<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Controlling Hallucinations at Word Level in Data-to-Text Generation Equal contribution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-07-09">9 Jul 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Rebuffel</surname></persName>
							<email>clement.rebuffel@lip6.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Roberti</surname></persName>
							<email>m.roberti@unito.it</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Soulier</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Scoutheeten</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rossella</forename><surname>Cancelliere</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Rebuffel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Roberti</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Soulier</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Scoutheeten</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bnp</forename><surname>Paribas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><forename type="middle">Rossella</forename><surname>Cancelliere</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Sorbonne Universit?</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>LIP6, F-75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Turin</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Sorbonne Universit?</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>LIP6, F-75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Turin</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Criteo AI Lab</orgName>
								<orgName type="institution" key="instit1">Sorbonne Universit?</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>LIP6, F-75005</postCode>
									<settlement>Paris, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Controlling Hallucinations at Word Level in Data-to-Text Generation Equal contribution</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-09">9 Jul 2021</date>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Data Mining and Knowledge Discovery manuscript No. (will be inserted by the editor) 2 C. Rebuffel, M. Roberti, L. Soulier, G. Scoutheeten, R. Cancelliere, P. Gallinari</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Data-to-Text Generation ? Hallucinations ? Controlled Text Genera- tion</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data-to-Text Generation (DTG) is a subfield of Natural Language Generation aiming at transcribing structured data in natural language descriptions. The field has been recently boosted by the use of neural-based generators which exhibit on one side great syntactic skills without the need of hand-crafted pipelines; on the other side, the quality of the generated text reflects the quality of the training data, which in realistic settings only offer imperfectly aligned structure-text pairs. Consequently, state-of-art neural models include misleading statementsusually called hallucinations-in their outputs. The control of this phenomenon is today a major challenge for DTG, and is the problem addressed in the paper.</p><p>Previous work deal with this issue at the instance level: using an alignment score for each table-reference pair. In contrast, we propose a finer-grained approach, arguing that hallucinations should rather be treated at the word level. Specifically, we propose a Multi-Branch Decoder which is able to leverage word-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>level labels to learn the relevant parts of each training instance. These labels are obtained following a simple and efficient scoring procedure based on co-occurrence analysis and dependency parsing. Extensive evaluations, via automated metrics and human judgment on the standard WikiBio benchmark, show the accuracy of our alignment labels and the effectiveness of the proposed Multi-Branch Decoder.</p><p>Our model is able to reduce and control hallucinations, while keeping fluency and coherence in generated texts. Further experiments on a degraded version of ToTTo show that our model could be successfully used on very noisy settings.</p><p>Keywords Data-to-Text Generation ? Hallucinations ? Controlled Text Generation 1 Introduction</p><p>Data-to-Text Generation (DTG) is the subfield of Computational Linguistics and Natural Language Generation (NLG) that is concerned with transcribing structured data into natural language descriptions, or, said otherwise, transcribing machine understandable information into a human understandable description <ref type="bibr" target="#b14">[15]</ref>. DTG objectives includes coverage, i.e. all the required information should be present in the text, and adequacy, i.e. the text should not contain information that is not covered by the input data. DTG is a domain distinct from other NLG task (e.g. machine translation <ref type="bibr" target="#b69">[70]</ref>, text summarization <ref type="bibr" target="#b27">[28]</ref>) with its own challenges <ref type="bibr" target="#b69">[70]</ref>, starting with the nature of inputs <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b39">40]</ref>. Such inputs include and are not limited to: databases of records, spreadsheets, knowledge bases, sensor readings. As an example, <ref type="figure" target="#fig_0">Fig. 1</ref> shows an instance of the WikiBio dataset, i.e. a data table containing information about Kian Emadi, paired with its corresponding natural language description found on Wikipedia.</p><p>Early approaches to DTG relied on static rules hand-crafted by experts, in which content selection (what to say) and surface realization (how to say it) are typically two separate tasks <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b9">10]</ref>. In recent years, neural models have blurred this distinction: various approaches showed that both content selection and surface realization can be learned in an end-to-end, data-driven fashion <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b48">49]</ref>. Based on the now-standard encoder-decoder architecture, with attention and copy mechanisms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b58">59]</ref>, neural methods for DTG are able to produce fluent text conditioned on structured data in a number of domains <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b49">50]</ref>, without relying on heavy manual work from field experts.</p><p>Such advances have gone hand in hand with the introduction of larger and more complex benchmarks. In particular, surface-realization abilities have been well studied on hand-crafted datasets such as E2E <ref type="bibr" target="#b42">[43]</ref> and WebNLG <ref type="bibr" target="#b13">[14]</ref>, while content-selection has been addressed by automatically constructed dataset such as WikiBio <ref type="bibr" target="#b28">[29]</ref> or RotoWire <ref type="bibr" target="#b69">[70]</ref>. These large corpora are often constructed from internet sources, which, while easy to access and aggregate, do not consist of perfectly aligned source-target pairs <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b6">7]</ref>. Consequently, model outputs are often subject to over-generation: misaligned fragments from training instances, namely divergences, can induce similarly misaligned outputs during inference, the so-called hallucinations.</p><p>In this paper, we specifically address the issue of hallucinations, which is currently regarded as a major issue in DTG <ref type="bibr" target="#b39">[40]</ref>. <ref type="bibr">Indeed</ref>  that real-life end-users of DTG systems care more about reliability than about readability <ref type="bibr" target="#b53">[54]</ref>, as unfaithful texts can potentially mislead decision makers, with dire consequences. Hallucinations-reduction methods such as the one presented here have applications in a broad range of tasks requiring high reliability, like news reports <ref type="bibr" target="#b30">[31]</ref>, in which hallucinations may give rise to fake news, or summaries of patient information in clinical contexts <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b1">2]</ref>. When corpora include a mild amount of noise, as in handcrafted ones (e.g. E2E, WebNLG), dataset regularization techniques <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b8">9]</ref> or hand crafted rules <ref type="bibr" target="#b20">[21]</ref> can help to reduce hallucinations. Unfortunately, these techniques are not suited to more realistic and noisier datasets, as for instance WikiBio <ref type="bibr" target="#b28">[29]</ref> or RotoWire <ref type="bibr" target="#b69">[70]</ref>. On these benchmarks, several techniques have been proposed, such as reconstruction loss terms <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b32">33]</ref> or Reinforcement Learning (RL) based methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b51">52]</ref>. These approaches suffer however from different issues: (1) the reconstruction loss relies on the hypothesis of one-to-one alignment between source and target which does not fit with content selection in DTG; (2) RL-trained models are based on instance-level rewards (e.g. BLEU <ref type="bibr" target="#b43">[44]</ref>, PARENT <ref type="bibr" target="#b6">[7]</ref>) which can lead to a loss of signal because divergences occur at the word level. In practice, parts of the target sentence express source attributes (in <ref type="figure" target="#fig_0">Fig. 1</ref> name and occupation fields are correctly realized), while others diverge (the birthday and nationality of Kian Emadi are not supported by the source table).</p><p>Interestingly, one can view DTG models as Controlled Text Generation (CTG) ones focused on controlling content, as most CTG techniques condition the generation on several key-value pairs of control factors (e.g. tone, tense, length) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b10">11]</ref>. Recently, Filippova <ref type="bibr" target="#b11">[12]</ref> explicitly introduced CTG to DTG by leveraging an hallucination score simply attached as an additional attribute which reflects the amount of noise in the instance. As an example, the table from <ref type="figure" target="#fig_0">Fig 1 can</ref> be augmented with an additional line (hallucination_score, 80%) 1 . However, this approach requires a strict alignment at the instance-level, namely between control factors and target text. A first attempt towards word-level approaches is proposed by Perez-Beltrachini and Lapata <ref type="bibr" target="#b46">[47]</ref> (also PB&amp;L in the following). They design word-level alignment labels, denoting the correspondence between the text and the input table, to bootstrap DTG systems. However, they incorporate these labels into a sentence-level RL-reward, which ultimately leads to a loss of this finer-grained signal.</p><p>In this paper, we go further in this direction with a DTG model by fully leveraging word-level alignment labels with a CTG perspective. We propose an original approach in which the word-level is integrated at all phases: we propose a word-level labeling procedure (Section 3), based on cooccurrences and sentence structure through dependency parsing. This mitigates the failure of strict word-matching procedure, while still producing relevant labels in complex settings. we introduce a weighted multi-branch neural decoder(Section 4), guided by the proposed alignment labels, acting as word-level control factors. During training, the model is able to distinguish between aligned and unaligned words and learns to generate accurate descriptions without being misled by un-factual reference information. Furthermore, our multi-branch weighting approach enables control at inference time.</p><p>We carry out extensive experiments on WikiBio, to evaluate both our labeling procedure and our decoder (Section 6). We also test our framework on ToTTo <ref type="bibr" target="#b44">[45]</ref>, in which models are trained with noisy reference texts, and evaluated on references reviewed and cleaned by human annotators to ensure accuracy. Evaluations are based on a range of automated metrics as well as human judgments, and show increased performances regarding hallucinations reduction, while preserving fluency. Importantly, our approach makes training neural models on noisy datasets possible, without the need to handcraft instances. This work shows the benefit of word-level techniques, which leverage the entire training set, instead of removing problematic training samples, which may form the great majority of the available data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Handling hallucinations in noisy datasets. The use of Deep Learning based methods to solve DTG tasks has led to sudden improvements in state of the art performances <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49]</ref>. As a key aspect in determining a model's performance is the quality of training data, several large corpora have been introduced to train and evaluate models' abilities on diverse tasks. E2E <ref type="bibr" target="#b42">[43]</ref> evaluates surface realization, i.e. the strict transcription of input attributes into natural language; RotoWire <ref type="bibr" target="#b69">[70]</ref> pairs statistics of basketball games with their journalistic descriptions, while WikiBio <ref type="bibr" target="#b28">[29]</ref> maps a Wikipedia info-box with the first paragraph of its associated article. Contrary to E2E, the latter datasets are not limited to surface realization. They were not constructed by human annotators, but rather created from Internet sources, and consist of loosely aligned table-reference pairs: in Wik-iBio, almost two thirds of the training instances contain divergences <ref type="bibr" target="#b6">[7]</ref>, and no instance has a 1-to-1 source-target alignment <ref type="bibr" target="#b45">[46]</ref>.</p><p>On datasets with a moderate amount of noise, such as E2E, data pre-processing has proven effective for reducing hallucinations. Indeed, rule-based <ref type="bibr" target="#b8">[9]</ref> or neuralbased methods <ref type="bibr" target="#b40">[41]</ref> have been proposed, specifically with table regularization techniques, where attributes are added or removed to re-align table and target descrip-tion. Several successful attempts have also been made in automatically learning alignments between the source tables and reference texts, benefiting from the regularity of the examples <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b16">17]</ref>. For instance, Juraska et al. <ref type="bibr" target="#b20">[21]</ref> leverage templating and hand-crafted rules to re-rank the top outputs of a model decoding via beam search; Gehrmann et al. <ref type="bibr" target="#b16">[17]</ref> also leverage the possible templating formats of E2E's reference texts, and train an ensemble of decoders where each decoder is associated to one template; and Kasner and Dusek <ref type="bibr" target="#b22">[23]</ref> produce template-based lexicalizations and improve them via a sentence fusion model. The previous techniques are not applicable in more complex, general settings. The work of Dusek et al. <ref type="bibr" target="#b8">[9]</ref> hints at this direction, as authors found that neural models trained on E2E were principally prone to omissions rather than hallucinations. In this direction, Shen et al. <ref type="bibr" target="#b60">[61]</ref> were able to obtain good results at increasing the coverage of neural outputs, by constraining the decoder to focus its attention exclusively on each table cell sequentially until the whole table was realized. On more complex datasets (e.g. WikiBio), a wide range of methods has been explored to deal with factualness such as loss design, either with a reconstruction term <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b67">68]</ref> or with RL-based methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b51">52]</ref>. Similarly to the coverage constraints, a reconstruction loss has proven only marginally efficient in these settings, as it contradicts the content selection task <ref type="bibr" target="#b67">[68]</ref>, and needs to be well calibrated using expert insight in order to bring improvements. Regarding RL, Perez-Beltrachini and Lapata <ref type="bibr" target="#b46">[47]</ref> build an instance-level reward which sums up word-level scores; Liu et al. <ref type="bibr" target="#b34">[35]</ref> propose a reward based on document frequency to favor words from the source table more than rare words; and Rebuffel et al. <ref type="bibr" target="#b51">[52]</ref> train a network with a variant of PARENT <ref type="bibr" target="#b6">[7]</ref> using self-critical RL. Note that data regularization techniques have also been proposed <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b67">68]</ref>, but these methods require heavy manual work and expert insights, and are not readily transposable from one domain to another.</p><p>From CTG to controlling hallucinations. Controlled Text Generation (CTG) is concerned with constraining a language model's output during inference on a number of desired attributes, or control factors, such as the identity of the speaker in a dialog setting <ref type="bibr" target="#b31">[32]</ref>, the politeness of the generated text or the text length in machine-translation <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b23">24]</ref>, or the tense in generated movie reviews <ref type="bibr" target="#b18">[19]</ref>. Earlier attempts at neural CTG can even be seen as direct instances of DTG as it is currently defined: models are trained to generate text conditioned on attributes of interest, where attributes are key-value pairs. For instance, in the movie review domain, Ficler and Goldberg <ref type="bibr" target="#b10">[11]</ref> proposed an expertly crafted dataset, where sentences are strictly aligned with control factors, being either content or linguistic style aspects (e.g. tone, length).</p><p>In the context of dealing with hallucinations in DTG, Filippova <ref type="bibr" target="#b11">[12]</ref> recently proposed a similar framework, by augmenting source tables with an additional attribute that reflects the degree of hallucinated content in the associated target description. During inference, this attribute acts as an hallucination handle used to produce more or less factual text. As mentioned in Section 1, we argue that a unique value can not accurately represent the correspondence between a table and its description, due to the phrase-based nature of divergences.</p><p>Based on the literature review, the lack of model control can be evidenced when loss modification methods are used <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b51">52]</ref>, although these approaches can be efficient and transposed from one domain to another. On the other hand, while <ref type="figure">Fig. 2</ref> The reference sentence of the example shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Every token is associated to its Part-of-Speech tag and hallucination score st. Words in red denote st &lt; ? . The dependency parsing is represented by labeled arrows that flow from parents to children. Important words are kian, emadi, 29, july, 1992, british, track, and cyclist.</p><p>CTG deals with control and enables choosing the defining features of generated texts <ref type="bibr" target="#b11">[12]</ref>, standard approaches rely on instance-level control factors that do not fit with hallucinations, which rather appear due to divergences at the word level. Our approach aims at gathering the merits of both trends of models and is guided by previous statements highlighting that word-level is primary in hallucination control. More particularly, our model differs from previous ones in several aspects:</p><p>(1) Contrasting with data-driven approaches (i.e. dataset regularization) which are costly in expert time, and loss-driven approaches (i.e. reconstruction or RL losses) which often do not take into account key subtasks of DTG (contentselection, world-level correspondences), we propose a multi-branch modeling procedure which allows the controllability of the hallucination factor in DTG. This multi-branch model can be integrated seamlessly in current approaches, allowing to keep peculiarities of existing DTG models, while deferring hallucination management to a parallel decoding branch. (2) Unlike previous CTG approaches <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> which propose instance-level control factors, the control of the hallucination factor is performed at the wordlevel to enable finer-grained signal to be sent to the model.</p><p>Our model is composed of two main components: (1) a word-level alignment labeling mechanism, which makes the correspondence between the input table and the text explicit, and (2) a multi-branch decoder guided by these alignment labels. The branches separately integrate co-dependent control factors (namely content, hallucination and fluency). We describe these components in Sections 3 and 4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Word-level Alignment Labels</head><p>We consider a DTG task, in which the corpus C is composed of a set of entitydescription pairs, (e, y). A single-entity table e is a variable-sized set of T e key-value pairs x := (k, v). A description y := y 1:T y is a sequence of T y tokens representing the natural language description of the entity; we refer to the tokens spanning from indices t to t of a description y as y t:t . A description is made of statements, defined as text spans expressing one single idea (Appendix A presents in detail the statement partitioning procedure). We refer to the first index of a statement as t i , so that y t i :t i+1 ?1 is the i th statement itself. <ref type="figure" target="#fig_0">Fig. 1</ref> shows a WikiBio entity made by 8 key-value pairs together with its associated description.</p><p>First, we aim at labeling each word from a description, depending on the presence of a correspondence with its associated table. We call such labels alignment labels. We drive the word-level labeling procedure on two intuitive constraints:</p><p>(1) important words (names, adjectives and numbers) should be labeled depending on their alignment with the table, and (2) words from the same statement should have the same label.</p><p>With this in mind, the alignment label for the t th token y t is a binary label:</p><p>l t := 1 {s t &gt;? } where s t refers to the alignment score between y t and the table, and ? is set experimentally (see Sec. 5.3). The alignment score s t acts as a normalized measure of correspondence between a token y t and the table e:</p><formula xml:id="formula_0">s t := norm(max x?e align(y t , x), y)<label>(1)</label></formula><p>where the function align estimates the alignment between token y t and a key-value pair x from the input table e, and norm is a normalization function based on the dependency structure of the description y. <ref type="figure">Fig. 2</ref> illustrates our approach: under each word we show its word alignment score, and words are colored in red if this score is lower than ? , denoting an alignment label equal to 0. Below, we describe these functions (Appendix A contains reproducibility details).</p><p>Co-occurrence-based alignment function (align(?, x)). This function assigns to important words a score in the interval [0, 1] proportional to their co-occurrence count (a proxy for alignment) with the key-value pair from the input table. If the word y t appears in the key-value pair x := (k, v), align(y t , x) outputs 1; otherwise, the output is obtained scaling the number of occurrences co y t ,x between y t and x through the dataset:</p><formula xml:id="formula_1">align(yt, x) := ? ? ? ? ? 1 if yt ? x a ? (coy t ,x ? m) 2 if m ? coy t ,x ? M 0 if 0 ? coy t ,x ? m (2)</formula><p>where M is the maximum number of word co-occurrences in the dataset vocabulary and the row x, m is a threshold value, and a := 1 (M ?m) 2 . Score normalization (norm(?, y)). According to the already stated assumption (2) -words inside the same statement should have the same score -, we first split the sentence y into statements y t i :t i+1 ?1 , via dependency parsing and its rule-based conversion to constituency trees <ref type="bibr" target="#b17">[18,</ref><ref type="bibr">73,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3]</ref>. Given a word y t associated to the score s t and belonging to statement y t i :t i+1 ?1 , its normalized score corresponds to the average score of all important words in this statement:</p><formula xml:id="formula_2">norm(s t , y) = 1 t i+1 ? t i t i+1 ?1 j=t i s j<label>(3)</label></formula><p>This in-statement average depends on both the specific word and its context, leading to coherent hallucination scores which can be thresholded without affecting the syntactical sentence structure, as shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multi-Branch Architecture</head><p>The proposed Multi-Branch Decoder (MBD) architecture aims at separating targeted co-dependent factors during generation. We build upon the standard DTG architecture, an encoder-decoder with attention and copy mechanism, which we modify by duplicating the decoder module into three distinct parallel modules. Each control factor (i.e. content, hallucination or fluency) is modeled via a single decoding module, also called branch, whose output representation can be weighted according to its desired importance. At training time, weights change depending on the word currently being decoded, inducing the desired specialization of each branch. During inference, weights are manually set, according to the desired tradeoff between information reliability, sentence diversity and global fluency. Text generation is thus controllable, and consistent with the control factors. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates a training step over the sentence "Giuseppe Mariani was an Italian art director ", in which Italian is a divergent statement (i.e. is not supported by the source table). While decoding factual words, the weight associated to the content (resp. hallucination) branch is set to 0.5 (resp. 0) while during the decoding of Italian, the weight associated to the content (resp. hallucination) branch is set to 0 (resp. 0.5). Note that the weight associated to the fluency branch is always set to 0.5, as fluency does not depend on factualness.</p><p>The decoding modules' actual architecture may vary, as we framed the MBD model from a high level perspective. Therefore, all types of decoder can be used, such as Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b56">[57]</ref>, Transformers <ref type="bibr" target="#b66">[67]</ref>, and Convolutional Neural Networks <ref type="bibr" target="#b15">[16]</ref>. The framework can be generalized to different merging strategies as well, such as late fusion, in which the final distributions are merged, instead of the presented early fusion, which works at the decoder states level.</p><p>In this paper, experiments are carried out on RNN-based decoders, weighting their hidden states. Section 4.1 presents the standard DTG encoder-decoder architecture; Section 4.2 shows how it can be extended to MBD, together with its peculiarities and the underlying objectives and assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Standard DTG architecture</head><p>Neural DTG approaches typically use an encoder-decoder architecture <ref type="bibr" target="#b69">[70]</ref> in which (1) the encoder relies on a RNN to encode each element of the source table into a fixed-size latent representation h j (elements of the input table are first embedded into T e N -dimensional vectors, and then fed sequentially to the RNN <ref type="bibr" target="#b69">[70]</ref>), and (2) the decoder generates a textual description y using a RNN augmented with attention and copy mechanisms <ref type="bibr" target="#b58">[59]</ref>. Words are generated in an auto-regressive way. The decoder's RNN updates its hidden state d t as:</p><formula xml:id="formula_3">d t := RNN(d t?1 , [y t?1 , c t ])<label>(4)</label></formula><p>where y t?1 is the previous word and c t is the context vector obtained through the attention mechanism. Finally, a word is drawn from the distribution computed via a copy mechanism <ref type="bibr" target="#b58">[59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Controlling Hallucinations via a Multi-Branch Model</head><p>Our objective is to enrich the decoder in order to be able to tune the content/hallucination ratio during generation, aiming at enabling generation of hallucination-free text when needed. Our key assumption is that the decoder's generation is conditioned by three co-dependent factors: -Content factor constrains the generation to realize only the information included in the input; -Hallucinating factor favors lexically richer and more diverse text, but may lead to hallucinations not grounded by the input; -Fluency factor 2 conditions the generated sentences toward global syntactic correctness, regardless of the relevance. Based on this assumption, we propose a multi-branch encoder-decoder network, whose branches are constrained on the above factors at word-level, as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. Our network has a single encoder and F = 3 distinct decoding RNNs, noted RNN f respectively, one for each factor. During each decoding step, the previously decoded word y t?1 is fed to all RNNs, and a final decoder state d t is computed using a weighted sum of all the corresponding hidden states,</p><formula xml:id="formula_4">d f t := RNN f (d f t?1 , [y t?1 , c t ]) (5) d t := F f =1 ? f t d f t (6)</formula><p>where d f t and ? f t are respectively the hidden state and the weight of the f th RNN at time t.</p><p>Weights are used to constrain the decoder branches to the desired control factors (? 0 t , ? 1 t , ? 2 t for the content, hallucination and fluency factors respectively) and sum to one.</p><p>During training, their values are dynamically set depending on the alignment label l t ? {0, 1} of the target token y t (see Sec. 5.3). While a number of mappings can be used to set the weights given the alignment label, early experiments have shown that better results were achieved when using a binary switch for each factor, i.e. activating/deactivating each branch, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref> (note that fluency should not depend on content and therefore its associated branch is always active).</p><p>During inference, the weights of the decoder's branches are set manually by a user, according to the desired trade-off between information reliability, sentence diversity and global fluency. Text generation is then controllable and consistent with the control factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We evaluated the model on two representative large size datasets. Both have been collected automatically and present a significant amount of table-text divergences for training. Both datasets involve content selection and surface realization, and represent a relatively realistic setting.</p><p>WikiBio <ref type="bibr" target="#b28">[29]</ref> contains 728, 321 tables, automatically paired with the first sentence of the corresponding Wikipedia English article. Reference text's average length is 26 words, and tables have on average 12 key-value pairs. We use the original data partition: 80% for the train set, and 10% for validation and test sets. This dataset has been automatically built from the Internet; concerning divergences, 62% of the references mention extra information not grounded by the table <ref type="bibr" target="#b6">[7]</ref>.</p><p>ToTTo <ref type="bibr" target="#b44">[45]</ref> contains 120, 761 training examples, and 7, 700 validation and test examples. For a given Wikipedia page, an example is built up by pairing its summary table and a candidate sentence, selected across the whole page via simple similarity heuristics. Such a sentence may accordingly realize whichever table cells, making content selection arbitrary; furthermore, its lexical form may strongly depend on the original context, because of pronouns or anaphoras. Divergences are of course present as well. Those issues have been addressed by Parikh et al. <ref type="bibr" target="#b44">[45]</ref> by <ref type="bibr" target="#b0">(1)</ref> highlighting the input cells realized by the output, and (2) removing divergences and making the sentence self-contained (e.g. replacing pronouns with their invoked noun or noun phrase). <ref type="figure">Fig. 6</ref> exemplifies the difference between noisy and clean ToTTo sentences. In our experiments, we limit the input to the highlighted cells and use the original, noisy sentence as output. Noisy texts' average length is 17.4 words, and 3.55 table cells are highlighted, on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We assess the accuracy and relevance of our alignment labels against the ones proposed by Perez-Beltrachini and Lapata <ref type="bibr" target="#b46">[47]</ref>, which is, to the best of our knowledge, the only work proposing such a fine-grained alignment labeling.</p><p>To evaluate our Multi-Branch Decoder (MBD), we consider five baselines: stnd [59], a LSTM-based encoder-decoder model with attention and copy mechanisms. This is the standard sequence-to-sequence recurrent architecture.</p><p>stnd filtered, the previous model trained on a filtered version of the training set: tokens deemed hallucinated according to their hallucination scores, are removed from target sentences.</p><p>hsmm <ref type="bibr" target="#b70">[71]</ref>, an encoder-decoder model with a multi-branch decoder. The branches are not constrained by explicit control factors. This is used as a baseline to show that the multi-branch architecture by itself does not guarantee the absence of hallucinations.</p><p>hier <ref type="bibr" target="#b33">[34]</ref>, a hierarchical sequence-to-sequence model, with a coarse-to-fine attention mechanism to better fit the attribute-value structure of the tables. This model is trained with three auxiliary tasks to capture more accurate semantic representations of the tables.</p><p>hal WO <ref type="bibr" target="#b11">[12]</ref>, a stnd -like model trained by augmenting each source table with an additional attribute (hallucination ratio, value).</p><p>We ran our own implementations of stnd, stnd filtered and hal WO . Authors of hier and hsmm models kindly provided us their WikiBio's test set outputs. The metrics described in Sec. 5.4 were directly applied on them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation Details</head><p>During training of our multi-branch decoder the fluency branch is always active (? 2 t = 0.5) while the content and hallucination branches are alternatively acti-vated, depending on the alignment label l t : ? 0 t = 0.5 (content factor) and ? 1 t = 0 (hallucination factor) when l t = 1, and conversely. The threshold ? used to obtain l t is set to 0.4 using human tuning to optimize for highest accuracy 3 . All hyperparameters were tuned in order to optimize the validation PARENT F-measure <ref type="bibr" target="#b6">[7]</ref>. In particular, we use the [0.4 0.1 0.5] weight combination during inference. See Sec. 6.2 for a discussion about weight combinations and Appendix B for other implementation details. <ref type="bibr" target="#b3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Metrics</head><p>To evaluate our model, we carried out (1) an automatic analysis and (2) a human evaluation for a qualitative analysis of generated sentences.</p><p>For the automatic analysis, we use five metrics: -BLEU [44] is a length-penalized precision score over n-grams, n ? 1, 4 , optionally improved with a smoothing technique <ref type="bibr" target="#b3">[4]</ref>. Despite being the standard choice, recent findings show that it correlates poorly with human evaluation, especially on the sentence level <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b52">53]</ref>, and that it is a proxy for sentence grammar and fluency aspects rather than semantics <ref type="bibr" target="#b6">[7]</ref>.</p><p>-PARENT <ref type="bibr" target="#b6">[7]</ref> computes smoothed n-gram precision and recall over both the reference and the input table. It is explicitly designed for DTG tasks, and its Fmeasure shows "the highest correlation with humans across a range of settings with divergent references in WikiBio." <ref type="bibr" target="#b6">[7]</ref> -The hallucination rate computes the percentage of tokens labeled as hallucinations (Sec. 3).</p><p>-The average generated sentence length in number of words.</p><p>-The classic readability Flesch index <ref type="bibr" target="#b12">[13]</ref>, which is based on words per sentence and syllables per word, and is still used as a standard benchmark <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref>.</p><p>Finally, we perform qualitative evaluations of the results obtained on WikiBIO and ToTTo, following the best practices outlined by van der Lee et al. <ref type="bibr" target="#b29">[30]</ref>. Our human annotators are from several countries across Europe, between 20 and 55 years old and proficient in English. They have been assigned two different tasks: (i) hallucination labeling, i.e. the selection of sentence pieces which include incorrect information, and (ii) sentence analysis, i.e. evaluating different realizations of the same table according to their fluency, factualness and coverage. Scores are presented as a 3-level Likert scale for Fluency (Fluent, Mostly fluent, or Not fluent) and Factualness (likewise), while coverage is the number of cells from the table that have been realized in the description. To avoid all bias, annotators are shown a randomly selected table at a time, together with its corresponding descriptions, both from the dataset and the models that are being evaluated. Sentences are presented each time in a different order. Following Tian et al. <ref type="bibr" target="#b65">[66]</ref>, we first tasked three expert annotators to annotate a pilot batch of 50 sentences. Once confirmed that Inter-Annotator Agreement was approx. 75% (a similar finding to Tian et al. <ref type="bibr" target="#b65">[66]</ref>), we asked 16 annotators to annotate a bigger sample of 300 instances (where each instance consists of one </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We perform an extensive evaluation of our scoring procedure and multi-branch architecture on the WikiBio dataset: we evaluate -the quality of the proposed alignment labels, both intrinsically using human judgment and extrinsically by means of the DTG downstream task and -the performance of our model with respect to the baselines. Additionally, we assess the applicability of our framework on the more noisy ToTTo benchmark, which represents a harder challenge for today's DTG models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Validation of Alignment Labels.</head><p>To assess the effectiveness of our alignment labels (Sec. 3), we first compare the alignment labels against human judgment, and then explore their impact on a DTG task. As a baseline for comparison we report performances of PB&amp;L.</p><p>Intrinsic performance. Tab. 1 (top) compares the labeling performance of our method and PB&amp;L against human judgment. Our scoring procedure significantly improves over PB&amp;L: the latter only achieves 46.9% accuracy and 29.7% F-measure, against 87.5% and 68.7% respectively for our proposed procedure. Perez-Beltrachini and Lapata <ref type="bibr" target="#b46">[47]</ref> report a F-measure of 36%, a discrepancy that can be explained by the difference between the evaluation procedures: PB&amp;L evaluate on 132 sentences, several of which can be tied to the same table, whereas we explicitly chose to evaluate on 300 sentences all from different tables in order to minimize correlation. We remark that beyond F-measure, the precision of PB&amp;L's scoring procedure is at 21.3% compared to 80.6% for ours, and recall stands at 49.2% against 59.8%. We argue that selecting a negative instance at random for training their classifier leads the network to incoherently label words, without apparent justification. See <ref type="figure" target="#fig_3">Figure 4</ref> for two examples of this phenomenon; and Appendix D for other comparisons. In contrast, our method is able to detect hallucinated statements inside a sentence, without incorrectly labeling the whole sentence as hallucinated.</p><p>Impact on a DTG downstream task. Additionally, we assess the difference of both scoring procedures using their impact on the WikiBio DTG task. Specifically, Tab. 1 (bottom) shows the results of training MBD using either PB&amp;L's or our labels. We observe significant improvements, especially in BLEU and PARENTrecall (40.5% vs 32.2% and 45% vs 39.3%), showing that our labeling procedure is more helpful at retaining information from training instances (the system better  picks up what humans picked-up, ultimately resulting in better BLEU and recall).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Automatic System Evaluation</head><p>Comparison with SOTA systems. Tab. 2 shows the performances of our model and all baselines according to the metrics of Sec. 5.4. Two qualitative examples are presented in <ref type="figure" target="#fig_4">Figure 5</ref> and more are available in Appendix D.</p><p>First of all, reducing hallucinations is reached with success, as highlighted by the hallucination rate (1.43% vs. 4.20% for a standard encoder-decoder and 10.10% for the best SOTA model on BLEU). The only model which gets a lower hallucination rate (0.74%, corroborated by its PARENT-precision of 80.9%), stnd filtered, achieves such a result at a high cost. As can be seen in <ref type="figure" target="#fig_4">Figure 5</ref> where its out-  <ref type="table">Table 2</ref> Comparison results on WikiBio. ? (resp. ? ) means higher (resp. lower) is better. "Gold" refers to the gold standard, i.e. the reference texts included in the dataset.</p><p>put is factual but cut short, its sentences are the shortest and the most naive in terms of the Flesch readability index, which is also reflected by a lower BLEU score. The high PARENT precision -mostly due to the shortness of the outputs -is counterbalanced by a low recall: the F-measure indicates the overall lack of competitiveness of this trade-off. This shows that the naive approach of simply filtering training instances is not the appropriate solution for hallucination reduction. This echoes <ref type="bibr" target="#b11">[12]</ref> who trained a vanilla network on the cleanest 20% of the data and found that predictions are more precise than those of a model trained on 100% but that PARENT-recall and BLEU scores are low. At the other extreme, the best model in terms of BLEU, hier, falls short regarding precision, suggesting that often the generated text is not matched in the input table; this issue is also reflected by the highest hallucination rate of all models (10.10%). A reason could be the introduction of their auxiliary training tasks which often drive the decoder to excess in mimicking human behavior. While BLEU score improves, overall factualness of outputs decreases, showing that the model picks up domain lingo (how to formulate ideas) but not domain insight (which ideas to formulate) (see <ref type="figure" target="#fig_4">Figure 5</ref>). This is in line with <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b11">12]</ref> who argue that BLEU is an inappropriate metric for generation tasks other than machine translation.</p><p>The analysis of hsmm, and especially of its relatively weak performance both in terms of BLEU and PARENT, highlights the insufficiency of the multi-branch architecture by itself. This reinforces the need of the additional hallucinations supervision provided by our labeling procedure.</p><p>Finally, in the comparisons with hal WO , we can see that while it achieves one of the highest performances in term of precision (79.5%), this comes at the cost of the lowest recall (40.5%) of all models and thus poor F-measure. This confirms our hypothesis that, while effective at producing mostly factual content, modeling hallucination only as a fixed value for a whole instance is detrimental to the content generation procedure. Finer-grain annotations are required, as shown by our model recall (46.4%), coupled with a robust precision (79.0%).</p><p>Weight impact on decoding. As we deal with a CTG system, we can guide our network at inference to generate sentences following desired attributes. The impact of different weight combinations is explored in Tab. 3. In particular, we can see that changing weights in favor of the hallucination factor (top five lines) leads to decreases in both precision and recall (from 80.37% to 57.88% and 44.96% 4.82% respectively). We also observe that strongly relying on the hallucinating branch dramatically impacts performances ([0.0 0.5 0.5] obtains near 0 BLEU and  F-measure), as it is never fed with complete, coherent sentences during training. However, some performance can still be restored via the fluency branch: [0.0 0.1 0.9] performs at 15.51% BLEU and 36.88% F-measure.</p><p>It is interesting to note that the relaxation of the strict constraint on the content factor in favor of the hallucination factor, ([0.4 0.1 0.5] ? [0.5 0.0 0.5]) obtains better performances (56.16% vs 55.29% F-measure). This highlights that strictly constraining on content yields sensibly more factual outputs (79% vs 80.37% precision), at the cost of constraining the model's generation creativity (46.40% vs 44.96% recall). The [0.4 0.1 0.5] variant has more "freedom of speech" and sticks more faithfully to domain lingo (recall and BLEU), without compromising too much in terms of content.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Human evaluation</head><p>To measure subtleties which are not captured by automatic metrics, we report in Tab. 4 human ratings of our model, two baselines and the gold. These baselines have been selected because they showcase interesting behaviors on automatic metrics: hier obtains the best BLEU score but a poor precision, and stnd filtered gets the best precision but poor BLEU, length and Flesch index. First, coherently with <ref type="bibr" target="#b6">[7]</ref>, we found that around two thirds of gold references contain divergences from their associated tables. Such data also confirm our analysis on the stnd filtered baseline: it's training on truncated sentences lead to an unquestionable ability to avoid hallucinations, while dramatically impacting both its fluency and coverage, leading to less desired outputs overall, despite the high PARENT-precision score.</p><p>The comparison between hier and MBD shows that both approaches lead to similar coverage, with MBD obtaining significantly better performances in terms of factualness. We also highlight that MBD is evaluated as being the most fluent one, even better than the reference (which can be explained by the imperfect pre-processing done by Lebret et al. <ref type="bibr" target="#b28">[29]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">ToTTo: a considerably noisy setting</head><p>The ToTTo dataset is used in the following experiments to explore models' robustness to the impact of extreme noise during training. As stated in Section 5.1, we use as inputs only the highlighted cells, as content selection is arbitrary (i.e. the cells were chosen depending on the target sentence, and not vice versa  <ref type="table">Table 5</ref> Comparison results on ToTTo. ? (resp. ? ) means higher (resp. lower) is better. In human evaluation for Fluency, reported are for "Fluent" and "Mostly Fluent", with only "Fluent" in parentheses. Same for Factualness. (b) <ref type="figure">Fig. 6</ref> Qualitative examples of MBD and hal WO on ToTTo. hal WO 's poor generation quality is not detected by discrete metrics. In contrast, MBD generates fluent and naively factual sentences. Note that stnd and stnd filtered have the same behavior as on WikiBio: the former produces fluent but nonsensical text; the latter generates very un-fluent, but factual, text.</p><p>other hand, we use as targets the noisy references, which may contain both divergences and lexical issues. This setting is particularly challenging and is more effective in recreating a representational, hallucination-prone real-life context than WikiBio. Other datasets <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b68">69]</ref> available in literature are too similar to Wik-iBio concerning their goals and challenges, and are therefore less interesting in this context. <ref type="table">Table 5</ref> reports the performances of stnd, stnd filtered, hal WO and MBD with regards to automatic metrics and human evaluation. Compared to their respective performances on WikiBio, all models show significantly decreased scores. They struggle at generating syntactically correct sentences but, at the same time, they have still learned to leverage their copy mechanism and to stick to the input. This behavior is illustrated in both examples of <ref type="figure">Fig. 6</ref>. In particular, hal WO 's high PARENT-precision score (77.64%) seems to be due to its tendency to blindly copy input data without framing them in a sentence structure, as its low BLEU and PARENT-recall scores suggests (17.06% and 22.65%). These lower scores are good indicators that the ToTTo task, as framed in this paper, is difficult. Following the same evaluation protocol than for WikiBio, we report human ratings of different models, also included in <ref type="table">Table 5</ref>.</p><p>MBD's factualness is judged favorably, with 55.9% hallucination-free texts, and up to 85.3% texts with a single error at most. In contrast, hal WO stands at 32.4% and 61.8% for error-free texts and single-error texts respectively. Interestingly, stnd filtered obtains the second best performance (70.6% texts with a single error).</p><p>Fluency scores are also meaningful: hal WO and MBD respectively obtain 61.7% and 91.2%. Word-based filtering is not suitable for noisy datasets, as shown by stnd filtered 's worse fluency score, 29.4%.</p><p>As for coverage performances, our model MBD obtains the maximum coverage score 3.613, surpassing all baselines by at least 0.789 slots (the second best coverage score is obtained by stnd at 2.824), and getting very close to the Gold value (which stands at 3.618). These performances, and qualitative examples of <ref type="figure">Figure 6</ref>, suggest that stnd filtered and hal WO try to reduce hallucinations at the cost of missing some input slot, while MBD effectively balances both goals.</p><p>The analysis of Factualness, Fluency and Coverage can be enhanced using qualitative error analysis on randomly sampled generated texts (we report two such examples in <ref type="figure">Figure 6</ref>). In particular, we want to highlight the following considerations:</p><p>-As most training examples are very noisy, sentence-level models fail at learning from them. stnd filtered has been trained on factual statements only, at the cost of using mostly incomplete sentences during training. On both examples of <ref type="figure">Figure 6</ref>, it generated truncated sentences, missing their subjects. Its relatively high Factualness and low Fluency scores indicate that it did not learn to produce diverging outputs, nor complete sentences. Differently, hal WO generates incorrectly ordered sequences of words extracted from the table <ref type="figure">(Fig. 6a</ref>), or repetitions <ref type="figure">(Fig. 6b)</ref>. The low number of training instances containing the input pair (hallucination ratio, 0) does not allow to learn what a non-hallucinated sentence actually consists in. -In contrast, our proposed finer-grained approach proves helpful in this setting, as shown by the human evaluation: sentences generated by MBD are more fluent and more factual. The multi-branch design enables the model to leverage the most of each training instance, leading to better performances overall. -Finally, we acknowledge that despite over-performing other models, MBD obtains only 55.9% of factual sentences. For instance, in <ref type="figure">Figure 6b</ref>, our model does not understand that a range consists of two numbers. The difficulty of current models to learn on very noisy and diverse datasets shows that there is still room for improvement in hallucination reduction in DTG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed a Multi-Branch decoder, able to leverage word-level alignment labels in order to produce factual and coherent outputs. Our proposed labeling procedure is more accurate than previous work, and outputs from our model are estimated, by automatic metrics and human judgment alike, more fluent, factual, and relevant.</p><p>We obtain state-of-the-art performances on WikiBio for PARENT F-measure, and show that our approach is promising in the context of a noisier setting. We designed our alignment procedure to be general and easily reproducible on any DTG dataset. One strength of our approach is that co-occurrences and dependency parsing can be used intuitively to extract more information from the tables than a naive word matching procedure. However, in the context of tables mainly including numbers (e.g., RotoWire), the effectiveness of the co-occurrence analysis is not guaranteed. A future work will be to improve upon the co-occurrence analysis to generalize to tables which contain less semantic inputs. For instance, the labeling procedure of Perez-Beltrachini and Lapata <ref type="bibr" target="#b46">[47]</ref> might be revised so that adverse instances are not selected randomly, which we hypothesize would result in more relevant labels.</p><p>Finally, experiments on ToTTo outline the narrow exposure to language of current models when used on very noisy datasets. Our model has shown interesting properties through the human evaluation but is still perfectible. Recently introduced large pretrained language models, which have seen significantly more varied texts, may attenuate this problem. In this direction, adapting the work of <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref> to our model could bring improvements to the results presented in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declarations</head><p>? Fundings: We would like to thank the H2020 project AI4EU (825619) and the ANR JCJC SESAMS (ANR-18-CE23-0001) for supporting this work. This research has been partially carried on in the context of the Visiting Professor Program of the Italian Istituto Nazionale di Alta Matematica (INdAM).</p><p>? Conflict of interest: No conflict of interest ? Code availability: Code is available at https://github.com/KaijuML/dtt-multi-branch ? Other items are not applicable (Availability of data and material, Additional declarations for articles in life science journals that report the results of studies involving humans and/or animals, Ethics approval, Consent to participate, Consent for publication)</p><p>State-of-the-art natural language processing. ArXiv abs/1910.03771 (2019) 73. Xia, F., Palmer, M.: Converting dependency structures to phrase structures.</p><p>In: HLT (2001)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Alignment labels reproducibility</head><p>We consider as important words, i.e. nouns, adjectives or numbers, those which are Part-of-Speech tagged as NUM, ADJ, NOUN and PROPN.</p><p>In order to apply the score normalization function norm(?, y), we separate sentences y into statements y t i :t i+1 ?1 . To do so, we identify the set of introductory dependency relation labels 7 , following previous work on rule-based systems for the conversion of dependency relations trees to constituency trees <ref type="bibr" target="#b17">[18,</ref><ref type="bibr">73,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Our segmentation algorithm considers every leaf token in the dependency tree, and seeks its nearest ancestor which is the root of a statement.</p><p>Two heuristics enforce the score normalization: (i) conjunctions and commas next to hallucinated tokens acquires these lasts' hallucination scores, and (ii) paired parentheses and quotes acquire the minimum inner tokens' hallucination score.</p><p>Part-of-Speech tagging has been done using the HuggingFace's Transformers library <ref type="bibr" target="#b71">[72]</ref> to fine-tune a BERT model <ref type="bibr" target="#b5">[6]</ref> on the UD English ParTUT dataset <ref type="bibr" target="#b57">[58]</ref>; Stanza <ref type="bibr" target="#b50">[51]</ref> has been exploited for dependency parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation details</head><p>Our system is implemented in Python 3.8 8 and PyTorch 1.4.0 9 . In particular, our multi-branch architecture is developed, trained and tested as an OpenNMT <ref type="bibr" target="#b25">[26]</ref> model. Sentence lengths and Flesch index <ref type="bibr" target="#b12">[13]</ref> are computed using the standard style Unix command.</p><p>Differently to Perez-Beltrachini and Lapata <ref type="bibr" target="#b46">[47]</ref>, we did not adapt the original WikiBio dataset 10 in any manner: as we work on the model side, we fairly preserve the dataset's noisiness.</p><p>Word-level alignment labels are computed setting m = 5, following Mikolov et al. <ref type="bibr" target="#b38">[39]</ref>. As stated in Sec. 5.3, the threshold ? 's value is optimized for highest accuracy via human tuning: <ref type="table" target="#tab_1">Table B1</ref> shows accuracy scores of our proposed automated labels for different values of ? .</p><p>We share the vocabulary between input and output, limiting its size to 20000 tokens. Hyperparameters were tuned using performances on the development set: Tab. B2 reports the performances of our best performing MBD on the development set. Our encoder consist of a 600-dimensional embedding layer followed by a 2-layered bidirectional LSTM network with hidden states sized 600. We use the general attention mechanism with input feeding <ref type="bibr" target="#b36">[37]</ref> and the same copy mechanism as See et al. <ref type="bibr" target="#b58">[59]</ref>. Each branch of the multi-branch decoder is a 2-layered LSTM network with hidden states sized 600 as well.</p><p>Training is performed using the Adam algorithm <ref type="bibr" target="#b24">[25]</ref> with learning rate ? = 10 ?3 , ? 1 = 0.9 and ? 2 = 0.999. The learning rate is decayed with a factor of 0.5 every 10000 steps, starting from the 5000th one. We used minibatches of size 64 and regularized via clipping the gradient norm to 5 and using a dropout rate of 0.3. We used beam search during inference, with a beam size of 10.</p><p>All experiments were performed on a single NVIDIA Titan XP GPU. Number of parameters and training times are shown in <ref type="table" target="#tab_4">Table B3</ref>. Same model's differences between WikiBio and ToTTo are justified by the different datasets' number of instances and input vocabulary sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Annotation interface</head><p>The human annotation procedure is done via a web application specifically developed for this research. <ref type="figure" target="#fig_0">Fig. C1a</ref> shows how the hallucination tagging user interface looked like in practice, while in <ref type="figure" target="#fig_0">Fig. C1b a typical</ref>   : susan maria blu -lrb-born july 12 , 1948 -rrb-, sometimes credited as sue blu , is an american voice actress , voice director and casting director in american and canadian cinema and television . PB&amp;L: susan maria blu -lrb-born july 12 , 1948 -rrb-, sometimes credited as sue blu , is an american voice actress , voice director and casting director in american and canadian cinema and television . Ours: susan maria blu -lrb-born july 12 , 1948 -rrb-, sometimes credited as sue blu , is an american voice actress , voice director and casting director in american and canadian cinema and television .   : ryan david moore -lrb-born december 5 , 1982 -rrb-is an american professional golfer , currently playing on the pga tour . PB&amp;L: ryan david moore -lrb-born december 5 , 1982 -rrb-is an american professional golfer , currently playing on the pga tour . Ours: ryan david moore -lrb-born december 5 , 1982 -rrb-is an american professional golfer , currently playing on the pga tour . <ref type="table">Table D2</ref> Hallucinated words according either to our scoring procedure or to the method proposed by Perez-Beltrachini and Lapata <ref type="bibr" target="#b46">[47]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold</head><p>prince frederick emil august of schleswig-holstein-sonderburgaugustenburg ( kiel , 23 august 1800 -beirut , 2 july 1865 ) , usually simply known by just his first name , frederick , " prince of no?r " , was a prince of the house of schleswig-holstein-sonderburg-augustenburg and a cadet-line descendant of the danish royal house . stnd prince frederick of schleswig-holstein-sonderburg-augustenburg ( 23 august 1800 -2 july 1865 ) was a member of the house of schleswigholstein-sonderburg-augustenburg . stnd filtered prince frederick of schleswig-holstein-sonderburg-augustenburg ( 23 august 1800 -2 july 1865 ) was a german . hsmm prince frederick of schleswig-holstein-sonderburg-augustenburg ( 23 august 1800 -2 july 1865 ) was a danish noblewoman . hier prince frederick of schleswig-holstein-sonderburg-augustenburg ( ) ( 23 august 1800 -2 july 1865 ) was a german prince of the house of schleswig-holstein-sonderburg-augustenburg . MBD[.4, .1, .5] prince frederick of schleswig-holstein-sonderburg-augustenburg ( ; 23 august 1800 -2 july 1865 ) was the son of frederick christian ii , duke of schleswig-holstein-sonderburg-augustenburg and princess louise auguste of denmark .      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold</head><p>pat burke is an irish gaelic footballer who plays for dublin and kilmacud crokes . stnd pat burke is a gaelic footballer from dublin , ireland . stnd filtered pat burke is a gaelic footballer for dublin . hsmm pat burke ( born in dublin ) is a former irish gaelic footballer who played as a gaelic footballer . hier pat burke is a former gaelic footballer for dublin . MBD[.4, .1, .5] pat burke is a gaelic footballer from county dublin .   This list ranks the top 10 islands of the Maldives by area . Some islands in the Maldives , although geographically one island , are divided into two administrative islands ( for example , Gan and Maandhoo in Laamu Atoll ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold</head><p>Hulhumeedhoo is the 5th largest island in Maldives . stnd</p><p>It has a area of Hulhumeedhoo km? ( Islands sq mi ) . stnd filtered is the fourth of the Maldives in Maldives .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>hal WO</head><p>Hulhumeedhoo is the largest islands of the Maldives by area size . MBD[.4, .1, .5] Hulhumeedhoo is the fifth largest island by area size . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold</head><p>In 2010 , Singular released its first single , " 24.7 ( Twenty-Four Seven ) " . stnd</p><p>The first single , 24.7 ( Twenty-Four Seven ) , was released in 2010 . stnd filtered</p><p>The band won the 24.7 ( Twenty-Four Seven ) . hal WO 24.7 ( Twenty-Four Seven ) . MBD[.4, .1, .5] Singular released their first album , 24.7 ( Twenty-Four Seven ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table D15</head><p>A ToTTo input table, coupled with the corresponding sentence and the modelsgenerated outputs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>An example of a WikiBio instance, composed by an input table and its (partially aligned) description.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>Our proposed decoder with three branches associated to content (in blue -left), hallucination (in red -middle) and fluency (in yellow -right). Semi-transparent branches are assigned the weight 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>patricia flores fuentes -lrb-born 25 july 1977 -rrb-is a mexican politician affiliated to the national action party . PB&amp;L: patricia flores fuentes -lrb-born 25 july 1977 -rrb-is a mexican politician affiliated to the national action party . Ours: patricia flores fuentes -lrb-born 25 july 1977 -rrb-is a mexican politician affiliated to the national action party . moore -lrb-golfer -rrb-Ref.: ryan david moore -lrb-born december 5 , 1982 -rrb-is an american professional golfer , currently playing on the pga tour . PB&amp;L: ryan david moore -lrb-born december 5 , 1982 -rrb-is an american professional golfer , currently playing on the pga tour . Ours: ryan david moore -lrb-born december 5 , 1982 -rrb-is an american professional golfer , currently playing on the pga tour .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4</head><label>4</label><figDesc>WikiBio instances' hallucinated words according either to our scoring procedure or to the method proposed by Perez-Beltrachini and Lapata<ref type="bibr" target="#b46">[47]</ref>. PB&amp;L labels word incoherently (a), and sometimes the whole reference text (b). In comparison, our approach leads to a fluent breakdown of the sentences in hallucinated/factual statements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5</head><label>5</label><figDesc>Qualitative examples of our model and baselines on the WikiBio test set. Note that: (1) gold references may contain divergences; (2) stnd and hsmm seem to perform well superficially, but often hallucinate; (3) stnd filtered doesn't hallucinate but struggles with fluency; (4) hier overgenerate "human-sounding" statements, that lacks facutalness; (5) MBD sticks to the fact contained by the table, in concise and fluent sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>flores fuentes Ref.: patricia flores fuentes -lrb-born 25 july 1977 -rrb-is a mexican politician affiliated to the national action party . PB&amp;L: patricia flores fuentes -lrb-born 25 july 1977 -rrb-is a mexican politician affiliated to the national action party . Ours: patricia flores fuentes -lrb-born 25 july 1977 -rrb-is a mexican politician affiliated to the national action party .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. C1</head><label>C1</label><figDesc>The human annotation tasks, as presented to the annotators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, experimental surveys show Ref.: kian emadi (born 29 july 1992) is a british track cyclist .</figDesc><table><row><cell>key</cell><cell>value</cell></row><row><cell>name</cell><cell>kian emadi</cell></row><row><cell>fullname</cell><cell>kian emadi-coffin</cell></row><row><cell>currentteam</cell><cell>retired</cell></row><row><cell>discipline</cell><cell>track</cell></row><row><cell>role</cell><cell>rider</cell></row><row><cell>ridertype</cell><cell>sprinter</cell></row><row><cell>proyears</cell><cell>2012-present</cell></row><row><cell>proteams</cell><cell>sky track cycling</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>table and four associated outputs), as Liu et al. [34]. 5 Performances of hallucination scores on WikiBio test set, w.r.t. human-designated labels (upper table) and MBD trained with different labeling procedures (lower table). Our model always significantly overpasses PB&amp;L (T-test with p &lt; 0.005).</figDesc><table><row><cell cols="3">Labels Accuracy Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell>PB&amp;L</cell><cell>46.9%</cell><cell>21.3%</cell><cell>49.2%</cell><cell>29.7%</cell></row><row><cell>ours</cell><cell>87.5%</cell><cell>80.6%</cell><cell>59.8%</cell><cell>68.7%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PARENT</cell><cell></cell></row><row><cell>Labels</cell><cell>BLEU</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell>PB&amp;L</cell><cell>32.15%</cell><cell>76.91%</cell><cell>39.28%</cell><cell>48.75%</cell></row><row><cell>ours</cell><cell>40.51%</cell><cell>77.71%</cell><cell>45.01%</cell><cell>54.57%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Performances of MBD on WikiBio validation set, with various weight settings. Weights' order is (content, hallucination, fluency).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PARENT ?</cell></row><row><cell>Weights</cell><cell cols="2">BLEU ?</cell><cell cols="2">Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell>0.5 0.0 0.5</cell><cell cols="2">38.90%</cell><cell>80.37%</cell><cell></cell><cell>44.96%</cell><cell>55.29%</cell></row><row><cell>0.4 0.1 0.5</cell><cell cols="2">41.56%</cell><cell>79.00%</cell><cell></cell><cell>46.40%</cell><cell>56.16%</cell></row><row><cell>0.3 0.2 0.5</cell><cell cols="2">42.68%</cell><cell>72.99%</cell><cell></cell><cell>45.81%</cell><cell>53.74%</cell></row><row><cell>0.2 0.3 0.5</cell><cell cols="2">22.64%</cell><cell>53.92%</cell><cell></cell><cell>32.96%</cell><cell>36.55%</cell></row><row><cell>0.1 0.4 0.5</cell><cell>2.03%</cell><cell></cell><cell>57.88%</cell><cell></cell><cell>4.82%</cell><cell>6.79%</cell></row><row><cell>0.0 0.5 0.5</cell><cell>0.32%</cell><cell></cell><cell>85.01%</cell><cell></cell><cell>1.02%</cell><cell>1.78%</cell></row><row><cell>0.0 0.4 0.6</cell><cell>1.07%</cell><cell></cell><cell>62.71%</cell><cell></cell><cell>2.47%</cell><cell>3.66%</cell></row><row><cell>0.0 0.3 0.7</cell><cell>2.81%</cell><cell></cell><cell>42.86%</cell><cell></cell><cell>6.15%</cell><cell>7.94%</cell></row><row><cell>0.0 0.2 0.8</cell><cell>7.30%</cell><cell></cell><cell>41.78%</cell><cell></cell><cell>16.58%</cell><cell>18.68%</cell></row><row><cell>0.0 0.1 0.9</cell><cell cols="2">15.51%</cell><cell>56.93%</cell><cell></cell><cell>32.85%</cell><cell>36.88%</cell></row><row><cell>Model</cell><cell cols="5">Fluency Factualness Coverage</cell></row><row><cell>Gold</cell><cell></cell><cell cols="2">98.7%</cell><cell cols="2">32.0%</cell><cell>4.47</cell></row><row><cell cols="2">stnd filtered</cell><cell cols="2">93.5%</cell><cell cols="2">86.1%</cell><cell>4.07</cell></row><row><cell>hier</cell><cell></cell><cell cols="2">97.4%</cell><cell cols="2">55.0%</cell><cell>4.45</cell></row><row><cell>MBD</cell><cell cols="3">99.6%</cell><cell cols="2">76.6%</cell><cell>4.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Results of the human evaluation on WikiBio 6 .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>). On the</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>PARENT ?</cell><cell></cell><cell cols="2">Human evaluation</cell><cell></cell></row><row><cell>Model</cell><cell>BLEU ?</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell><cell>Fluency ?</cell><cell>Factualness ?</cell><cell>Coverage</cell></row><row><cell>Gold(noisy)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>97.1% (97.1)</cell><cell>91.2% (79.4)</cell><cell>3.618</cell></row><row><cell>stnd</cell><cell>21.27%</cell><cell>56.60%</cell><cell>25.16%</cell><cell>29.71%</cell><cell>55.9% (26.5)</cell><cell>53.0% (20.6)</cell><cell>2.824</cell></row><row><cell>stnd filtered</cell><cell>19.48%</cell><cell>56.69%</cell><cell>22.31%</cell><cell>27.18%</cell><cell>29.4% (8.8)</cell><cell>70.6% (50.0)</cell><cell>2.706</cell></row><row><cell>hal WO</cell><cell>17.06%</cell><cell>77.64%</cell><cell>22.65%</cell><cell>29.38%</cell><cell>61.7% (38.2)</cell><cell>61.8% (32.4)</cell><cell>2.725</cell></row><row><cell>MBD</cell><cell>18.35%</cell><cell>50.44%</cell><cell>25.25%</cell><cell>28.25%</cell><cell>91.2% (50.0)</cell><cell>85.3% (55.9)</cell><cell>3.613</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>sentence analysis page is shown.</figDesc><table><row><cell cols="5">Threshold Accuracy F-measure Precision Recall</cell></row><row><cell>0.0</cell><cell>70.2%</cell><cell>56.8%</cell><cell>42.2%</cell><cell>86.7%</cell></row><row><cell>0.4</cell><cell>86.0%</cell><cell>70.6%</cell><cell>67.0%</cell><cell>74.6%</cell></row><row><cell>0.8</cell><cell>85.8%</cell><cell>62.8%</cell><cell>77.3%</cell><cell>52.9%</cell></row><row><cell cols="5">Table B1 Accuracy scores of our proposed word-level automated labels for different values</cell></row><row><cell>of the threshold ? .</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>PARENT</cell><cell></cell></row><row><cell>Model</cell><cell>BLEU</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell cols="2">MBD[.4, .1, .5] 42.50%</cell><cell>79.26%</cell><cell>46.09%</cell><cell>55.95%</cell></row><row><cell cols="5">Table B2 The performances of our model on the WikiBio validation set.</cell></row><row><cell>Dataset</cell><cell>Model</cell><cell cols="3">Size [M] Training time [h]</cell></row><row><cell></cell><cell>stnd</cell><cell>41</cell><cell>5</cell><cell></cell></row><row><cell>WikiBio</cell><cell>stnd filtered halWO</cell><cell>41 41</cell><cell>5 5</cell><cell></cell></row><row><cell></cell><cell>MBD</cell><cell>55</cell><cell>10</cell><cell></cell></row><row><cell></cell><cell>stnd</cell><cell>62</cell><cell>4</cell><cell></cell></row><row><cell>ToTTo</cell><cell>stnd filtered halWO</cell><cell>62 62</cell><cell>4 4</cell><cell></cell></row><row><cell></cell><cell>MBD</cell><cell>76</cell><cell>8</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table B3</head><label>B3</label><figDesc>Sizes and training times of the implemented models. coupled with the corresponding sentences, either as found in the dataset, or as generated by our models and baselines.</figDesc><table><row><cell>D Qualitative examples Tables D1 and D2 show word-level labeling of WikiBio training examples. Underlined, red words are hallucinated according either to our scoring procedure or to the method proposed by Perez-Beltrachini and Lapata [47]. In the subsequent tables, some WikiBio (D3 to D12) and ToTTo (D13 to D15) inputs value name susan blu birth name susan maria blupka birth date 12 july 1948 birth place st paul , minnesota , u.s. occupation actress , director , casting director yearsactive 1968 --present article title susan blu are shown, key Ref.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table D1</head><label>D1</label><figDesc>Hallucinated words according either to our scoring procedure or to the method proposed by Perez-Beltrachini and Lapata<ref type="bibr" target="#b46">[47]</ref>.</figDesc><table><row><cell>key</cell><cell>value</cell></row><row><cell>name</cell><cell>alex wilmot sitwell</cell></row><row><cell>birth date</cell><cell>16 march 1961</cell></row><row><cell>birth place</cell><cell>uk</cell></row><row><cell>occupation</cell><cell>president , europe and emerging markets -lrb-</cell></row><row><cell></cell><cell>ex-asia -rrb-of bank of america merrill lynch</cell></row><row><cell cols="2">article title alex wilmot-sitwell</cell></row><row><cell cols="2">Ref.: alex wilmot-sitwell heads bank of america merrill lynch</cell></row><row><cell cols="2">'s businesses across europe and emerging markets excluding</cell></row><row><cell>asia .</cell><cell></cell></row><row><cell cols="2">PB&amp;L: alex wilmot-sitwell heads bank of america merrill lynch</cell></row><row><cell cols="2">'s businesses across europe and emerging markets excluding</cell></row><row><cell>asia .</cell><cell></cell></row><row><cell cols="2">Ours: alex wilmot-sitwell heads bank of america merrill lynch</cell></row><row><cell cols="2">'s businesses across europe and emerging markets excluding</cell></row><row><cell>asia .</cell><cell></cell></row><row><cell>key</cell><cell>value</cell></row><row><cell>name</cell><cell>ryan moore</cell></row><row><cell>spouse</cell><cell>nichole olson -lrb-m. 2011 -rrb-</cell></row><row><cell>children</cell><cell>tucker</cell></row><row><cell>college</cell><cell>unlv</cell></row><row><cell>yearpro</cell><cell>2005</cell></row><row><cell>tour</cell><cell>pga tour</cell></row><row><cell>prowins</cell><cell>4</cell></row><row><cell>pgawins</cell><cell>4</cell></row><row><cell>masters</cell><cell>t12 2015</cell></row><row><cell>usopen</cell><cell>t10 2009</cell></row><row><cell>open</cell><cell>t10 2009</cell></row><row><cell>pga</cell><cell>t9 2006</cell></row><row><cell cols="2">article title ryan moore -lrb-golfer -rrb-</cell></row><row><cell>Ref.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table D3 A</head><label>D3</label><figDesc>WikiBio input table, coupled with the corresponding sentence and the modelsgenerated outputs. MBD[.4, .1, .5] godgory was a swedish melodic death metal band from karlstad .</figDesc><table><row><cell>name</cell><cell>godgory</cell></row><row><cell>background</cell><cell>group or band</cell></row><row><cell>origin</cell><cell>karlstad , sweden</cell></row><row><cell>genre</cell><cell>melodic death metal doom metal gothic metal</cell></row><row><cell>years active</cell><cell>1992 --2004</cell></row><row><cell>label</cell><cell>nuclear blast</cell></row><row><cell cols="2">current members matte andersson erik andersson</cell></row><row><cell>past members</cell><cell>mikael dahlqvist stefan grundel henrik lindstr?m fredric</cell></row><row><cell></cell><cell>danielsson thomas heder</cell></row><row><cell>article title</cell><cell>godgory</cell></row><row><cell>Gold</cell><cell>godgory was a swedish melodic death metal or death doom metal band</cell></row><row><cell></cell><cell>that was formed august 1992 by erik andersson ( drums ) and matte</cell></row><row><cell></cell><cell>andersson ( vocals ) of no relation .</cell></row><row><cell>stnd</cell><cell>godgory was a melodic death metal band from karlstad , sweden .</cell></row><row><cell>stnd filtered</cell><cell>godgory was a swedish metal band .</cell></row><row><cell>hsmm</cell><cell>godgory ( 10 august 2004 -4 january 2010 ) was a melodic death metal</cell></row><row><cell></cell><cell>band from karlstad , sweden .</cell></row><row><cell>hier</cell><cell>godgory was a melodic death metal band from sweden , sweden .</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table D4 A</head><label>D4</label><figDesc>WikiBio input table, coupled with the corresponding sentence and the modelsgenerated outputs. MBD[.4, .1, .5] christy mihos ( born june 13 , 1949 , in brockton , massachusetts ) is a former member of the massachusetts turnpike commission .</figDesc><table><row><cell>name</cell><cell></cell><cell>christy mihos</cell></row><row><cell>image</cell><cell></cell><cell>christy2006wp.jpeg</cell></row><row><cell cols="2">px|office</cell><cell>massachusetts turnpike commission member</cell></row><row><cell cols="2">term start</cell><cell>july 1 , 1999</cell></row><row><cell cols="2">term end</cell><cell>july 1 , 2004</cell></row><row><cell cols="2">successor</cell><cell>daniel grabauskas</cell></row><row><cell cols="2">birth date</cell><cell>13 june 1949</cell></row><row><cell cols="2">birth place</cell><cell>brockton , massachusetts</cell></row><row><cell cols="2">nationality</cell><cell>american</cell></row><row><cell cols="2">occupation</cell><cell>convenience store franchise owner</cell></row><row><cell>spouse</cell><cell></cell><cell>andrea mihos ( 1974-2013 ; divorce )</cell></row><row><cell cols="2">religion</cell><cell>greek orthodox</cell></row><row><cell cols="2">alma mater</cell><cell>stonehill college</cell></row><row><cell>party</cell><cell></cell><cell>republican</cell></row><row><cell cols="3">article title christy mihos</cell></row><row><cell>Gold</cell><cell cols="2">christy peter mihos is an american politician and businessman from</cell></row><row><cell></cell><cell cols="2">massachusetts .</cell></row><row><cell>stnd</cell><cell cols="2">christy mihos ( born june 13 , 1949 in brockton , massachusetts ) is a</cell></row><row><cell></cell><cell cols="2">former republican member of the massachusetts house of representa-</cell></row><row><cell></cell><cell>tives .</cell></row><row><cell>stnd filtered</cell><cell cols="2">christy mihos ( born june 13 , 1949 in brockton , massachusetts ) is an</cell></row><row><cell></cell><cell cols="2">american politician .</cell></row><row><cell>hsmm</cell><cell cols="2">christy mihos ( born june 13 , 1949 ) is an american politician from</cell></row><row><cell></cell><cell cols="2">the state of massachusetts .</cell></row><row><cell>hier</cell><cell cols="2">christy mihos ( born june 13 , 1949 ) is an american politician who</cell></row><row><cell></cell><cell cols="2">served as the massachusetts state representative from 1999 to 2004 .</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table D5 A</head><label>D5</label><figDesc>WikiBio input table, coupled with the corresponding sentence and the modelsgenerated outputs. Gold duncan eagleson is an american self-trained painter and former graffiti artist . stnd duncan eagleson is an american painter , writer , and penciller . stnd filtered duncan eagleson is an american painter . hsmm duncan eagleson ( born 1975 ) is an american comic book painter and writer . hier duncan eagleson is an american painter , illustrator , and designer . MBD[.4, .1, .5] duncan eagleson is an american painter , writer , and sculptor .</figDesc><table><row><cell>name</cell><cell>duncan eagleson</cell></row><row><cell>nationality</cell><cell>american</cell></row><row><cell>area</cell><cell>painter , writer , penciller , colorist , sculptor , designer</cell></row><row><cell cols="2">article title duncan eagleson</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table D6 A</head><label>D6</label><figDesc>WikiBio input table, coupled with the corresponding sentence and the modelsgenerated outputs. Gold gerald warner brace ( september 24 , 1901 -july 20 , 1978 ) was an american novelist , writer , educator , sailor and boat builder . stnd gerald warner brace ( september 24 , 1901 -july 20 , 1978 ) was an american novelist , writer , and boat builder . stnd filtered gerald warner brace ( september 24 , 1901 -july 20 , 1978 ) was an american novelist . hsmm gerald warner brace ( september 24 , 1901 -july 20 , 1978 ) was an american novelist and writer . hier gerald warner brace ( september 24 , 1901 -july 20 , 1978 ) was an american novelist , short story writer , educator , and sailor . MBD[.4, .1, .5] gerald warner brace ( september 24 , 1901 -july 20 , 1978 ) was an american author , educator , sailor , and boat builder .</figDesc><table><row><cell>name</cell><cell>gerald warner brace</cell></row><row><cell>imagesize</cell><cell>208px</cell></row><row><cell>birth date</cell><cell>24 september 1901</cell></row><row><cell>birth place</cell><cell>islip , long island , suffolk county , new york</cell></row><row><cell>death date</cell><cell>20 july 1978</cell></row><row><cell>death place</cell><cell>blue hill , maine</cell></row><row><cell>occupation</cell><cell>novelist , writer , educator , sailor , boat builder</cell></row><row><cell>nationality</cell><cell>american</cell></row><row><cell>genre</cell><cell>fiction , non-fiction</cell></row><row><cell cols="2">article title gerald warner brace</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table D7 A</head><label>D7</label><figDesc>WikiBio input table, coupled with the corresponding sentence and the modelsgenerated outputs. MBD[.4, .1, .5] polina miller ( born 23 november 1988 in saint petersburg , russian sfsr , soviet union ) is a russian gymnast .</figDesc><table><row><cell>name</cell><cell></cell><cell>polina miller</cell></row><row><cell>gender</cell><cell></cell><cell>f</cell></row><row><cell cols="2">birth date</cell><cell>23 november 1988</cell></row><row><cell cols="2">birth place</cell><cell>saint petersburg , russian sfsr , soviet union</cell></row><row><cell cols="2">discipline</cell><cell>wag</cell></row><row><cell cols="3">article title polina miller</cell></row><row><cell>Gold</cell><cell cols="2">polina miller ( , born november 23 , 1988 in saint petersburg ) is a</cell></row><row><cell></cell><cell cols="2">russian gymnast .</cell></row><row><cell>stnd</cell><cell cols="2">polina miller ( born november 23 , 1988 ) is a russian artistic gymnast</cell></row><row><cell></cell><cell>.</cell></row><row><cell>stnd filtered</cell><cell cols="2">polina miller ( born november 23 , 1988 ) is a .</cell></row><row><cell>hsmm</cell><cell cols="2">polina miller ( born 23 november 1988 in saint petersburg ) is a russian</cell></row><row><cell></cell><cell cols="2">artistic gymnast .</cell></row><row><cell>hier</cell><cell cols="2">polina miller ( born 23 november 1988 ) is a russian rhythmic gymnast</cell></row><row><cell></cell><cell>.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table D10 A</head><label>D10</label><figDesc>WikiBio input table, coupled with the corresponding sentence and the modelsgenerated outputs.</figDesc><table><row><cell>name</cell><cell>pat burke</cell></row><row><cell>irish</cell><cell>p?draig de b?rca</cell></row><row><cell>sport</cell><cell>gaelic football</cell></row><row><cell>code</cell><cell>football</cell></row><row><cell>county</cell><cell>dublin</cell></row><row><cell>province</cell><cell>leinster</cell></row><row><cell>clposition</cell><cell>corner forward</cell></row><row><cell>club</cell><cell>kilmacud crokes</cell></row><row><cell>clubs</cell><cell>kilmacud crokes</cell></row><row><cell>counties</cell><cell>dublin</cell></row><row><cell>icprovince</cell><cell>1</cell></row><row><cell>birth place</cell><cell>dublin , ireland</cell></row><row><cell cols="2">article title pat burke ( gaelic footballer )</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table D11 A</head><label>D11</label><figDesc>WikiBio input table, coupled with the corresponding sentence and the modelsgenerated outputs. Gold odiakes ( born march 22 ) is a japanese composer from tokyo , japan who has worked for a variety of visual novel companies . stnd , better known by his stage name odiakes , is a japanese composer . stnd filtered odiakes is a japanese composer . hsmm odiakes " odiakes " ( born march 22 ) is a japanese composer . hier composer ( born march 22 ) is a japanese j-pop player . MBD[.4, .1, .5] odiakes ( born march 22 in tokyo , japan ) is a japanese composer .</figDesc><table><row><cell>name</cell><cell>odiakes</cell></row><row><cell>background</cell><cell>non vocal instrumentalist</cell></row><row><cell>birth date</cell><cell>march 22</cell></row><row><cell>origin</cell><cell>tokyo , japan</cell></row><row><cell>instrument</cell><cell>keyboard , synthesizer</cell></row><row><cell>genre</cell><cell>j-pop , techno</cell></row><row><cell>occupation</cell><cell>composer</cell></row><row><cell>years active</cell><cell>1998 --present</cell></row><row><cell cols="2">article title odiakes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table D12 A</head><label>D12</label><figDesc>WikiBio input table, coupled with the corresponding sentence and the modelsgenerated outputs. Umarbayev signed for Tajik League FC Istiklol . stnd Umarbayev joined Tajik League side Istiklol in 2015 . stnd filtered 2015 , he joined Istiklol in the Tajik League . hal WO Parvizdzhon joined Tajik League club Istiklol in 2015 . MBD[.4, .1, .5] Umarbayev signed with Istiklol ahead of the 2015 Tajik League season .</figDesc><table><row><cell></cell><cell>Club</cell><cell>Istiklol</cell></row><row><cell></cell><cell>Season</cell><cell>2015</cell></row><row><cell></cell><cell>League</cell><cell>Tajik League</cell></row><row><cell></cell><cell>page title</cell><cell>Parvizdzhon Umarbayev</cell></row><row><cell></cell><cell cols="2">section title Club</cell></row><row><cell></cell><cell>section text</cell><cell>As of match played 29 July 2018</cell></row><row><cell>Gold</cell><cell>In 2015 ,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table D13 A</head><label>D13</label><figDesc>ToTTo input table, coupled with the corresponding sentence and the modelsgenerated outputs.</figDesc><table><row><cell>Rank</cell><cell>5</cell></row><row><cell>Island</cell><cell>Hulhumeedhoo</cell></row><row><cell>page title</cell><cell>List of islands of the Maldives</cell></row><row><cell cols="2">section title Islands by area size</cell></row><row><cell>section text</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table D14 A</head><label>D14</label><figDesc>ToTTo input table, coupled with the corresponding sentence and the modelsgenerated outputs.</figDesc><table><row><cell>Single</cell><cell>24.7 ( Twenty-Four Seven )</cell></row><row><cell>page title</cell><cell>Singular ( band )</cell></row><row><cell cols="2">section title 2010</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The reader may disagree with such a strong hallucination score. Indeed, while the birthdate and nationality are clearly divergences, the rest of the sentence is correct. This illustrates the complexity of handling divergences in complex datasets, where alignment cannot be framed as a simple word-matching task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Wiseman et al.<ref type="bibr" target="#b70">[71]</ref> showed that the explicit modeling of a fluency latent factor improves performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that accuracy is not heavily impacted by different choices of ? . We report in Appendix B the respective accuracy scores of our proposed automated labels for different values of ? .<ref type="bibr" target="#b3">4</ref> Code is given to reviewers and will be available upon acceptance.<ref type="bibr" target="#b4">5</ref> An eyesight of our platform is available in Appendix C.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Fluency reports the sum of "fluent" and "mostly fluent", as "mostly fluent" often comes from misplaced punctuation and doesn't really impact readability. However, Factualness reports only the count of "factual", as "mostly factual" sentences contain hallucinations and cannot be considered "factual".</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">acl, advcl, amod, appos, ccomp, conj, csubj, iobj, list, nmod, nsubj, obj, orphan, parataxis, reparandum, vocative, xcomp; every dependency relation is documented in the Universal Dependencies website. 8 http://www.python.org 9 http://www.pytorch.org 10 https://github.com/DavidGrangier/wikipedia-biography-dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Table D9A WikiBio input table, coupled with the corresponding sentence and the modelsgenerated outputs.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold</head><p>vice admiral robert b. murrett was the fourth director of the national geospatial-intelligence agency , from 7 july 2006 through july 2010 . stnd robert b. murrett is a retired vice admiral of the united states navy . stnd filtered robert b. murrett is the director of the national geospatial-intelligence agency . hsmm robert b. " bob " murrett ( born 1956 ) is an american naval officer and the director .  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards NLG for physiological data monitoring with body area networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Banaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loutfi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ENLG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Children&apos;s grammars grow more abstract with age -evidence from an automatic procedure for identifying the productive units of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Borensztajn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Zuidema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bod</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>topiCS</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A systematic comparison of smoothing techniques for sentence-level BLEU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>WMT@ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Few-shot NLG with pre-trained language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eavani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NAACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Handling divergent reference texts when evaluating table-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning to generate product reviews from attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semantic noise matters for neural natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dusek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Howcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rieser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>INLG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural datato-text generation: A comparison between pipeline and end-to-end architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Van Der Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Miltenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Controlling linguistic style aspects in neural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ficler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Stylistic Variation @ ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Controlled hallucinations: Learning to generate faithfully from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Filippova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Flesch</surname></persName>
		</author>
		<title level="m">The Art of Readable Writing</title>
		<imprint>
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Creating training corpora for NLG micro-planners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Survey of the state of the art in natural language generation: Core tasks, applications and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">End-to-end content and plan selection for data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>INLG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Handling stuctural divergences and recovering dropped arguments in a korean/english machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lavoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Kittredge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Korelsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>AMTA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Toward controlled generation of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bootstrapping parsers via syntactic projection across parallel texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Cabezas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kolak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A deep ensemble model with slot alignment for sequence-to-sequence natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Juraska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NAACL-HLT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Text-to-text pre-training for data-to-text tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rastogi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>INLG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Data-to-text generation with iterative text editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dusek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>INLG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Controlling output length in neural encoder-decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okumura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">OpenNMT: Opensource toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Twitter user profiling: Bot and gender identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kosmajac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Keselj</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Evaluating the factual consistency of abstractive text summarization abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kryscinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Neural text generation from structured data with application to the biography domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Best practices for the human evaluation of automatically generated text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Van Der Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Miltenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wubben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Krahmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>INLG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Datadriven news generation for automated journalism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lepp?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Munezero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Granroth-Wilding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>INLG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A persona-based neural conversation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Recordto-text generation with style imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Hierarchical encoder with auxiliary supervision for neural table-to-text generation: Learning better representation for tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Towards comprehensive description generation from factual attribute-value tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ACLs</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Table-to-text generation by structure-aware seq2seq learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">What to talk about and how? selective generation using lstms with coarse-to-fine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gardent</surname></persName>
		</author>
		<title level="m">Deep Learning Approaches to Text Production</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A simple recipe towards reducing hallucination in neural surface realisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Why we need new evaluation metrics for NLG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dusek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Curry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rieser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The E2E dataset: New challenges for endto-end generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dusek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGdial Meeting on Discourse and Dialogue</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<title level="m">ToTTo: A Controlled Table-To-Text Generation Dataset</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Analysing data-to-text generation benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gardent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bootstrapping generators from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Automatic generation of textual summaries from neonatal intensive care data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Portet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sripada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sykes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Data-to-text generation with content selection and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Data-to-text generation with entity modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Stanza: A Python natural language processing toolkit for many human languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">System Demonstrations @ ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Parenting via modelagnostic reinforcement learning to correct pathological behaviors in data-totext generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rebuffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Soulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Scoutheeten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>INLG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A structured review of the validity of BLEU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An investigation into the validity of some metrics for automatically evaluating natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Belz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Building applied natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Copy mechanism and tailored training for character-based data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bonetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cancelliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<editor>ECML-PKDD</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Parttut: The turin university parallel treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sanguinetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Delmonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<editor>Simi, M.</editor>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PARLI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Controlling politeness in neural machine translation via side constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Measuring the readability of sustainability reports: A corpus-based analysis through standard formulae and nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smeuninx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Clerck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aerts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Business Communication</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">When shallow is good enough: Automatic assessment of conceptual text complexity using shallow semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stajner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hulpus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>LREC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Coco: A tool for automatically assessing conceptual complexity of texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stajner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nisioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hulpus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>LREC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Studying the Impact of Filling Information Gaps on the Output Quality of Neural Data-to-Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sripada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>INLG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Sticking to the facts: Confident decoding for faithful data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<idno>abs/1910.08684</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Revisiting challenges in data-to-text generation with fact grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>INLG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Learning neural templates for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brew</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Huggingface&apos;s transformers</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
