<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Toward Hierarchical Self-Supervised Monocular Absolute Depth Estimation for Autonomous Driving Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guirong</forename><surname>Zhuo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyuan</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wufei</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyue</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><forename type="middle">H</forename><surname>Ang</surname><genName>Jr</genName></persName>
						</author>
						<title level="a" type="main">Toward Hierarchical Self-Supervised Monocular Absolute Depth Estimation for Autonomous Driving Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, self-supervised methods for monocular depth estimation has rapidly become an significant branch of depth estimation task, especially for autonomous driving applications. Despite the high overall precision achieved, current methods still suffer from a) imprecise object-level depth inference and b) uncertain scale factor. The former problem would cause texture copy or provide inaccurate object boundary, and the latter would require current methods to have an additional sensor like LiDAR to provide depth ground-truth or stereo camera as additional training inputs, which makes them difficult to implement. In this work, we propose to address these two problems together by introducing DNet. Our contributions are twofold: a) a novel dense connected prediction (DCP) layer is proposed to provide better object-level depth estimation and b) specifically for autonomous driving scenarios, dense geometrical constrains (DGC) is introduced so that precise scale factor can be recovered without additional cost for autonomous vehicles. Extensive experiments have been conducted and, both DCP layer and DGC module are proved to be effectively solving the aforementioned problems respectively. Thanks to DCP layer, object boundary can now be better distinguished in the depth map and the depth is more continues on object level. It is also demonstrated that the performance of using DGC to perform scale recovery is comparable to that using groundtruth information, when the camera height is given and the ground point takes up more than 1.03% of the pixels. Code is available at https://github.com/TJ-IPLab/DNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Estimating an accurate depth map from single RGB image is of great significance in 3D scene understanding as well as in many real-world applications such as augmented reality and autonomous driving. Compared to traditional handcrafted feature-based methods <ref type="bibr" target="#b0">[1]</ref>, supervised <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> and stereo self-supervised <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> learning has been proved to be able to achieve better performance on this task. Unfortunately, these methods either require a large amount of high-quality annotated ground-truth, which is difficult to obtain, or need complex stereo calibration. Therefore, monocular self-supervised learning methods became the focus of research. Some recent works <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> revealed its great potential to tackle monocular depth estimation task.</p><p>Despite its potential to reach satisfying performances, current methods have two shortcomings. One of them is that they are only able to estimate relative depth rather than the absolute one. For evaluation, scale factor is calculated by <ref type="figure">Fig. 1</ref>. Structure difference of DNet with other self-supervised monocular depth estimation methods. Solid lines indicates our work flow and dotted lines are that of other methods. Dense connected prediction (DCP) layer is introduced to generate hierarchical features for better object-level depth inference, and dense geometrical constraint (DGC) is introduced to directly estimate absolute depth from monocular images. Performance comparison of DGC and ground-truth based scale recovery is indicated in the top-right plot.</p><p>ratio between the medians of ground-truth (given by LiDAR) and predicted depth <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, as can be seen from <ref type="figure">Fig. 1</ref>. Theoretically, it is a decent solution. However, for practical uses, obtaining ground-truth in real applications using other sensors not only raises the cost, it also complexes the system, leading to complicated joint calibration processes and synchronization problems.</p><p>Another problem is that because the decoder of current methods predict depth in different resolutions separately, some details on object-level is omitted. For example, object boundary can be blurred and the depth of texture on the object may be predicted differently than the object itself.</p><p>In this paper, we propose DNet, a novel self-supervised monocular depth estimation pipeline that exploits densely connected hierarchical features to obtain more precise objectlevel depth inference, and uses dense geometrical constraint to eliminate the dependence on additional sensors or depth ground-truth to perform scale recovery, so that it is easier to be brought into practical use.</p><p>Our contributions are listed as follows:</p><p>? We improve the former multi-scale estimation strategy by proposing a novel dense connected prediction (DCP) layer. Instead of predicting depth and computing reconstruction loss separately under different scales, the proposed DCP layer exploits hierarchical feature so that object-level depth inference can be made based on multi-scale prediction features, refining object boundary Split <ref type="bibr" target="#b1">[2]</ref>, where the results not only showed the capability of DCP layer to improve the performance of objectlevel depth inference, but also proved that DNet using DGC module has competitive performance against those methods using depth ground-truth to determine scale factor. Ablation studies demonstrated module effectiveness as well as sensitivity of DNet to ground points ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>A. Self-supervised monocular depth estimation Monocular depth estimation has always been an important aspect of scene understanding. Some works apply supervised <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> or stereo self-supervised <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> methods to tackle the problem. However, due to the difficulty of obtaining large amount of labeled data or complex stereo calibration to train the depth estimation network, monocular self-supervised method was proposed instead <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>Proposed by the pioneering work <ref type="bibr" target="#b14">[15]</ref>, the basic idea is to use photometric reconstruction loss calculated by comparing the target image with the target view reconstructed from nearby source views. However, it assumes that the scene is static and that no occlusion is present between different consecutive frames. <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> explicitly established different motion models to resolve the moving scene problem. <ref type="bibr" target="#b19">[20]</ref> introduced 3D surface normal by constructing two additional layers for better depth estimation. <ref type="bibr" target="#b13">[14]</ref> replaced the original photometric reconstruction error with per-pixel minimum reprojection error, which partially enabled it to tackle occlusion. It also used up-sampling and proposed automasking of stationary pixels to avoid 'holes' of infinite depth generated by low-texture and moving objects respectively.</p><p>However, all aforementioned works predict only relative depth, which means there still exists a scale gap between the prediction and true depth. For evaluation purpose, ratio between medians of ground-truth and current prediction is employed to acquire absolute depth. Unfortunately, in real application scenarios, ground-truth is either too difficult or financially expensive to obtain. Therefore, a scale recovering approach which is free of depth ground truth is called for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Monocular scale recovery</head><p>Scale uncertainty has always been a problem for 3D vision for monocular camera. To recover scale factor and achieve absolute depth estimation, <ref type="bibr" target="#b20">[21]</ref> utilizes pose information and <ref type="bibr" target="#b21">[22]</ref> uses stereo data to pretrain network, both introducing additional sensor information but the results were no as satisfactory. Besides depth estimation, a typical example of this is monocular visual SLAM. In order to mitigate this, <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref> integrate object detection algorithms into monocular visual SLAM system and take advantage of object size prior to recover scale. However, in addition to the significant increase of computation complexity, these methods show limited robustness under scenes without known object classes.</p><p>Handling the geometrical relationship between camera and ground is also an effective approach to tackle this problem. This geometrical constrain is broadly used in autonomous driving tasks, for ground is commonly seen in images captured by on-board cameras. The main task of these methods is to estimate a relative camera height using camera-ground geometrical constrains, and thus infer scale with absolute camera height prior. <ref type="bibr" target="#b25">[25]</ref> extracted ground using trained classifier, but it doesn't possess an excellent generalization power. <ref type="bibr" target="#b26">[26]</ref> extracted the ground points densely in region of interest similar to <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>, but it requires dense stereo to be added to the system, which can potentially raise cost and increase complexity. In <ref type="bibr" target="#b29">[29]</ref>, the most similar work to this one, used surface normal to extract ground points and thus calculate camera height. However, due to the sparsity originated in its key-point-based strategy, data association through consecutive frames are needed, makes this method hardly integrated into monocular depth estimation tasks which use only single image as input. In additon, this method regards the ground as a whole, flat panel with single surface normal, which is a strong assumption for autonomous driving scenarios. In contrast, our method is free of data association, which means it can be integrated into both monocular depth estimation and visual SLAM tasks. Furthermore, our method achieve per-pixel surface normal calculation and ground segmentation, makes the algorithm robust to different road conditions for autonomous driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section, a novel pipeline called DNet specifically designed for monocular absolute depth estimation in autonomous driving applications is proposed. The pipeline can be divided into two parts, respectively relative depth estimation, with dense connected prediction (DCP) layer to improve object-level depth inference, and scale recovery based on dense geometrical constraint, without needing any additional sensor signals or depth ground-truth. The overview of DNet can be seen in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Relative depth estimation</head><p>The proposed DNet is based on Monodepth2 <ref type="bibr" target="#b13">[14]</ref>. As all self-supervised depth estimation methods, its objectlevel inference can still have texture copy and imprecise object boundaries. In this section, we will first introduce Monodepth2 and then resolve this issue by introducing DCP layer to replace full resolution module used in Monodepth2.  After that, scale recovery module pops in. It utilizes geometrical relationship between the ground and the camera, using extracted ground points to densely calculate camera heights point by point. Median value of all camera heights is then selected to be the final estimated value and used to obtain scale factor. Combined with relative depth map generated in the first step, scale recovery module outputs absolute depth of the given monocular image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Baseline: Monodepth2 w/o full resolution:</head><p>Architecture: Two networks are required in monocular selfsupervision architecture, respectively a depth network and a pose network. Single image I t of the t-th frame is taken as the input of the depth network. Depth network outputs a dense relative depth map D rel t . Pose network takes {I t?1 , I t } and {I t , I t+1 } sequentially as inputs and then outputs camera poses of the t-th image relative to that of the (t ? 1)-th and (t + 1)-th images, i.e., {T rel t?t?1 , T rel t?t+1 }. Self-supervision loss: Two parts constitute the overall loss, respectively per-pixel minimum reconstruction loss L p and inverse depth smoothness loss L s . Reconstruction loss is calculated by firstly inverse warping source images {I t?1 , I t+1 } to rebuild two target images {I t?1?t , I t+1?t }. After that, photometric error (PE) between reconstructed image and target image is calculated combining structural similarity index (SSIM) <ref type="bibr" target="#b30">[30]</ref> and L1 norm between two images I a , I b as follows:</p><formula xml:id="formula_0">PE(I a , I b ) = ? 1 ? SSIM(I a , I b ) 2 + (1 ? ?) I a ? I b 1 ,<label>(1)</label></formula><p>where ? is used for weight adjustment. Per-pixel minimum loss L p is then calculated as follows:</p><formula xml:id="formula_1">L p = min I (P E(I , I t )) ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">I ? {I t?1?t , I t+1?t , I t?1 , I t+1 }.</formula><p>Combined with edge-aware smoothness loss L s :</p><formula xml:id="formula_3">L s = |? x d * t |e ?|?xIt| + |? y d * t |e ?|?yIt| ,<label>(3)</label></formula><p>where d * t = d t /d t is the mean-normalized inverse depth, overall loss can be constructed with two hyper-parameters ? and ? as:</p><formula xml:id="formula_4">L i = i (?L p,i + ?w i L s,i ) ,<label>(4)</label></formula><p>where subscript i denotes different resolution layers of the decoder. w i is determined according to the resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) DNet &amp; Dense connected prediction layer:</head><p>Overall loss: Because the photometric error of low resolution depth prediction can be the result of wrong network prediction or the aliasing of down-sampling, using the same weight in loss for low-res and high-res results can mislead the network to converge in non-optimal values. Additionally, in consideration that features with lower resolution are reused for multiple times, the weight of error in lower resolution depth prediction is reduced as follows:</p><formula xml:id="formula_5">L i = i (?v i L p,i + ?w i L s,i ) .<label>(5)</label></formula><p>where v i &lt; 1 is introduced as weight adjustment parameter. DCP layer: In order to handle local gradient caused by bilinear sampling <ref type="bibr" target="#b31">[31]</ref> and local minima, current works <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> including our baseline Monodepth2 use multi-scale depth prediction strategy. This strategy implicitly uses low-res features to predict depth by repeated upsampling layers, which has the tendency of depth artifacts ( <ref type="figure">Fig. 9</ref>). Motivated by reducing the depth artifacts and acquiring more reasonable object-level depth inference, we propose a novel DCP layer that explicitly combines features in different scales hierarchically. The intuition is based on the observation that low-res layers of decoder network can provide more reliable object-level depth inference and high-res layers focus more on local depth details.</p><p>Formally, the numbers of feature channels in different scales are reduced to eight using a convolutional layer in the DCP layer, so that the number of channels are uniformed and calculations afterwards can be simplified. Features in lowres layers are then up-sampled and concatenated to higherres layer features. By doing this, we introduce more precise object-level inference into higher resolution depth predictions that originally care less about object-level depth. The final depth estimation is performed based on the hierarchical features provided by densely connected feature layers. Detailed structure can be seen in <ref type="figure" target="#fig_1">Fig. 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Scale recovery</head><p>Scale recovery is performed after relative depth is predicted so that absolute depth map can be generated solely relying on monocular image. Dense geometrical constraint (DGC) is thus introduced. DGC is specifically designed for autonomous driving applications. It works under the assumption that there are enough ground points in the monocular image, which is usually the case for autonomous driving. Unlike the scale recovery employed by feature-based visual odometry, ground points are densely extracted by DGC from the monocular images to form a dense ground point map. Each point in the map is used to estimate one camera height, as can be seen in <ref type="figure">Fig. 5</ref>. A large number of camera heights can thus be obtained. By applying statistical methods for overall camera height estimation, outliers can barely harm the estimation result of the scale factor. 1) Surface normal calculation: The first step is to determine a surface normal for each pixel in the input image. All the pixel points need to be projected to 3D space according to the following equation:</p><formula xml:id="formula_6">D rel t (p i,j )p i,j = KP i,j ,<label>(6)</label></formula><p>where p i,j = [i, j, 1] refers to the pixel on the i-th row and the j-th column in 2D space with one homogeneous coordinate, and P i,j = [X, Y, Z] is the corresponding 3D point, D rel t (p i,j ) is the depth of that specific point, and K is the camera intrinsic matrix.</p><p>Similar to <ref type="bibr" target="#b19">[20]</ref>, for each pixel point , 8-neighbor convention is used to determine several planes around it, as in <ref type="figure" target="#fig_2">Fig. 4</ref>. All 8 neighbors of p i,j are grouped into 4 pairs. Two vectors of p i,j connected respectively to two points in one pair form a 90-degree angle, i.e., G(P i,j ) = {[P i+1,j , P i,j?1 ], [P i+1,j?1 , P i?1,j?1 ]...}. Four pairs of vector constitutes 4 surfaces, thus generating 4 surface normals, which can be calculated by:</p><formula xml:id="formula_7">n g = ? ???? ? P i,j G g,1 ? ? ???? ? P i,j G g,2 ,<label>(7)</label></formula><p>where G a,b denotes the b-th element of the a-th pair in G(P i,j ) and g = 1, 2, 3, 4. The final normalized surface normal of point P i,j is given by normalizing and averaging four estimated normals:</p><formula xml:id="formula_8">N(P i,j ) = g n/ n g 2 4 .<label>(8)</label></formula><p>2) Ground point detection: Ground points usually refers to the points that has a normalized normal close to ideal ground normal, i.e.,? = (0, 1, 0) . With this ideal target normal and the calculated normalized surface normal, we propose a similarity function s(P i,j ) based on absolute value of cosine function. The calculated similarity S can be used as a simple criteria to determine whether P i,j is a ground point or not. </p><formula xml:id="formula_9">S = s(P i,j ) = | (?, N(P i,j ))| = |arccos? ? P i,j ? P i,j | ,<label>(9)</label></formula><p>where operator ? denotes the inner product operation.</p><p>Considering the uncertainty produced by estimating the surface normal and the y-axis of camera coordinate system is not strictly perpendicular to the ground as in <ref type="figure">Fig. 5</ref>, a threshold S max is set. For S &lt; S max , the pixel point is considered as ground points. After determination for ground points has finished for all pixel points, a set of ground points GP = {P i,j |s(P i,j ) &lt; S max , y(P i,j ) &gt; 0} is detected, where y(P i,j ) denotes the y-axis value of P i,j . A ground mask is thereafter generated.</p><p>3) Camera height estimation: When all the ground points have been densely identified from the image, the geometrical relationship between ground points and camera itself is ready to be exploited. As can be seen from <ref type="figure">Fig. 5</ref>, camera height is the projection of vector ???? OP i,j in the direction of surface normal of point P i,j , i.e., N(P i,j ). Therefore, camera height of P i,j can be calculated as follows:</p><formula xml:id="formula_10">h(P i,j ) = N(P i,j ) ? ???? OP i,j ,<label>(10)</label></formula><p>where ???? OP i,j = P i,j = [X, Y, Z] . This operation is done for all P i,j ? GP. Now a set of camera heights H = {h(P i,j )|P i,j ? GP} with element number equal to that of ground points is obtained. But for overall scale factor, one single camera height should be estimated for the relative depth map. After careful experiments, median of all estimated camera heights h M = M edian(H) is selected as the final camera height. 4) Scale factor calculation: Given the camera height estimated for current relative depth map for I t , in order to calculate the scale factor, all that is still needed is the real height of the camera h R . The scale factor for the current relative depth estimation is simply determined as follows:</p><formula xml:id="formula_11">f t = h R h M .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Absolute depth estimation</head><p>After successfully estimated the scale factor for current relative depth map D rel t , absolute depth can be thus pixelwise calculated:</p><formula xml:id="formula_12">D abs t = f t D rel t .<label>(12)</label></formula><p>where D abs t denotes the absolute depth estimated for current image I t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head><p>Thorough experiments are presented here for evaluation of DNet pipeline. Quantitative results show our proposed DNet is able to achieve competitive performance on both relative depth estimation and scale recovery. Also, ablation study is performed to prove the effectiveness of our proposed DCP layer. And due to the dependency of enough visible ground, experiments under different ground point ratio show the robustness of DGC scale recovery module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation details</head><p>The same training parameters and method as Monodepth2 are used. Specifically, we set ? = 1, ? = 0.001, and ? for SSIM is equal to 0.85. Only monocular image sequence is used during training. For scale recovery, angle threshold S max = 5. Low values are assigned to v i and w i for low-res predictions, i.e., v = w = {1/8, 1/4, 1/2, 1}.</p><p>The experiments are run on a computer with Intel Xeon 8163 CPU (2.5GHz) and NVIDIA RTX 2080 Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation dataset</head><p>All experiments for evaluation of DNet are conducted on the Eigen split [2] of KITTI <ref type="bibr" target="#b15">[16]</ref> 2015 containing 697 test images. For evaluation of depth estimation results, it contains ground truth projected from LiDAR 3D point clouds to 2D depth maps. However, there is no ground truth for scale factors to transfer relative depth maps to absolute depth maps. Usually used method is to use the ratio between medians of LiDAR detected depth values and estimated ones as ground truth of scale factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Quantitative evaluation</head><p>Thorough quantitative evaluation is presented to show the overall performance of DNet pipeline on both relative and absolute depth estimation. Commonly used metrics are adopted for evaluation. <ref type="table" target="#tab_3">Table I</ref> demonstrates the overall depth estimation performance of DNet, both using ground-truth (GT) and DGC scale recovery, in comparison with 14 self-supervised monocular depth estimators. DNet with GT scale recovery is first evaluated to demonstrate its relative depth estimation performance. As can be seen from the table, DNet with GT scale recovery has achieved a satisfactory result. It has improved compared to Monodepth2 on former four metrics by respectively 1.74%, 4.32%, 1.05% and 1.04%.</p><p>In terms of absolute depth estimation, DGC performs almost as well as GT scale recovery. Compared to Roussel et al. <ref type="bibr" target="#b21">[22]</ref>, DNet achieves improvement on former four metrics by respectively 32.57%, 41.64%, 28.73% and 29.18%. The performance of DGC module can even outperform most early depth estimator using GT scale recovery. These indicate that DGC scale recovery method, in spite of its simplicity, can carry out a satisfactory scale recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation study</head><p>In order to better show the benefit of our proposed modules and the robustness against ground point ratio, comprehensive ablation study is conducted.  1) Benefit of DCP layer: In order to show the effectiveness of hierarchical feature generated by DCP layer, comparisons are made between baseline and DNet as can be seen in <ref type="table" target="#tab_3">Table II</ref>. It can be seen that, our proposed DCP layer can boost the performance on the former four metrics by respectively 3.42%, 3.36%, 1.78%, 2.05%.</p><p>2) Benefit of DCP layer on object-level prediction: Depth estimation on objects can be challenging for the irrgular boundary and texture copy effects. To show the improvement of DCP layer on object-level prediction, Mask-RCNN <ref type="bibr" target="#b37">[37]</ref> is used to generate object masks as shown in <ref type="figure">Fig.6</ref> on test files and error metrics are calculated only within the masked <ref type="figure">Fig. 6</ref>. Object masks extracted by Mask-RCNN <ref type="bibr" target="#b37">[37]</ref>, object level performance is calculated only on pixels in the mask. areas. <ref type="table" target="#tab_3">Table III</ref> compares performance between baseline and DNet on the object-level depth prediction. Our proposed DCP layer improves the object-level prediction performance on the former four metrics by respectively 11.01%, 23.45%, 5.80%, 2.14%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Robustness of DGC scale recovery against visible ground:</head><p>Since DGC scale recovery largely depends on the ground points extraction, the relationship of its performance and the proportion of ground points in a single frame should be carefully evaluated. We evaluate 697 test images in Eigen split and plot ground points ratio and corresponding scale error of each frame. Result is shown in <ref type="figure">Fig. 7</ref>, where the xaxis is ground point ratio and y-axis is DGC?GT GT . It can be seen that when the ground point ratio is larger than 1.03%, the proposed DGC module can perform uniformly and robustly comparable to GT scale recovery. But with extreme low ground points ratio, scale may be incorrectly estimated.</p><p>Ground points ratio less than 1.03% <ref type="figure">Fig. 7</ref>. Robustness evaluation of DGC scale recovery module under different ground point ratios. The result shows that when detected ground points take up more than 1.03% of all pixels, our proposed DGC module can perform comparable to GT scale recovery. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative evaluation</head><p>Qualitative results are demonstrated in <ref type="figure">Fig. 8</ref> and <ref type="figure">Fig. 9</ref>. <ref type="figure">Fig. 8</ref> shows the overall absolute depth estimation results as well as intermediate results such as surface normal and ground point mask. <ref type="figure">Fig. 9</ref> demonstrates intuitively the improvement brought by introducing DCP in comparison with baseline. It can be seen that object boundary is more precise and depth artifacts are to some extent eliminated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth estimation</head><p>Surface normal map Ground point estimation  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Additional DGC and GT comprarisons</head><p>There are also results showing that in some cases, DGC scale recovery works even better than GT scale recovery, especially in those scenes, where ground point ratio is relatively large. Some example of those scenes can be seen in <ref type="figure">Fig. 10</ref>. The performance in those frames can be seen in <ref type="table" target="#tab_7">Table V</ref>. Surprisingly, in at least 31.7% and at most 45.2% of the frames, DGC scale recovery module performs better in terms of four metrics. Detailed result of the ratio of frames where DGC performs favorably against GT scale recovery can be seen in <ref type="table" target="#tab_3">Table VI.</ref> V. CONCLUSIONS In this work, a novel pipeline for self-supervised monocular absolute depth estimation is presented. DCP layer is proposed to generate hierarchical features for high resolution depth inferences, so that object boundary can be more </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame #106</head><p>Frame #395 Frame #330 Frame #183 <ref type="figure">Fig. 10</ref>. Scenes where DGC scale recovery performs better than GT scale recovery. It can be intuitively seen that ground point ratio are all relatively high in those scenes. accurate and depth artifacts can be better addressed. In order for the self-supervised monocular depth estimation to be more easily adapted to and used in autonomous driving applications, DGC module is introduced to perform absolute depth prediction without additional sensors and depth ground truth. Extensive experiments were conducted to demonstrate the effectiveness and robustness of the proposed DNet pipeline as well as DCP and DGC module. In future, this work provides intuition for better use of hierarchical features and can serve as the basis for further explorations of scale recovery methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Overall structure of proposed DNet pipeline. The algorithm first estimates relative depth using the depth estimation network with our proposed DCP layer (pink layer in the figure) to generate and exploit hierarchical features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Structure of proposed DCP layer. Different from baseline multiscale prediction strategy (directly uses feature in different resolutions independently), features are densely connected from low to high resolution to form hierarchical features in DCP layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>2D to 3D projection and pairing of 8-neighbors in surface normal calculation. Points with the same color is paired to form two vectors respectively with the center point. Four surface normals can be calculated from four vector pairs and used to form one surface normal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Qualitative results of DNet absolute depth estimation result as well as components in DGC scale recovery module on KITTI 2015 Eigen Split. Qualitative results of our proposed DCP layer on KITTI 2015 Eigen Split. Compared to baseline, DNet with DCP layer is able to present more precise object boundary (green) and significantly reduce depth artifacts (blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>A novel dense geometrical constraints (DGC) module is introduced to perform high-quality scale recovery for autonomous driving. Based on relative depth estimation, DGC module can finish per-pixel ground segmentation and estimate a camera height from every ground point. Statistical method is applied to determine the camera height so that outliers of ground point extraction can be robustly suppressed. Scale factor can be determined through comparision between the given and estimated camera height.</figDesc><table /><note>arXiv:2004.05560v2 [cs.CV] 9 Sep 2020and reducing visual artifacts.?? DNet is extensively evaluated on KITTI[16] Eigen</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I</head><label>I</label><figDesc>QUANTITATIVE RESULTS. PERFORMANCE OF DNET PIPELINE COMPARED TO EXISTING METHODS. THE EXPERIMENT RESULTS OF EXISTING METHODS ARE FROM RESPECTIVE PAPERS. FOR SCALE FACTOR, GT IS USING LIDAR DEPTH GROUND TRUTH, P IS USING ADDITIONAL POSE INFORMATION, S IS USING STEREO PRETRAINED NETWORK AND DGC IS OUR PROPOSED DENSE GEOMETRICAL CONSTRAIN METHOD. BOLD AND UNDERLINED DATA DENOTES THE BEST AND SECOND BEST PERFORMANCE RESPECTIVELY.</figDesc><table><row><cell>Method</cell><cell>Scale Factor</cell><cell cols="6">Lower is better Abs Rel Sq Rel RMSE RMSE log ? &lt; 1.25 ? &lt; 1.25 2 Higher is better</cell><cell>? &lt; 1.25 3</cell></row><row><cell>Zhou et al. [15]CVPR'17</cell><cell>GT</cell><cell>0.183</cell><cell>1.595</cell><cell>6.709</cell><cell>0.270</cell><cell>0.734</cell><cell>0.902</cell><cell>0.959</cell></row><row><cell>Yang et al. [20]AAAI'18</cell><cell>GT</cell><cell>0.182</cell><cell>1.481</cell><cell>6.501</cell><cell>0.267</cell><cell>0.725</cell><cell>0.906</cell><cell>0.963</cell></row><row><cell>Mahjourian et al. [32]CVPR'18</cell><cell>GT</cell><cell>0.163</cell><cell>1.240</cell><cell>6.220</cell><cell>0.250</cell><cell>0.762</cell><cell>0.916</cell><cell>0.968</cell></row><row><cell>LEGO [13]CVPR'18</cell><cell>GT</cell><cell>0.162</cell><cell>1.352</cell><cell>6.276</cell><cell>0.252</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DDVO [33]CVPR'18</cell><cell>GT</cell><cell>0.151</cell><cell>1.257</cell><cell>5.583</cell><cell>0.228</cell><cell>0.810</cell><cell>0.936</cell><cell>0.974</cell></row><row><cell>DF-Net [34]ECCV'18</cell><cell>GT</cell><cell>0.150</cell><cell>1.124</cell><cell>5.507</cell><cell>0.223</cell><cell>0.806</cell><cell>0.933</cell><cell>0.973</cell></row><row><cell>GeoNet [18]CVPR'18</cell><cell>GT</cell><cell>0.149</cell><cell>1.060</cell><cell>5.567</cell><cell>0.226</cell><cell>0.796</cell><cell>0.935</cell><cell>0.975</cell></row><row><cell>EPC++ [17]TPAMI'18</cell><cell>GT</cell><cell>0.141</cell><cell>1.029</cell><cell>5.350</cell><cell>0.216</cell><cell>0.816</cell><cell>0.941</cell><cell>0.976</cell></row><row><cell>Struct2Depth [12]AAAI'19</cell><cell>GT</cell><cell>0.141</cell><cell>1.026</cell><cell>5.291</cell><cell>0.215</cell><cell>0.816</cell><cell>0.945</cell><cell>0.979</cell></row><row><cell>CC [35]CVPR'19</cell><cell>GT</cell><cell>0.139</cell><cell>1.032</cell><cell>5.199</cell><cell>0.213</cell><cell>0.827</cell><cell>0.943</cell><cell>0.977</cell></row><row><cell>Bian et al. [36]NIPS'19</cell><cell>GT</cell><cell>0.128</cell><cell>1.047</cell><cell>5.234</cell><cell>0.208</cell><cell>0.846</cell><cell>0.947</cell><cell>0.976</cell></row><row><cell>Monodepth2 [14]ICCV'19</cell><cell>GT</cell><cell>0.115</cell><cell>0.903</cell><cell>4.863</cell><cell>0.193</cell><cell>0.877</cell><cell>0.959</cell><cell>0.981</cell></row><row><cell>DNet (Ours)</cell><cell>GT</cell><cell>0.113</cell><cell>0.864</cell><cell>4.812</cell><cell>0.191</cell><cell>0.877</cell><cell>0.960</cell><cell>0.981</cell></row><row><cell>Pinard et al. [21]ECCV'18</cell><cell>P</cell><cell>0.271</cell><cell>4.495</cell><cell>7.312</cell><cell>0.345</cell><cell>0.678</cell><cell>0.856</cell><cell>0.924</cell></row><row><cell>Roussel et al. [22]IROS'19</cell><cell>S</cell><cell>0.175</cell><cell>1.585</cell><cell>6.901</cell><cell>0.281</cell><cell>0.751</cell><cell>0.905</cell><cell>0.959</cell></row><row><cell>DNet (Ours)</cell><cell>DGC</cell><cell>0.118</cell><cell>0.925</cell><cell>4.918</cell><cell>0.199</cell><cell>0.862</cell><cell>0.953</cell><cell>0.979</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II ABLATION</head><label>II</label><figDesc>STUDY. COMPARISON ON THE PERFORMANCE BETWEEN BASELINE AND OUR DNET WITH PROPOSED DCP LAYER. SCALE FACTOR IS DETERMINED USING LIDAR GROUND TRUTH.</figDesc><table><row><cell>Method</cell><cell cols="5">Lower is better Abs Rel Sq Rel RMSE RMSE log ? &lt; 1.25 ? &lt; 1.25 2 ? &lt; 1.25 3 Higher is better</cell></row><row><cell cols="2">Baseline 0.117 0.894 4.899</cell><cell>0.195</cell><cell>0.871</cell><cell>0.958</cell><cell>0.981</cell></row><row><cell>Ours</cell><cell>0.113 0.864 4.812</cell><cell>0.191</cell><cell>0.877</cell><cell>0.960</cell><cell>0.981</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III ABLATION</head><label>III</label><figDesc>STUDY. COMPARISON ON THE OBJECT LEVEL PREDICTION PERFORMANCE BETWEEN BASELINE AND OUR DNET WITH PROPOSED DCP LAYER. SCALE FACTOR IS DETERMINED USING LIDAR GROUND TRUTH.</figDesc><table><row><cell>Method</cell><cell cols="5">Lower is better Abs Rel Sq Rel RMSE RMSE log ? &lt; 1.25 ? &lt; 1.25 2 ? &lt; 1.25 3 Higher is better</cell></row><row><cell cols="2">Baseline 0.227 3.680 8.430</cell><cell>0.327</cell><cell>0.690</cell><cell>0.857</cell><cell>0.924</cell></row><row><cell>Ours</cell><cell>0.202 2.817 7.941</cell><cell>0.310</cell><cell>0.725</cell><cell>0.875</cell><cell>0.932</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV SPEED</head><label>IV</label><figDesc>PERFORMANCE OF DNET.</figDesc><table><row><cell>Stage</cell><cell>Time consumption</cell></row><row><cell>Inference</cell><cell>50.0ms</cell></row><row><cell>DGC scale recovery</cell><cell>4.1ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V</head><label>V</label><figDesc>QUANTITATIVE RESULT IN SCENES LISTED IN FIG. 10. WHEN GROUND POINT RATIO IS RELATIVELY HIGH, DGC USUALLY PERFORMS BETTER THAN GT SCALE RECOVERY IN AT LEAST ON METRIC.</figDesc><table><row><cell>Frame</cell><cell cols="4">Scale Factor Abs Rel Sq Rel RMSE RMSE log Lower is better</cell></row><row><cell>#106</cell><cell>GT</cell><cell>0.195</cell><cell>1.443 6.416</cell><cell>0.320</cell></row><row><cell>#106</cell><cell>DGC</cell><cell>0.110</cell><cell>1.105 5.799</cell><cell>0.319</cell></row><row><cell>#183</cell><cell>GT</cell><cell>0.194</cell><cell>1.175 5.888</cell><cell>0.231</cell></row><row><cell>#183</cell><cell>DGC</cell><cell>0.127</cell><cell>0.995 5.745</cell><cell>0.229</cell></row><row><cell>#330</cell><cell>GT</cell><cell>0.211</cell><cell>1.100 4.138</cell><cell>0.270</cell></row><row><cell>#330</cell><cell>DGC</cell><cell>0.144</cell><cell>0.844 4.174</cell><cell>0.271</cell></row><row><cell>#395</cell><cell>GT</cell><cell>0.353</cell><cell>2.181 5.837</cell><cell>0.418</cell></row><row><cell>#395</cell><cell>DGC</cell><cell>0.273</cell><cell>1.754 5.834</cell><cell>0.470</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI RATIO</head><label>VI</label><figDesc>OF THE FRAMES WHERE DGC SCALE RECOVERY PERFORMS BETTER THAN GT IN TERMS OF DIFFERENT METRICS. IT CAN BE SEEN THAT ESPECIALLY IN ABSOLUTE RELATIVE ERRORS, DGC PERFORMS BETTER IN MANY FRAMES.</figDesc><table><row><cell></cell><cell cols="2">Evaluation metrics</cell><cell></cell></row><row><cell>Abs Rel</cell><cell>Sq Rel</cell><cell>RMSE</cell><cell>RMSE log</cell></row><row><cell>45.2%</cell><cell>38.5%</cell><cell>39.3%</cell><cell>31.7%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This research was supported in part by the National Natural Science Foundation of China under Grant U19A2069, the National Research Foundation Singapore through the Singapore MIT Alliance for Research and Technology's Future Urban Mobility IRG research programme, and the Singapore Land Transport Authority Urban Mobility Grand Challenge Funding Initiative.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Depth extraction from video using non-parametric sampling-supplemental material</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on Computer Vision</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Suh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10326</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5684" to="5693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">High quality monocular depth estimation via transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11941</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learn stereo, infer mono: Siamese networks for self-supervised, monocular, depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Superdepth: Self-supervised, super-resolved monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambru?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9250" to="9256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning monocular depth estimation with unsupervised trinocular assumptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="324" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8001" to="8008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lego: Learning edge with geometry all at once by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3828" to="3838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Every pixel counts++: Joint learning of geometry and motion with 3d holistic understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06125</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1983" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07804</idno>
		<title level="m">Sfm-net: Learning of structure and motion from video</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unsupervised learning of geometry with edge-aware depth-normal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03665</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning structure-from-motion from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pinard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chevalley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Manzanera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Monocular depth estimation in new environments with absolute scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roussel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Eycken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1735" to="1741" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Object-aware bundle adjustment for correcting monocular scale drift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4770" to="4776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Probabilistic global scale estimation for monoslam based on generic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Hayet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="48" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What does ground tell us? monocular visual odometry under planar motion constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Joung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-I</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 11th International Conference on Control, Automation and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1480" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High accuracy monocular sfm and scale correction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Guest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="730" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Monocular visual odometry using a planar road model to solve scale ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Kitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schonbein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lategahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ground-plane-based absolute scale estimation for monocular visual odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Monocular visual odometry scale recovery using geometrical constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="988" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5667" to="5675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2022" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Df-net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="36" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="240" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unsupervised scale-consistent depth and ego-motion learning from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10553</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
