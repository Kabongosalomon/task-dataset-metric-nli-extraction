<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ScatSimCLR: self-supervised contrastive learning with pretext task regularization for small-scale datasets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Kinakh</surname></persName>
							<email>vitaliy.kinakh@unige.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Taran</surname></persName>
							<email>olga.taran@unige.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svyatoslav</forename><surname>Voloshynovskiy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ScatSimCLR: self-supervised contrastive learning with pretext task regularization for small-scale datasets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we consider a problem of self-supervised learning for small-scale datasets based on contrastive loss between multiple views of the data, which demonstrates the state-of-the-art performance in classification task. Despite the reported results, such factors as the complexity of training requiring complex architectures, the needed number of views produced by data augmentation, and their impact on the classification accuracy are understudied problems. To establish the role of these factors, we consider an architecture of contrastive loss system such as SimCLR, where baseline model is replaced by geometrically invariant "hand-crafted" network ScatNet with small trainable adapter network and argue that the number of parameters of the whole system and the number of views can be considerably reduced while practically preserving the same classification accuracy. In addition, we investigate the impact of regularization strategies using pretext task learning based on an estimation of parameters of augmentation transform such as rotation and jigsaw permutation for both traditional baseline models and ScatNet based models. Finally, we demonstrate that the proposed architecture with pretext task learning regularization achieves the state-ofthe-art classification performance with a smaller number of trainable parameters and with reduced number of views. Code: https://github.com/vkinakh/scatsimclr</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Self-supervised learning refers to the learning of data representations that are not based on labeled data. The recent techniques of self-supervised learning such as SimCLR <ref type="bibr" target="#b9">[10]</ref>, SwAV <ref type="bibr" target="#b7">[8]</ref>, SeLa <ref type="bibr" target="#b2">[3]</ref> and BYOL <ref type="bibr" target="#b13">[14]</ref> demonstrate a classification performance close to their supervised counterparts. The main common idea behind these self-supervised approaches is to learn an embedding that produces an invariant representation under various data augmentations rang-* S. Voloshynovskiy is a corresponding author. Gray dots indicate other self-supervised methods. Our method, ScatSimCLR, is shown in red. Our implementation of SimCLR is shown in green. The results are obtained with models trained for 1000 epochs.</p><p>ing from image filtering to geometrical transformations. In most cases, some powerful neural network such as for example ResNet <ref type="bibr" target="#b9">[10]</ref> is used to implement this embedding. It is demonstrated <ref type="bibr" target="#b9">[10]</ref> that the classification accuracy of these systems increases with the increase of the complexity of ResNet represented by the larger number of parameters capable of producing the invariance of visual representation under the broad family of augmentations. Typically the number of parameters of such networks ranges from 5M to 500M that makes their training quite a complex and time consuming task and requires a lot of training data.</p><p>At the time, in many practical applications it is infeasible to collect a lot of training data. Moreover, in many cases the amount of labeled data is limited. We refer to these cases as a "small dataset" problem. These restrictions lead to the overfitting of large scale models such as ResNet and result in their poor generalization. Therefore, to benefit from the recent advancements of self-supervised learning, which performance is generally demonstrated on the large scale datasets such as ImageNet <ref type="bibr" target="#b10">[11]</ref>, it is important to develop efficient representation learning techniques adapted to the small dataset problem.</p><p>In this paper, we try to address the problem of selfsupervised learning based on contrastive loss in the application to the small dataset problem by replacing complex ResNet network by networks with a smaller number of parameters. More particularly, we investigate a question whether such complex networks as ResNet are really needed to achieve the targeted representation invariance assuming that the invariance to some families of augmentations can be achieved by a hand-crafted embedding. One candidate for such an invariant hand-crafted embedding is ScatNet <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>, which is known to produce stable embeddings under the deformations in terms of Lipschitz continuity property. As a by-product of such an invariance, one might assume that the number of augmentations needed for the training of invariant embedding can be reduced accordingly. Finally, the overall complexity of training might also be lower. To investigate these questions, we propose a ScatSimCLR architecture where the complex ResNet is replaced by ScatNet followed by a simple adapter network. We demonstrate that ScatSimCLR with a reduced number of training parameters and a reduced number of used augmentations can achieve similar performance and in some cases even outperform SimCLR. Furthermore, we demonstrate that the introduction of pretext task learning regularization, yet another popular technique of self-supervised learning, is beneficial for representation learning both for basic neural networks like SimCLR as well as for the proposed architectures.</p><p>Main contributions are:</p><p>1. We propose a model with the reduced number of parameters of the embedding network while preserving the same classification performance. This is achieved due to the usage of the geometrically invariant network ScatNet. <ref type="figure" target="#fig_0">Figures 1 and 8</ref> demonstrate the performance of ScatSimCLR on STL-10 and CIFAR100-20 1 <ref type="bibr" target="#b0">[1]</ref> as a function of the number of parameters with respect to the other state-of-the-art methods. The ScatSimCLR outperforms the state-of-the-art SCAN <ref type="bibr" target="#b40">[40]</ref> and RUC <ref type="bibr" target="#b34">[34]</ref> methods known to produce the top result for STL-10 and CIFAR100-20 datasets, while using even lower complexity networks.</p><p>2. We investigate the impact of pretext task regularization on the classification performance. This includes the regularization based on the estimation of parameters 1 CIFAR100-20 is CIFAR100 dataset with 20 superclasses. <ref type="figure">Figure 2</ref>. ScatNet <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> filter bank for J = 5 (number of scales) and L = 6 (number of rotations). The top left image corresponds to a low-pass filter. The first left half image corresponds to the real parts of ScatNet filters arranged according to the scales (rows) and orientations (columns). The right half image corresponds to the imaginary part of ScatNet filters.</p><p>of applied augmentation transform such as the rotation angle and jigsaw permutation.</p><p>3. We investigate the impact of the ScatNet and pretext task regularization on several datasets such as STL-10 and CIFAR100-20 and establish that the ScatSim-CLR achieves state-of-the-art performance even with the smaller number of parameters.</p><p>4. We investigate the role of augmentations in the context of representation learning based on the geometrically invariant ScatNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>We demonstrate that the data agnostic ScatNet is applicable to the datasets with different statistics and labels and does not require extensive training as in the case of ResNet used for SimCLR contrastive learning.</p><p>6. Finally, we demonstrate that individual contributions of ScatNet and pretext tasks improves the performance of the model on classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>We briefly summarize the related work to the concepts used in this paper.</p><p>Contrastive learning is considered among the state-ofthe-art techniques for self-supervised learning <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b9">10]</ref>. The contrastive learning is based on a parameterized encoding or embedding that produces a lowdimensional data representation such that minimizes some distance between similar (positive) data pairs and maximizes for dissimilar (negative) ones. One of the central questions in contrastive learning is the generation or selection of positive and negative examples without labels. It is a common practice to generate positive examples by a data augmentation when multiple "views" for a given image are created by applying different crops <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b25">26]</ref>, various geometrical transformations of affine or projective families <ref type="bibr" target="#b12">[13]</ref>, jigsaw image permutations <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b11">12]</ref>, splitting image into luminance and chrominance components  <ref type="figure">Figure 3</ref>. Examples of ScatNet <ref type="bibr" target="#b1">[2]</ref> feature vectors for L = 4 and J = 2 for STL-10 <ref type="bibr" target="#b0">[1]</ref> images. ScatNet transform is applied to each color channel separately, then each channel is normalized and merged into a RGB image for better visualization. <ref type="bibr" target="#b38">[38]</ref>, applying low-pass and high-pass filtering <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref>, predicting one view from another <ref type="bibr" target="#b44">[44]</ref>, etc. The overall idea is to create a sort of "associations" between different parts of the same object or scene via a common latent space representation. The negative pairs are generally considered as images or parts of images randomly sampled from unlabeled data. The recent study <ref type="bibr" target="#b39">[39]</ref> demonstrates the role of positive and negative example selection and generation and their impact on the overall classification accuracy.</p><p>Hand-crafted geometrically invariant transform -Scat-Net 2 is a class of Convolutional Neural Networks (CNNs) designed with fixed weights <ref type="bibr" target="#b5">[6]</ref> that has a set of important properties. (1) Deformation stability: in contrast to the Fourier transformation that is generally unstable to deformations at high frequencies <ref type="bibr" target="#b2">3</ref> , ScatNet is stable to deformations in terms of Lipschitz continuity property. The stability is gained due to the use of non-linearity and average pooling. (2) Hand-crafted design: ScatNet is considered as a deep convolution network with fixed filters in a form of wavelet basis functions independent of a specific dataset that at the same time provides (3) sparse representation. (4) Interpretable representation: in contrast to the most deep convolutional networks that output only the last layer, ScatNet outputs all layers representing the different signal scales. <ref type="figure">Figure 2</ref> shows typical ScatNet filters for the depth J = 5 and number of orientations L = 6. A set of features produced by ScatNet for the STL-10 [1] samples is shown in <ref type="figure">Figure 3</ref>.</p><p>Hand-crafted pretext task and clustering based pseudolabeling are used to compensate for the lack of labeled data. The hand-crafted pretext task is considered as a sort of self- <ref type="bibr" target="#b1">2</ref> The efficient GPUs' implementation are provided in <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b1">2]</ref> 3 The Fourier transform is invariant to translation. supervised learning when the input data are manipulated to extract a supervised signal in the form of a pretext task learning. The hand-crafted pretext task has been widely used in various settings to predict the patch context <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28]</ref>, solve jigsaw puzzles from the same <ref type="bibr" target="#b28">[29]</ref> and different images <ref type="bibr" target="#b31">[31]</ref>, colorize images <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b24">25]</ref>, predict noise <ref type="bibr" target="#b4">[5]</ref>, count <ref type="bibr" target="#b30">[30]</ref>, estimate parameters of rotations <ref type="bibr" target="#b12">[13]</ref>, inpaint patches <ref type="bibr" target="#b35">[35]</ref>, spot artifacts <ref type="bibr" target="#b19">[20]</ref>, generate images <ref type="bibr" target="#b36">[36]</ref> as well as for predictive coding <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b17">18]</ref> and instance discrimination <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b26">27]</ref>. We refer the reader to <ref type="bibr" target="#b21">[22]</ref> for the details of these methods. At the same time, clustering based pseudo-labeling can be used as pseudo-labels to learn visual representations <ref type="bibr" target="#b6">[7]</ref>. Recent work <ref type="bibr" target="#b7">[8]</ref> extends this idea to soft cluster assignment in contrast to hard-assignment. In this work we only consider pretext task learning based on rotation and jigsaw parameters' estimation.</p><formula xml:id="formula_0">x ! x j ? ? t (x) ? t (x) ! x i f ? ResNet a j f ? Scat h j z j f ? h a i f ? Scat h i f ? h Maximize agreement Representation z i g ? z f ? ResNet Contrastive loss g ? z</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ScatSimCLR</head><p>The proposed architecture of self-supervised representation learning is shown in <ref type="figure" target="#fig_2">Figure 4</ref> and it is based on the SimCLR framework.</p><p>For the batch size N , given {x k } N k=1 in the batch, Sim-CLR produces two augmented versionsx 2k?1 = ? t (x k ) andx 2k = ? t ? (x k ) of each x k using parameterized augmentation transform ? t with parameters t and t ? for each view. Both augmented images are first processed by the feature extraction network f ? ResNet thus producing two representations h 2k?1 = f ? ResNet (x 2k?1 ) and h 2k = f ? ResNet (x 2k ) and then by the projection network g ?z that produces two vectors z 2k?1 = g ?z (h 2k?1 ) and z 2k = g ?z (h 2k ).</p><p>SimCLR contrastive loss is defined as:</p><formula xml:id="formula_1">L SimCLR (? ResNet , ? z ) = 1 2N N k=1 [?(2k ? 1, 2k)+ ?(2k, 2k ? 1)],<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">?(i, j) = ? log exp(si,j /? ) 2N k=1 1 |k? =i] exp(s i,k /? ) with s i,j = z ? i z j / (?z i ? ?z j ?) denotes a pairwise similarity for all pairs i ? {1, . . . , 2N } and j ? {1, . . . , 2N } and 1 [k? =i] ? {0, 1}</formula><p>is an indicator function evaluating to 1 iff k ? = i and ? denotes a temperature parameter.</p><p>SimCLR demonstrates the increasing performance in classification accuracy as shown in <ref type="figure" target="#fig_0">Figure 1</ref> with the growth of the number of parameters of ResNet f ? ResNet from about 11M to 28M. Thus it is commonly assumed that this increase in performance is due to the increase of f ? ResNet network capacity and its ability to learn more complex associations between different parts of objects. Obviously, all the parameters of the network should be trained to efficiently encode these associations.</p><p>In contrast to this, we argue that the complex trainable ResNet f ? ResNet can be replaced by the hand-crafted nontrainable ScatNet network f ? Scat and small capacity trainable adapter network f ? h . ScatNet network f ? Scat is a hand-crafted network with the fixed parameters and it is agnostic to a particular dataset and corresponding inter-object associations. It produces invariant low-level image representation a. At the same time, the low capacity adapter network f ? h aggregates the output of ScatNet and produces the visual representation h. Therefore, one should only train the parameters of an adapter network that is just a fraction of ResNet. Similarly to the results presented in <ref type="figure" target="#fig_0">Figure 1</ref>, one can change the complexity of the adapter network and investigate its impact on the overall system performance. For the fair comparison, we keep the remaining architecture the same as in SimCLR.</p><p>To process color images, we simply apply ScatNet to each color channel as shown in <ref type="figure">Figure 5</ref>. We have used RGB representation but YCbCr or YUV spaces might be even more suited due to the properties of Y component reflecting grayscale images.</p><p>We will refer to SimCLR network with the replaced ResNet by ScatNet and the adapter network as ScatSim-CLR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Additional regularizer as a pretext task selflearning</head><p>In this section, we introduce an additional form of regularization that does not require any labeling, pseudolabeling or mining for positive or negative neighbors as <ref type="figure">Figure 5</ref>. The encoding network for color images. An image x is represented by three color components {xR, xG, xB}. Each color component is processed by ScatNet network f ? Scat and then the adapter network f ? h aggregates the outputs to produce the representation h.</p><formula xml:id="formula_3">x x B f ? Scat a B f ? h h x G f ? Scat a G x R f ? Scat a R</formula><p>x <ref type="figure">Figure 6</ref>. ScatSimCLR with an additional regularization based on the estimation of parameter t and t ? of augmentation transform ?t via a network g ? t applied to both left and right channels (schematically shown only for the left channel in green).</p><formula xml:id="formula_4">! x j ? ? t (x) ? t (x) ! x i a j f ? Scat h j z j f ? h a i f ? Scat h i f ? h Maximize agreement Representation z i Contrastive loss g ? t L t (? h , ? t ) t t Maximize agreement g ? z g ? z L ScatSimCLR (? h , ? z )</formula><p>in <ref type="bibr" target="#b40">[40]</ref> that can be surely applied to our framework. Instead for fair comparison with SimCLR we will stay in the scope of the same self-supervised framework and try to explore another direction by investigating the role of latent space regularization via estimation of parameters of applied augmentation transformation. The pretext task regularization methods are not new and have been used in a standalone self-supervised architectures as described in Section 2. However, up to our best knowledge these regularization techniques have not been considered in the scope of contrastive representation learning. Thus a hypothesis to verify is whether creating more semantics about the inter-object or inter-scene associations would lead to more meaningful latent space representation.</p><p>In our study we define the parameters t of the augmentation transform ? t under the pretext task estimation to be the rotation or jigsaw permutation. We used 4 rotation angles (0?, 90?, 180?and 270?) and 35 jigsaw permutations. We apply only one augmentation (either rotation or jigsaw permutation) at time. These parameters are encoded as onehot-encoding for each augmentation and the corresponding classifier g ?t is used to estimate them from the visual representation h = f ? Scat (f ? h (x)) extracted from the augmented viewx = ? t (x) as shown in <ref type="figure">Figure 6</ref>.</p><p>The pretext task loss is defined as the parameters estimation loss between the applied parameters t and estimated onest = g ?t (h):</p><formula xml:id="formula_5">L t (? h , ? t ) = 1 2N 2N i=1 d(t i , g ?t (h i )),<label>(2)</label></formula><p>where d(., .) denotes the cross-entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Final loss and training</head><p>We define ScatSimCLR loss similarly to SimCLR loss (1) with the only difference that instead of h = f ? ResNet (x), we consider h = f ? h (f ? Scat (x)). Thus, the loss of Scat-SimCLR is denoted as L ScatSimCLR (? h , ? z ).</p><p>The final loss of ScatSimCLR with the pretext task regularization is defined as:</p><formula xml:id="formula_6">L(? h , ? z , ? t ) = L ScatSimCLR (? h , ? z ) + ?L t (? h , ? t ),<label>(3)</label></formula><p>where ? controls the relative weight of the second loss term.</p><p>The parameters estimation is based on the minimization problem:</p><formula xml:id="formula_7">(? h ,? z ,? t ) = argmin (? h ,?z,?t) L(? h , ? z , ? t ),<label>(4)</label></formula><p>in practical implementation for the first 40 epochs we assume ? = 0 in (3) and then ? = 0.3 for the rest. We have noticed that the network converges better, if it is pre-trained with only contrastive loss at the beginning. The parameter ? is selected to equalize the amplitude of contrastive and cross-entropy losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental results</head><p>In this section, we evaluate ScatSimCLR performance on several datasets in the image classification task. At first, the proposed model is pretrained on a particular dataset based on (4) using unlabeled data and then a logistic one-layer classifier is applied to the learned representation to map it to the class labels encoded based on one-hot-vector encoding.</p><p>Datasets. The experimental evaluation is performed on STL-10 <ref type="bibr" target="#b0">[1]</ref> and CIFAR100-20 <ref type="bibr" target="#b22">[23]</ref> datasets. The experiments aim at investigating the impact of ScatSimCLR architecture and image augmentations on the classification performance. The results are reported as a top-1 result from 5 different runs. In this section, we investigate the impact of ScatNet parameters on the overall performance of ScatSimCLR. We use two datasets STL-10 and CIFAR100-20 with the images of size 96x96 to fit ScatNet. It should be noted that CIFAR100-20 is up-sampled from the size 32x32 to 96x96 using LANCZOS interpolation <ref type="bibr" target="#b23">[24]</ref>. The system is trained with respect to the contrastive loss L ScatSimCLR (? h , ? z ) and with adapter network fixed to 12 ResBlock layers and fixed depth of ScatNet to be 2. The pretext task loss was not used and the training was performed for the first 5 epochs only to reflect the dynamics of learning. We have considered a range of ScatNet scaling parameters J from 1 to 4. We experimentally established that for the current architecture of ScatNet applied to the investigated datasets with the images of size 96x96, the best scaling parameter J is 2 as shown in <ref type="table" target="#tab_0">Table 1</ref>. It should be pointed out that the increase of the scaling leads to the usage of larger filter sizes. As a consequence, the size of resulting images on the output of ScatNet, representing the feature vector, decreases. In turns, this represents a trade-off between the desirable robustness to the scaling and undesirable loss of details in the produced images. This might explain the optimality of the scaling factor J=2 as opposed to J=4. <ref type="table" target="#tab_0">Table 1</ref> also demonstrates the impact of rotation parameter L on the classification performance for the considered scale factors J. The investigation of the rotation parameter L was performed in the range from 4 till 16 with the step size equals to 4 for each scale factor. For both considered datasets, the increase of the number of rotations clearly leads to the increase of the classification performance that can be explained by the increase of the rotation invariance in the produced feature space. In contrast to the scaling, the increase of the rotation factor L preserves the dimensionality of the produced feature map for a given fixed scaling and only leads to the increase of the number of rotation channels in the network output. This might explain the increase of the rotation parameter leads to overall performance enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Impact of the number of layers in the adapter network</head><p>In this section, we investigate the impact of the adapter network parameters on the classification accuracy. The experiments are performed on the dataset STL-10 with the image size 96x96. The training loss is defined by (3). As the pretext task network we used a classifier consisting of two fully-connected layers followed by the traditional dropout and ReLu activation. The last layer activation is softmax. ScatNet parameters were chosen according to the best results of section 6.1.1, i.e., J=2 and L=16. We investigate the adapter network with the different number of layers, namely 8, 12, 16 and 30. ScatSimCLR was trained during 1000 epochs for each considered adapter network. The results presented in <ref type="table" target="#tab_1">Table 2</ref> are obtained as the top-1 results on the validation set. The obtained results clearly indicate that the increase of the adapter network complexity increases the performance in the classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Regularization ablations</head><p>In this section we investigate the impact of the regularization techniques. We compare the performance of the model trained with and without pretext task based on the estimation of augmentation transform: rotation and jigsaw estimation. We run experiment with all models presented in <ref type="table" target="#tab_1">Table 2</ref> and ScatSimCLR based on ResNet18 to compare the performance in a function of model complexity. ScatNet parameters were chosen according to the best results presented in 6.1.1. We use the STL-10 dataset for our comparison experiments. As the pretext task estimator we used a classifier consisting of two fully-connected layers with ReLU activation and the softmax at the end. We trained each model for  <ref type="table" target="#tab_2">Table 3</ref>, the introduction of the pretext task improves the classification accuracy for both considered models: (i) ScatNet based SimCLR and (ii) vanilla SimCLR. For all models, rotation augmentation pretext task provides higher increase in classification performance in comparison to jigsaw. It can be explained by the fact that in the process of jigsaw pretext task, an image is split into 9 patches without an intersection, and then each patch is resized using Lanczos interpolation, so they fit the network input size. Applying interpolation introduces some artifacts. In the considered pretext task based on rotating by 90, 180 and 270 degrees the interpolation is not applied as such.</p><p>Therefore the introduction of pretext task regularization improves the classification performance of the models trained with contrastive loss. The proposed ScatSimCLR8 with 6.1 M of parameters outperforms SimCLR (ResNet18) with 11.5 M of parameters for all considered pretext tasks and also without pretext task. This confirms the importance of geometrical invariance of ScatNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Ablations of image augmentations</head><p>In this section we investigate the impact of image augmentations on the classification performance. We use the STL-10 dataset with image size 96x96. To exclude the impact of batch size and other model hyperparameters, we use the fixed setup with batch size = 256, ScatNet parameters: J=2, L=16 and depth=2. We study (i) geometric transformations: random cropping, horizontal flipping and random affine transformations and (ii) color transformations: color jitter, Gaussian blur and grayscaling. We tried to investigate the effect of augmentation ablation considering different combinatorics of augmentations.</p><p>The obtained results are shown in <ref type="figure">Figure 7</ref>. The baseline system performance is shown by the green bar. The baseline uses all considered augmentation similarly to SimCLR. It is interesting to point out that the removal of affine transformation augmentations leads to the performance enhancement with about 2% with respect to the baseline system with all considered augmentations. This is an important result confirming the invariance of ScatNet to geometrical trans- <ref type="figure">Figure 7</ref>. Impact of removing the augmentations on the performance of ScatSimCLR for STL-10: "Baseline" denotes ScatSim-CLR trained with all augmentations (cropping, flipping, color, grayscale, Gaussian blur and affine augmentations). The following labels denote: 1 -the baseline without the affine augmentations; 2 -only cropping and color augmentations; 3 -the baseline without the horizontal flipping; 5 -the baseline without Gaussian blur augmentations; 6 -the baseline without cropping and Gaussian blur augmentations; 7 -the baseline without color and Gaussian blur augmentations; 8 -the baseline without grayscale and Gaussian blur augmentations; 9 -the baseline without cropping and grayscale augmentations; 10 -the baseline without color augmentations; 11 -the baseline without cropping augmentations; 12 -the baseline without grayscale augmentations; 13 -only cropping augmentations; 14 -the baseline without color and grayscale augmentations; 15 -only color augmentations; 16 -the baseline without crop and color augmentations; 17 -no augmentations.</p><p>formations. Therefore, these augmentations can be further excluded from training. In turns, it might lead to the lower complexity of training under a smaller number of augmentations. The next interesting result is obtained when the only image cropping and color transformations were used as the augmentations. It leads to about 0.5% enhancement over the baseline system. Finally, the same enhancement is observed when the flipping was removed from the baseline augmentations. The performance of ScatSimCLR without any augmentations is about 24% lower with respect to the baseline system.</p><p>Summarizing the obtained results, we can conclude that the most important augmentations for ScatSimCLR are cropping and color ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Comparison with the state-of-the-art</head><p>We compare the results obtained for the proposed ScatSim-CLR on STL-10 <ref type="bibr" target="#b0">[1]</ref> and CIFAR100-20 <ref type="bibr" target="#b22">[23]</ref> with the stateof-the-art results reported in ADC <ref type="bibr" target="#b14">[15]</ref>, DeepCluster <ref type="bibr" target="#b6">[7]</ref>, DAC <ref type="bibr" target="#b8">[9]</ref>, IIC <ref type="bibr" target="#b20">[21]</ref>, TSUK <ref type="bibr" target="#b15">[16]</ref>, SCAN <ref type="bibr" target="#b40">[40]</ref>, RUC <ref type="bibr" target="#b34">[34]</ref> and SimCLR <ref type="bibr" target="#b9">[10]</ref> on the <ref type="figure" target="#fig_0">Figures 1 and 8</ref>. <ref type="figure" target="#fig_0">Figures 1 and 8</ref> show the performance of image classifi- cation using ScatSimCLR with the linear evaluation layer. We compare the model performance not only in terms of classification accuracy but also in terms of number of trainable parameters. For the STL-10 dataset as shown on <ref type="figure" target="#fig_0">Figure 1</ref>, we not only achieve SOTA classification accuracy but also our model achieves better performance, compared to previous SOTA <ref type="bibr" target="#b40">[40]</ref> with only a half of its parameters. The same tendency is shown for CIFAR100-20 [23] dataset on <ref type="figure" target="#fig_3">Figure 8</ref>; all proposed ScatSimCLR models achieve SOTA classification accuracy: 58.0% , 59.4% and 63.8%, with 7M, 10.4M and 14M parameters respectively, while previous SOTA RUC <ref type="bibr" target="#b34">[34]</ref> achieves 54.3% with 14M trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and discussions</head><p>In this paper, we address the problem of self-supervised learning for small dataset problems. More particularly, we answer the question whether the complex encoding network used for the contrastive learning can be partially replaced by the simpler hand-crafted network ensuring geometric invariance.</p><p>We demonstrate that the proposed model based on geometrically invariant ScatNet with reduced number of trainable parameters can achieve the state-of-the-art performance on STL-10 and CIFAR100-20 datasets.</p><p>We demonstrate that introduction of pretext task regularization based on the estimation of augmentation transform improves the performance of the proposed ScatSim-CLR models as well as SimCLR with ResNet.</p><p>We demonstrate that by using a geometrically invariant ScatNet model, we are able to reduce the great portion of augmentations used to simulate the geometrical transformations at the training. Also, we confirm that the main benefit in the considered contrastive learning comes from the color and cropping augmentations. This indicates that a promising direction in further reduction of the number of augmentations is to use more efficient color coding schemes and to introduce local windowed encoding in contrast to the whole image encoding considered in the paper.</p><p>The performed extensive experiments explain the architectural and design particularities of the considered approach. The obtained results represent the state-of-the-art performance on several datasets among the networks with the same number of parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>STL-10 [1] Top-1 accuracy of self-supervised methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Contrastive learning of visual representation according to SimCLR architecture. In this work, an encoding network f ? ResNet producing a representation h is replaced by ScatNet network f ? Scat and adapter network f ? h . In the rest, the architecture remains the same as for SimCLR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>CIFAR100-20 Top-1 accuracy of self-supervised methods. Gray dots indicate other self-supervised methods. ScatSim-CLR is shown in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Impact of scale J and rotations L parameters of ScatNet on the classification accuracy after 5 epochs.</figDesc><table><row><cell>J L</cell><cell cols="2">Accuracy STL-10 Accuracy CIFAR100-20</cell></row><row><cell>1 4</cell><cell>61.90%</cell><cell>36.88%</cell></row><row><cell>1 8</cell><cell>62.75%</cell><cell>39.43%</cell></row><row><cell>1 12</cell><cell>63.00%</cell><cell>40.56%</cell></row><row><cell>1 16</cell><cell>63.70%</cell><cell>41.72%</cell></row><row><cell>2 4</cell><cell>63.12%</cell><cell>44.52%</cell></row><row><cell>2 8</cell><cell>63.71%</cell><cell>46.09%</cell></row><row><cell>2 12</cell><cell>63.34%</cell><cell>46.25%</cell></row><row><cell>2 16</cell><cell>64.03%</cell><cell>46.73%</cell></row><row><cell>3 4</cell><cell>60.10%</cell><cell>42.50%</cell></row><row><cell>3 8</cell><cell>60.80%</cell><cell>43.85%</cell></row><row><cell>3 12</cell><cell>60.90%</cell><cell>44.01%</cell></row><row><cell>3 16</cell><cell>61.20%</cell><cell>44.59%</cell></row><row><cell>4 4</cell><cell>45.12%</cell><cell>34.39%</cell></row><row><cell>4 8</cell><cell>46.58%</cell><cell>35.00%</cell></row><row><cell>4 12</cell><cell>48.10%</cell><cell>35.96%</cell></row><row><cell>4 16</cell><cell>49.91%</cell><cell>36.74%</cell></row><row><cell cols="3">6.1. Impact of ScatSimCLR parameters</cell></row></table><note>6.1.1 Impact of scaling and rotation channels</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Impact of the number of layers in the adapter network of ScatSimCLR on STL-10 dataset for 1000 epochs.</figDesc><table><row><cell cols="3">Num. of layers Num. of parameters Accuracy STL-10</cell></row><row><cell>8</cell><cell>6.1M</cell><cell>76.47%</cell></row><row><cell>12</cell><cell>7.8M</cell><cell>83.53%</cell></row><row><cell>16</cell><cell>10.4M</cell><cell>84.01%</cell></row><row><cell>30</cell><cell>14.1M</cell><cell>85.11%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Impact of the pretext task regularization on the classification accuracy on STL-10 dataset.</figDesc><table><row><cell></cell><cell cols="2">Accuracy on STL-10</cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>Without</cell><cell cols="2">With pretext</cell><cell>Num.</cell></row><row><cell>model</cell><cell>pretext</cell><cell cols="2">Rotation Jigsaw</cell><cell>of paramers</cell></row><row><cell>ScatSimCLR 8</cell><cell cols="3">74.78% 77.86% 76.36%</cell><cell>6.1 M</cell></row><row><cell>ScatSimCLR 12</cell><cell cols="3">76.57% 78.43% 77.78%</cell><cell>7.8 M</cell></row><row><cell>ScatSimCLR 16</cell><cell>77.03%</cell><cell>78.5%</cell><cell>77.91%</cell><cell>10.5 M</cell></row><row><cell>ScatSimCLR 30</cell><cell cols="2">77.86% 79.11%</cell><cell>78.4%</cell><cell>14.1 M</cell></row><row><cell cols="4">SimCLR (ResNet18) 71.90% 76.36% 75.22%</cell><cell>11.5 M</cell></row><row><cell cols="5">100 epochs. The batch size differs depending on the size of</cell></row><row><cell>the model.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>As it is shown in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research was partially funded by the Swiss National Science Foundation SNF project CRSII5 193716.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scattering transforms in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Mathieu Andreux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Angles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Exarchakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaspar</forename><surname>Leonarduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Rochette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Thiry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Zarka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>And?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belilovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">60</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">Markus</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05371</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15535" to="15545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05310</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep adaptive image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5880" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Associative deep clustering: Training a classification network with no labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Plapp</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljalbout</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cremers</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mitigating embedding and class assignment mismatch in unsupervised image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungkyu</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sundong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meeyoung</forename><surname>Cha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-supervised feature learning by learning to spot artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2733" to="2742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9865" to="9874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An iteration method for the solution of the eigenvalue problem of linear differential and integral operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lanczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of research of the National Bureau of Standards</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="255" to="282" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6874" to="6883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Curl: Contrastive unsupervised representations for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning<address><addrLine>Vienna, Austria, PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improvements to context based self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>T Nathan Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry Y</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9339" to="9348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5898" to="5906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9359" to="9367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scattering networks for hybrid representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Belilovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2208" to="2221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sundong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungkyu</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.11150</idno>
		<title level="m">Seunghoon Hong, and Meeyoung Cha. Improving unsupervised image clustering with robust learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cross-domain selfsupervised multi-task feature learning using synthetic imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="762" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multiclass n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="268" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer vision and pattern recognition</title>
		<meeting>the IEEE Conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6210" to="6219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1058" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
