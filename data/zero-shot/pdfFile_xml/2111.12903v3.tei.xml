<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Perturbed and Strict Mean Teachers for Semi-supervised Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengbei</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universit?t Ulm</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Perturbed and Strict Mean Teachers for Semi-supervised Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Consistency learning using input image, feature, or network perturbations has shown remarkable results in semisupervised semantic segmentation, but this approach can be seriously affected by inaccurate predictions of unlabelled training images. There are two consequences of these inaccurate predictions: 1) the training based on the "strict" cross-entropy (CE) loss can easily overfit prediction mistakes, leading to confirmation bias; and 2) the perturbations applied to these inaccurate predictions will use potentially erroneous predictions as training signals, degrading consistency learning. In this paper, we address the prediction accuracy problem of consistency learning methods with novel extensions of the mean-teacher (MT) model, which include a new auxiliary teacher, and the replacement of MT's mean square error (MSE) by a stricter confidenceweighted cross-entropy (Conf-CE) loss. The accurate prediction by this model allows us to use a challenging combination of network, input data and feature perturbations to improve the consistency learning generalisation, where the feature perturbations consist of a new adversarial perturbation. Results on public benchmarks show that our approach achieves remarkable improvements over the previous SOTA methods in the field. 1 Our code is available at https://github.com/yyliu01/PS-MT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is an essential pixel-wise classification task that has reached remarkable success in recent years. However, the training of such a task is known to be data-hungry, where the labelling process is particularly costly and time-consuming <ref type="bibr" target="#b33">[34]</ref>. To tackle this limitation, semi-supervised semantic segmentation has become an important research direction that has drawn a growing attention recently <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34]</ref>. This problem relies on a small set of pixel-level labelled images and a large set of unlabelled images, where both types of images are drawn from the same <ref type="figure" target="#fig_4">Figure 1</ref>. In (a), we compare our method (red star) to the current SOTA on Pascal VOC 2012 under different partition protocols based on the augmented set <ref type="bibr" target="#b15">[16]</ref>, and (b) shows a simple diagram of our consistency-based model, which includes two mean teachers (denoted by the encoders t <ref type="bibr" target="#b0">(1)</ref> enc. and t <ref type="bibr" target="#b1">(2)</ref> enc. and decoders t <ref type="bibr" target="#b0">(1)</ref> dec. and t <ref type="bibr" target="#b1">(2)</ref> dec. ) that produce accurate segmentation maps for unlabelled images x u and the student (encoder senc. and decoder s dec. ), with three types of perturbations (network, feature and input image) that are used in the minimisation of our strict Conf-CE loss. data distribution. The challenge is how to extract additional and useful training signal from the unlabelled images to allow the training of the model to generalise beyond the small labelled set.</p><p>Current state-of-the-art (SOTA) semi-supervised semantic segmentation models are based on consistency learning, which enforces the agreement between the outputs from different views of unlabelled images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44]</ref>. These different views can be obtained via perturbations applied to the input image with data augmentation <ref type="bibr" target="#b43">[44]</ref> or to the feature space with noise injection <ref type="bibr" target="#b33">[34]</ref>. Another way of obtaining different views is with network perturbation, which encourages similar predictions between multiple models trained from different initialization, and has been shown to enable better consistency regularization than input image and feature perturbations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref>. One potential weakness of consistency learning is that it assumes accurate predictions for unlabelled images, such that the perturbation does not push the image feature to the wrong side of the true (hidden) classification decision boundary. Unfortunately, in prac-tice this assumption is not always met by SOTA methods, making the training signal of consistency learning methods potentially incorrect. This problem is exacerbated for consistency learning based on network perturbation because incorrect predictions from one model will deteriorate the training for the other model, and vice versa. Another consequence of these inaccurate predictions is that consistency learning methods that rely on a "strict" cross-entropy (CE) loss can easily overfit prediction mistakes, which can lead to confirmation bias.</p><p>In this paper, we address the prediction accuracy problem of consistency based methods by extending the mean teacher (MT) model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37]</ref> with a new auxiliary teacher, and the replacement of MT's means square error (MSE) loss by a stricter confidence-weighted CE loss (Conf-CE) that has better training convergence. These accurate predictions enable the use of more challenging perturbations, combining input image, feature and network perturbations to improve the generalisation of consistency learning. Furthermore, we propose a new type of adversarial feature perturbation that learns the perturbation to be applied to the student model using virtual adversarial training <ref type="bibr" target="#b32">[33]</ref> from the teachers (T-VAT), instead of injecting different types of noise in the image features <ref type="bibr" target="#b33">[34]</ref>. To summarise, our contributions are:</p><p>? New consistency based semi-supervised semantic segmentation MT model designed to improve the segmentation accuracy of unlabelled training images with a new auxiliary teacher and a replacement of MT's MSE loss by a stricter confidence-weighted CE loss (Conf-CE) that allows stronger convergence and overall better training accuracy;</p><p>? A new challenging combination of input data, feature and network perturbations to improve model generalisation; and</p><p>? A new type of feature perturbation, called T-VAT, based on an adversarial noise learned from the both teachers of our MT model and applied to the student model, which results in the generation of challenging noise to promote an effective training of the student model.</p><p>Our experimental evaluation shows that our approach achieves the best results on Pascal VOC 2012 <ref type="bibr" target="#b11">[12]</ref>. Our approach also shows the best performance on Cityscapes <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Below, we first discuss supervised semantic segmentation, then semi-supervised learning, and then we describe pseudo-labelling and consistency-based SSL methods. Supervised semantic segmentation consists of classifying all image pixels into a closed set of visual classes. Current models are based on fully convolutional neural networks (FCN) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref> and extensions that explore: 1) multiscale aspects of the image <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29]</ref>, 2) pyramidal feature maps <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b42">43]</ref>, 3) dilated convolutions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b39">40]</ref>, and 4) attention mechanisms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref>. SOTA semi-supervised semantic segmentation models rely on the supervised semantic segmentation models DeeplabV3+ <ref type="bibr" target="#b4">[5]</ref> and PSPNet <ref type="bibr" target="#b42">[43]</ref> as backbone architectures. Semi-supervised learning (SSL) trains a model using labelled and unlabelled images. Current SSL solutions are formulated based on three assumptions <ref type="bibr" target="#b37">[38]</ref>: 1) smoothness: similar images have similar labels; 2) low-density: decision boundary does not pass through high-density areas of the feature space; and 3) manifold: samples on the same low-dimensional manifold embedded in the feature space have the same label. SSL methods can be loosely classified into pseudo-label based SSL <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b35">36]</ref> and consistency based SSL <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref>, with the former generally presenting worse accuracy than the latter. We believe that this is due to the fact that pseudo-label methods disregard part of the unlabelled training set during training, which can reduce their generalisation ability. Below, we focus on consistency based SSL given its superior accuracy on public benchmarks. Consistency-based SSL methods aims to enforce the agreement between the predictions of perturbed unlabelled images, where perturbations can be applied to the input image, to the feature representation, or to the network. The effectiveness of consistency-based SSL depends on the prediction accuracy for the unlabelled images and the perturbations to challenge the model training. In general, more challenging perturbations target an improved generalisation, but if this perturbation is applied to inaccurate predictions, it can cause the consistency-based method to learn from wrong labels. Prediction accuracy can be improved in many ways, but a simple model ensemble strategies <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37]</ref>. Perturbations can be applied to the input image <ref type="bibr" target="#b43">[44]</ref>, feature representation <ref type="bibr" target="#b33">[34]</ref> or network <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref>. Independently of how they are applied, perturbations tend to be more effective when they challenge the classification process by, for example, moving perturbed feature closer to true (but hidden) classification boundaries, such as with virtual adversarial training (VAT) <ref type="bibr" target="#b32">[33]</ref>. Consistency-based semi-supervised semantic segmentation methods have shown more competitive results <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34]</ref> than pseudo-label approaches. Among the SOTA consistency-based semi-supervised semantic segmentation methods, PseudoSeg <ref type="bibr" target="#b43">[44]</ref> relies on a new pseudo-labelling strategy and data augmentation consistency training to calibrate pseudo-labels, but its dependence on the generally inaccurate class activation maps can lead to poor training performance. Cross-consistency training (CCT) <ref type="bibr" target="#b33">[34]</ref> applies different types of feature perturbations to enforce the <ref type="figure">Figure 2</ref>. Illustration of our approach. The unlabelled image x u is weakly augmented for the mean teachers (encoders parameterised by ? t1,t2 h and decoders by ? t1,t2 g ) that jointly predict the segmentation? u . The same unlabelled image is strongly augmented for the student (with encoder ? s h and decoder ? s g ) that suffers T-VAT feature perturbation before predicting the segmentation? u . This prediction of the unlabelled image and the prediction? l of the labelled image x l (also perturbed by T-VAT) are used to minimise the consistency loss ?con(.) and the supervised loss ?sup(.), respectively, to train the student. The mean teachers are trained with EMA of the student model. agreement between their semantic segmentation results and the segmentation from the non-perturbed feature. Although the feature perturbations used in CCT are effective, more targeted and accurate adversarial noise can be more helpful for the consistency regularization. Other methods have explored network perturbation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref>, where consistency is enforced between the responses of differently initialised models. Perturbation models depend on the ability of the models to produce accurate segmentation results, and as mentioned above, such ability can be improved with the use of model ensembling <ref type="bibr" target="#b36">[37]</ref>. French et al. <ref type="bibr" target="#b13">[14]</ref> explore model ensembling <ref type="bibr" target="#b36">[37]</ref> together with network perturbation and input image perturbation <ref type="bibr" target="#b41">[42]</ref>. This is one of the closest methods to our proposal, but we add a more effective model ensembling with multiple mean teachers, and a new adversarial feature perturbation with VAT <ref type="bibr" target="#b32">[33]</ref> and challenging input image perturbation with CutMix <ref type="bibr" target="#b41">[42]</ref> and Zoom In/Out <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28]</ref> data augmentation. Also comparing with <ref type="bibr" target="#b13">[14]</ref>, the more accurate segmentation results produced by our multiple mean teachers allows us to train the model for unlabelled images with the CE loss instead of the MSE used in <ref type="bibr" target="#b13">[14]</ref>, providing better training convergence and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Before we describe our model and training process, we introduce our dataset for semi-supervised semantic segmentation. We have a small labelled training set</p><formula xml:id="formula_0">D L = {(x i , y i )} |D L | i=1</formula><p>, where x i ? X ? R H?W ?C is the input image of size H ? W with C colour channels, and y i ? Y ? {0, 1} H?W ?Y is the segmentation map, with the number of visual classes denoted by Y . We also have a large unlabelled training set</p><formula xml:id="formula_1">D U = {x i } |D U | i=1 is provided, with |D L | &lt;&lt; |D U |.</formula><p>These datasets are used to train our proposed MT model with an auxiliary teacher, described in Sec. 3.1. The training of our new MT model, exploring network, feature and input image perturbations, and a strict Conf-CE loss is described in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multiple Mean Teachers and Student Models</head><p>As explained in Sec. 1, we aim to improve the accuracy of the segmentation of unlabelled training images. To achieve that, we propose the inclusion of an auxiliary teacher, exploring the idea of a double ensembling process to improve segmentation accuracy, namely the ensemble of teacher models, each representing a temporal ensemble of the student model <ref type="bibr" target="#b36">[37]</ref>. The teachers and student models have the same network structure denoted by f ? : X ? R H?W ?Y , where ? is the model parameter. This model is decomposed into an encoder h ? h : X ? Z and a decoder g ?g :</p><formula xml:id="formula_2">Z ? Y, where Z ? R Z represents the feature space of dimension Z. Hence, f ? = g ?g ? h ? h , where ? = {? g , ? h }.</formula><p>The probability output of the network is achieved by applying the pixel-wise softmax function ?(.) over the Y classes, as in p ? (x) = ?(f ? (x)). The multiple mean teacher-student model is represented with the respective parameter superscripts: ? t1 = {? t1 g , ? t1 h } and ? t2 = {? t2 g , ? t2 h } for the teachers, and ? s = {? s g , ? s h } for the student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training with Multiple Perturbations and a Strict Confidence-weighted CE Loss</head><p>In this section, we present the training process of our new MT model using a confidence-weighted CE loss, which is optimised with perturbations to the network, feature representations and input images.</p><p>Training. The full training loss for the student model is</p><formula xml:id="formula_3">?(D L , D U , ? s ) = ? sup (D L , ? s ) + ?? con (D U , ? s ), (1)</formula><p>where the first loss is the supervised segmentation loss, defined as:</p><formula xml:id="formula_4">? sup (D L , ? s ) = 1 |DL||?| (x,y)?DL ??? ?(y(?), p ? s (x)(?)),<label>(2)</label></formula><p>where ? is the image lattice of size H ?W , and ?(.) denotes the CE loss between the annotation y and segmentation prediction from p ? s (x) at pixel address ? ? ?. The second term in <ref type="formula">(1)</ref> is the consistency loss, denoted by the confidence weighted CE loss (Conf-CE), with ? ? R weighting its contribution and its definition being as follows:</p><formula xml:id="formula_5">? con (D U , ? s ) = 1 |DU ||?| x?DU ??? c(?)?(?(?), p ? s (x)(?)),<label>(3)</label></formula><p>where ?(.) represents the CE loss, ? denotes the pixel address of the output lattice ? of the segmentation map,</p><formula xml:id="formula_6">y(?) ? {0, 1} Y is the segmentation prediction from the teacher models at ?, p ? s (x)(?) ? [0, 1] Y is the student model segmentation prediction at ?, and c(?) ? [0, 1]</formula><p>represents the segmentation prediction confidence from the teacher models at ?, defined below in <ref type="bibr" target="#b3">(4)</ref>.</p><p>The network perturbation is obtained from the predictions of the mean teacher models and the student model. The soft segmentation map produced by the ensemble of the mean teachers is estimated as:</p><formula xml:id="formula_7">y = ?(0.5 ? (f ? t1 (x) + f ? t2 (x))),<label>(4)</label></formula><p>where ?(.) denotes the softmax function. The hard segmentation prediction by the ensemble of teachers, denoted by? ? Y in <ref type="formula" target="#formula_5">(3)</ref>, is obtained from the one-hot representation computed from? ? [0, 1] H?W ?Y in (4). The segmentation prediction confidence c(?) in <ref type="formula" target="#formula_5">(3)</ref> is computed</p><formula xml:id="formula_8">with c(?) =?(?) ?? (?) ? I(?(?) ?? (?) &gt; ? ), where I(.)</formula><p>denotes an indicator function and ? is a minimum confidence to enable a value larger than zero for c(?). Following the MT framework <ref type="bibr" target="#b36">[37]</ref>, while the student model is trained via stochastic gradient descent (SGD) to minimise the cost function in <ref type="formula" target="#formula_5">(3)</ref>, both teacher models are trained with exponential moving average (EMA) <ref type="bibr" target="#b36">[37]</ref> of the student model and batch norm (BN) parameters <ref type="bibr" target="#b3">[4]</ref>, with:</p><formula xml:id="formula_9">? k = ? ? ? k + (1 ? ?) ? ? s ,<label>(5)</label></formula><p>where k ? {t 1 , t 2 }, and ? ? (0, 1) controls the transfer weight between epochs. For the training of teacher models, we update the parameters of only one of the two teachers at each training epoch.</p><p>The feature perturbation consists of a challenging adversarial feature perturbation that is designed to violate the cluster, or low-density, assumption <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b37">38]</ref> by pushing the image features, computed from the model encoder, toward the classification boundaries in the feature space. One effective way to produce such adversarial feature noise is with virtual adversarial training (VAT) <ref type="bibr" target="#b32">[33]</ref> that optimises a perturbation vector to maximise the divergence between correct and adversarial classification. Current methods estimate the adversarial noise using the same single network where the consistency loss will be applied <ref type="bibr" target="#b33">[34]</ref>. However, the use of VAT to perturb the training of MT in semisupervised semantic segmentation is new, to the best of our knowledge. In an MT model, the feature perturbation can be applied to the student model, but given that it has less accurate predictions than the teacher model, this approach may not be conducive to effective training. Hence we propose to estimate the adversarial noise using the more accurate teachers, and then apply this estimated noise to the feature of the student model -we call this feature perturbation T-VAT. The student output to be used in the loss in <ref type="formula" target="#formula_5">(3)</ref> </p><formula xml:id="formula_10">is p ? s (x) = ?(g ? s g (h ? s h (x) + r adv )),</formula><p>where the adversarial feature perturbation r adv ? Z is estimated from the response of the ensemble of teacher models with:</p><formula xml:id="formula_11">maximise d ? 0.5 ? (g ? t1 g (z s ) + g ? t2 g (z s )) , ? 0.5 ? (g ? t1 g (z s + r adv ) + g ? t2 g (z s + r adv ) , subject to ||r adv || 2 &lt;= ?,<label>(6)</label></formula><p>where z s = f ? s (x), d(.) is the the sum of the pixel-wise Kullback-Leibler (KL) divergence between the original and perturbed pixel predictions.</p><p>The input image perturbation is based on the weakstrong augmentation pairs <ref type="bibr" target="#b23">[24]</ref>, where weak augmentations (image flipping, cropping and scaling) are applied to the images to be processed by the teacher models, and in addition to those weak augmentation, strong augmentations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref> (e.g., colour jitter, randomise grayscale and blur) are applied to the images fed to the student model to improve the overall generalization capability.</p><p>On top of the strong augmentations, we also apply the CutMix <ref type="bibr" target="#b41">[42]</ref> and Zoom In/Out <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28]</ref> data augmentations to the student model images. As defined in <ref type="bibr" target="#b13">[14]</ref>, the Cut-Mix augmentation is achieved by applying a binary mask m ? {0, 1} H?W that combines two images using the function ?(x i , x j , m) = (1 ? m) ? x i + m ? x j . We can apply CutMix by combining two input images and minimise the consistency loss (3) with the prediction from (4) <ref type="bibr" target="#b8">[9]</ref> (referred to as CutMix before prediction), or we can minimise the consistency loss using the CutMix combination of the images and their predictions, as in</p><formula xml:id="formula_12">?(?(? i ,? j , m), p ? s (?(x i , x j , m))),<label>(7)</label></formula><p>with? defined in <ref type="bibr" target="#b3">(4)</ref>. The perturbation used in <ref type="formula" target="#formula_12">(7)</ref> is referred to as CutMix after prediction, which we argue to produce a cleaner prediction for the consistency loss than CutMix before prediction because its prediction does not contain the artifacts introduced by the prediction from the CutMix images. The Zoom In/Out augmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28]</ref> is defined by the function ?(x, s) that zooms in or out the image using the parameter s ? R + . The input image consistency loss in (3) for the zoom in/out augmentation for the ensemble results of teacher models is defined by</p><formula xml:id="formula_13">?(?(?, s), p ? s (?(x, s))),<label>(8)</label></formula><p>and? is defined in (4). The segmentation loss ?(.) in <ref type="formula" target="#formula_5">(3)</ref> for previous consistency-based semi-supervised semantic segmentation models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref> is usually based on the L2 loss. Even though L2 loss is known to be robust, which is advantageous when dealing with the noisy predictions produced by consistencybased methods, it is also known to have poor converge and to possibly lead to vanishing gradients. Given the reliability of the segmentation predictions produced by our extended MT model, we instead use the more effective cross entropy (CE) loss, constrained to be computed at regions of high-confidence segmentation results, represented by c(?) in (3), following the strategy applied in self-training approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Inference. The semantic segmentation of a test image is obtained from the teachers, as described in (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We firstly introduce the experimental setting that we used to evaluate our approach. In Sec. 4.2 we demonstrate our approach for both datasets under different partition protocols by comparing them with the supervised baselines and other previous SOTA approaches. Moreover, we also carry out detailed results based on the few supervision studies in Sec. 4.3. Lastly, we perform the ablation study in Sec. 4.4 and demonstrate an extension experiment based on the exploring of the image-level data in Sec. 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets. Pascal VOC 2012 <ref type="bibr" target="#b11">[12]</ref> is the standard dataset used for evaluating the performance of the semi-supervised segmentation models. The dataset contains more than 13, 000 images with 21 classes, providing 1, 464 images with pixel-level labels for training, 1, 449 images for validation and 1, 456 for testing. Following previous papers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34]</ref>, we adopt the additional labels from <ref type="bibr" target="#b15">[16]</ref> and our entire training set contains 10, 582 images. Note that the labels from <ref type="bibr" target="#b15">[16]</ref> are of low-quality, and may contain noise. Cityscapes <ref type="bibr" target="#b9">[10]</ref> is the urban driving scene dataset, which consists of 2, 975 images for training, 500 for validation and 1, 525 for testing. Each image in the dataset has resolution 2, 048 ? 1, 024, and there are 19 classes in total.</p><p>Following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref>, we random split the full labelled set in Pascal VOC 2012 and Cityscapes with different ratios. We also provide the results based on the official labelled set (with 1, 464 images for Pascal VOC 2012) <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b43">44]</ref>. Implementation details. Most results are based on using our method to train the DeeplabV3+ <ref type="bibr" target="#b4">[5]</ref> model. We load the ImageNet pre-trained checkpoint, and the segmentation heads are initialized randomly. Following previous papers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34]</ref>, we utilise the following polynomial learning-rate decay: (1 ? iter max iter ) 0.9 . We also test our method on PSPNet <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34]</ref> to show the generalization of our approach.</p><p>During training, we apply data augmentation with random scaling in {0.5, 0.75, 1.25} and random flipping of both labelled and unlabelled images. On Pascal VOC 2012, we crop images to 512 ? 512 pixels for DeeplabV3+, train for 80 epochs with base learning rate set to 0.01, using batch size 32, for both labelled and unlabelled images, following <ref type="bibr" target="#b8">[9]</ref>. For PSPNet, we follow <ref type="bibr" target="#b17">[18]</ref> and crop images to 321 ? 321 pixels and use batch size 8. On Cityscapes, due to hardware limitation, we crop images to 712 ? 712 pixels, train for 450 epochs with base learning rate set to 0.0038 with batch size 8 for both architectures. Because the teacher's predictions are unstable at the early stage of the training, we apply the Gaussian ramp-up to the consistency loss weight ? in <ref type="bibr" target="#b0">(1)</ref>. For both datasets, the supervised loss is the cross-entropy loss.</p><p>Evaluation metrics. Following previous papers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref>, we report the mean Intersection-over-Union (mIoU) for validation set for both datasets. All the results are based on the single scale inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on Different Partition Protocols</head><p>In this section, following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref>, we evaluate our method based on sub-sampling the datasets with ratio 1/n for the labelled set and (1 ? 1/n) unlabelled set. Specifically, in the Pascal VOC 2012 dataset, we split the entire training set (with 10, 582 images) with the ratios of 1/16, 1/4, 1/8, 1/2 for be labelled set. In the Cityscapes, we similarly utilise the ratios 1/8, 1/4, 1/2 for the labelled set. All the partition protocols are from <ref type="bibr" target="#b8">[9]</ref>.</p><p>Improvements over Supervised Baselines. We first compare our results with fully supervised learning (trained with the same ratio of labelled set) using DeepLabV3+ architecture, and illustrate the improvements in <ref type="figure" target="#fig_1">Fig. 3</ref>. This figure demonstrates that our approach successfully exploits unlabelled data, with a dramatic performance boost. On Pascal VOC 2012, <ref type="figure" target="#fig_1">Fig. 3-(a)</ref> shows that our approach outperforms the supervised baseline by a large gap, especially for small labelled partitions. Specifically, in the 1/16 ratio (with 662 labelled images), our approach surpasses the fully supervised baseline with 6.01% and 5.97% for the ResNet50 and ResNet101, respectively. On the other set- <ref type="table">Table 1</ref>. Comparison with SOTA on Pascal VOC 2012. All approaches are based on the DeeplabV3+ architecture, under different partition protocols <ref type="bibr" target="#b8">[9]</ref>. The * indicates the approaches re-implemented by <ref type="bibr" target="#b8">[9]</ref>. Best results are in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>ResNet-50 tings, our approach also shows consistent improvements between 2% and 5% for ratios 1/8, 1/4, and 1/2. On Cityscapes, we use the sliding evaluation to evaluate our final results following <ref type="bibr" target="#b8">[9]</ref>. <ref type="figure" target="#fig_1">Fig. 3-(b)</ref> shows that our approach surpasses the supervised baseline by 2% and 6% for ResNet50 and ResNet101 for all protocols. Comparison to SOTA. For Pascal VOC 2012, Tab. 1 shows that our approach is the best for all partition protocols, using DeepLabV3+ and ResNet50 and ResNet101 backbones. Comparing to French et al. <ref type="bibr" target="#b13">[14]</ref>, which is considered a strong baseline, our approach improves by 3% to 6% in all cases. Our approach also provides a significant boost for the original MT in all cases. The results also show that our approach is better than the current SOTA CPS <ref type="bibr" target="#b8">[9]</ref> by around 1% to 2% for all cases. In some partition protocols, our approach is better than the CPS <ref type="bibr" target="#b8">[9]</ref> with fewer labelled samples. For example, our approach trained with 1, 323 labelled images outperforms CPS <ref type="bibr" target="#b8">[9]</ref> trained with 2, 646 labelled images using both backbones. This demonstrates that our perturbed and strict mean teachers yield more accurate results than any other method in the field. On Cityscapes, we use the settings from <ref type="bibr" target="#b21">[22]</ref> and show results that use similar settings (in terms of image resolution, batch size, and supervised loss function) for fair comparison. Our approach outperforms CAC <ref type="bibr" target="#b21">[22]</ref> by nearly 4.6% and 2.4% for the 1/8 and 1/4 partition protocols. The sliding evalu-  ation also boosts our performance by approximately 1.30% for all the ratios. This shows that the the sliding process improves the performance of our approach in large resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Official Labelled Set of Pascal VOC 2012</head><p>In this section, we report the results based on the official 1, 464 high quality labelled data of the Pascal VOC 2012. We compare our approach for both PSPNet and DeepLabV3+ architectures. <ref type="table" target="#tab_1">Table 3</ref> shows that our approach outperforms the SOTA methods for each architecture and backbone settings. For PSPNet, our approach outperforms DARS <ref type="bibr" target="#b17">[18]</ref> by 0.7% mIoU and CCT <ref type="bibr" target="#b33">[34]</ref> by 5.19%. In the experiments, our approach outperforms other SOTA approaches by a large gap. Yuan et. al <ref type="bibr" target="#b40">[41]</ref> only utilize the single network to produce the pseudo labels in a self-training manner. For instance, our approach outperforms Yuan et. al <ref type="bibr" target="#b40">[41]</ref> by 5.01% mIoU, which shows the value of our teachers and student network and several perturbation strategies, compared with their self-training single network approach. Few-supervision study. We subsample with partitions 1/2, 1/4, 1/8 and 1/16 using the official 1, 464 labelled images. The remaining data are combined with the augmented set <ref type="bibr" target="#b15">[16]</ref> (around 9K images) to be the unlabelled data in the experiments. <ref type="table">Table 4</ref> shows that our approach yields the best mIoU results for all cases. For example, our approach outperforms CPS [9] by 4.86% for 366 labelled images. We argue that our effective perturbations allowed the better generalisation of our model under this limited labelled data conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In this section, we study the roles of the confidence weighted CE loss (conf-CE), T-VAT perturbation and auxiliary teacher (AT) of our approach. All the experiments are run on Pascal VOC 2012 under 1/8 ratio, and we use DeeplabV3+ to evaluate our results. <ref type="table" target="#tab_2">Table 5</ref> demonstrates the improvements of each component mentioned above,  <ref type="figure">Figure 4</ref>. T-VAT and CutMix Perturbation Effectiveness. (a) mIoU under different feature perturbations. (b) mIoU of validation set during training for CutMix applied before or after prediction, as described in <ref type="formula" target="#formula_12">(7)</ref>.</p><p>where we use MT <ref type="bibr" target="#b36">[37]</ref> trained with the input image perturbations from Sec. 3.2 and MSE loss as baseline. We note that by replacing MSE by our conf-CE increases mIoU by 2.30% and 2.81% for the ResNet50 and ResNet101. T-VAT perturbation yields nearly 1% improvements, showing the effectiveness of our proposed feature perturbation. The more accurate predictions by the auxiliary teacher allows a further improvement of 0.83% and 0.84% for the two backbones.</p><p>T-VAT perturbation. <ref type="figure">Fig. 4-(a)</ref> shows the performance under different types of feature perturbations, namely: original (no feature perturbation), uniform (feature noise randomly sampled from uniform distribution), vat (VAT noise learned from the student model), t-vat (T-VAT noise learned from the teacher model as in <ref type="formula" target="#formula_11">(6)</ref>). Our proposed T-VAT outperforms uniform and VAT perturbations by 0.93% and 0.62%, respectively. Additionally, it also surpasses original by 1.10%.</p><p>Empirical results of the CutMix before or after prediction, as described in <ref type="bibr" target="#b6">(7)</ref>. In <ref type="figure">Fig 4-(b)</ref>, we show the mIoU results on the validation set during training epochs. Applying CutMix before predictions may introduce extra semantic complexity and yield inaccurate pseudo-labels, leading to ineffective optimisation. In contrast, the result indicates that applying the CutMix after the prediction improves mIoU by around 3%.</p><p>Average gradient magnitudes difference between MSE and Conf-CE. <ref type="figure" target="#fig_2">Fig. 5</ref> shows the average gradient magnitudes per layer of the student model after being trained with MSE and Conf-CE losses to optimise consistency in (3), at the last stages of training (70 th out of 80  epochs). Note that our Conf-CE shows larger gradient magnitudes than MSE <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref>, suggesting that it can lead to stronger convergence than MSE. <ref type="figure" target="#fig_5">Figure 6</ref> shows the supervised, student and mean teachers results on Pascal VOC 2012 images. The supervised results display the worst accuracy in column (c), caused by the limited labelled training samples. Our final results in column (e) significantly improves the baseline performance, which demonstrates the effectiveness of our approach. Moreover, our final results in column(e) are also more accurate than the student results in column (d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Combining Pseudo-label and Consistencybased losses</head><p>Current consistency-based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref> also include a pseudo-labelling loss, involving the use of class activation maps (CAM) from the model to generate pseudolabels?. We follow a similar strategy as in CCT <ref type="bibr" target="#b33">[34]</ref> and add the CAM loss below to the cost function in <ref type="bibr" target="#b0">(1)</ref> to train the student model:  </p><p>where c(?) is the segmentation confidence from the teachers defined in (3), and ?(.) is the CE loss. Hence, the non- confident predictions from the teachers are then handled by this CAM loss in <ref type="bibr" target="#b8">(9)</ref>.</p><p>We run experiments based on the official 1, 464 labelled images on Pascal VOC 2012 with the additional ? 9K images used to minimise the CAM loss in <ref type="bibr" target="#b8">(9)</ref>. Results on Tab. 6 show that our method outperforms all previous works that use a similar strategy <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44]</ref>. Moreover, the addition of this CAM loss in (9) to our cost function boosts the performance by 0.63% and 1.18% for the two backbones under DeeplabV3+ architecture, compared with our previous results in Tab. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a new consistency-based semi-supervised semantic segmentation method. Among our contributions, we introduced a new MT model, based on multiple mean teachers and a student network, which shows more accurate predictions for unlabelled images that facilitate consistency learning, allowing us to use a stricter confidence-based CE loss than the original MT's MSE loss. This more accurate predictions also allowed us to use a challenging combination of network, feature and input image perturbations that showed better generalisation. Furthermore, we proposed a new adversarial feature perturbation, called T-VAT, that further improved the generalisation of our approach. Our method outperforms previous methods on Pascal VOC 2012 and Cityscapes, becoming the new SOTA for the semi-supervised semantic segmentation field. Regarding the limitations of our model, it can be argued that the strict Conf-CE loss has the potential to overfit the remaining prediction mistakes, so we will focus on improving the robustness of the Conf-CE loss. Another limitation that we plan to address is to work on an approach that can handle high-resolution images without using the time-consuming sliding evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Improvements over the supervised baseline. mIoU vs. partition protocols results of our approach and the supervised baseline on Pascal VOC 2012 (top) and Cityscapes (bottom) using ResNet50 and ResNet101 backbones and DeepLabV3+.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Different gradient magnitudes. Average gradient magnitudes per layer of the student model after being trained with MSE and Conf-CE losses, at the last stages of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?</head><label></label><figDesc>cam (D U , ? s ) = 1 |D U ||?| x?D U ???</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 1 ?</head><label>1</label><figDesc>c(?))?(?(?), f ? s (x)(?)),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results from Pascal VOC 2012. (a) input images, (b) ground truth, (c) supervised baseline results, (d) student results and (e) results by our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Comparison using the official (1, 464) labelled images on the Pascal VOC 2012 under different network settings. Best results are in bold.</figDesc><table><row><cell>Method</cell><cell cols="3">Architecture Backbone</cell><cell>mIoU</cell></row><row><cell>CCT [34]</cell><cell>PSPNet</cell><cell cols="2">ResNet50</cell><cell>69.40</cell></row><row><cell>DARS [18]</cell><cell>PSPNet</cell><cell cols="2">ResNet50</cell><cell>73.89</cell></row><row><cell>ours</cell><cell>PSPNet</cell><cell cols="2">ResNet50</cell><cell>74.59</cell></row><row><cell>CAC [22]</cell><cell cols="3">DeeplabV3+ ResNet50</cell><cell>74.50</cell></row><row><cell>ours</cell><cell cols="3">DeeplabV3+ ResNet50</cell><cell>78.08</cell></row><row><cell cols="5">PseudoSeg [44] DeeplabV3+ ResNet101 73.20</cell></row><row><cell cols="5">Yuan et al. [41] DeeplabV3+ ResNet101 75.00</cell></row><row><cell>ours</cell><cell cols="4">DeeplabV3+ ResNet101 80.01</cell></row><row><cell cols="5">Table 4. Comparison with SOTA approaches with few-</cell></row><row><cell cols="5">supervision using the official (1, 464) labelled images on the</cell></row><row><cell cols="5">Pascal VOC 2012. Our approach follows the same protocols as</cell></row><row><cell cols="4">CPS [9] and PseudoSeg [44]. Best results are in bold.</cell><cell></cell></row><row><cell>Method</cell><cell>732</cell><cell>366</cell><cell>183</cell><cell>92</cell></row><row><cell cols="5">AdvSemSeg [20] 65.27 59.97 47.58 39.69</cell></row><row><cell>CCT [34]</cell><cell cols="4">62.10 58.80 47.60 33.10</cell></row><row><cell>MT [37]</cell><cell cols="4">69.16 63.01 55.81 48.70</cell></row><row><cell>GCT [21]</cell><cell cols="4">70.67 64.71 54.98 46.04</cell></row><row><cell>VAT</cell><cell cols="4">63.34 56.88 49.35 36.92</cell></row><row><cell cols="5">French et al. [14] 69.84 68.36 63.20 55.58</cell></row><row><cell>PseudoSeg [44]</cell><cell cols="4">72.41 69.14 65.50 57.60</cell></row><row><cell>CPS [9]</cell><cell cols="4">75.88 71.71 67.42 64.07</cell></row><row><cell>ours</cell><cell cols="4">78.42 76.57 69.58 65.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Ablation study using the 1/8 labelled ratio on Pascal VOC 2012 under DeeplabV3+ architecture.</figDesc><table><row><cell cols="5">MT conf-CE T-VAT AT</cell><cell cols="2">Backbone ResNet-50 ResNet-101</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>71.49</cell><cell>73.50</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>73.79</cell><cell>76.39</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>74.87</cell><cell>77.36</cell></row><row><cell>? (mIoU) 0.78 0.79</cell><cell>? original uniform vat t-vat</cell><cell>?</cell><cell>?</cell><cell cols="3">75.70 (mIoU) 0.78 0.77 0.75 0.76 CutMix after Pred. 78.20 CutMix before Pred.</cell></row><row><cell>0.77</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.74</cell></row><row><cell>0.76</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.73</cell><cell>1/4</cell><cell>2/4</cell><cell>3/4</cell><cell>(epochs) 4/4</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Comparison using the official (1, 464) labelled images with combined pseudo-label and consistency-based losses on the Pascal VOC 2012 under different network settings. Best results per architecture are in bold.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">Architecture Backbone</cell><cell>mIoU</cell></row><row><cell>CCT [34]</cell><cell></cell><cell>PSPNet</cell><cell>ResNet50</cell><cell>73.2</cell></row><row><cell>ours</cell><cell></cell><cell>PSPNet</cell><cell>ResNet50</cell><cell>75.74</cell></row><row><cell cols="2">AdvCAM [26]</cell><cell>Deeplabv2</cell><cell cols="2">ResNet101 77.8</cell></row><row><cell cols="4">PseudoSeg [44] DeeplabV3+ ResNet50</cell><cell>73.8</cell></row><row><cell cols="2">CAC [22]</cell><cell cols="2">DeeplabV3+ ResNet50</cell><cell>76.1</cell></row><row><cell>ours</cell><cell></cell><cell cols="2">DeeplabV3+ ResNet50</cell><cell>78.71</cell></row><row><cell>ours</cell><cell></cell><cell cols="3">DeeplabV3+ ResNet101 81.19</cell></row><row><cell>0.03 0.04</cell><cell>MSE Conf-CE</cell><cell></cell><cell></cell></row><row><cell>0.02</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.01</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.00</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pseudo-labeling and confirmation bias in deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exponential moving average normalization for self-supervised and semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="194" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with cross pseudo supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2613" to="2622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3992" to="4000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation via dynamic self-training and classbalanced curriculum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08514</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation needs strong, high-dimensional perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Finlayson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="519" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive pyramid context network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongying</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7519" to="7528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Re-distributing biased pseudo labels for semi-supervised semantic segmentation: A baseline investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.11279</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7014" to="7023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adversarial learning for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Ting</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07934</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Guided collaborative training for pixel-wise semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaican</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson Wh</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="429" to="445" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIII 16</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with directional context-aware consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuotao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1205" to="1214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">896</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ficklenet: Weakly and semi-supervised semantic image segmentation using stochastic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5267" to="5276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Antiadversarially manipulated attributions for weakly and semisupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4071" to="4080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Expectation-maximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9167" to="9176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-scale context intertwining for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-supervised segmentation based on error-correcting supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis Antonio De</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Jo?o Paulo Papa, and Christoph Palm</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image segmentation using deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><forename type="middle">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasser</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demetri</forename><surname>Kehtarnavaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semisupervised semantic segmentation with cross-consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myriam</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="12674" to="12684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli B Juditsky</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01780</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A survey on semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jesper E Van Engelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">St++: Make self-training work better for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A simple baseline for semi-supervised semantic segmentation with strong data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07256</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pseudoseg: Designing pseudo labels for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09713</idno>
	</analytic>
	<monogr>
		<title level="m">Jia-Bin Huang, and Tomas Pfister</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
