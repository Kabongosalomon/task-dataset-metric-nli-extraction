<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Blind Video Super-resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songsheng</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Blind Video Super-resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>b) HR patch (c) Bicubic (d) RCAN [43] (e) DUF [14]</p><p>(a) Ground truth high-resolution (HR) image (f) TOF <ref type="bibr" target="#b38">[39]</ref> (g) RBPN [12]   (h) w/o kernel modeling (i) Ours <ref type="figure">Figure 1</ref>. Video super-resolution result (?4). Existing video super-resolution algorithms usually assume the blur kernel in the degradation is known and do not model the blur kernel in the restoration process. We show that without modeling the blur kernel does not effectively capture the intrinsic characteristics of the video super-resolution problem which thus leads to over-smoothed results (see (c)-(h)). Our algorithm jointly estimates blur kernels, motion fields, and latent frames, which is able to generate clearer results and better recovers structural details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Existing video super-resolution (SR) algorithms usually assume that the blur kernels in the degradation process are known and do not model the blur kernels in the restoration. However, this assumption does not hold for video SR and usually leads to over-smoothed super-resolved images. In this paper, we propose a deep convolutional neural network (CNN) model to solve video SR by a blur kernel modeling approach. The proposed deep CNN model consists of motion blur estimation, motion estimation, and latent image restoration modules. The motion blur estimation module is used to provide reliable blur kernels. With the estimated blur kernel, we develop an image deconvolution method based on the image formation model of video SR to generate intermediate latent images so that some sharp image contents can be restored well. However, the generated intermediate latent images may contain artifacts. To generate high-quality images, we use the motion estimation module to explore the information from adjacent frames, where the motion estimation can constrain the deep CNN model for better image restoration. We show that the proposed algorithm is able to generate clearer images with finer structural details. Extensive experimental results show that the pro-posed algorithm performs favorably against state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(b) HR patch (c) Bicubic (d) RCAN <ref type="bibr" target="#b42">[43]</ref> (e) DUF <ref type="bibr" target="#b13">[14]</ref> (a) Ground truth high-resolution (HR) image (f) TOF <ref type="bibr" target="#b38">[39]</ref> (g) RBPN <ref type="bibr" target="#b11">[12]</ref> (h) w/o kernel modeling (i) Ours <ref type="figure">Figure 1</ref>. Video super-resolution result (?4). Existing video super-resolution algorithms usually assume the blur kernel in the degradation is known and do not model the blur kernel in the restoration process. We show that without modeling the blur kernel does not effectively capture the intrinsic characteristics of the video super-resolution problem which thus leads to over-smoothed results (see (c)-(h)). Our algorithm jointly estimates blur kernels, motion fields, and latent frames, which is able to generate clearer results and better recovers structural details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Existing video super-resolution (SR) algorithms usually assume that the blur kernels in the degradation process are known and do not model the blur kernels in the restoration. However, this assumption does not hold for video SR and usually leads to over-smoothed super-resolved images. In this paper, we propose a deep convolutional neural network (CNN) model to solve video SR by a blur kernel modeling approach. The proposed deep CNN model consists of motion blur estimation, motion estimation, and latent image restoration modules. The motion blur estimation module is used to provide reliable blur kernels. With the estimated blur kernel, we develop an image deconvolution method based on the image formation model of video SR to generate intermediate latent images so that some sharp image contents can be restored well. However, the generated intermediate latent images may contain artifacts. To generate high-quality images, we use the motion estimation module to explore the information from adjacent frames, where the motion estimation can constrain the deep CNN model for better image restoration. We show that the proposed algorithm is able to generate clearer images with finer structural details. Extensive experimental results show that the pro-posed algorithm performs favorably against state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video super-resolution (SR) aims to estimate highresolution (HR) frames from a low-resolution (LR) sequence. It is a fundamental problem in the vision and graphics communities and has received active research efforts within the last decade as high-definition devices have been widely used in our daily lives. As the HR sequences are usually contaminated by unknown blur, it is quite challenging to restore HR images from low-resolution sequences.</p><p>Since video SR is an ill-posed problem, conventional methods usually estimate underlying motion and latent images simultaneously in a variational approach <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref>. To improve the performance, kinds of hand-crafted priors on the latent images and motion fields have been widely used in these methods. In spite of achieving decent results, these algorithms usually need to complex energy functions or complex matching processes and the performance is limited by the hand-crafted priors. In addition, most of these algorithms usually use known blur kernels (e.g., Gaussian blur kernel, Bicubic kernel) and do not model blur kernels in the restoration, which cannot effectively capture the intrinsic characteristics of video SR <ref type="bibr" target="#b20">[21]</ref>.</p><p>Motivated by the first end-to-end trainable network for single image SR <ref type="bibr" target="#b5">[6]</ref>, lots of methods based on deep convolutional neural networks (CNNs) have been proposed <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18]</ref>. These algorithms achieve decent results in single image SR. However, directly using these algorithms cannot solve the video SR problem well. To overcome this problem, most existing algorithms focus on developing effective motion fields and alignment estimation methods. For example, the subpixel motion compensation based on optical flow <ref type="bibr" target="#b34">[35]</ref>, deformable alignment networks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38]</ref>, and spatial alignment networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b38">39]</ref>. To better restore latent images, the recurrent approaches and Generative Adversarial Networks (GANs) have been developed <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref>. These methods significantly promote the progress of video SR. However, they usually assume the blur kernel is known (e.g., Bicubic kernel). Therefore, without modeling the blur kernel usually leads to over-smoothed results ( <ref type="figure">Figure 1</ref>).</p><p>To overcome this problem, several algorithms explicitly estimate blur kernels for SR <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. These algorithms show that using the estimated blur kernels for image SR is able to improve the results significantly <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b32">33]</ref>. However, these algorithms are mainly developed for single image SR which cannot be extended to video SR directly. The methods by <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref> simultaneously estimate underlying motion and blur kernels for image restoration. However, the performance is limited by the hand-crafted image priors.</p><p>To overcome the above problems, we propose an effective video SR algorithm that simultaneously estimates underlying motion blur, motion field, and latent image by deep CNN models so that our method can not only avoid the hand-crafted priors and but also effectively estimate blur kernels and motion fields for better image restoration. The proposed algorithm mainly consists of motion blur estimation, motion field estimation, and latent image restoration modules. The motion blur estimation generates blur kernels based on the image formation of video SR and is able to provide intermediate latent images with sharp contents. The motion field estimation is used to explore the spatiotemporal information from adjacent frames so that it can guide the deep CNN model for better image restoration. By training the proposed algorithm in an end-to-end manner, it is able to generate clearer images with finer structural details ( <ref type="figure">Figure 1</ref>).</p><p>The main contributions are summarized as follows:</p><p>? We propose an effective video SR algorithm that simultaneously estimates blur kernels, motion fileds, and latent images by deep CNN models.</p><p>? We develop an effective kernel estimation method and image deconvolution algorithm based on the image formation of video SR. To restore high-quality images, we explore the spatio-temporal information from the adjacent frames so that it can guide the deep CNN model for better image restoration.</p><p>? We both quantitatively and qualitatively evaluate the proposed algorithm on benchmark datasets and realworld videos and show that it performs favorably against state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We briefly discuss methods most relevant to this work and put this work in proper context. Variational approach. Since video SR is highly ill-posed, early approaches mainly focus on developing effective priors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30]</ref> on the HR images to solve this problem. As these methods usually use known blur kernels to approximate the real ones which will lead to over-smoothed results. Several methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref> simultaneously estimate motion fileds, blur kernels, and latent images in a Maximum a posteriori (MAP) framework. In <ref type="bibr" target="#b20">[21]</ref>, Liu and Sun solve video SR by a Bayesian framework, where the motion fileds, blur kernels, latent images, and noise levels are estimated simultaneously. Ma et al. <ref type="bibr" target="#b23">[24]</ref> propose an effective Expectation Maximization (EM) framework to jointly solve video SR and blur estimation. Although promising results have been achieved, these algorithms require solving complex optimization problems. In addition, the performance is limited by the hand-crafted priors. Deep learning approach. Motivated by the success of deep learning-based single image SR <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18]</ref>, several methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b35">36]</ref> explore the spatio-temporal information for video SR. Huang et al. <ref type="bibr" target="#b12">[13]</ref> develop an effective bidirectional recurrent convolutional network to model the long-term contextual information. Some algorithms <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15]</ref> first estimate motion fields based on the hand-crafted priors and then use a deep CNN model to restore high-quality images. In <ref type="bibr" target="#b2">[3]</ref>, Caballero et al. develop an effective motion compensation and explore the spatio-temporal information for video SR. Liu et al. <ref type="bibr" target="#b21">[22]</ref> develop a temporal adaptive neural network and a spatial alignment network to better explore the temporal information. In <ref type="bibr" target="#b34">[35]</ref>, Tao et al. propose an effective subpixel motion compensation layer based on the estimated motion fields for video SR. Xue et al. <ref type="bibr" target="#b38">[39]</ref> demonstrate the effect of optical flow on video image restoration and propose an effective video restoration framework to solve general video restoration problems. Instead of explicitly using optical flow for alignment, Jo et al. <ref type="bibr" target="#b13">[14]</ref> dynamically estimate upsampling filters. In <ref type="bibr" target="#b11">[12]</ref>, Haris et al. extend the deep back-projection method <ref type="bibr" target="#b10">[11]</ref> by a recurrent network. Wang et al. <ref type="bibr" target="#b37">[38]</ref> improve the deformable convolution <ref type="bibr" target="#b35">[36]</ref> and develop an effective temporal and spatial attention to solve video restoration. This algorithm wins the champions in the NTIRE19 video restoration <ref type="bibr" target="#b25">[26]</ref>. To generate more realistic images, GANs have been used to solve the both single <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b1">2]</ref> and video <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref> SR problems. These algorithms generate decent results on video SR. However, these algorithms either explicitly or implicitly assume that the blur kernels are known and do not model the blur kernels for SR, which accordingly leads to over-smoothed results.</p><p>Estimating blur kernels has been demonstrated effective for image SR, especially for the details restoration <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42]</ref>. However, these algorithms are designed for single image SR. Few of them have been developed for video SR. Different from these methods, we propose a deep CNN model to simultaneously estimate blur kernels, motion fields, and latent frames so that high-quality videos can be better-restored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Revisiting Variational Methods</head><p>The proposed algorithm is motivated by the variational methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref> for video SR. In this section, we first revisit how these variational methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref> solve video SR and then introduce the proposed algorithm.</p><p>Following the definitions of <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref>, the degradation model for video SR is:</p><formula xml:id="formula_0">L j = SKF ui?j I i + n j ,<label>(1)</label></formula><p>where {L j } i+N j=i?N denote a set of LR images with 2N + 1 frames; I i denotes the HR image; n j denotes image noise, S and K denote the matrix form of down-sampling and blur kernel; F ui?j denotes the warping matrix w.r.t. optical flow u i?j , and u i?j denotes the optical flow from I i to I j .</p><p>Based on the degradation model (1), the HR image I i , optical flow u i?j , and blur kernel K can be estimated by a Maximum a posteriori (MAP):</p><formula xml:id="formula_1">{I * i , K * , {u * i?j }} = arg max Ii,K,{ui?j } p(I i , K, {u i?j }|{L j }), = p(I i )p(K) j p(u i?j )p(L i |I i , K) j =i p({L j }|I i , K, {u i?j })<label>(2)</label></formula><p>Using hand-crafted image priors ?(I i ), ?(u i?j ), and ?(K) on the HR image I i , optical flow u i?j , and blur kernel K, respectively, the video SR process can be achieved by alternatively minimizing <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_2">I * i = arg min Ii SKI i ? L i + i+N j=i?N,j =i SKF ui?j I i ? L j + ?(I i ),<label>(3)</label></formula><formula xml:id="formula_3">u * i?j = arg min ui?j SKF ui?j I i ? L j + ?(u i?j ),<label>(4)</label></formula><p>and</p><formula xml:id="formula_4">K * = arg min K ST Ii K ? L i + ?(K),<label>(5)</label></formula><p>where T Ii is a matrix of latent HR image I i w.r.t. K <ref type="bibr" target="#b20">[21]</ref>.</p><p>Although the video SR algorithms <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref> based on above model have been demonstrated effective in both benchmark datasets and real-world videos, they need to define the hand-crafted image priors ?(I i ), ?(u i?j ), and ?(K) which usually lead to highly non-convex objective function <ref type="bibr" target="#b1">(2)</ref>. This makes the video SR problem more difficult to solve. In addition, the performance of video SR is limited by the hand-crafted image priors. We further note that most existing deep learning-based methods usually employ deep CNN models to solve video SR problem. Although these methods do not need to define hand-crafted priors, they cannot capture the intrinsic characteristics of video SR as the blur kernel is assumed to be known (e.g., Bicubic <ref type="bibr" target="#b37">[38]</ref>, Gaussian <ref type="bibr" target="#b28">[29]</ref>) and do not model it in the SR process, which accordingly lead to over-smoothed results.</p><p>To overcome these problems, we develop an effective deep CNN model which consists of motion blur estimation, motion field estimation, and latent image restoration for video SR. The proposed model does not need the handcrafted priors and is able to capture the intrinsic characteristics of degradation process in video SR by modeling blur kernels. Thus, it can generate much better super-resolved videos with clearer structural details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Algorithm</head><p>The overview of the proposed method is shown in <ref type="figure">Figure</ref> 2. In the following, we explain the main ideas for each component in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Motion blur estimation</head><p>We note that the motion blur estimation based on (5) needs to define a hand-crafted prior ?(K) which usually leads to a complex optimization process. To avoid handcrafted priors and the complex optimization process, we develop a deep CNN model N k to effectively estimate motion blur kernels. The network N k takes initialized Gaussian kernels as input and refine it based on the degradation model <ref type="bibr" target="#b0">(1)</ref>. Given the HR images {I i } and the corresponding LR images {L i }, we use the first term of (5) (which is related to the degradation model (1)) to constrain the deep CNN model N k :</p><formula xml:id="formula_5">L k = SKI i ? L i 1 ,<label>(6)</label></formula><p>whereK denotes the output of the deep CNN model N k and 1 norm is used. Similar to <ref type="bibr" target="#b26">[27]</ref>, the motion blur estimation network N k consists of two fully connected layers, where the first fully connected layer is followed by a ReLU  <ref type="figure">Figure 2</ref>. An overview of the proposed method. The proposed algorithm takes three adjacent frames and initialized input kernel as the input and super-resolves the center image (i.e., Li). First, we use N k to estimate blur kernels from initialized input kernel, where the Gaussian blur kernel is used as the initialized kernel. Then, we generate an intermediate HR image (? * i ) based on an image deconvolution method with the estimated blur kernels. To remove the artifacts in? * i , we compute the optical flow based on the Bicubic usampling results of three adjacent frames and generate the warped images (i.e.,? b i+1 ,? b i?1 ) to guide the restoration of? * i based on NI . The proposed algorithm is jointly trained in an end-to-end manner and generates better high-quality images. The mathematical operators are detailed in main contents. activation function and the second one is follow by a Softmax function to ensure that each element of the blur kernel is nonnegative and the summation of all elements is 1. <ref type="figure" target="#fig_1">Figure 3</ref>(c) shows the estimated blur kernel from bicubic downsampling LR images. We note that the shape of the blur kernel is quite similar to that of Bicubic blur kernel <ref type="bibr" target="#b7">[8]</ref>. We will demonstrate the effectiveness of the motion blur estimation in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Intermediate latent image restoration</head><p>With the blur kernel K, we can estimate HR image from input LR image L i according to <ref type="bibr" target="#b2">(3)</ref>. However, solving (3) needs the optical flow and image prior. Recent algorithms <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> show that the image prior can be efficiently learned by deep CNN models so that the restoration process can be achieved by alternatively solving a simple model to restore intermediate latent images and using deep CNN models to remove the noise and artifacts in the intermediate latent images. Motivated by the success of <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>, we first estimate an intermediate HR image by a simple image deconvolution model and then explore the information of adjacent frames and deep CNN models to restore high-quality images.</p><p>To obtain the intermediate HR image efficiently, we propose an image deconvolution model based on the image formation (1) by:</p><formula xml:id="formula_6">I * i = arg min Ii SKI i ? L i 2 + ? ?I i 2 ,<label>(7)</label></formula><p>where ?I i 2 is used to make the problem well-posed and L 2 norm is used to make the problem be efficiently solved, and ? denotes the gradient operator. Note that (8) is a least square problem. We can get the closed-form solution by:</p><formula xml:id="formula_7">I * i = K S SK + ?(D v D v + D h D h ) ?1K S L i ,<label>(8)</label></formula><p>where D h and D v denote the matrices of derivative filters in horizontal and vertical directions. <ref type="figure" target="#fig_1">Figure 3</ref>(c) shows the estimated intermediate HR imag? I * i . Note that though? * i contains noise and artifacts, it also contains some clear contents which facilitate the following image restoration, especially for the structural details restoration <ref type="figure">(Figure 1(i)</ref>). In the following, we will use the adjacent frames and deep CNN models to remove noise and artifacts in? * i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Optical flow estimation</head><p>The optical flow is used to warp adjacent frames to the reference frame and provide more reliable information for the reference frame restoration. In this work, we use the PWC-Net <ref type="bibr" target="#b33">[34]</ref> as the proposed optical flow estimation algorithm given its small model size and decent performance. We note that the intermediate HR image contains artifacts and noise which may interfere the optical flow estimation. Thus, we use the Bicubic upsampling result of each LR image to compute the initial optical flow.</p><p>Given any three adjacent frames L i?1 , L i , and L i+1 , we first use the Bicubic upsampling to obtain I b i?1 , I b i , and I b i+1 , respectively. Then, the PWC-Net (denoted as N o in <ref type="figure">Figure 2</ref>) is used to compute optical flow u i?1?i and u i+1?i based on the Bicubic upsampling results, where the PWC-Net for the computations of u i?1?i and u i+1?i shares the same parameters. Based on the estimated optical flow, we use the bilinear interpolation method to obtain the warped images I b i?1 (x + u i?1?i ) and I b i+1 (x + u i+1?i ) according to <ref type="bibr" target="#b33">[34]</ref>  <ref type="figure">Figure 2</ref>).</p><formula xml:id="formula_8">(i.e.,? b i+1 ,? b i?1 in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Guided image restoration</head><p>Using the warped images? b i?1 and? b i+1 as the guidance, we can employ existing deep CNN models for image restoration to estimate a high-quality image from? * i . In this paper, we use the deep CNN model by <ref type="bibr" target="#b42">[43]</ref> to restore high-quality images, where we change the network input as the concatenation of? b i?1 ,? b i+1 , and? * i . However, as</p><formula xml:id="formula_9">I b i?1 ,? b i+1</formula><p>, and? * i are HR images, they will increase the computational cost. To overcome this problem, we adopt the space-to-depth transformation <ref type="bibr" target="#b28">[29]</ref> to divide these HR images into LR ones. Thus, the high-quality image can be obtained by</p><formula xml:id="formula_10">I * i = N I (C(S(? b i+1 ; S(? * i ); S(? b i?1 ))),<label>(9)</label></formula><p>where N I denotes the restoration network, C denotes the concatenation operation, and S denotes the space-to-depth transformation. We use the following loss function to constrain the network N I :</p><formula xml:id="formula_11">L = I * i ? I i 1 .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Implementation details</head><p>Training datasets. We train the proposed algorithm using the REDS dataset <ref type="bibr" target="#b25">[26]</ref>, where the REDS dataset contains 300 videos, each video contains 100 frames with an image size of 720 ? 1280 pixels. Among 300 videos, 240 videos are used for training, 30 videos are used for validation, and the remaining 30 videos are used for test. During the training, we randomly choose 45 consecutive frames from each video in the training dataset to train the proposed algorithm.</p><p>Parameter settings and training details. We empirically set ? = 0.02. We use the similar data augmentation method to <ref type="bibr" target="#b37">[38]</ref> to generate training data. The batch size is set to be 8, and the size of each image patch is 64 ? 64 pixels.</p><p>In the training process, we use the ADAM optimizer <ref type="bibr" target="#b16">[17]</ref> with parameters ? 1 = 0.9, ? 2 = 0.999, and = 10 ?8 . The motion blur estimation network N k takes the Gaussian kernels as the input, where the settings of the Gaussian kernels are the same as <ref type="bibr" target="#b30">[31]</ref>. The size of Gaussian kernel is empirically set to be 15 ? 15 pixels. The sizes of the two fully connected layers are set to be 1000 and 225, respectively. The output size of N k is set to be 15 ? 15 pixels. The optical flow estimation network N o is initialized by the pretrained model <ref type="bibr" target="#b33">[34]</ref>. Both the kernel estimation network N k and the image restoration network N I use the random initialization and are trained from scratch. The learning rates for both kernel estimation network N k and image restoration network N I are initialized to be 10 ?4 . As we use the pre-trained model <ref type="bibr" target="#b33">[34]</ref> to initialize the optical flow estimation network, the learning rate for this network is initialized to be 10 ?6 . All the learning rates decrease to 0.2 times after every 50 epochs. During the training process, we first train N k and then jointly train N o and N I in an end-to-end manner. The algorithm is implemented based on the PyTorch. More experimental results are included in the supplemental material. The training code and test model are available at https://github.com/jspan/blindvsr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>In this section, we compare the proposed algorithm against state-of-the-art methods using publicly available benchmark datasets. Quantitative evaluations. We compare the proposed algorithm against state-of-the-art methods including the variational methods <ref type="bibr" target="#b23">[24]</ref> and deep CNN-based methods including DUF <ref type="bibr" target="#b13">[14]</ref>, TOFlow <ref type="bibr" target="#b38">[39]</ref>, RBPN <ref type="bibr" target="#b11">[12]</ref>, EDVR <ref type="bibr" target="#b37">[38]</ref>. In addition, we compare the proposed method with state-ofthe-art deep CNNs-based single image SR <ref type="bibr" target="#b42">[43]</ref> (RCAN). We use the PSNR and SSIM as the evaluation metrics to evaluate the quality of each restored image on synthetic datasets. The PSNR and SSIM values of each restored image are calculated using RGB channels based on the script by <ref type="bibr" target="#b37">[38]</ref>. <ref type="table">Table 1</ref> shows the quantitative evaluation results on 4 videos from the REDS test dataset <ref type="bibr" target="#b25">[26]</ref>, where these 4 videos are also used in <ref type="bibr" target="#b37">[38]</ref> for test. Overall, the proposed method achieves comparable results compared to the EDVR algorithm and outperforms other algorithms by a large margin. <ref type="figure">Figure 4</ref> shows some results with a scale factor of 4 by the top-performing methods on the REDS dataset <ref type="bibr" target="#b25">[26]</ref>. We note that the state-of-the-art single image SR method <ref type="bibr" target="#b42">[43]</ref> does not recover the structural details well as shown in <ref type="figure">Figure 4(d)</ref>. Tao et al. <ref type="bibr" target="#b34">[35]</ref> develop an effective warping layer for video SR. However, this structural details in the superresolved image are not sharp. Xue et al. <ref type="bibr" target="#b38">[39]</ref> explicitly use optical flow and warping operations for video restoration. However, this algorithm assumes the blur kernel is known and takes the Bicubic upsampled images as inputs which are blurry <ref type="figure" target="#fig_1">(Figure 3(b)</ref>) and thus accordingly affect the details restoration <ref type="figure">(Figure 4(g)</ref>). Jo et al. <ref type="bibr" target="#b13">[14]</ref> develop an effective algorithm to dynamically estimate upsampling filters <ref type="table">Table 1</ref>. Quantitative evaluations on the REDS dataset <ref type="bibr" target="#b25">[26]</ref> in terms of PSNR and SSIM. All the results are generated according to the published models for fair comparisons. The best two results are shown in red and blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Bicubic RCAN <ref type="bibr" target="#b42">[43]</ref>  (a) Ground truth HR image (f) DUF <ref type="bibr" target="#b13">[14]</ref> (g) TOFlow <ref type="bibr" target="#b38">[39]</ref> (h) RBPN <ref type="bibr" target="#b11">[12]</ref> (i) Ours <ref type="figure">Figure 4</ref>. Video SR result (?4) on the REDS dataset <ref type="bibr" target="#b25">[26]</ref>. The proposed algorithm recovers high-quality images with clearer structures. and residual images for video SR. However, the structural details are not restored well due to the inaccurate upsampling filters <ref type="figure">(Figure 4(h)</ref>). As the proposed algorithm develops a motion blur estimation which provides an intermediate latent HR image with sharp contents, it generates much clearer images with finer details <ref type="figure">(Figure 4(i)</ref>). We then evaluate the proposed algorithm on the test dataset by Liu et al. <ref type="bibr" target="#b20">[21]</ref> (Vid4) and Tao et al. <ref type="bibr" target="#b34">[35]</ref> (SPMCS). <ref type="table" target="#tab_1">Table 2</ref> shows the quantitative results on the Vid4 and SPMCS datasets. We note that the variational modelbased method <ref type="bibr" target="#b20">[21]</ref> estimates blur kernels from video sequences to restore images. However, the performance of these methods is limited by the hand-crafted image priors. The deep CNN-based methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b11">12]</ref> generate the results with higher PSNR and SSIM values than the variational model-based methods. In contrast, our algorithm generates favorable results in terms of PSNR and SSIM due to the use of motion blur estimation. <ref type="figure">Figure 5</ref> shows some SR results with a scale factor of 4 by the top-performing methods on the Vid4 <ref type="bibr" target="#b20">[21]</ref> and SPMCS <ref type="bibr" target="#b34">[35]</ref> datasets. State-of-the-art methods do not recover the structural details well. In contrast, the proposed method jointly estimates motion blur, motion fields, and la- Qualitative evaluations. We further qualitatively evaluate the proposed algorithm against state-of-the-art methods on real videos. <ref type="figure">Figure 6</ref> shows a real example from <ref type="bibr" target="#b18">[19]</ref>. The results by state-of-the-art methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38]</ref> are still blurry. In contrast, our algorithm generates the images with clearer detailed structures, which demonstrate that the proposed algorithm generalizes well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Analysis and Discussions</head><p>We have shown that using the motion blur estimation is able to help details restoration in video SR. In this section, we further analyze the effect of the proposed algorithm. Effectiveness of the motion blur estimation. The proposed motion blur estimation process provides blur kernels (b1) HR patch (c1) Bicubic (d1) RCAN <ref type="bibr" target="#b42">[43]</ref> (e1) SPMC <ref type="bibr" target="#b34">[35]</ref> (a1) Ground truth HR image (f1) DUF <ref type="bibr" target="#b13">[14]</ref> (g1) TOFlow <ref type="bibr" target="#b38">[39]</ref> (h1) RBPN <ref type="bibr" target="#b11">[12]</ref> (i1) Ours (b1) HR patch (c2) Bicubic (d2) RCAN <ref type="bibr" target="#b42">[43]</ref> (e2) SPMC <ref type="bibr" target="#b34">[35]</ref> (a2) Ground truth HR image (f2) DUF <ref type="bibr" target="#b13">[14]</ref> (g2) TOFlow <ref type="bibr" target="#b38">[39]</ref> (h2) RBPN <ref type="bibr" target="#b11">[12]</ref> (i2) Ours <ref type="figure">Figure 5</ref>. Video SR results (?4) on the Vid4 <ref type="bibr" target="#b20">[21]</ref> and SPMCS <ref type="bibr" target="#b34">[35]</ref> datasets. The proposed algorithm generates much clearer images.  <ref type="table" target="#tab_2">Table 3</ref> shows the quantitative evaluations on the REDS dataset <ref type="bibr" target="#b25">[26]</ref>. The average PSNR by our method is 0.33dB higher than that by BaselineLR, which demonstrates that using motion blur estimation is able to generate much better results.</p><p>The visualizations in <ref type="figure">Figure 7</ref> further demonstrate that directly estimating the HR images using deep CNN models without motion blur estimation does not generates the results with clearer structural details, while the proposed method generates much clearer images. Analysis on motion blur kernels. Different from existing video SR methods that use known blur kernels (e.g., Gaus-sian blur kernel <ref type="bibr" target="#b28">[29]</ref>) and do not model them in the video SR process, we develop a motion blur estimation method to estimate blur kernels for video SR. To examine accuracy of the estimated blur kernels, we apply the estimated blur kernels and downsampling operation to the ground truth HR images to generate LR images. We apply the Bicubic downsampling to the HR images to obtain the LR ones as the ground truth LR images. The quality of the regenerated LR images is used to measure the accuracy of the estimated blur kernels. <ref type="table" target="#tab_3">Table 4</ref> shows that regenerated LR images by the proposed method are closed to the ground truth LR images, which indicates the proposed algorithm is able to estimate more accurate blur kernels than those conventional variational model-based method <ref type="bibr" target="#b23">[24]</ref>. <ref type="figure">Figure 8</ref> shows the visualizations of estimated blur kernels by N k when the degradation process is the Bicubic downampling. We note that shape of the estimated blur kernels are similar to that of Bicubic kernel as demonstrated in <ref type="bibr" target="#b7">[8]</ref>. Thus, both the quantitative and qualitative results demonstrate that the proposed algorithm is able to capture the degradation process well. Video SR with motion blur. The proposed algorithm is able to super-resolve videos containing small motion blur to some extent. <ref type="figure">Figure 6</ref>(a) shows a LR frame from a LR video in <ref type="bibr" target="#b23">[24]</ref>, where the LR video contains small motion blur. As most video SR methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38]</ref> do (b1) Bicubic (c1) RCAN <ref type="bibr" target="#b42">[43]</ref> (d1) SPMC <ref type="bibr" target="#b34">[35]</ref> (e1) DUF <ref type="bibr" target="#b13">[14]</ref> (a1) Input (f1) TOF <ref type="bibr" target="#b38">[39]</ref> (g1) RBPN <ref type="bibr" target="#b11">[12]</ref> (h1) EDVR <ref type="bibr" target="#b37">[38]</ref> (i1) Ours (b2) Bicubic (c2) RCAN <ref type="bibr" target="#b42">[43]</ref> (d2) SPMC <ref type="bibr" target="#b34">[35]</ref> (e2) DUF <ref type="bibr" target="#b13">[14]</ref> (a2) Input (f2) TOFlow <ref type="bibr" target="#b38">[39]</ref> (g2) RBPN <ref type="bibr" target="#b11">[12]</ref> (h2) EDVR <ref type="bibr" target="#b37">[38]</ref> (i2) Ours <ref type="figure">Figure 6</ref>. Video SR results on real videos (?4). The proposed algorithm generates much clearer images.</p><p>(a) HR patch (b) Bicubic (c) DUF <ref type="bibr" target="#b13">[14]</ref> (d) BaselineLR (e) BaselineHR (f) Ours <ref type="figure">Figure 7</ref>. Effectiveness of the motion blur estimation on video SR (?4). Using motion blur estimation is able to generate the results with clearer structural details. not model the motion blur, the generated results are blurry. Although the method <ref type="bibr" target="#b23">[24]</ref> is able to solve video SR with motion blur, it is limited to the hand-crafted priors. In contrast, the proposed method generates much clearer images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Concluding Remarks</head><p>We have proposed an effective video super-resolution algorithm. The proposed algorithm consists of motion blur estimation, motion field estimation, and latent image (a) Initial kernel (b) Estimated kernel <ref type="figure">Figure 8</ref>. Visualizations of the estimated motion blur kernels generated by the network N k . We use the Bicubic downampling as the degradation process of video SR for test. restoration modules. The motion blur estimation module is able to provide reliable blur kernels. With the estimated blur kernel, we develop an image deconvolution method based on the image formation model of video super-resolution to generate intermediate latent images so that some sharp image contents can be restored well. To generate high-quality images, we use the motion estimation module to explore the information from adjacent frames to constrain the deep CNN model for better image restoration. We have shown that the proposed method is able to generate much clearer images with finer structural details due to the use of motion blur kernel estimation. Both quantitative evaluations and qualitative evaluations show that the proposed algorithm performs favorably against state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Effect of the intermediate latent image restoration and motion blur estimation. Using the estimated blur kernel to deblur LR images generates sharper images (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative evaluations on the Vid4 dataset<ref type="bibr" target="#b20">[21]</ref> and SPMCS dataset<ref type="bibr" target="#b34">[35]</ref> in terms of PSNR and SSIM. All the results are generated according to the published models for fair comparisons. * means the values from the reported results<ref type="bibr" target="#b38">[39]</ref>. The best two results are shown in red and blue.</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="6">Bicubic BayesianSR [24]* RCAN [43] SPMC [35] DUF [14] TOFlow [39]</cell><cell>Ours</cell></row><row><cell>Vid4 dataset [21]</cell><cell>Avg. PSNRs Avg. SSIMs</cell><cell>21.91 0.5825</cell><cell>21.95 0.7369</cell><cell>24.03 0.7206</cell><cell>24.39 0.7534</cell><cell>25.84 0.8151</cell><cell>24.22 0.7396</cell><cell>25.35 0.7868</cell></row><row><cell>SPMCS dataset [35]</cell><cell>Avg. PSNRs Avg. SSIMs</cell><cell>25.16 0.6962</cell><cell>--</cell><cell>28.60 0.8253</cell><cell>28.19 0.8164</cell><cell>29.31 0.8554</cell><cell>27.62 0.8048</cell><cell>29.54 0.8532</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Effectiveness of the motion blur estimation on video SR (?4). The results are obtained from the REDS dataset<ref type="bibr" target="#b25">[26]</ref>.</figDesc><table><row><cell>Methods</cell><cell cols="2">BaselineLR BaselineHR</cell><cell>Ours</cell></row><row><cell>Avg. PSNRs</cell><cell>30.18</cell><cell>30.48</cell><cell>30.51</cell></row><row><cell>Avg. SSIMs</cell><cell>0.8609</cell><cell>0.8669</cell><cell>0.8674</cell></row><row><cell cols="4">tent images. The motion blur estimation is able to generate</cell></row><row><cell cols="4">intermediate HR image with clear details, which thus lead</cell></row><row><cell cols="4">to much clearer images with finer structural details.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Accuracy of the estimated motion blur kernels.</figDesc><table><row><cell>Methods</cell><cell cols="2">Ma et al. [24] Gaussian kernel</cell><cell>Ours</cell></row><row><cell>Avg. PSNRs</cell><cell>31.24</cell><cell>25.48</cell><cell>42.06</cell></row><row><cell>Avg. SSIMs</cell><cell>0.9272</cell><cell>0.8726</cell><cell>0.9962</cell></row><row><cell cols="4">which thus leads to the intermediate latent images with clear</cell></row><row><cell cols="4">contents for better details restoration. To demonstrate the</cell></row><row><cell cols="4">effectiveness of this method, we disable this step in the pro-</cell></row><row><cell cols="4">posed algorithm for fair comparisons. Thus, the proposed</cell></row><row><cell cols="4">method reduces to the method without using the motion blur</cell></row><row><cell cols="4">estimation and intermediate latent image restoration. For</cell></row><row><cell cols="4">this case, the inputs of this baseline method can be either the</cell></row><row><cell cols="4">bicubic upsampling results (i.e., I b i , I b i+1 , and I b i?1 ) (Base-</cell></row><row><cell cols="4">lineHR for short) or the original LR images (BaselineLR for</cell></row><row><cell>short).</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Motion deblurring and super-resolution from an image sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedicte</forename><surname>Bascle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">To learn image super-resolution, use a GAN to learn how to do image degradation first</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="187" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2848" to="2857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Armstrong. Projection-based image registration in the presence of fixed-pattern noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Majeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><forename type="middle">E</forename><surname>Hayat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1860" to="1872" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Temporally coherent gans for video super-resolution (tecogan)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyu</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Thuerey</surname></persName>
		</author>
		<idno>abs/1811.09393</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate blur models vs. image priors in single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Netalee</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Apartsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boaz</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast and robust multiframe super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Dirk</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Blind super-resolution with iterative kernel correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent back-projection network for video superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3897" to="3906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional networks for multi-frame superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3224" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video super-resolution with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghwan</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="122" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video super-resolution via deep draft-ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1132" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On bayesian adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="346" to="360" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust video super-resolution with learned temporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2526" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative adversarial networks and perceptual losses for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><forename type="middle">Lopez</forename><surname>Tapia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Handling motion blur in multi-frame superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nonparametric blind super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">NTIRE 2019 challenge on video deblurring and superresolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyong</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokil</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Neural blind deconvolution using deep priors. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Dongwei Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4491" to="4500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Frame-recurrent video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Space-time super-resolution from a single video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oded</forename><surname>Shahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3353" to="3360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fast image/video upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaorong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<idno>153:1-153:7</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">zero-shot&quot; super-resolution using deep internal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Blind super-resolution kernel estimation using an internal-GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detail-revealing deep video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4482" to="4490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">TDAN: temporally deformable alignment network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<idno>abs/1812.02898</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Blind image super-resolution with spatially variant degradations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornill?re</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djelouah</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorkine-Hornung</forename><surname>Olga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schroers</forename><surname>Christopher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-GRAPH Asia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">EDVR: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
		<title level="m">Video enhancement with task-oriented flow. IJCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="1106" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning fully convolutional networks for iterative non-blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Rynson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6969" to="6977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning deep CNN denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2808" to="2817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep plug-andplay super-resolution for arbitrary blur kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1671" to="1681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="294" to="310" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
