<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Learning for Compact Single Image Dehazing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
							<email>yyqu@xmu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Fujian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Tencent Youtu Lab</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhi</forename><surname>Qiao</surname></persName>
							<email>ruizhiqiao@tencent.com</email>
							<affiliation key="aff2">
								<orgName type="department">Tencent Youtu Lab</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
							<email>lzma@cs.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Learning for Compact Single Image Dehazing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single image dehazing is a challenging ill-posed problem due to the severe information degeneration. However, existing deep learning based dehazing methods only adopt clear images as positive samples to guide the training of dehazing network while negative information is unexploited. Moreover, most of them focus on strengthening the dehazing network with an increase of depth and width, leading to a significant requirement of computation and memory. In this paper, we propose a novel contrastive regularization (CR) built upon contrastive learning to exploit both the information of hazy images and clear images as negative and positive samples, respectively. CR ensures that the restored image is pulled to closer to the clear image and pushed to far away from the hazy image in the representation space.</p><p>Furthermore, considering trade-off between performance and memory storage, we develop a compact dehazing network based on autoencoder-like (AE) framework. It involves an adaptive mixup operation and a dynamic feature enhancement module, which can benefit from preserving information flow adaptively and expanding the receptive field to improve the network's transformation capability, respectively. We term our dehazing network with autoencoder and contrastive regularization as AECR-Net. The extensive experiments on synthetic and real-world datasets demonstrate that our AECR-Net surpass the state-of-the-art approaches. The code is released in https://github. com/GlassyWu/AECR-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Haze is an important factor to cause noticeable visual quality degradation in object appearance and contrast. In-* Equal contribution. ? Corresponding author.</p><p>(a) Hazy input (b) Only L1 loss <ref type="bibr" target="#b33">[34]</ref> (c) Prior <ref type="bibr" target="#b41">[42]</ref> (d) KDDN <ref type="bibr" target="#b22">[23]</ref> (e) Our CR (f) Ground-truth put images captured under hazy scenes significantly affect the performance of high-level computer vision tasks, such as object detection <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b7">8]</ref> and scene understanding <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>. Therefore, image dehazing has received a great deal of research focus on image restoration for helping to develop effective computer vision systems.</p><p>Recently, various end-to-end CNN-based methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b41">42]</ref> have been proposed to simplify the dehazing problem by directly learning hazy-to-clear image translation via a dehazing network. However, there exists several issues: (1) Less effectiveness of only positiveorient dehazing objective function. Most existing methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b9">10]</ref> typically adopt clear images (a.k.a. groundtruth) as positive samples 1 to guide the training of dehazing network via L1/L2 based image reconstruction loss without any regularization. However, only image reconstruction loss is unable to effectively deal with the details of images, which may lead to color distortion in the restored images (see <ref type="figure" target="#fig_0">Fig. 1(b)</ref>). Recently, additional knowledge from posi- tive samples based regularization <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b29">30]</ref> has been proposed to make the dehazing model generate more natural restored images. For example, Hong et al. <ref type="bibr" target="#b22">[23]</ref> introduced an additional teacher network to transfer knowledge from the intermediate representation of the positive image extracted by the teacher to the student/dehazing network as positive samples based regularization. Although they utilize the information of positive images as an upper bound, the artifacts or unsatisfied results still happen due to the unexploited information of negative images as an lower bound (see <ref type="figure" target="#fig_0">Fig. 1(d)</ref>). (2) Parameter-heavy dehazing networks. Previous works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">29]</ref> focus on improving the dehazing performance by significantly increasing the depth or width of the dehazing models without considering memory or computation overhead, which prohibits their usage on resource-limited environments, such as mobile or embedded devices. For example, TDN <ref type="bibr" target="#b28">[29]</ref>, the champion model on NTIRE 2020 Challenge <ref type="bibr" target="#b2">[3]</ref> in the dehazing task has 46.18 million parameters. More state-ofthe-art (SOTA) models about their performance and parameters are presented in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>To address these issues, we propose a novel contrastive regularization (CR), which is inspired by contrastive learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>As shown in the right panel of <ref type="figure">Fig. 3</ref>, we denote a hazy image, its corresponding restored image generated by a dehazing network and its clear image (i.e. ground-truth) as negative, anchor and positive respectively. There are two "opposing forces"; One pulls the prediction closer to the clear image, the other one pushes the prediction farther away from the hazy image in the representation space. Therefore, CR constrains the anchor images into the closed upper and lower bounds via contrastive learning, which better help the dehazing network approximate the positive images and move away from the negative images. Furthermore, CR improves the performance for image dehazing without introducing additional computation/parameters during testing phase, since it can be directly removed for inference.</p><p>To achieve the best trade-off between performance and parameters, we also develop a compact dehazing network by adopting autoencoder-like (AE) framework to make dense convolution computation in the low-resolution space and also reduce the number of layers, which is presented in <ref type="figure">Fig. 3</ref>. The information loss from the reduction of parameters can be made up by adaptive mixup and dynamic feature enhancement (DFE). Adaptive mixup enables the information of shallow features from the downsampling part adaptively flow to high-level features from the upsampling one, which is effective for feature preserving. Inspired by deformable convolution <ref type="bibr" target="#b53">[54]</ref> with strong transformation modeling capability, DFE module dynamically expands the receptive field for fusing more spatially structured information, which significantly improves the performance of our dehazing network. We term the proposed image dehazing framework as AECR-Net by leveraging contrastive regularization into the proposed AE-like dehazing network.</p><p>Our main contributions are summarized as follows:</p><p>? We propose a novel ACER-Net to effectively generate high quality haze-free images by contrastive regularization and highly compact autoencoder-like based dehazing network. AECR-Net achieves the best parameter-performance trade-off, compared to the state-of-the-art approaches.</p><p>? The proposed contrastive regularization as a universal regularization can further improve the performance of various state-of-the-art dehazing networks.</p><p>? Adaptive mixup and dynamic feature enhancement module in the proposed autoencoder-like (AE) dehazing network can help the dehazing model preserve information flow adaptively and enhance the network's transformation capability, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Single Image Haze Removal</head><p>Single image dehazing aims to generate the haze-free images from the hazy observation images, which can be categorized into prior-based methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b3">4]</ref> and learning-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Prior-based Image Dehazing Methods. These methods depend on the physical scattering model <ref type="bibr" target="#b30">[31]</ref> and usually remove the haze using handcraft priors from empirical observation, such as contrast maximization <ref type="bibr" target="#b44">[45]</ref>, dark channel prior (DCP) <ref type="bibr" target="#b16">[17]</ref>, color attenuation prior <ref type="bibr" target="#b52">[53]</ref> and nonlocal prior <ref type="bibr" target="#b3">[4]</ref>. Although these prior-based methods achieve promising results, the priors depend on the relative assumption and specific target scene, which leads to less robustness  <ref type="figure">Figure 3</ref>. The architecture of the proposed AECR-Net. It consists of autoencoder-like (AE) dehazing network and constrative regularization (CR). AE has light parameters with one 4? downsampling module, six FA blocks, one DFE module, one 4? upsampling module and two adaptive mixup operations. We jointly minimize the L1 based reconstruction loss and constrative regularization to better pull the restored image (i.e. anchor) to the clear (i.e. positive) image and push the restored image to the hazy (i.e. negative) image.</p><p>in the complex practical scene. For instance, DCP <ref type="bibr" target="#b16">[17]</ref> cannot well dehaze the sky regions, since it does not satisfy with the prior assumption.</p><p>Learning-based Image Dehazing Methods. Different from prior-based methods, learning-based methods are data-driven, which often use deep neural networks to estimate the transmission map and atmospheric light in the physical scattering model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b50">51]</ref> or directly learn hazy-to-clear image translation <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Early works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b50">51]</ref> focus on directly estimating the transmission map and atmospheric light. However, these methods may cause a cumulative error to generate the artifacts, since the inaccurate estimation or some estimation bias on the transmission map and the global atmospheric light results in large reconstruction error between the restored images and the clear ones. Besides, it is difficult or expensive to collect the ground-truth about transmission map and global atmospheric light in the real world.</p><p>Recently, various end-to-end methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b9">10]</ref> have been proposed to directly learn hazy-toclear image translation without using atmospheric scattering model. Most of them <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b9">10]</ref> focus on strengthening the dehazing network and adopt clear images as positive samples to guide the dehazing network via image reconstruction loss without any regularization on images or features. For instance, Qin et al. <ref type="bibr" target="#b33">[34]</ref> proposed a feature fusion attention mechanism network to enhance flexibility by dealing with different types of information, which only uses L1 based reconstruction loss between the restored image and ground-truth. Dong et al. <ref type="bibr" target="#b9">[10]</ref> proposed a boosted decoder to progressively restore the haze-free image by only considering the reconstruction error using ground-truth as supervision. To better use the knowledge from positive samples, Hong et al. <ref type="bibr" target="#b22">[23]</ref> introduced an additional teacher network to transfer knowledge from the intermediate representation of the positive image extracted by the teacher to the student/dehazing network. Although these methods utilize the information of positive images as an upper bound, the artifacts or unsatisfied results still happen due to the unexploited information of negative images as an lower bound. Moreover, these methods are also performance-oriented to significantly increase the depth of the dehazing network, which leads to heavy computation and parameter costs.</p><p>Different from these methods, we propose a novel contrastive regularization to exploit both the information of negative images and positive images via contrastive learning. Furthermore, our dehazing network is compact by reducing the number of layers and spatial size based on autoencoder-like framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Contrastive Learning</head><p>Contrastive learning are widely used in self-supervised representation learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7]</ref>, where the contrastive losses are inspired by noise contrastive estimation <ref type="bibr" target="#b13">[14]</ref>, triplet loss <ref type="bibr" target="#b21">[22]</ref> or N-pair loss <ref type="bibr" target="#b43">[44]</ref>. For a given anchor point, contrastive learning aims to pull the anchor close to positive points and push the anchor far away from negative points in the representation space. Previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b11">12]</ref> often apply contrastive learning into highlevel vision tasks, since these tasks inherently suit for modeling the contrast between positive and negative samples/features. Recently, the work in <ref type="bibr" target="#b32">[33]</ref> has demonstrated that contrastive learning can improve unpaired image-toimage translation quality. However, there are still few works to apply constrative learning into image dehazing, as the speciality of this task on constructing contrastive samples and contrastive loss. Moreover, different from <ref type="bibr" target="#b32">[33]</ref>, we proposed a new sampling method and a novel pixel-wise contrastive loss (a.k.a. contrastive regularization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head><p>In this section, we first describe the notations. Then, we present the proposed autoencoder-like (AE) dehazing network using adaptive mixup for better feature preserving and a dynamic feature enhancement module for fusing more spatially structured information. Finally, we employ contrastive regularization as a universal regularization applied into our AE-like dehazing network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notations</head><p>End-to-end single image dehazing methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b41">42]</ref> remove haze images by using two losses, image reconstruction loss and regularization term on the restored image, which can be formulated as:</p><formula xml:id="formula_0">arg min w J ? ?(I, w) + ??(?(I, w)),<label>(1)</label></formula><p>where I is a hazy image, J is the corresponding clear image, and ?(?, ?) is the dehazing network with parameter w. J ? ?(I, w) is the data fidelity term, which often uses L1/L2 norm based loss. ?(?) is the regularization term to generate a nature and smooth dehazing image, where TVnorm <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b27">28]</ref>, DCP prior <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b27">28]</ref> are widely used in the regularization term. ? is a penalty parameter for balancing the data fidelity term and regularization term. Different from the previous regularization, we employ a contrastive regularization to improve the quality of the restored images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Autoencoder-like Dehazing Network.</head><p>Inspired by FFA-Net <ref type="bibr" target="#b33">[34]</ref> with high effective FA blocks, we use the FA block as our basic block in the proposed autoencoder-like (AE) network. Different from FFA-Net, we significantly reduce the memory storage to generate a compact dehazing model. As presented in <ref type="figure">Fig. 3</ref>, the AElike network first adopts 4? downsampling operation (e.g. one regular convolution with stride 1 and two convolution layers all with stride 2) to make dense FA blocks learn the feature representation in the low-resolution space, and then employ the corresponding 4? upsampling and one regular convolution to generate the restored image. Note that we significantly reduce the number of FA blocks by only using 6 FA blocks (vs. 57 FA blocks in FFA-Net). To improve the information flow between layers and fuse more spatially structured information, we propose two different connectivity patterns: (1) Adaptive mixup dynamically fuses the features between the downsampling layers and the upsampling layers for feature preserving. (2) Dynamic feature enhancement (DFE) module enhances the transformation capability by fusing more spatially structured information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Adaptive Mixup for Feature Preserving</head><p>Low-level features (e.g. edges and contours) can be captured in the shallow layers of CNNs <ref type="bibr" target="#b48">[49]</ref>. However, with   an increase of the network's depth, the shallow features degrades gradually <ref type="bibr" target="#b17">[18]</ref>. To deal with this issue, several previous works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b17">18]</ref> integrate the shallow and deep features to generate new features via the skip connections with an addition or concatenation operation. Actually, FA block <ref type="bibr" target="#b33">[34]</ref> also use addition based skip connections to fuse the internal input and output features. However, there are missing connection between the features from the downsampling layers and upsampling layers in our image dehazing network, which causes shallow features (e.g. edge and corner) lost. Thus, we apply the adaptive mixup operation <ref type="bibr" target="#b49">[50]</ref> to fuse the information from these two layers for feature preserving (see <ref type="figure" target="#fig_3">Fig. 4</ref>). In our case, we consider two downsampling layers and two upsampling layers, such that the final output of the mixup operations can be formulated as:</p><formula xml:id="formula_1">f ?2 = Mix(f ?1 , f ?1 ) = ?(?1) * f ?1 + (1 ? ?(?1)) * f ?1 , f ? = Mix(f ?2 , f ?2 ) = ?(?2) * f ?2 + (1 ? ?(?2)) * f ?2 ,<label>(2)</label></formula><p>where f ?i and f ?i are feature maps from the i-th downsampling and upsampling layer, respectively. f ? is the final output. ?(? i ), i = 1, 2 is the i-th learnable factor to fuse the inputs from the i-th downsampling layer and the i-th upsampling one, whose value is determined by the sigmoid operator ? on parameter ? i . During training, we can effectively learn these two learnable factors, which achieves better performance than the constant factors (see Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Dynamic Feature Enhancement</head><p>Previous works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b41">42]</ref> usually employ the fixed grid kernel (e.g. 3x3) as shown in <ref type="figure" target="#fig_4">Fig. 5</ref> middle, which limits the receptive field and cannot exploit the structured information in the feature space <ref type="bibr" target="#b46">[47]</ref>. Alternatively, the dilated convolutional layer <ref type="bibr" target="#b47">[48]</ref> is introduced to expanse the receptive field. However, it will potentially cause the gridding artifacts. On the other hand, the shape of receptive field is also important to enlarge the receptive field. As shown in <ref type="figure" target="#fig_4">Fig. 5</ref> right, the deformable convolution can capture more important information since the kernel is dynamic and flexible. In fact, the work <ref type="bibr" target="#b46">[47]</ref> has demonstrated that spatiallyinvariant convolution kernels could result in corrupted image textures and over-smoothing artifacts, such that the deformable 2D kernels was proposed to enhance the feature for image denoising. Therefore, we introduce dynamic feature enhancement module (DFE) via deformable convolution <ref type="bibr" target="#b8">[9]</ref> to expand receptive field with adaptive shape and improve the model's transformation capability for better image dehazing. In particular, we employ two deformable convolutional layers to enable more free-form deformation of the sampling grid, as shown in <ref type="figure">Fig. 3</ref>. As such, the network can dynamically pay more attention to the computation of the interest region to fuse more spatially structured information. We also find that DFE deployed after the deep layer achieves better performance than the shallow layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Contrastive Regularization</head><p>Inspired by contrastive learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7]</ref>, it aims to learn a representation to pull "positive " pairs in some metric space and push apart the representation between "negative" pairs. We propose a new contrastive regularization (CR) to generate better restored images. Therefore, we need to consider two aspects in CR: one is to construct the "positive" pairs and "negative" pairs, the other one is to find the latent feature space of these pairs for contrast. In our CR, the positive pair and negative pair are generated by the group of a clear image J and its restored image? by the AE-like dehazing network ?, and the group of? and a hazy image I, respectively. For simplicity, we call the restored image, the clear image and the hazy image as anchor, positive and negative, respectively. For the latent feature space, we select the common intermediate feature from the same fixed pre-trained model G, e.g. VGG-19 <ref type="bibr" target="#b42">[43]</ref>. Thus, the objective function in Eq. (1) can be reformulated as:</p><formula xml:id="formula_2">min J ? ?(I, w) + ? ? ? G(I), G(J), G(?(I, w)) ,<label>(3)</label></formula><p>where the first term is the reconstruction loss to align between the restored image and its ground-truth in the data field. We employ L1 loss, as it achieves the better performance compared to L2 loss <ref type="bibr" target="#b51">[52]</ref>. The second term ? G(I), G(J), G(?(I, w)) is the contrastive regularization among I, J and ?(I, w) under the same latent feature space, which plays a role of opposing forces pulling the restored image ?(I, w) to its clear image J and pushing ?(I, w) to its hazy image I. ? is a hyperparameter for balancing the reconstruction loss and CR. To enhance the contrastive ability, we extract the hidden features from different layers of the fixed pre-trained model. Therefore, the overall dehazing loss function Eq. (3) can be further formulated as:</p><formula xml:id="formula_3">min J ? ?(I, w) 1 + ? n i=1</formula><p>?i ? D Gi(J), Gi(?(I, w)) D Gi(I), Gi(?(I, w)) , <ref type="bibr" target="#b3">(4)</ref> where G i , i = 1, 2, ? ? ? n extracts the i-th hidden features from the fixed pre-trained model. D(x, y) is the L1 distance between x and y. ? i is a weight coefficient. Eq. (4) can be trained via an optimizer (e.g. Adam) in an end-to-end manner. Related to our CR, perceptual loss <ref type="bibr" target="#b23">[24]</ref> measures the visual difference between the prediction and the ground truth by leveraging multi-layer features extracted from a pre-trained deep neural network. Different from the perceptual loss with positive-oriented regularization, we also adopt hazy image (input of dehazing network) as negatives to constrain the solution space, and experiments demonstrate our CR outperforms it for image dehazing (see Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup</head><p>Implementation Details. Our AECR-Net is implemented by PyTorch 1.2.0 and MindSpore with one NVIDIA TITAN RTX GPU. The models are trained using Adam optimizer with exponential decay rates ? 1 and ? 2 equal to 0.9 and 0.999, respectively. The initial learning rate and batchsize are set to 0.0002 and 16, respectively. We use cosine annealing strategy <ref type="bibr" target="#b18">[19]</ref> to adjust the learning rate. We empirically set the penalty parameter ? to 0.1 and the total number of epoch to 100. We set the L1 distance loss in Eq. (4) after the latent features of the 1st, 3rd, 5th, 9th and 13th layers from the fixed pre-trained VGG-19, and their corresponding coefficients ? i , i = 1, ? ? ? , 5 to 1 32 , 1 16 , 1 8 , 1 4 and 1, respectively. Datasets. We evaluate the proposed method on synthetic dataset and real-world datasets. RESIDE <ref type="bibr" target="#b26">[27]</ref> is a widely used synthetic dataset, which consists of five subsets: Indoor Training Set (ITS), Outdoor Training Set (OTS), Synthetic Objective Testing Set (SOTS), Real World taskdriven Testing Set (RTTS), and Hybrid Subjective Testing Set (HSTS). ITS, OTS and SOTS are synthetic datasets, RTTS is the real-world dataset, HSTS consists of synthetic and real-word hazy images. Following the works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b9">10]</ref>, we select ITS and SOTS indoor as our training and testing datasets. In order to further evaluate the robustness of our method in the real-world scene, we also adopt two real-world datasets: Dense-Haze <ref type="bibr" target="#b0">[1]</ref> and NH-HAZE <ref type="bibr" target="#b1">[2]</ref>. More details are provided in the supplementary.</p><p>Evaluation Metric and Compatitors. To evaluate the performance of our method, we adopt the Peak Signal to Noise Ratio (PSNR) and the Structural Similarity index (SSIM) as the evaluation metrics, which are usually used as criteria to evaluate image quality in the image dehazing task. We compare with the prior-based method (e.g.  <ref type="bibr" target="#b16">[17]</ref> (c) DehazeNet <ref type="bibr" target="#b4">[5]</ref> (d) AOD-Net <ref type="bibr" target="#b24">[25]</ref> (e) GridDehazeNet <ref type="bibr" target="#b29">[30]</ref> (f) FFA-Net <ref type="bibr" target="#b33">[34]</ref> (g) MSBDN <ref type="bibr" target="#b9">[10]</ref> (h) KDDN <ref type="bibr" target="#b22">[23]</ref> (i) Ours (j) Ground-truth <ref type="figure">Figure 6</ref>. Visual results comparison on SOTS <ref type="bibr" target="#b26">[27]</ref> dataset. Zoom in for best view.</p><p>DCP <ref type="bibr" target="#b16">[17]</ref>), physical model based methods (e.g. DehazeNet <ref type="bibr" target="#b4">[5]</ref> and AOD-Net <ref type="bibr" target="#b24">[25]</ref>), and hazy-to-clear image translation based methods (e.g. GridDehazeNet <ref type="bibr" target="#b29">[30]</ref>, FFA-Net <ref type="bibr" target="#b33">[34]</ref>, MSBDN <ref type="bibr" target="#b9">[10]</ref> and KDDN <ref type="bibr" target="#b22">[23]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-art Methods</head><p>Results on Synthetic Dataset. In <ref type="table" target="#tab_1">Table 1</ref>, we summarize the performance of our AECR-Net and SOTA methods on RESIDE dataset <ref type="bibr" target="#b26">[27]</ref> (a.k.a, SOTS). Our AECR-Net achieves the best performance with 37.17dB PSNR and 0.9901 SSIM, compared to SOTA methods. In particular, compared to FFA-Net <ref type="bibr" target="#b33">[34]</ref> with the second top performance, our AECR-Net achieves 0.78dB PSNR and 0.0015 SSIM performance gains with the significant reduction of 2M parameters. We also compare our AECR-Net with SOTA methods on the quality of the restored images, which is shown in <ref type="figure">Fig. 6</ref>. We can observe that DCP <ref type="bibr" target="#b16">[17]</ref> and DehazeNet <ref type="bibr" target="#b4">[5]</ref> and AOD-Net <ref type="bibr" target="#b24">[25]</ref> cannot successfully remove dense haze, and suffer from the color distortion (see <ref type="figure">Fig. 6</ref>(b)-6(d)). Compared to DCP, DehazeNet and AOD-Net, the hazy-to-clear image translation based methods in an end-to-end manner (e.g. GridDehazeNet <ref type="bibr" target="#b29">[30]</ref>, FFA-Net <ref type="bibr" target="#b33">[34]</ref>, MSBDN <ref type="bibr" target="#b9">[10]</ref> and KDDN <ref type="bibr" target="#b22">[23]</ref>) achieve the restored images with higher quality. However, they still generate some gray mottled artifacts as shown in <ref type="figure">Fig. 6</ref>(e)-6(f) and cannot completely remove the haze in some regions (see the red rectangles of <ref type="figure">Fig. 6</ref>(g)-6(h)). Our method generates the most natural images and achieves the similar patterns to the ground-truth both in low and high frequency regions. More examples can be found in the supplementary.</p><p>Results on Real-world Datasets. We also compare our AECR-Net with SOTA methods on Dense-Haze <ref type="bibr" target="#b0">[1]</ref> and NH-HAZE <ref type="bibr" target="#b1">[2]</ref> datasets. As shown in <ref type="table" target="#tab_1">Table 1</ref>, we can observe: (1) Our AECR-Net outperforms all SOTA methods with 19.88dB PSNR and 0.7173 SSIM on NH-HAZE dataset. (2) Our AECR-Net also achieves the highest PSNR of 15.80dB, compared to SOTA methods. Note that MS-BDN achieves only about 0.02 higher SSIM, but with 12? parameters, compared to our AECR-Net. (3) Compared to RESIDE dataset, Dense-Haze and NH-HAZE dataset are more difficult to remove the haze, especially on Dense-Haze dataset. This is due to the real dense haze which leads to the severe degradation of information. We also compare our AECR-Net with SOTA methods on the quality of restored images, which are presented in <ref type="figure">Fig. 7</ref> and <ref type="figure">Fig. 8</ref>. Obviously, our AECR-Net generates the most natural images, compared to other methods. The restored images by DCP <ref type="bibr" target="#b16">[17]</ref>, DehazeNet <ref type="bibr" target="#b4">[5]</ref>, AOD-Net <ref type="bibr" target="#b24">[25]</ref>, GridDehazeNet <ref type="bibr" target="#b29">[30]</ref>, FFA-Net <ref type="bibr" target="#b33">[34]</ref> and KDDN <ref type="bibr" target="#b22">[23]</ref> suffer from the serious color distortion and texture loss. Besides, there are still (a) Hazy input (b) DCP <ref type="bibr" target="#b16">[17]</ref> (c) DehazeNet <ref type="bibr" target="#b4">[5]</ref> (d) AOD-Net <ref type="bibr" target="#b24">[25]</ref> (e) GridDehazeNet <ref type="bibr" target="#b29">[30]</ref> (f) FFA-Net <ref type="bibr" target="#b33">[34]</ref> (g) MSBDN <ref type="bibr" target="#b9">[10]</ref> (h) KDDN <ref type="bibr" target="#b22">[23]</ref> (i) Ours (j) Ground-truth <ref type="figure">Figure 7</ref>. Visual comparison on the Dense-Haze dataset.</p><p>(a) Hazy input (b) DCP <ref type="bibr" target="#b16">[17]</ref> (c) DehazeNet <ref type="bibr" target="#b4">[5]</ref> (d) AOD-Net <ref type="bibr" target="#b24">[25]</ref> (e) GridDehazeNet <ref type="bibr" target="#b29">[30]</ref> (f) FFA-Net <ref type="bibr" target="#b33">[34]</ref> (g) MSBDN <ref type="bibr" target="#b9">[10]</ref> (h) KDDN <ref type="bibr" target="#b22">[23]</ref> (i) Ours (j) Ground-truth <ref type="figure">Figure 8</ref>. Visual comparison on NH-HAZE datasets. some thick haze existed in the restored images by MSBDN <ref type="bibr" target="#b9">[10]</ref> and KDDN <ref type="bibr" target="#b22">[23]</ref>. More examples can be found in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>To demonstrate the effectiveness of the proposed AECR-Net, we conduct ablation study to analyze different elements, including mixup, DFE and CR.</p><p>We first construct our base network as the baseline of dehazing network, which mainly consists of two downsampling layers, six FA blocks and two upsampling layers. Subsequently, we add the different modules into base network as: <ref type="formula" target="#formula_0">(1)</ref>  We employ L1 loss as image reconstruction loss (i.e. the first term in Eq. (4)), and use RESIDE <ref type="bibr" target="#b26">[27]</ref> dataset for both training and testing. The performance of these models are summarized in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Effect of Adaptive Mixup Operation. Adaptive mixup operation can improve the dehazing network with additional negligible parameter, which provides additional flexibility to fuse the different features. In <ref type="table" target="#tab_2">Table 2</ref>, it can improve the performance of our base network, e.g. the increases of 0.19dB and 0.7dB in PSNR from base to base+mixup and from base+DFE to base+DFE+mixup, respectively. Furthermore, we compare our adaptive mixup operation with skip connection (SC) operation. The factors (i.e. ?(? 1 ) and ?(? 2 ) in Eq. <ref type="formula" target="#formula_1">(2)</ref>) in our adaptive mixup operation are learn-able, while SC has the identical information fusion. Adaptive mixup operation achieves 0.61dB PSNR gains over SC.</p><p>Effect of DFE Module. DFE module significantly improves the performance from base to base+DFE with an increase of 1.65dB PSNR and from base+mixup to base+DFE+mixup with an increase of 2.16dB PSNR. Therefore, DFE is an more important factor than adaptive mixup, due to the higher performance gains. We also evaluate the effect of DFE positions before and after 6 FA blocks. The results demonstrate that DFE deployed after the deeper layers achieves better performance than the shallow layers. The detailed performance are shown in the supplementary.</p><p>Effect of Contrastive Regularization.</p><p>We consider the effect of CR whether uses negative samples. CR* represents only positive samples are used for training, which is similar to perceptual loss <ref type="bibr" target="#b23">[24]</ref>. Compared to base+DFE+mixup, adding CR* on that (i.e. base+DFE+mixup+CR*) only achieves slightly higher PSNR and SSIM with the gains of 0.26dB and 0.002, respectively. Our AECR-Net employs the proposed CR adding both negative and positive samples for training, which significantly achieves performance gains over base+DFE+mixup+CR*. For example, our AECR-Net achieves a higher PSNR of 37.17dB, compared to base+DFE+mixup+CR* with 36.46dB PSNR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Universal Contrastive Regularization</head><p>To evaluate the universality of the proposed CR, we add our CR into various SOTA methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23]</ref>. As presented in <ref type="table" target="#tab_3">Table 3</ref>, CR can further improve the performance of SOTA methods. In other words, our CR is modelagnostic to train the dehazing networks effectively. Furthermore, our CR cannot increase the additional parameters for inference, since it can be directly removed for testing.</p><p>CR can also enhance the visual quality of SOTA methods. For example, adding our CR into SOTA methods can reduce the effect of black spots and color distortion (see supplementary on these examples). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Discussion</head><p>We further explore the effect of different rates (i.e. r) between positive and negative samples on CR. If the number of negative samples is r, we will take the current hazy input as one sample and randomly select the other r ? 1 negative samples from the the same batch to the input haze image. For positive samples, we select the corresponding clear images to the selected negative samples as positive ones. We select our AECR-Net with the rate of 1:1 as baseline, and conduct all experiments on RESIDE dataset. Additionally, we consider at most 10 positive or negative samples, because of the limited GPU memory size. As shown in <ref type="table" target="#tab_4">Table 4</ref>, adding more negative samples into CR achieves the better performance, while adding more positive samples achieves the opposite results. We conjecture this is due to the different positive pattern that confuses the anchor to learn good pattern. For negative samples, the more negative samples, the farther away from the worse pattern in the hazy images. Therefore, our AECR-Net with the rate of 1:10 achieves the best performance. However, it takes longer training time when increasing the number of negative samples. For example, Our AECR-Net with the rate of 1:10 takes about 200 hours in total (i.e. 2?) for training, compare to total 100 hours at the rate of 1:1 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel AECR-Net for single image dehazing, which consists of contrastive regularization (CR) and autoencoder-like (AE) network. CR is built upon contrastive learning to ensure that the restored image is pulled to closer to the clear image and pushed to far away from the hazy image in representation space. AElike dehazing network based on the adaptive mixup operation and a dynamic feature enhancement module is compact and benefits from preserving information flow adaptively and expanding the receptive field to improve the network's transformation capability. We have comprehensively evaluated the performance of AECR-Net on synthetic and realworld datasets, which demonstrates the superior performance gains over the SOTA methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Comparison with only positive-orient supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The best PSNR-parameter trade-off of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Adaptive mixup. The first and second rows are downsampling and upsampling operations, respectively.Normal grid kernelDeformable kernel Hazy input</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Dynamic feature enhancement module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>base+mixup: Add the mixup operation into base-line. (2) base+DFE: Add the DFE module into baseline. (3) base+DFE+mixup: Add both DFE module and mixup operation into baseline, a.k.a. our AE-like dehazing network. (4) base+DFE+mixup+CR*: Add CR without using negative samples into our AE-like dehazing network. It means that only positive samples are utilized to train the dehazing network. (5) Ours: The combination of our AE-like dehazing network and the proposed CR, which allows both negative and positive samples for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparisons with SOTA methods on the synthetic and real-world dehazing datasets.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="5">SOTS [27] PSNR SSIM PSNR SSIM PSNR SSIM Dense-Haze [1] NH-HAZE [2]</cell><cell># Param</cell></row><row><cell cols="2">(TPAMI'10) DCP [17]</cell><cell cols="5">15.09 0.7649 10.06 0.3856 10.57 0.5196</cell><cell>-</cell></row><row><cell cols="2">(TIP'16) DehazeNet [5]</cell><cell cols="5">20.64 0.7995 13.84 0.4252 16.62 0.5238</cell><cell>0.01M</cell></row><row><cell cols="2">(ICCV'17) AOD-Net [25]</cell><cell cols="6">19.82 0.8178 13.14 0.4144 15.40 0.5693 0.002M</cell></row><row><cell cols="7">(ICCV'19) GridDehazeNet [30] 32.16 0.9836 13.31 0.3681 13.80 0.5370</cell><cell>0.96M</cell></row><row><cell cols="2">(AAAI'20) FFA-Net [34]</cell><cell cols="5">36.39 0.9886 14.39 0.4524 19.87 0.6915</cell><cell>4.68M</cell></row><row><cell cols="2">(CVPR'20) MSBDN [10]</cell><cell cols="6">33.79 0.9840 15.37 0.4858 19.23 0.7056 31.35M</cell></row><row><cell cols="2">(CVPR'20) KDDN [23]</cell><cell cols="5">34.72 0.9845 14.28 0.4074 17.39 0.5897</cell><cell>5.99M</cell></row><row><cell cols="2">(ECCV'20) FDU [11]</cell><cell>32.68 0.9760</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell></cell><cell cols="5">37.17 0.9901 15.80 0.4660 19.88 0.7173</cell><cell>2.61M</cell></row><row><cell>(a) Hazy input</cell><cell>(b) DCP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on AECR-Net. * denotes only positive samples are used for training. SC means skip connection.</figDesc><table><row><cell>Model</cell><cell></cell><cell>CR</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>base</cell><cell></cell><cell>-</cell><cell>33.85</cell><cell>0.9820</cell></row><row><cell>base+mixup</cell><cell></cell><cell>-</cell><cell>34.04</cell><cell>0.9838</cell></row><row><cell>base+DFE</cell><cell></cell><cell>-</cell><cell>35.50</cell><cell>0.9853</cell></row><row><cell>base+DFE+SC</cell><cell></cell><cell>-</cell><cell>35.59</cell><cell>0.9858</cell></row><row><cell>base+DFE+mixup base+DFE+mixup+CR* Ours</cell><cell>?</cell><cell>-(w/o negative) ?</cell><cell>36.20 36.46 37.17</cell><cell>0.9869 0.9889 0.9901</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Results of applying CR into SOTA methods.</figDesc><table><row><cell>Method</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell cols="3">GridDehazeNet [30] 32.99 (? 0.83) 0.9863 (? 0.0027)</cell></row><row><cell>FFA-Net [34]</cell><cell cols="2">36.74 (? 0.35) 0.9906 (? 0.0020)</cell></row><row><cell>KDDN [23]</cell><cell cols="2">35.18 (? 0.46) 0.9854 (? 0.0009)</cell></row><row><cell>MSBDN [10]</cell><cell cols="2">34.45 (? 0.66) 0.9861 (? 0.0021)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparisons of different positive and negative sample rates on CR. The baseline is AECR-Net with the rate of 1:1.</figDesc><table><row><cell cols="4">Rate # Positive # Negative PSNR SSIM</cell></row><row><cell>1:1</cell><cell>1</cell><cell>1</cell><cell>37.17 0.9901</cell></row><row><cell>1:r</cell><cell>1</cell><cell>10</cell><cell>37.41 0.9906</cell></row><row><cell>r:1</cell><cell>10</cell><cell>1</cell><cell>35.61 0.9862</cell></row><row><cell>r:r</cell><cell>10</cell><cell>10</cell><cell>35.65 0.9861</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this paper, positive samples, clear images and ground-truth are the same concept in the image dehazing task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">All tables and figures report the results of the rate 1:1, exceptTable 4.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dense haze: A benchmark for image dehazing with dense-haze and haze-free images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Codruta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosmin</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateu</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Sbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">NH-HAZE: An image dehazing benchmark with nonhomogeneous hazy and haze-free images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPRW</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ntire 2020 challenge on nonhomogeneous dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Codruta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosmin</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florin-Alexandru</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Vasluianu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shai Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1674" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dehazenet: An end-to-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangmin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunmei</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5187" to="5198" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gated context aggregation network for image dehazing and deraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingnan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1375" to="1383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-scale boosted dehazing network with dense feature fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Physics-based feature dehazing networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="188" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dense scene information estimation network for dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkateswararao</forename><surname>Cherukuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Monga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distilling image dehazing with heterogeneous task imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3462" to="3471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aod-net: All-in-one dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiulian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4770" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">End-to-end united video dehazing and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiulian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03919</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Benchmarking singleimage dehazing and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengpan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2766" to="2779" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Trident dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Griddehazenet: Attention-based multi-scale network for image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongrui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Optics of the atmosphere: scattering by molecules and particles. nyjw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccartney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15651,2020.3</idno>
		<title level="m">Contrastive learning for unpaired image-to-image translation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ffa-net: Feature fusion attention network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Xu Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="11908" to="11915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Enhanced pix2pix dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingying</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Single image dehazing via multiscale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wenqi Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gated fusion network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3253" to="3261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Model adaptation with synthetic and real data for semantic dense foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="973" to="992" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Time-contrastive networks: Self-supervised learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmine</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1134" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Changxin Gao, and Nong Sang. Domain adaptation for image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lerenhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multiclass n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visibility in bad weather from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning deformable kernels for image and video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06903</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Densely connected pyramid dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Loss functions for image restoration with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iuri</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on computational imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="57" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Single image dehazing using color attenuation prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingsong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11168</idno>
		<title level="m">Deformable convnets v2: More deformable, better results</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
