<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The World is Not Binary: Learning to Rank with Grayscale Data for Dialogue Response Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibo</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
							<email>xiaojiangliu84@hotmail.com</email>
							<affiliation key="aff3">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
							<email>zheng.haitao@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
							<email>shumingshi@tencent.com</email>
							<affiliation key="aff3">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The World is Not Binary: Learning to Rank with Grayscale Data for Dialogue Response Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Response selection plays a vital role in building retrieval-based conversation systems. Despite that response selection is naturally a learning-to-rank problem, most prior works take a point-wise view and train binary classifiers for this task: each response candidate is labeled either relevant (one) or irrelevant (zero). On the one hand, this formalization can be sub-optimal due to its ignorance of the diversity of response quality. On the other hand, annotating grayscale data for learning-to-rank can be prohibitively expensive and challenging. In this work, we show that grayscale data can be automatically constructed without human effort. Our method employs off-the-shelf response retrieval models and response generation models as automatic grayscale data generators. With the constructed grayscale data, we propose multi-level ranking objectives for training, which can (1) teach a matching model to capture more fine-grained context-response relevance difference and (2) reduce the traintest discrepancy in terms of distractor strength. Our method is simple, effective, and universal. Experiments on three benchmark datasets and four state-of-the-art matching models show that the proposed approach brings significant and consistent performance improvements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Building intelligent conversation systems <ref type="bibr" target="#b19">(Shum et al., 2018;</ref><ref type="bibr" target="#b10">Kollar et al., 2018)</ref> is gaining more and more attention in recent years. A core module in such kind of conversation systems is response selection <ref type="bibr" target="#b16">(Ritter et al., 2011;</ref><ref type="bibr" target="#b7">Hu et al., 2014;</ref><ref type="bibr" target="#b27">Wu et al., 2017;</ref>: Identifying the best response from a set of possible candidates given a dialogue context, i.e., conversation history. For the response selection problem, the trendy practice * Equal contribution. Work was done during internship at Tencent AI Lab. is to build neural matching models <ref type="bibr" target="#b8">(Ji et al., 2014;</ref><ref type="bibr" target="#b24">Wang et al., 2015;</ref><ref type="bibr" target="#b27">Wu et al., 2017;</ref><ref type="bibr" target="#b14">Lu et al., 2019)</ref> for scoring the adequacy of individual response candidates in the dialogue context. Most prior works on this topic focus on fine-grained text encoding and better interactions between dialogue context and response candidates, typically via sophisticated and powerful matching networks <ref type="bibr" target="#b27">(Wu et al., 2017;</ref><ref type="bibr" target="#b14">Lu et al., 2019;</ref><ref type="bibr" target="#b5">Gu et al., 2019)</ref>. Despite their differences, in almost all these previous works, the matching models are trained with binary classification objective. Each response in the training data is either labeled positive (i.e., a correct response to the dialogue context) or negative (i.e., an incorrect response). Often, the negative responses are automatically constructed by random sampling. One limitation of the above training strategy is that this formalization downplays the nuance of fine-grained response quality; the matching model is only informed to predict a binary label, either correct or incorrect. However, the quality of possible response candidates may be quite diverse, thus letting the matching model be aware of which re-sponse candidates are more incorrect or less incorrect than others may more effectively increase the model capacity. Another limitation is that in real-world scenarios the matching models are often confronted with more difficult tasks: to select the best response from a set of strong response candidates instead of random ones. An example is given in <ref type="table" target="#tab_0">Table 1</ref>. During training, the matching models are trained to distinguish the ground truth G and the randomly sampled response R1, where R1 shows little relevance to the dialogue context. Matching models trained on such training data have little experience to identify the groundtruth response G from a set of strong distractor responses such as R2 and R3. Intuitively, a good matching model should be able to not only distinguish good responses from random ones (usually totally irrelevant), as conveyed by the binary classification objective, but also capture the more subtle differences for competitive candidates.</p><p>One natural solution to the above problems is to collect grayscale data for training; if we consider the quality of all possible response candidates falls in the interval [0, 1], the golden-truth and random responses usually cover the two endpoints only, and our goal is to obtain a list of grayscale responses locate in between 0 and 1. However, grayscale data are hard to obtain in reality owing to the expense of human annotation and the subjectivity of individual human annotators.</p><p>In this work, we propose to automatically construct grayscale data from standard dialogue datasets, where only golden dialogue context and response pairs are provided. To meet this goal, we resort to off-the-shelf retrieval algorithms and generation models. Our idea is inspired by the observation that, in most cases, the responses from retrieval models or generation models are better than randomly sampled ones but worse than the ground-truth response. We believe that this progressive relationship, such as "ground truth &gt; retrieval &gt; random", can be utilized for training a better matching model. Concretely, we propose a multi-level ranking objective to make full use of such relationships. Our multi-level ranking objective jointly combines multiple binary contrastive estimations. In addition, the grayscale data partly simulates the real-world response distractors and thus reduces the gap between training and testing, leading to a better distinguishing ability for strong response distractors.</p><p>Our method is simple, effective, and orthogonal to prior efforts for modeling designs. It can be conveniently implemented with most existing matching models. Experimental results on four state-of-the-art matching models and three benchmark datasets demonstrate that our new training approach leads to remarkable performance improvement consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Early research for response selection is devoted to single-turn conversations <ref type="bibr" target="#b23">(Wang et al., 2013;</ref><ref type="bibr" target="#b20">Tan et al., 2015;</ref>. Recently, researchers have started to study on multi-turn conversations <ref type="bibr" target="#b13">(Lowe et al., 2015;</ref><ref type="bibr" target="#b27">Wu et al., 2017;</ref><ref type="bibr" target="#b31">Zhang et al., 2018)</ref>. In the current literature, the task of response selection is formulated as follows. Given a dialogue dataset D = {(c i , r i )}, where c i represents a dialogue context, and r i is the human-written ground-truth response. The goal is to build a matching model s(?, ?) from D so that s(c, r) accurately measures the adequacy of a response candidate r for a dialogue context c.</p><p>Rapid progress has been made for building such matching models in recent years. Concretely, various neural architectures <ref type="bibr" target="#b32">(Zhou et al., 2016;</ref><ref type="bibr" target="#b27">Wu et al., 2017;</ref><ref type="bibr" target="#b5">Gu et al., 2019;</ref><ref type="bibr" target="#b30">Yuan et al., 2019)</ref> have been proposed for fine-grained text encoding and better dialogue context and response interactions modeling. To train such matching models, binary-labeled training sets are constructed <ref type="bibr" target="#b13">(Lowe et al., 2015;</ref><ref type="bibr" target="#b27">Wu et al., 2017;</ref><ref type="bibr" target="#b31">Zhang et al., 2018)</ref>: The humanwritten ground-truth response is designated as positive instances (labeled as 1), and a set of randomly sampled responses N i are treated as negative ones (labeled as 0). The learning objective of s(?, ?) is then to maximize the following binary classification loss function:</p><formula xml:id="formula_0">log s(c i , r i ) + E r ? ?N i log (1 ? s(c i , r ? )). (1)</formula><p>Different from previous works, our study questions the effectiveness of the binary-labeled training data and the corresponding binary classification objective. We argued that the binary classification paradigm is sub-optimal as most of the randomly sampled negative responses are distant from the corresponding positive responses in terms of matching degree, which could lead to serious drawbacks when some strong distractors are presented during testing <ref type="bibr" target="#b31">Zhang et al., 2018)</ref>. Our <ref type="figure">Figure 1</ref>: The illustration of our training approach. For each dialogue, we first extract a number of grayscale data from heterogeneous sources. Then, the multi-level ranking objective is applied to learn the progressive relationships between different responses. work starts with enriching the range of the negative sample set N i in terms of response quality and leads to a simple but new learning strategy that aimed at capturing more fine-grained response quality differences.</p><p>3 Proposed Approach 3.1 Overview <ref type="figure">Figure 1</ref> depicts an overview of our approach. First, different responses are acquired from various sources, such as retrieval models, generation models, and random sampling. Then, the collected responses are sorted by estimated quality to form progressive relationships. Lastly, a multi-level ranking objective is designed to learn such relationships. We first present our methods for automatically constructing grayscale data in Section 3.2, followed by the multi-level ranking objective introduced in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Grayscale Data Acquisition</head><p>Our goal is to construct a set of responses with diverse quality. Specifically, we construct three types of responses for each dialogue context and rank them in three tiers. It should be noted that our data acquisition only relies on standard dialogue datasets, which only provide human-to-human dialogue context and response pairs.</p><p>Zero &amp; One First of all, the corresponding responses for dialogues context in the standard dialogue dataset are considered as our ground-truth responses. These human-written responses are often informative and relevant. As a result, the groundtruth samples are ranked as tier-1. Similar to previous work, we also utilize randomly sampled responses for contrastive estimation. The random responses are sampled from the responses of other dialogue contexts in the training data. We rank random responses as tier-3 because they often show little relevance to the dialogue context. The groundtruth responses and random responses constitute the "zero &amp; one" binary training data used in the prior work.</p><p>Grayscale We now delve into describing the grayscale data construction procedures. We consider two types of frequently-used toolkits for automatic response generation to produce grayscale data, namely, the retrieval-based models and the generation-based models.</p><p>The retrieval-based models <ref type="bibr" target="#b8">(Ji et al., 2014;</ref><ref type="bibr" target="#b7">Hu et al., 2014)</ref> directly copy an existing response from the training corpus when receiving a response request. Since the returning responses are always human utterances in real-world conversations, they are informative and grammatical. However, the response quality of such systems varies as it depends on the lexical similarity of the given dialogue con-text and those in the training corpus. Typically, the retrieval results are better than random responses because they are more or less relevant to the dialogue context. However, most retrieval results are worse than the ground truth. The retrieval results are ranked tier-2.</p><p>Specifically, we split the multi-turn dialogue into a series of single-turn input-response pairs. Then we index the input-response pairs with the BM25 algorithm <ref type="bibr" target="#b17">(Robertson and Zaragoza, 2009</ref>). We retrieve response candidates using the last utterance of the dialogue context.</p><p>The generation-based models <ref type="bibr" target="#b18">(Shang et al., 2015;</ref><ref type="bibr" target="#b12">Li et al., 2016)</ref> generate a new utterance from scratch after training. While those models have better generalization capacity in rare dialogue contexts, the generation responses tend to be universal and noninformative (e.g., "I don't know", "I think so" etc.) <ref type="bibr" target="#b12">(Li et al., 2016)</ref>. Similar to the retrieval responses, the generation responses are usually better than the random responses but worse than the ground-truth responses. However, compared to retrieval models that merely rely on lexical overlapping, generation results can capture deeper semantic interactions. The different characteristics of retrieval and generation models make their results complement each other in terms of response quality, which we consider beneficial for training.</p><p>Specifically, we train a Seq2Seq model with the attention mechanism <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> for response generation. We adopt the same corpus used in the retrieval model to train the generation model. The generation response is produced by feeding the dialogue context to a trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion on Extendibility</head><p>Note that there can be many more sophisticated ways to construct the grayscale data. For example, one may employ the results from different retrieval models and/or generation models. Responses from different models can be further divided into sub-groups according to the relative strengths of the corresponding models. For instance, responses that are generated from more advanced and competent generation models (e.g., a model based on GPT2 <ref type="bibr" target="#b15">(Radford et al., 2019)</ref>) can be considered better than those from less competent models (e.g., a vanilla seq2seq model). However, in this paper, we only showcase the results with basic retrieval and generation models for keeping our idea simple and neat. Nevertheless, this simple setting, as we will demonstrate, already leads to remarkable performance improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Level Ranking Objectives</head><p>Our grayscale data acquisition provides ground for carrying out more principled and sufficient training paradigms. To make full use of the grayscale data, we propose multi-level ranking objectives. Unlike prior work that minimizes binary classification errors, our training objective better fits the learning-to-rank nature of response selection, that is, minimizes ranking errors of possible responses <ref type="bibr" target="#b3">(Cao et al., 2007)</ref>. Also, as the grayscale data exhibit various response quality, training with such data rather than random negatives better simulate testing environments.</p><p>We start formal descriptions with some notation: the training set can be re-organized as D =</p><formula xml:id="formula_1">{(c i , R i )} N i=1</formula><p>, where c i denotes the dialogue context and R i = {r i , e i , g i ,r i } is the response set enhanced by grayscale data. Concretely, r i , e i , g i , andr i refer to ground-truth responses, retrieval responses, generation responses, and random responses, respectively. We consider three ordered list as follows.</p><p>? ground truth&gt;retrieval&gt;random This ordered list considers the progressive relationships between ground-truth responses, retrieval responses, and random responses. We use margin ranking losses for implementation, the formula are given below:</p><formula xml:id="formula_2">L Ret = max{0, ? ? s(c, r i ) + s(c, e i )} + max{0, ? ? s(c, e i ) + s(c,r i )}.</formula><p>where ? is a hyperparameter and represent the minimum acceptable score margin between two tiers, and s(?, ?) is the matching score given by a matching model.</p><p>? ground truth&gt;generation&gt;random This ordered list considers the progressive relationships between ground-truth responses, generation responses, and random responses. The loss function is given below.</p><formula xml:id="formula_3">L Gen = max{0, ? ? s(c, r i ) + s(c, g i )} + max{0, ? ? s(c, g i ) + s(c,r i )},</formula><p>? ground truth&gt;random</p><formula xml:id="formula_4">L Ran = max{0, ? ? s(c, r i ) + s(c,r i )},</formula><p>this loss function directly models the relationship between the ground-truth samples r i and random samplesr i .</p><p>Our final training objective is an unite of all above. It models the integrated relationship between tiers "ground truth&gt;retrieval &amp; generation&gt;random" and "ground truth &gt; random" simultaneously:</p><formula xml:id="formula_5">L U ni = L Ran + L Ret + L Gen .</formula><p>4 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>We test on three benchmark datasets for multi-turn response selection.</p><p>Ubuntu Dialogue Corpus It consists of English multi-turn dialogues about technical support collected from the Ubuntu Forum <ref type="bibr" target="#b13">(Lowe et al., 2015)</ref>. The dataset contains 500K, 50K and 50K chat logs for training, validation, and test respectively. Each test dialogue is paired with 9 distractor responses. Following conventions, the response selection performance is evaluated by R n @k scores. R n @k is the recall rate at position k in n candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Douban Conversation Corpus It consists of</head><p>Chinese multi-turn daily conversations crawled from Douban group <ref type="bibr" target="#b27">(Wu et al., 2017)</ref>. The dataset contains 500K, 25K and 1K chat logs for training, validation, and test respectively. Each test dialogue is paired with 10 candidate responses. Following prior work, besides R n @k scores, we also report Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) and the precision at position 1 (P@1).</p><p>E-commerce It consists of Chinese conversations between customers and customer service staff from Taobao <ref type="bibr" target="#b31">(Zhang et al., 2018)</ref>. The dataset sizes and settings is the same as Douban corpus. R n @k scores are commonly employed for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Models</head><p>We compare with the following baseline models.</p><p>Single-turn Matching Models These models concatenate all context utterances together into one single long utterance then compute the matching scores between the long utterance and response candidates, including RNN <ref type="bibr" target="#b13">(Lowe et al., 2015)</ref>, CNN <ref type="bibr" target="#b13">(Lowe et al., 2015)</ref>, LSTM <ref type="bibr" target="#b13">(Lowe et al., 2015)</ref>, Bi-LSTM <ref type="bibr" target="#b9">(Kadlec et al., 2015)</ref>, Match-LSTM <ref type="bibr" target="#b25">(Wang and Jiang, 2016)</ref> and <ref type="bibr">MV-LSTM (Wan et al., 2016)</ref>.</p><p>Multi-turn Matching Models These models aggregate the information of context utterances in more advanced ways, including DL2R , Multi-View <ref type="bibr" target="#b32">(Zhou et al., 2016)</ref>, DUA <ref type="bibr" target="#b31">(Zhang et al., 2018)</ref>, SMN <ref type="bibr" target="#b27">(Wu et al., 2017)</ref>, DAM , IOI , and MSN <ref type="bibr" target="#b30">(Yuan et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>For grayscale data construction, we train a seq2seq generation model and build a BM25 retrieval system using the training set for each dataset. We consider the top 100 responses from BM25 retrieval and the top 5 responses from seq2seq generation (via beam search) as the grayscale responses. To facilitate further research, we have made our collected grayscale data publicly available. 1 During training, we use these grayscale responses in a way adaptive to the training matching model. At each training epoch, ten different grayscale responses are used: the top 5 retrieval responses ranked by the current matching model and all 5 seq2seq generation responses. We experiment our new training approach on four latest state-of-the-art models as follows:</p><p>? SMN <ref type="bibr" target="#b27">(Wu et al., 2017)</ref> interacts each utterance of a dialogue context with a response and then transforms interaction matrices into matching vectors with CNN. The matching vectors are finally mapped into a matching score with an RNN.</p><p>? DAM  obtains matching vectors of text segments at different granularities with the stacked self-attention. The matching vectors are then distilled with the cross-attention and finally fused into a matching score via a single-layer perceptron.</p><p>? IOI  pairs each utterance of a context with a response via stacking multiple interaction blocks and then aggregates matching information from all the pairs as a matching score in an iterative fashion.</p><p>? MSN <ref type="bibr" target="#b30">(Yuan et al., 2019)</ref> utilizes a multi-hop selector to select the relevant utterances as context and then matches the filtered context with the given response candidate to obtain a matching score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Douban Ubuntu E-commerce M AP M RR P @1 R 10 @1 R 10 @2 R 10 @5 R 2 @1 R 10 @1 R 10 @2 R 10 @5 R 10 @1 R 10 @2 R 10 @5  Results of all baselines are directly copied from the previous works <ref type="bibr" target="#b30">Yuan et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LRan LRet</head><p>LGen  Specifically, we first pre-train a model with objective L ran only then switch to L U ni . We find that such a treatment makes the training process more stable.</p><formula xml:id="formula_6">SMN DAM P @1 R10@1 R10@2 R10@5 P @1 R10@1 R10@2 R10@5 ? ? 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Results</head><p>The experimental results are listed in <ref type="table" target="#tab_2">Table 2</ref>, where G-X indicates X with our grayscale enhanced training approach. We can see that our training approach significantly improves the performance of all four matching models in terms of various metrics. The improvements are consistent across different datasets and different models, indicating the university of our approach. Moreover, one interesting observation is that a less-accurate matching architecture with the proposed training approach can outperform a stronger matching architecture with the traditional training paradigm, e.g., G-IOI vs. MSN. This suggests that while the choice of learning objective is often overlooked, it could be decisive for building a competitive response selection model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effect of Different Grayscale Data</head><p>We then turn to conduct an ablation study for understanding the roles of different grayscale data in performance enhancement. We choose SMN as well as DAM as the baselines models. We train the models with three additional settings by removing either retrieval responses or generation responses and removing both of them.</p><p>The results are shown in <ref type="table" target="#tab_4">Table 3</ref>, we can find that both retrieval data and generation data make irreplaceable contributions to the overall performance and the combination of both worlds makes the best results, which confirms our hypotheses that responses from heterogeneous sources complement each other. We can also find that the help from retrieval data has a greater influence than generation data when used alone. This can be attributed to that the seq2seq-based generation model tends to output general and dull responses. Such general responses are less informative than the retrieval data, thus can provide limited help for distinguishing the nuance of fine-grained response quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of Multi-level Ranking Objectives</head><p>Next, we study the effect of the multi-level ranking objective <ref type="bibr">(MRO)</ref>. Recall that we adopt the MRO in order to make use of the progressive relationship in different tiers. However, a simpler alternative is to treat all grayscale data as negative samples and use the learning objective in Eq. 2. It can be regarded  as a simple data augmentation technique, enlarging the set of negative examples with retrieval and generation results. We implement such an idea to test whether the proposed MRO is necessary and quantify the benefit of the MRO. As shown in <ref type="table" target="#tab_6">Table 4</ref>, the performance of models trained without MRO falls behind those trained with MRO. Besides, the improvements of grayscale data without MRO are quite limited compared to the original counterparts without grayscale data. This indicates that the proposed multi-level ranking objective is essential for performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effect of Margin Size</head><p>The hyperparameter margin size (?) denotes the minimum distance between two tiers in matching scores, which may affect the performance of a matching model. We conduct a series of sensitivity analysis experiments to study how the margin affects the performance of our training. 2 All models are evaluated in terms of R 10 @1.</p><p>Referring to <ref type="figure" target="#fig_0">Figure 2</ref>, we can see that both SMN and DAM have a similar trend on Douban: the curves first increase and then drop as the margin increases. This is mainly because response candidates on Douban are of high relevance. When the margin is too large, matching models have no idea to handle strongly relevant distractors. However, when the margin is too small, matching models will become too sensitive and sometimes mistakenly give high scores for responses with less relevance to dialogue context. Results on Ubuntu show a completely different behavior: the performances grow in step with the margin. The reason may be that 2 We also tried to use different margins for different pairs but the improvements are limited. the response distractors of Ubuntu have relatively large margins in semantic and matching models need to make strong discrimination between the ground truth and other grayscale samples. As a result, models learned with the large margin can fit such data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Compatiblity with Co-teaching</head><p>We have noticed that  adopts the co-teaching framework to train a robust matching model. From their experiment, the co-teaching framework with dynamic margins is proven to eliminate the effect from random sampled noisy responses effectively. We believe that our approach and co-teaching framework can benefit each other. Therefore, we combine our training approach with the co-teaching framework taking margins strategy as an instance to train the matching models. From the results in <ref type="table">Table 5</ref>, we can see that models trained with our approach outperform those trained with the co-teaching framework. More importantly, the SMN+CoT and DAM+CoT obtain further improvements after adding our multi-level ranking objectives. This demonstrates that our approach is compatible with the co-teaching framework and shows strong portability and practicability to act as a generalized approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Case Study</head><p>As shown in case 1 of <ref type="table">Table 6</ref>, response 2 contains some irrelevant content about the comic "One Piece", but it is still selected by DAM as the best response. In case 2, SMN selects the totally irrelevant response 2 as the best response, which may because this response has some overlapped words with the dialogue. These are consistent with the problem  <ref type="table">Table 5</ref>: Experimental results of matching models trained with our approach and the co-teaching framework.</p><p>X+CoT indicates models trained with the co-teaching framework. We copy the results of SMN+CoT and DAM+CoT from  on Douban, and we supplement the results of two models trained with the co-teaching framework on Ubuntu. <ref type="table">Table 6</ref>: Two cases from the test set of Douban are listed above, and both of them have Response 1 as a groundtruth response. Though each dialogue has ten candidates, we show only two of them due to space limitations. The dialogues are in Chinese (the left) and we also provide their translated version in English (the right).</p><p>introduced in Section 2 that these models may mistake the fuzzy-candidate with few improper details for the best response due to the gap between training and testing. In contrast, after adopting our training approach, the G-SMN and G-DAM correctly identify the improper content in the negative responses and successfully select response 1 as the best response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Some researchers also studied how to improve the performance of existing matching models with a better learning method.  proposed to leverage a Seq2Seq model as a weak annotator to assign a score for each response candidate of the dialogue and learn matching models through the scores.  introduced the coteaching framework <ref type="bibr" target="#b6">(Han et al., 2018)</ref> for eliminat-ing the effect of training noises. The learning approach maintains two matching models and makes them teach each other.  attempted to neglect the effect of false negatives and trivial true responses by adopting four negative sampling strategies to choose negative samples during training dynamically. Different from those previous works, our approach makes use of grayscale data from heterogeneous sources and learns progressive quality relationships. In addition, our work enhances retrieval models with generation models, which is on par with recent attempts <ref type="bibr">(Cai et al., 2019a,b)</ref> to strengthen generation models via retrieval models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We presented a novel approach for training response selection models for multi-turn conversa-tions. It automatically constructs different types of grayscale data and uses a multi-level ranking objective. The proposed approach can teach a matching model to capture fine-grained quality differences better and reduce the train-test discrepancy in distractor strength. Experimental results on three benchmark datasets and four state-of-the-art models demonstrated the effectiveness of the proposed training approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The effect of margin size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dialogue Context Between Speakers A and B Relevance A: Would you please share some useful experience for improving spoken English? B: Sure! Watching English movies helped a lot. A: Agreed. I watched Friends many times. B: Me too! I bought the DVDs and they went broken due to my frequent use. Distractor Responses in Real-world Scenario R2: It's said that a DVD can be preserved for decades. ? R3: Friends is an American television sitcom. Dialogue context (conversation history) between Speakers A and B. R1 is a random sample used as a negative instance during training. R2 and R3 are real distractors during testing.</figDesc><table><row><cell>Ground Truth</cell><cell></cell></row><row><cell>G: Hah! Then your English should be very good!</cell><cell>+</cell></row><row><cell>Distractor Response During Training</cell><cell></cell></row><row><cell>R1: Why didn't the British police come?</cell><cell>???</cell></row></table><note>?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results of all models trained with our approach on Douban, Ubuntu and, E-commerce datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of our approach on Douban datasets with SMN and DAM.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Effect of multi-level ranking objectives. Here, all metrics are evaluated in Douban corpus.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Related resources can be found at https: //ai.tencent.com/ailab/nlp/dialogue/ datasets/grayscale_data_release.zip</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Skeleton-to-response: Dialogue generation guided by retrieval memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Retrievalguided dialogue response generation via a matchingto-generation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1866" to="1875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to rank: from pairwise approach to listwise approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a matching model with co-teaching for multi-turn response selection in retrieval-based dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiazhan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3805" to="3815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interactive matching network for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Chen</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2321" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An information retrieval approach to short text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongcheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.6988</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03753</idno>
		<title level="m">Improved deep learning baselines for ubuntu corpus dialogs</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The alexa meaning representation language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauren</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolina</forename><surname>Owczarzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tagyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lambert</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kayser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradford</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Matsoukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics, NAACL 2018</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics, NAACL 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sampling matters! an empirical study of negative sampling strategies for learning of matching models in retrievalbased dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Wei Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1291" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Constructing interpretive spatio-temporal features for multiturn responses selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenbin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeying</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">Chao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenglin</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics, ACL 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="44" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data-driven response generation in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1577" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">From eliza to xiaoice: challenges and opportunities with social chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Dong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Information Technology &amp; Electronic Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="26" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">LSTM-based deep learning models for non-factoid answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04108</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">One time of interaction may not be enough: Go deep with an interaction-over-interaction network for response selection in dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics, ACL 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Match-srnn: Modeling the recursive matching structure with spatial rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Shengxian Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">IJCAI</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A dataset for research on short-text conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Syntax-based deep matching of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1354" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning natural language inference with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning matching models with weak supervision for response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="420" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="496" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Incorporating loose-structured knowledge into LSTM with recall gate for conversation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05110</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to respond with deep neural networks for retrievalbased human-computer conversation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-hop selector network for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling multiturn conversation with deep utterance aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongshen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics, ACL 2018</title>
		<meeting>the 27th International Conference on Computational Linguistics, ACL 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3740" to="3752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-view response selection for human-computer conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-turn response selection for chatbots with deep attention matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
