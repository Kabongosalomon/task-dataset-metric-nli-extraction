<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">No-Reference Video Quality Assessment Using Space-Time Chips</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">P</forename><surname>Ebenezer</surname></persName>
							<email>joshuaebenezer@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Image and Video Engineering (LIVE)</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaixi</forename><surname>Shang</surname></persName>
							<email>zxshang@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Image and Video Engineering (LIVE)</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Wu</surname></persName>
							<email>yongjuw@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Image and Video Engineering (LIVE)</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wei</surname></persName>
							<email>haiwei@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Image and Video Engineering (LIVE)</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
							<email>bovik@ece.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Image and Video Engineering (LIVE)</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">No-Reference Video Quality Assessment Using Space-Time Chips</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>? Amazon Prime Video</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video quality assessment</term>
					<term>Space-time chips</term>
					<term>Natural Video Statistics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new prototype model for no-reference video quality assessment (VQA) based on the natural statistics of space-time chips of videos. Space-time chips (ST-chips) are a new, quality-aware feature space which we define as space-time localized cuts of video data in directions that are determined by the local motion flow. We use parametrized distribution fits to the bandpass histograms of space-time chips to characterize quality, and show that the parameters from these models are affected by distortion and can hence be used to objectively predict the quality of videos. Our prototype method, which we call ChipQA-0, is agnostic to the types of distortion affecting the video, and is based on identifying and quantifying deviations from the expected statistics of natural, undistorted ST-chips in order to predict video quality. We train and test our resulting model on several large VQA databases and show that our model achieves high correlation against human judgments of video quality and is competitive with state-of-the-art models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Video content accounts for a very large portion of traffic on the Internet and continues to surge in volume, as more people stream content on smartphones, tablets, and high-definition screens. Being able to predict perceived video quality is important to content providers for monitoring and controlling streaming quality and thereby enhance customer satisfaction. Video quality is affected when the video is distorted, which occurs due to a number of reasons as the video is being captured, transmitted, received, and displayed. The task of predicting the quality of a distorted video without a pristine video of the same content to compare it with, which is called no-reference (NR) VQA, is difficult. NR VQA is a hard problem because less information is available than models that use a reference, and conventional notions of signal fidelity are not applicable. Here we describe a new NR VQA algorithm based on the statistics of local windows oriented in space and time.</p><p>We briefly review state-of-the-art NR VQA algorithms. V-BLIINDS <ref type="bibr" target="#b0">[1]</ref> is a distortion agnostic NR VQA algorithm that models the statistics of DCTs of frame differences to predict video quality. These features are based on models of the human visual system (HVS). HVS-based algorithms posit that natural video statistics have a regularity from which ? Equal contribution the statistics of distorted videos deviate, and which the HVS is attuned to. Human perceptual judgments of quality are influenced by the degree to which the statistics of distorted videos deviate from those of natural videos. Such models thus attempt to understand and mimic operations of the HVS in order to identify the regularity in natural video statistics. VI-IDEO <ref type="bibr" target="#b1">[2]</ref> is another algorithm that is belongs to this category. VIIDEO is completely "blind," in the sense that it does not require any training and can be directly deployed. VIIDEO makes use of observed high inter subband correlations of natural videos to predict quality. TLVQM <ref type="bibr" target="#b2">[3]</ref> takes a different approach to the problem, where the goal is to define features that capture distortion, and not to characterize naturalness per se. A number of spatiotemporal features are defined at two computational levels, collectively designed to capture a wide range of distortions. TLVQM has about 30 different user-defined parameters, which may affect its performance on databases it was not exposed to when it was designed.</p><p>It has also been observed <ref type="bibr" target="#b3">[4]</ref> that NR image quality assessment algorithms work reasonably well when applied frame-byframe to distorted videos of user-generated content because of a lack of temporal variation in such videos. FRIQUEE <ref type="bibr" target="#b4">[5]</ref> is a state-of-the-art algorithm for NR IQA that uses a bag of perceptually motivated features. BRISQUE <ref type="bibr" target="#b5">[6]</ref> is another NR IQA algorithm that models the statistics of spatially bandpassed coefficients of images, motivated by the fact that the early stages of the HVS perform spatial bandpassing. NIQE <ref type="bibr" target="#b6">[7]</ref> also models statistics of spatially bandpassed coefficients, but generates an opinion score by quantifying the deviation of a distorted image from the statistical fit to a corpus of natural images and does not require training. CORNIA <ref type="bibr" target="#b7">[8]</ref> approaches the problem differently from HVS-based models, and builds a dictionary to represent images effectively for quality assessment.</p><p>Videos are spatiotemporal signals and distortions affecting videos can be spatial or temporal or a combination of both. An effective NR VQA algorithm must be able to build a robust representation of the spatiotemporal information in videos. Primary visual cortex (area V1) is implicated in decomposing visual signals into orientation and scale-tuned spatial and temporal channels. V1 neurons are sensitive to specific local orientations of motion. This decomposition is passed on to other areas of the brain, including area middle temporal (MT), where further motion processing occurs. Extra-striate cortical area MT is known to contain neurons that are sensitive to motion over larger spatial fields <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. We propose a perceptually motivated NR VQA model, ChipQA, that captures both spatial and temporal distortions, by building a representation of local spatiotemporal data that is attuned to local orientations of motion but is studied over large spatial fields. We show how observed statistical regularities of spatially bandpassed coefficients can be extended temporally and introduce the notion of space-time chips, which follow natural video statistics (NVS). We evaluate the new model on a number of databases and show that we are able to achieve state-of-the-art performance at reasonable computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROPOSED ALGORITHM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Space-time Chips</head><p>If we consider videos as space-time volumes of data, Spacetime (ST) chips can be defined as chips of this volume in any direction or orientation. ST-Chips are similar to space-time slices <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, but are highly localized in space and time. We are interested in finding the specific directions and orientations of these chips that capture the regularity of natural videos. Consider a frame I T at time instant T , and its preceding frames</p><formula xml:id="formula_0">I T ?T +1 ..I T ?1 from time instant T ? T + 1 onward, all of size M ? N .</formula><p>There are T frames in this space-time volume. Assume that we have coherent motion vectors available to us at time instant T . To the best of our knowledge, localized groups of pixels are in motion along the directions of these vectors. Assuming T is small enough, a local spatiotemporal chip of this volume that is oriented perpendicular to the motion vector at each spatial location would capture the local areas that are in motion at each location and across time.</p><p>Taking into account the smoothness of motion across time for natural videos, we expect ST-chips to follow similar regularities as we would expect to find for stationary frames. Spatially bandpassed coefficients are known to follow a generalized Gaussian distribution (GGD) in the first order, and their second order statistics can be approximated as following an asymmetric generalized Gaussian distribution (AGGD) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Accordingly, we define the mean subtracted contrast normalized (MSCN) coefficient?</p><formula xml:id="formula_1">I T (i, j) = I T (i, j) ? ? T (i, j) ? T (i, j) + C<label>(1)</label></formula><p>where (i, j) are spatial coordinates and we define the local mean and local variance as</p><formula xml:id="formula_2">? T (i, j) = k=K k=?K k=L k=?L w(k, l)I T (i + k, j + l) (2) ? T (i, j) = k=K k=?K k=L k=?L w(k, l)(I T (i + k, j + l) ? ? T (i, j)) 2</formula><p>(3) respectively. w = {w(k, l), k ? ?K, .., K, l ? ?L, .., L} is a 2D circularly-symmetric Gaussian weighting function sampled out to 3 standard deviations and rescaled to unit volume. Motion has been used to guide video quality prediction in several full-reference models <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, but few NR models <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Given a dense motion flow field at time T , we first find the median of the flow field across a spatial window of size R ? R. This gives us a more robust estimate of the flow in each spatial window. We cut the video volume of MSCN coefficients across time from? T ?T +1 to? T at each window in the direction perpendicular to the motion vectors at each spatially localized window, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, and then aggregate these chips across the spatial windows to form a single "frame" S T of ST-Chips at each time instance. Each chip is constrained to pass through the center of the spatial R ? R patch, and to be perpendicular to the median flow of that patch. R pairs of x and y coordinates obtained from the relevant line equations are rounded to integers for sampling from the video.In this way, the chips at each patch are uniquely defined by the spatial location of the patch and the direction of the median motion vector. These frames do not have well defined axes because each ST chip is oriented differently in space-time, but they contain important spatiotemporal data. For simplicity, we define R = T . S T then has dimensions M ? N , where M = T M T and N = T N T . Ideally, the directions we have defined are the ones most likely to capture objects in motion. To see this, consider an object in motion in a fixed direction. An ST chip that is perpendicular to the direction of motion of the object is a plane that cuts through the cross-section of the object as it moves along time. A concrete example of this is given in <ref type="figure" target="#fig_2">Fig.  2</ref> for a video before its MSCN coefficients are computed.</p><p>ST-Chips are collected over all spatial patches of MSCN coefficients for every group of preceding T frames at each time instant T and are then aggregated. We use the Farneback <ref type="bibr" target="#b16">[17]</ref> optical flow algorithm in all our experiments. Due to the smoothness of motion over time and the fact that we expect these chips to have captured natural objects in motion, we hypothesize that this aggregated spatiotemporal data will follow a regular form of natural video statistics. Spatiotemporal distortions are expected to affect the optical flow as well as spatial data. The motion estimates from optical flow can be distorted for distorted videos, and hence we would expect to see a deviation from the statistics of chips computed from pristine videos. We would also expect to see a deviation in the statistics of the MSCN coefficients collected across time, as distortions affect their regularity across time and across space. Both the distortion of MSCN coefficients and the distortion of optical flow are expected to contribute to a deviation from the statistics of a pristine video, and we confirm this empirically, as shown in <ref type="figure" target="#fig_3">Figs. 3, 4</ref>, and 5. ST-Chips of MSCN coefficients are found to follow a generalized Gaussian distribution (GGD) of the form:</p><formula xml:id="formula_3">f (x; ?; ?) = ? 2??( 1 ? ) exp(?( |x| ? ) ? )<label>(4)</label></formula><p>where ?(.) is the gamma function:</p><formula xml:id="formula_4">?(?) = ? 0 t ??1 exp(?t)dt.<label>(5)</label></formula><p>The shape parameter ? of the GGD and the variance of the distribution are estimated using the moment-matching method described in <ref type="bibr" target="#b17">[18]</ref>. We also model the second-order statistics of ST-Chips. Define the collection of ST-Chips aggregated at each time instance T as S T , and define the pairwise products</p><formula xml:id="formula_5">H T (i, j) = S T (i, j)S T (i, j + 1) (6) V T (i, j) = S T (i, j)S T (i + 1, j) (7) D1 T (i, j) = S T (i, j)S T (i + 1, j + 1) (8) D2 T (i, j) = S T (i, j)S T (i + 1, j ? 1)<label>(9)</label></formula><p>These pairwise products of neighboring ST chip values along four orientations are modeled as following an asymmetric generalized Gaussian distribution (AGGD), which is given by:</p><formula xml:id="formula_6">f (x; ?, ? 2 l , ? 2 r ) = ? (? l +?r)?( 1 ? ) exp(?(? x ? l ) ? ) x &lt; 0 ? (? l +?r)?( 1 ? ) exp(?( x ?r ) ? ) x &gt; 0 (10) where ? l = ? l ?( 1 ? ) ?( 3 ? )<label>(11)</label></formula><formula xml:id="formula_7">? r = ? r ?( 1 ? ) ?( 3 ? )<label>(12)</label></formula><p>? controls the shape of the distribution and ? l and ? r control the spread on each side of the mode. The parameters (?, ?, ? 2 l , ? 2 r ) are extracted from the best AGGD fit to each pairwise product, where</p><formula xml:id="formula_8">? = (? r ? ? l ) ?( 2 ? ) ?( 1 ? ) .<label>(13)</label></formula><p>Videos are affected at multiple scales by distortions, and so all the features defined above are extracted at a reduced resolution as well. Each frame is low-pass filtered and downsampled by a factor of 2. Motion vectors are computed at the reduced scale and ST-Chips are extracted, as described previously, from volumes of MSCN coefficients at the reduced scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatiotemporal Gradient Chips</head><p>The spatial gradients of videos contain important information about edges and corners. Distortions are often more noticeable around edges and affect gradient fields. For this reason, we compute the magnitude of the gradient for each frame using the Sobel filter. We then extract ST-Chips from spatiotemporal volumes of the MSCN coefficients of the gradient magnitude at two scales. The first-order and secondorder statistics are modeled as GGD and AGGD, respectively, as described earlier for MSCN coefficients of pixel data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spatial features</head><p>Spatial features and a naturalness score are computed frameby-frame with the image naturalness index NIQE. These capture purely spatial aspects of distortion that may not be completely contained in ST-Chips, and hence boost performance. <ref type="table" target="#tab_0">Table I</ref> gives a summary of all the features used in the prototype algorithm, ChipQA-0. A total of 109 features are extracted at each time instance, starting from the T = 5 th frame. Note that the way in which ST-Chips are extracted could vary, and the use of optical flow in this prototype algorithm is just one of many ways in which these chips could be defined for the task of quality assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Quality assessment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Databases</head><p>We evaluated our algorithm on four databases, as described in the following section.</p><p>1) LIVE-APV Livestream VQA Database: This new database, which we call the LIVE-APV Livestream VQA database, and which will soon be released, was created for the purpose of developing tools for the quality assessment of live streamed videos. The database contains 367 videos, including both synthetically and authentically distorted videos. Of these, 52 videos are authentically distorted videos of different events. The remaining 315 videos were created by applying 6 different distortions on 45 unique contents. The 6 distortions are aliasing, compression, flicker, judder, interlacing, and frame drop. All the videos are of 4K resolution and were shown on a 4K TV in a human study in which 37 subjects participated.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain</head><p>Description Feature index ST-Chip Shape and scale parameters from GGD fits at two scales.</p><formula xml:id="formula_9">f 1 ? f 4 ST-Chip</formula><p>Four parameters from AGGD fitted to pairwise products at two scales. f 5 ? f 36 ST Gradient Chips Shape and scale parameters from GGD fits at two scales.</p><p>f 37 ? f 40 ST Gradient Chips Four parameters from AGGD fitted to pairwise products at two scales. f 41 ? f 72 Spatial Features and scores of spatial naturalness index NIQE.</p><p>f 73 ? f 109 2) Konvid-1k: Konvid-1k <ref type="bibr" target="#b18">[19]</ref> is a database of 1200 videos of user-generated content with authentic distortions. Many videos in this database do not have significant temporal variation, and it has been found that NR IQA algorithms applied frame-by-frame often achieve high performance on Konvid-1k without making use of temporal information. All videos are of resolution 960?540.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) LIVE Video Quality Challenge (VQC):</head><p>The LIVE VQC database <ref type="bibr" target="#b19">[20]</ref> has 585 videos of authentically distorted videos, each labeled by an average of 240 human opinion scores. The videos are of user-generated content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) LIVE Mobile Database:</head><p>The LIVE Mobile <ref type="bibr" target="#b20">[21]</ref> database has 200 distorted videos created from 10 reference videos. The synthetically applied distortions are compression, wirelesspacket loss, temporally varying compression levels, and framefreezes. This study was conducted on mobile and tablet devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Protocol</head><p>The LIVE-APV database contains a mix of synthetically and authentically distorted content. Each algorithm was tested on 1000 random train-test splits, where 80% of the data was used for training, and 20% for testing. 5-fold cross validation was performed while ensuring content separation between training and validation data for each fold. Videos of the same content were not allowed to mix between folds. On the other databases, we implemented a 80-20 train-test split. A support vector regressor (SVR) was used to learn mappings from features to mean opinion scores. Cross validation was used to find the best parameters for the SVR for each algorithm. NIQE and VIIDEO are completely blind algorithms and were not trained, but were evaluated against the test set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METHOD</head><p>SROCC/LCC NIQE <ref type="bibr" target="#b6">[7]</ref> 0.3559/0.3860 BRISQUE <ref type="bibr" target="#b5">[6]</ref> (1 fps) 0.5876/0.5989 HIGRADE <ref type="bibr" target="#b22">[23]</ref> (1 fps) 0.7310/0.7390 FRIQUEE <ref type="bibr" target="#b4">[5]</ref> (1 fps) 0.7414/0.7486 CORNIA <ref type="bibr" target="#b7">[8]</ref> (1 fps) 0.7685/0.7671 TLVQM <ref type="bibr" target="#b2">[3]</ref> 0.7749/0.7715 VIIDEO <ref type="bibr" target="#b1">[2]</ref> 0.3107/0.3269 V-BLIINDS <ref type="bibr" target="#b0">[1]</ref> 0.7127/0.7085 ChipQA-0 0.6973/0.6943</p><p>We report the Spearman's rank ordered correlation coefficient (SROCC) and the Pearsons linear correlation coefficient (LCC) between the scores predicted by the different algorithms and the subjective mean opinion scores. The predicted score was passed through the non-linearity described in <ref type="bibr" target="#b21">[22]</ref> before the LCC was computed. We report results for 1000 splits for all algorithms on the LIVE-APV database, and 100 splits on all other databases. Results are shown in Tables II, III, IV, and V. We did not compute NR IQA features for every frame of each video in the databases, but computed them for at least 1 frame every second for each video. We averaged the features obtained by the NR IQA algorithms across frames and trained them with an SVR to map to mean opinion scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METHOD</head><p>SROCC/LCC BRISQUE <ref type="bibr" target="#b5">[6]</ref> (1 fps) 0.4876/0.5215 VIIDEO <ref type="bibr" target="#b1">[2]</ref> 0.2751/0.3439 VBLIINDS <ref type="bibr" target="#b0">[1]</ref> 0.7960/0.8585 TLVQM <ref type="bibr" target="#b2">[3]</ref> 0.8247/0.8744 ChipQA-0 0.7898/0.8435</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance</head><p>ChipQA-0 performs better than competing algorithms on the new LIVE-APV database, in which there are a number of commonly occurring temporal distortions, such as interlacing, -0.0336/-0.0064 VBLIINDS <ref type="bibr" target="#b0">[1]</ref> 0.7005/0.7251 TLVQM <ref type="bibr" target="#b2">[3]</ref> 0.8026/0.7999 ChipQA-0 0.6692/0.6965  <ref type="bibr" target="#b7">[8]</ref> 1797 FRIQUEE <ref type="bibr" target="#b4">[5]</ref> 924000 VIIDEO <ref type="bibr" target="#b1">[2]</ref> 4950 VBLIINDS <ref type="bibr" target="#b0">[1]</ref> 10774 TLVQM <ref type="bibr" target="#b2">[3]</ref> 892 ChipQA-0 2284 judder, frame drop, and temporal variation of compression levels. We found the individual performance of each feature space on the LIVE-APV database, and the results are shown in <ref type="table" target="#tab_0">Table II</ref>. It is clear that ST-Chips and ST Gradient chips provide quality-aware information that boosts the performance of the algorithm on fast-moving, livestreamed videos. Studies have shown that UGC videos are dominated by spatial distortions <ref type="bibr" target="#b23">[24]</ref>. Both Konvid-1k and LIVE VQC are known to not contain significant temporal variation and therefore NR IQA algorithms perform well on them without the need for temporal information or processing. Nevertheless, ChipQA-0 achieves competitive performance on these databases as well. It also performs competitively on the LIVE Mobile database, which has a mix of spatial and temporal distortions. We perform a one-sided t test on the 1000 SROCCs of the various algorithms on the live streaming database with a 95% confidence level to evaluate the statistical signifiance of the results. The results show that ChipQA-0 is statistically superior to all other algorithms on the LIVE-APV Livestream VQA database. <ref type="table" target="#tab_0">Table VI</ref> shows the computation times required to extract features for each algorithm on a single 4K video from the LIVE-APV database. Costs for the IQA algorithms were estimated by multiplying the computation time for a single frame by the total number of frames in the video. VIIDEO, VBLIINDS, and ChipQA-0 were implemented with Python. The other algorithms were implemented with MATLAB ? . It is not possible to directly compare these numbers because they were implemented on different platforms with different optimization strategies, but they can serve as a rough estimate, and ChipQA-0 is reasonably efficient. It was run on an Intel i9 9820X CPU with 10 cores and a maximum frequency of IV. CONCLUSION We have proposed the novel concept of ST-Chips, and defined how they are extracted and described why they are relevant to video quality. We used the statistics of these chips to model 'naturalness' and deviations from naturalness, and proposed parameterized statistical fits to their statistics. We further used the parameters from these statistical fits to map videos to subjective opinions of video quality without explicitly finding distortion-specific features and without reference videos. We showed that our prototype distortion-agnostic, noreference video quality assessment algorithm, ChipQA-0, is highly competitive with other state-of-the-art models on a number of databases. We continue to refine the model, with one aim being to eliminate the need for an optical flow algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Computational cost</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ACKNOWLEDGMENTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>978- 1 -</head><label>1</label><figDesc>4799-7492-4/15/$31.00 ?2020 IEEE arXiv:2008.00031v3 [eess.IV] 23 Aug 2020</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Extracting ST-chips. On the left is a spatiotemporal volume of frames from time T ? T + 1 to T . The green arrows represent motion vectors at each spatial patch at time T . ST-chips are cut perpendicular to these across time and aggregated across spatial patches to form the frame on the right at each time instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>ST-Chips capture views of objects in motion. In this video, the person in the center moves to their right over time. Consequently, a chip taken in the proximity of their face over this duration and perpendicular to their motion captures their face itself.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Empirical distributions of ST-Chips. Pristine (original) distributions are in black and distorted distributions are in red. (a) Compressed and pristine(b) Frame Drop and pristine (c) Flicker and pristine (d) Judder and pristine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Empirical distributions of ST Gradient chips. Pristine (original) distributions are in black and distorted distributions are in red. (a) Aliased and pristine (b) Interlaced and pristine (c) Compressed and pristine (d) Frame drop and pristine Empirical distributions of paired products of ST-Chips. Pristine (original) distributions are in black and distorted distributions are in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DESCRIPTIONS</head><label>I</label><figDesc>OF FEATURES IN CHIPQA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II MEDIAN</head><label>II</label><figDesc>SROCC AND LCC FOR 1000 SPLITS ON THE LIVE-APV LIVESTREAM VQA DATABASE</figDesc><table><row><cell>METHOD</cell><cell cols="2">SROCC LCC</cell></row><row><cell>NIQE [7]</cell><cell>0.3395</cell><cell>0.4962</cell></row><row><cell>BRISQUE [6] (1 fps)</cell><cell>0.6224</cell><cell>0.6843</cell></row><row><cell cols="2">HIGRADE [23] (1 fps) 0.7159</cell><cell>0.7388</cell></row><row><cell>CORNIA [8] (1 fps)</cell><cell>0.6778</cell><cell>0.7076</cell></row><row><cell>TLVQM [3]</cell><cell>0.7597</cell><cell>0.7743</cell></row><row><cell>VIIDEO [2]</cell><cell>-0.0039</cell><cell>0.2155</cell></row><row><cell>V-BLIINDS [1]</cell><cell>0.7264</cell><cell>0.7646</cell></row><row><cell>Spatial</cell><cell>0.6770</cell><cell>0.7370</cell></row><row><cell>ST-Chips</cell><cell>0.6742</cell><cell>0.7235</cell></row><row><cell>ST Gradient Chips</cell><cell>0.7450</cell><cell>0.7611</cell></row><row><cell>ChipQA-0</cell><cell>0.7802</cell><cell>0.8054</cell></row><row><cell cols="2">TABLE III</cell><cell></cell></row><row><cell cols="3">MEDIAN SROCC AND LCC FOR 100 SPLITS ON THE KONVID DATABASE</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV MEDIAN</head><label>IV</label><figDesc>SROCC AND LCC FOR 100 SPLITS ON THE LIVE MOBILE DATABASE</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V MEDIAN</head><label>V</label><figDesc>SROCC AND LCC FOR 100 SPLITS ON THE LIVE VQC</figDesc><table><row><cell cols="2">DATABASE</cell></row><row><cell>METHOD</cell><cell>SROCC/LCC</cell></row><row><cell>BRISQUE [6] (1 fps)</cell><cell>0.6192/0.6519</cell></row><row><cell>VIIDEO [2]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI</head><label>VI</label><figDesc></figDesc><table><row><cell cols="2">COMPUTATION TIME FOR A SINGLE 3840X2160 VIDEO WITH 210 FRAMES</cell></row><row><cell cols="2">FROM THE LIVE-APV LIVESTREAM VQA DATABASE</cell></row><row><cell>METHOD</cell><cell>Time (s)</cell></row><row><cell>BRISQUE [6]</cell><cell>273</cell></row><row><cell cols="2">HIGRADE [23] 14490</cell></row><row><cell>CORNIA</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII RESULTS</head><label>VII</label><figDesc>OF ONE-SIDED T-TEST PERFORMED BETWEEN SROCC VALUES OF VARIOUS ALGORITHMS ON THE LIVE-APV DATABASE. '1' ('-1') INDICATES THAT THE ROW ALGORITHM IS STATISTICALLY SUPERIOR (INFERIOR) TO THE COLUMN ALGORITHM. THE MATRIX IS SYMMETRIC</figDesc><table><row><cell>METHOD</cell><cell cols="2">NIQE BRISQUE</cell><cell cols="3">HIGRADE CORNIA TLVQM</cell><cell cols="2">VIIDEO V-BLIINDS</cell><cell>ChipQA-0</cell></row><row><cell>NIQE</cell><cell>-</cell><cell>-1</cell><cell>-1</cell><cell>-1</cell><cell>-1</cell><cell>1</cell><cell>-1</cell><cell>-1</cell></row><row><cell>BRISQUE</cell><cell>1</cell><cell>-</cell><cell>-1</cell><cell>-1</cell><cell>-1</cell><cell>1</cell><cell>-1</cell><cell>-1</cell></row><row><cell>HIGRADE</cell><cell>1</cell><cell>1</cell><cell>-</cell><cell>1</cell><cell>-1</cell><cell>1</cell><cell>-1</cell><cell>-1</cell></row><row><cell>CORNIA</cell><cell>1</cell><cell>1</cell><cell>-1</cell><cell>-</cell><cell>-1</cell><cell>1</cell><cell>-1</cell><cell>-1</cell></row><row><cell>TLVQM</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell>-1</cell></row><row><cell>VIIDEO</cell><cell>-1</cell><cell>-1</cell><cell>-1</cell><cell>-1</cell><cell>-1</cell><cell>-</cell><cell>-1</cell><cell>-1</cell></row><row><cell>V-BLIINDS</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>-1</cell><cell>1</cell><cell>-</cell><cell>-1</cell></row><row><cell>ChipQA-0</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>-</cell></row><row><cell cols="5">4.1 GHz. All other algorithms were run on a AMD Ryzen 5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">3600 with a maximum frequency of 4.2 GHz.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors thank the Texas Advanced Computing Center (TACC) at The University of Texas at Austin for providing HPC resources that have contributed to the research results reported in this paper. URL: http://www.tacc.utexas.edu.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Blind prediction of natural video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1352" to="1365" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A completely blind video integrity oracle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Two-level approach for no-reference consumer video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5923" to="5938" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">UGC-VQA: Benchmarking blind video quality assessment for user generated content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14354</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Perceptual quality prediction on authentically distorted images using a bag of features approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vision</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="32" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Making a completely blind image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning framework for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1098" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Motion tuned spatio-temporal quality assessment of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="350" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Natural image statistics and divisive normalization: Modeling nonlinearity and adaptation in cortical neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="203" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video quality assessment based on motion structure partition similarity of spatiotemporal slice images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ViS3: an algorithm for video quality assessment via analysis of spatial and spatiotemporal slices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video quality assessment via gradient magnitude similarity deviation of spatial and spatiotemporal slices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mobile Devices and Multimedia: Enabling Technologies</title>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9411</biblScope>
			<biblScope unit="page" from="182" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A structural similarity metric for video based on motion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">869</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An optical flow-based full reference video quality assessment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Manasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Channappayya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2480" to="2492" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An optical flow-based no-reference video quality assessment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Manasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Channappayya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2400" to="2404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Very high accuracy velocity estimation using orientation tensors, parametric motion, and simultaneous segmentation of the motion field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farneback</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Computer Vision</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="171" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimation of shape parameter for generalized gaussian distributions in subband decompositions of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sharifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon-Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="56" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Konstanz natural video database (Konvid-1k)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jenadeleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Szir?nyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Quality of Multimedia Experience</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale study of perceptual video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="612" to="627" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video quality assessment on mobile devices: Subjective, behavioral and objective studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">De</forename><surname>Veciana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="652" to="671" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A statistical evaluation of recent full reference image quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Sabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3440" to="3451" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">No-reference quality assessment of tone-mapped HDR pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2957" to="2971" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A comparative evaluation of temporal pooling methods for blind video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10651</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
