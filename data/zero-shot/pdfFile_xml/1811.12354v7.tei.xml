<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Chen</surname></persName>
							<email>hchen@asapp.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
							<email>suhr@cs.cornell.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipendra</forename><surname>Misra</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
							<email>snavely@cs.cornell.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">ASAPP Inc</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science &amp; Cornell Tech</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of jointly reasoning about language and vision through a navigation and spatial reasoning task. We introduce the TOUCHDOWN task and dataset, where an agent must first follow navigation instructions in a real-life visual urban environment, and then identify a location described in natural language to find a hidden object at the goal position. The data contains 9,326 examples of English instructions and spatial descriptions paired with demonstrations. Empirical analysis shows the data presents an open challenge to existing methods, and qualitative linguistic analysis shows that the data displays richer use of spatial reasoning compared to related resources. 1 * Work done at Cornell University. <ref type="bibr" target="#b0">1</ref> The data is available at https://github.com/lil-lab/touchdown.</p><p>Turn and go with the flow of traffic. At the first traffic light turn left. Go past the next two traffic light, As you come to the third traffic light you will see a white building on your left with many American flags on it. Touchdown is sitting in the stars of the first flag.</p><p>the dumpster has a blue tarp draped over the end closest to you. touchdown is on the top of the blue tarp on the dumpster.</p><p>LINGUNET The model correctly predicts the location of Touchdown, putting most of the predicted distribution (green) on the top-left of the dumpster at the center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>TEXT2CONV The model incorrectly predicts the location of Touchdown to the top of the car on the far right. While some of the probability mass is correctly placed on the dumpster, the pixel with the highest probability is on the car.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>CONCATCONV The model correctly predicts the location of Touchdown. The distribution is heavily concentrated at a couple of nearby pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>CONCAT The prediction is similar to CONCATCONV.</p><p>3 <ref type="figure">Figure 9</ref>. Three of the models are doing fairly well. Only TEXT2CONV fails to predict the location of Touchdown.</p><p>turn to your right and you will see a green trash barrel between the two blue benches on the right. click to the base of the green trash barrel to find touchdown.</p><p>LINGUNET The model accurately predicts the green trash barrel on the right as Touchdown's location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>41</head><p>TEXT2CONV The model predicts successfully as well. The distribution is focused on a smaller area compared to LIN-GUNET closer to the top of the object. This possibly shows a learned bias towards placing Touchdown on the top of objects that TEXT2CONV is more suceptible to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>41</head><p>CONCATCONV The model prediction is correct. The distribution is focused on fewer pixels compared to LINGUNET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>41</head><p>CONCAT The model prediction is correct. Similar to CONCATCONV, it focuses on a few pixels. 41 <ref type="figure">Figure 10</ref>. All the models predict the location of Touchdown correctly. Trash can is a relatively common object that workers use to place Touchdown in the dataset .</p><p>on your right is a parking garage, there is a red sign with bikes parked out in front of the garage, the bear is on the red sign.</p><p>LINGUNET The model predicted the location of Touchdown correctly to the red stop sign on the right side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>59</head><p>TEXT2CONV The model predicts the location of Touchdown correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>59</head><p>CONCATCONV The model predicts the location of Touchdown correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>59</head><p>CONCAT The model predicts the location of Touchdown correctly.</p><p>59 <ref type="figure">Figure 11</ref>. All the models predict the location of Touchdown correctly. Reference to a red sign are relatively common in the data <ref type="figure">(Figure 8</ref>) potentially simplifying this prediction.</p><p>on your right is a parking garage, there is a red sign with bikes parked out in front of the garage, the bear is on the red sign.</p><p>LINGUNET The model misidentifies the red sign on the left hand side as the correct answer. It fails to resolve the spatial description, instead focusing on a more salient red object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>61</head><p>TEXT2CONV The model fails to predict the correct location, instead focusing on the red sign closer to the center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>61</head><p>CONCATCONV The model fails to predict the correct location, instead focusing on the red sign closer to the center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>61</head><p>CONCAT The model fails to predict the correct location, instead focusing on the red sign close to the center of the image. <ref type="figure">Figure 14</ref>. All the models fail to identify the correct location. They focus unanimously on the red sign on the left hand side. They all ignore the reference to the garage, which is hard to resolve visually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>61</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Consider the visual challenges of following natural language instructions in a busy urban environment. <ref type="figure" target="#fig_13">Figure 1</ref> illustrates this problem. The agent must identify objects and their properties to resolve mentions to traffic light and American flags, identify patterns in how objects are arranged to find the flow of traffic, and reason about how the relative position of objects changes as it moves to go past objects. Reasoning about vision and language has been studied extensively with various tasks, including visual question answering <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34]</ref>, visual navigation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25]</ref>, interactive question answering <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>, and referring expression resolution <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. However, existing work has largely focused on relatively simple visual input, including object-focused photographs <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28]</ref> or simulated environments <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref>. While this has enabled significant progress in visual understanding, the use of real-world visual input not only increases the challenge of the vision task, it also drastically changes the kind of language it elicits and requires fundamentally different reasoning. <ref type="bibr">Figure</ref> 1. An illustration of the task. The agent follows the instructions to reach the goal, starting by re-orientating itself (top image) and continuing by moving through the streets (two middle images). At the goal (bottom), the agent uses the spatial description (underlined) to locate Touchdown the bear. Touchdown only appears if the guess is correct (see bottom right detail).</p><p>In this paper, we study the problem of reasoning about vision and natural language using an interactive visual navigation environment based on Google Street View. <ref type="bibr" target="#b1">2</ref> We design the task of first following instructions to reach a goal position, and then resolving a spatial description at the goal by identifying the location in the observed image of Touchdown, a hidden teddy bear. Using this environment and task, we release TOUCHDOWN, <ref type="bibr" target="#b2">3</ref> a dataset for navigation and spatial reasoning with real-life observations.</p><p>We design our task for diverse use of spatial reasoning, including for following instructions and resolving the spatial descriptions. Navigation requires the agent to reason about its relative position to objects and how these relations change as it moves through the environment. In contrast, understanding the description of the location of Touchdown requires the agent to reason about the spatial relations between observed objects. The two tasks also diverge in their learning challenges. While in both learning requires relying on indirect supervision to acquire spatial knowledge and language grounding, for navigation, the training data includes demonstrated actions, and for spatial description resolution, annotated target locations. The task can be addressed as a whole, or decomposed to its two portions.</p><p>The key data collection challenge is designing a scalable process to obtain natural language data that reflects the richness of the visual input while discouraging overly verbose and unnatural language. In our data collection process, workers write and follow instructions. The writers navigate in the environment and hide Touchdown. Their goal is to make sure the follower can execute the instruction to find Touchdown. The measurable goal allows us to reward effective writers, and discourages overly verbose descriptions.</p><p>We collect 9,326 examples of the complete task, which decompose to the same number of navigation tasks and 27,575 spatial description resolution (SDR) tasks. Each example is annotated with a navigation demonstration and the location of Touchdown. Our linguistically-driven analysis shows the data requires significantly more complex reasoning than related datasets. Nearly all examples require resolving spatial relations between observable objects and between the agent and its surroundings, and each example contains on average 5.3 commands and refers to 10.7 unique entities in its environment.</p><p>We empirically study the navigation and SDR tasks independently. For navigation, we focus on the performance of existing models trained with supervised learning. For SDR, we cast the problem of identifying Touchdown's location as an image feature reconstruction problem using a languageconditioned variant of the UNET architecture <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b24">25]</ref>. This approach significantly outperforms several strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work and Datasets</head><p>Jointly reasoning about vision and language has been studied extensively, most commonly focusing on static visual input for reasoning about image captions <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b2">3</ref> Touchdown is the unofficial mascot of Cornell University. <ref type="bibr" target="#b31">32</ref>] and grounded question answering <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b33">34]</ref>. Recently, the problem has been studied in interactive simulated environments where the visual input changes as the agent acts, such as interactive question answering <ref type="bibr">[9, 12, ]</ref> and instruction following <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. In contrast, we focus on an interactive environment with real-world observations.</p><p>The most related resources to ours are R2R <ref type="bibr" target="#b1">[2]</ref> and Talk the Walk <ref type="bibr" target="#b9">[10]</ref>. R2R uses panorama graphs of house environments for the task of navigation instruction following. It includes 90 unique environments, each containing an average of 119 panoramas, significantly smaller than our 29,641 panoramas. Our larger environment requires following the instructions closely, as finding the goal using search strategies is unlikely, even given a large number of steps. We also observe that the language in our data is significantly more complex than in R2R (Section 5). Our environment setup is related to Talk the Walk, which uses panoramas in small urban environments for a navigation dialogue task. In contrast to our setup, the instructor does not observe the panoramas, but instead sees a simplified diagram of the environment with a small set of pre-selected landmarks. As a result, the instructor has less spatial information compared to TOUCH-DOWN. Instead the focus is on conversational coordination.</p><p>SDR is related to the task of referring expression resolution, for example as studied in ReferItGame <ref type="bibr" target="#b15">[16]</ref> and Google Refexp <ref type="bibr" target="#b21">[22]</ref>. Referring expressions describe an observed object, mostly requiring disambiguation between the described object and other objects of the same type. In contrast, the goal of SDR is to describe a specific location rather than discriminating. This leads to more complex language, as illustrated by the comparatively longer sentences of SDR (Section 5). Kitaev and Klein <ref type="bibr" target="#b17">[18]</ref> proposed a similar task to SDR, where given a spatial description and a small set of locations in a fully-observed simulated 3D environment, the system must select the location described from the set. We do not use distractor locations, requiring a system to consider all areas of the image to resolve a spatial description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Environment and Tasks</head><p>We use Google Street View to create a large navigation environment. Each position includes a 360 ? RGB panorama. The panoramas are connected in a graph-like structure with undirected edges connecting neighboring panoramas. Each edge connects to a panorama in a specific heading. For each panorama, we render perspective images for all headings that have edges. Our environment includes 29,641 panoramas and 61,319 edges from New York City. <ref type="figure" target="#fig_14">Figure 2</ref> illustrates the environment.</p><p>We design two tasks: navigation and spatial description resolution (SDR). Both tasks require recognizing objects and the spatial relations between them. Navigation focuses on egocentric spatial reasoning, where instructions refer to the agent's relationship with its environment, including the  <ref type="figure" target="#fig_14">Figure 2</ref>. An illustration of the environment. Left: part of the graph structure with polarly projected panoramas illustrating positions linked by edges, each labeled with its heading. Heading angles shown closer to each panorama represent the outgoing angle from that panorama; for example, the heading from Pano A to Pano B is 31 ? . Right: the area in New York City covered by the graph. objects it observes. The SDR task displays more allocentric reasoning, where the language requires understanding the relations between the observed objects to identify the target location. While navigation requires generating a sequence of actions from a small set of possible actions, SDR requires choosing a specific pixel in the observed image. Both tasks present different learning challenges. The navigation task could benefit from reward-based learning, while the SDR task defines a supervised learning problem. The two tasks can be addressed separately, or combined by completing the SDR task at the goal position at the end of the navigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Navigation</head><p>The agent's goal is to follow a natural language instruction and reach a goal position. Let S be the set of all states. A state s ? S is a pair (I, ?), where I is a panorama and ? is the heading angle indicating the agent heading. We only allow states where there is an edge connecting to a neighboring panorama in the heading ?. Given a navigation instructionx n and a start state s 1 ? S, the agent performs a sequence of actions. The set of actions A is {FORWARD, LEFT, RIGHT, STOP}. Given a state s and an action a ? A, the state is deterministically updated using a transition function T : S ? A ? S. The FORWARD action moves the agent along the edge in its current heading. Formally, if the environment includes the edge</p><formula xml:id="formula_0">(I i , I j ) at head- ing ? in I i , the transition is T ((I i , ?), FORWARD) = (I j , ? ).</formula><p>The new heading ? is the heading of the edge in I j with the closest heading to ?. The LEFT (RIGHT) action changes the agent heading to the heading of the closest edge on the left (right). Formally, if the position panorama I has edges at headings ? &gt; ? &gt; ? , T ((I, ?), LEFT) = (I, ? ) and T ((I, ?), RIGHT) = (I, ? ). Given a start state s 1 and a navigation instructionx n , an execution? is a sequence of state-action pairs (s 1 , a 1 ), ..., (s m , a m ) , where T (s i , a i ) = s i+1 and a m = STOP.</p><p>Evaluation We use three evaluation metrics: task completion, shortest-path distance, and success-weighted edit distance. Task completion (TC) measures the accuracy of completing the task correctly. We consider an execution correct if the agent reaches the exact goal position or one of its neighboring nodes in the environment graph. Shortest-path distance (SPD) measures the mean distance in the graph between the agent's final panorama and the goal. SPD ignores turning actions and the agent heading. Success weighted by edit distance (SED) is</p><formula xml:id="formula_1">1 N N i=1 S i (1 ? lev(?,?)</formula><p>max(|?|,|?|) ), where the summation is over N examples, S i is a binary task completion indicator,? is the reference execution,? is the predicted execution, lev(?, ?) is the Levenshtein edit distance, and | ? | is the execution length. The edit distance is normalized and inversed. We measure the distance and length over the sequence of panoramas in the execution, and ignore changes of orientation. SED is related to success weighted by path length (SPL) <ref type="bibr" target="#b0">[1]</ref>, but is designed for instruction following in graphbased environments, where a specific correct path exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatial Description Resolution (SDR)</head><p>Given an image I and a natural language descriptionx s , the task is to identify the point in the image that is referred to by the description. We instantiate this task as finding the location of Touchdown, a teddy bear, in the environment. Touchdown is hidden and not visible in the input. The image I is a 360 ? RGB panorama, and the output is a pair of (x, y) coordinates specifying a location in the image.</p><p>Evaluation We use three evaluation metrics: accuracy, consistency, and distance error. Accuracy is computed with regard to an annotated location. We consider a prediction as correct if the coordinates are within a slack radius of the annotation. We measure accuracy for radiuses of 40, 80, and 120 pixels and use euclidean distance. Our data collection process results in multiple images for each sentence. We use this to measure consistency over unique sentences, which is measured similar to accuracy, but with a unique sentence considered correct only if all its examples are correct <ref type="bibr" target="#b10">[11]</ref>. We compute consistency for each slack value. We also measure the mean euclidean distance between the annotated location and the predicted location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Data Collection</head><p>We frame the data collection process as a treasure-hunt task where a leader hides a treasure and writes directions to find it, and a follower follows the directions to find the treasure. The process is split into four crowdsourcing tasks ( <ref type="figure">Figure 3</ref>). The two main tasks are writing and following. In the writing task, a leader follows a prescribed route and hides Touchdown the bear at the end, while writing instructions that describe the path and how to find Touchdown.</p><p>Task I: Instruction Writing The worker starts at the beginning of the route facing north (a). The prescribed route is shown in the overhead map (bottom left of each image). The worker faces the correct direction and follows the path, while writing instructions that describe these actions (b). After following the path, the worker reaches the goal position, places Touchdown, and completes writing the instructions (c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Place Touchdown</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Can't Place Touchdown</head><p>Turn so that the trees are to your left. At the first intersection, turn left and stop. Touchdown is on top of the blue mailbox on the right hand corner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Place Touchdown</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Can't Place Touchdown</head><p>Turn so that the trees are to your left. At the first intersection, turn left and stop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Place Touchdown</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Can't Place Touchdown</head><p>Turn so that the trees are to your left. At the first intersection, turn left and stop. Touchdown is on top of the blue mailbox on the right hand corner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Place Touchdown</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bear is Occluded</head><p>Turn so that the trees are to your left. At the first intersection, turn left and stop. Touchdown is on top of the blue mailbox on the right hand corner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remaining</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attempts: 2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>You Found Touchdown!</head><p>Touchdown is on top of the blue mailbox on the right hand corner.</p><p>Turn so that the trees are to your left. At the first intersection, turn left and stop. Touchdown is on top of the blue mailbox on the right hand corner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Location</head><p>Instructions:</p><formula xml:id="formula_2">Submit (a) (b) (c)</formula><p>Place Touchdown</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Can't Place Touchdown</head><p>Turn so that the trees are to your left. At the first intersection, turn left and stop. Touchdown is on top of the blue mailbox on the right hand corner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Place Touchdown</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Can't Place Touchdown</head><p>Turn so that the trees are to your left. At the first intersection, turn left and stop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Place Touchdown</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Can't Place Touchdown</head><p>Turn so that the trees are to your left. At the first intersection, turn left and stop.</p><p>Touchdown is on top of the blue mailbox on the right hand corner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Place Touchdown</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bear is Occluded</head><p>Turn so that the trees are to your left. At the first intersection, turn left and stop. Touchdown is on top of the blue mailbox on the right hand corner. Task IV: Instruction Segmentation The instructions are shown (left). The worker highlights segments corresponding to the navigation and target location subtasks. The highlighted segment is shown to the worker (right). <ref type="figure">Figure 3</ref>. Illustration of the data collection process.</p><p>The following task requires following the instructions from the same starting position to navigate and find Touchdown. Additional tasks are used to segment the instructions into the navigation and target location tasks, and to propagate Touchdown's location to panoramas that neighbor the final panorama. We use a customized Street View interface for data collection. However, the final data uses a static set of panoramas that do not require the Street View interface.</p><p>Task I: Instruction Writing We generate routes by sampling start and end positions. The sampling process results in routes that often end in the middle of a city block. This encourages richer language, for example by requiring to describe the goal position rather than simply directing to the next intersection. The route generation details are described in the Supplementary Material. For each task, the worker is placed at the starting position facing north, and asked to follow a route specified in an overhead map view to a goal position. Throughout, they write instructions describing the path. The initial heading requires the worker to re-orient to the path, and thereby familiarize with their surroundings better. It also elicits interesting re-orientation instructions that often include references to the direction of objects (e.g., flow of traffic) or their relation to the agent (e.g., the umbrellas are to the right). At the goal panorama, the worker is asked to place Touchdown in a location of their choice that is not a moving object (e.g., a car or pedestrian) and to describe the location in their instructions. The worker goal is to write instructions that a human follower can use to correctly navigate and locate the target without knowing the correct path or location of Touchdown. They are not permitted to write instructions that refer to text in the images, including street names, store names, or numbers.</p><p>Task II: Target Propagation to Panoramas The writing task results in the location of Touchdown in a single panorama in the Street View interface. However, resolving the spatial description to the exact location is also possible from neighboring panoramas where the target location is visible. We use a crowdsourcing task to propagate the loca-Orient yourself in the direction of the red ladder. Go straight and take a left at the intersection with islands. Take another left at the intersection with a gray trash can to the left. Go straight until near the end of the fenced in playground and court to the right near the end of the fenced in playground and court to the right. Touchdown is on the last basketball hoop to the right.  tion of Touchdown to neighboring panoramas in the Street View interface, and to the identical panoramas in our static data. This allows to complete the task correctly even if not stopping at the exact location, but still reaching a semantically equivalent position. The propagation in the Street View interface is used for our validation task. The task includes multiple steps. At each step, we show the instruction text and the original Street View panorama with Touchdown placed, and ask for the location for a single panorama, either from the Street View interface or from our static images. The worker can indicate if the target is occluded. The propagation annotation allows us to create multiple examples for each SDR, where each example uses the same SDR but shows the environment from a different position.</p><p>Task III: Validation We use a separate task to validate each instruction. The worker is asked to follow the instruction in the customized Street View interface and find Touchdown. The worker sees only the Street View interface, and has no access to the overhead map. The task requires navigation and identifying the location of Touchdown. It is completed correctly if the follower clicks within a 90-pixel radius 4 of the ground truth target location of Touchdown. This requires the follower to be in the exact goal panorama, or in one of the neighboring panoramas we propagated the location to. The worker has five attempts to find Touchdown. Each attempt is a click. If the worker fails, we create another task for the same example to attempt again. If the second worker fails as well, the example is discarded.</p><p>Task IV: Segmentation We annotate each token in the instruction to indicate if it describes the navigation or SDR tasks. This allows us to address the tasks separately. First, a worker highlights a consecutive prefix of tokens to indicate the navigation segment. They then highlight a suffix of tokens for the SDR task. The navigation and target location segments may overlap <ref type="figure" target="#fig_1">(Figure 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Workers and Qualification</head><p>We require passing a qualification task to do the writing task. The qualifier task requires   correctly navigating and finding Touchdown for a predefined set of instructions. We consider workers that succeed in three out of the four tasks as qualified. The other three tasks do not require qualification. <ref type="table">Table 1</ref> shows how many workers participated in each task.</p><p>Payment and Incentive Structure The base pay for instruction writing is $0.60. For target propagation, validation, and segmentation we paid $0.15, $0.25, and $0.12. We incentivize the instruction writers and followers with a bonus system. For each instruction that passes validation, we give the writer a bonus of $0.25 and the follower a bonus of $0.10. Both sides have an interest in completing the task correctly. The size of the graph makes it difficult, and even impossible, for the follower to complete the task and get the bonus if the instructions are wrong.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Data Statistics and Analysis</head><p>Workers completed 11,019 instruction-writing tasks, and 12,664 validation tasks. 89.1% examples were correctly validated, 80.1% on the first attempt and 9.0% on the second. <ref type="bibr" target="#b4">5</ref> While we allowed five attempts at finding Touchdown during validation tasks, 64% of the tasks required a single attempt. The value of additional attempts decayed quickly: only 1.4% of the tasks were only successful after five attempts. For the full task and navigation-only, TOUCH-DOWN includes 9,326 examples with 6,526 in the training set, 1,391 in the development set, and 1,409 in the test set. For the SDR task, TOUCHDOWN includes 9,326 unique descriptions and 25,575 examples with 17,880 for training, 3,836 for development, and 3,859 for testing. We use our initial paths as gold-standard demonstrations, and the placement of Touchdown by the original writer as the reference location. <ref type="table" target="#tab_3">Table 2</ref> shows basic data statistics. The mean instruction length is 108.0 tokens. The average overlap between navigation and SDR is 11.4 tokens. <ref type="figure">Figure 5</ref> shows the distribution of text lengths. Overall, TOUCHDOWN contains a larger vocabulary and longer navigation instructions than related corpora. The paths in TOUCHDOWN are longer than in R2R <ref type="bibr" target="#b1">[2]</ref>, on average 35.2 panoramas compared to 6.0. SDR segments have a mean length of 29.8 tokens, longer than in common referring expression datasets; Refer-ItGame <ref type="bibr" target="#b15">[16]</ref> expressions 4.4 tokens on average and Google RefExp <ref type="bibr" target="#b21">[22]</ref> expressions are 8.5.</p><p>We perform qualitative linguistic analysis of TOUCH-DOWN to understand the type of reasoning required to solve the navigation and SDR tasks. We identify a set of phenomena, and randomly sample 25 examples from the development set, annotating each with the number of times each phenomenon occurs in the text. <ref type="table">Table 3</ref> shows results comparing TOUCHDOWN with R2R. <ref type="bibr" target="#b5">6</ref> Sentences in TOUCH-DOWN refer to many more unique, observable entities (10.7 vs 3.7), and almost all examples in TOUCHDOWN include coreference to a previously-mentioned entity. More examples in TOUCHDOWN require reasoning about counts, sequences, comparisons, and spatial relationships of objects. Correct execution in TOUCHDOWN requires taking actions only when certain conditions are met, and ensuring that the agent's observations match a described scene, while this is rarely required in R2R. Our data is rich in spatial reasoning. We distinguish two types: between multiple objects (allocentric) and between the agent and its environment (egocentric). We find that navigation segments contain more egocentric spatial relations than SDR segments, and SDR segments require more allocentric reasoning. This corresponds to the two tasks: navigation mainly requires moving the agent relative to its environment, while SDR requires resolving a point in space relative to other objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Spatial Reasoning with LINGUNET</head><p>We cast the SDR task as a language-conditioned image reconstruction problem, where we predict a distribution of the location of Touchdown over the entire observed image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Model</head><p>We use the LINGUNET architecture <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b4">5]</ref>, which was originally introduced for goal prediction and planning in instruction following. LINGUNET is a language-conditioned variant of the UNET architecture <ref type="bibr" target="#b28">[29]</ref>, an image-to-image encoder-decoder architecture widely used for image segmentation. LINGUNET incorporates language into the image reconstruction phase to fuse the two modalities. We <ref type="bibr" target="#b5">6</ref> See the Supplementary Material for analysis of SAIL and LANI.                modify the architecture to predict a probability distribution over the input panorama image. We process the description text tokensx s = x 1 , x 2 , . . . , x l using a bi-directional Long Short-term Memory (LSTM) recurrent neural network to generate l hidden states. The forward computation is h f</p><formula xml:id="formula_3">L x v H u N 2 z S d s = " &gt; A A A C L X i c b V D L S s N A F J 3 4 r P H R q k s 3 g 0 V w V R I R d C m 4 c S F S w a r Q h D K Z 3 N b B m U m Y u V F L y J e 4 1 b V f 4 0 I Q t / 6 G 0 9 q F V g 9 c O J x z X 5 w k l 8 J i E L x 5 M 7 N z 8 w u L t S V / e W V 1 r d 5 Y 3 7 i 0 W W E 4 d H g m M 3 O d M A t S a O i g Q A n X u Q G m E g l X y e 3 x y L + 6 A 2 N F p i 9 w m E O s 2 E C L v u A M n d R r 1 C O E B y x P h R 5 0 z g C r X q M Z t I I x 6 F 8 S T k i T T N D u r X s L U Z r x Q o F G L p m 1 3 T D I M S 6 Z Q c E l V H 5 U W M g Z v 2 U D 6 D q q m Q I b l + P P K 7 r j l J T 2 M + N K I x 2 r P y d K p q w d q s R 1 K o Y 3 d t o b i f 9 6 q R 0 t n L q O / c O 4 F D o v E D T / P t 4 v J M W M j q K h q T D A U Q 4 d Y d w I 9 z / l N 8 w w j i 5 A 3 4 8 M a L j n m V J M p 2 X E q 2 4 Y l 2 V k F G 2 G V e W</formula><formula xml:id="formula_4">L x v H u N 2 z S d s = " &gt; A A A C L X i c b V D L S s N A F J 3 4 r P H R q k s 3 g 0 V w V R I R d C m 4 c S F S w a r Q h D K Z 3 N b B m U m Y u V F L y J e 4 1 b V f 4 0 I Q t / 6 G 0 9 q F V g 9 c O J x z X 5 w k l 8 J i E L x 5 M 7 N z 8 w u L t S V / e W V 1 r d 5 Y 3 7 i 0 W W E 4 d H g m M 3 O d M A t S a O i g Q A n X u Q G m E g l X y e 3 x y L + 6 A 2 N F p i 9 w m E O s 2 E C L v u A M n d R r 1 C O E B y x P h R 5 0 z g C r X q M Z t I I x 6 F 8 S T k i T T N D u r X s L U Z r x Q o F G L p m 1 3 T D I M S 6 Z Q c E l V H 5 U W M g Z v 2 U D 6 D q q m Q I b l + P P K 7 r j l J T 2 M + N K I x 2 r P y d K p q w d q s R 1 K o Y 3 d t o b i f 9 6 q R 0 t n L q O / c O 4 F D o v E D T / P t 4 v J M W M j q K h q T D A U Q 4 d Y d w I 9 z / l N 8 w w j i 5 A 3 4 8 M a L j n m V J M p 2 X E q 2 4 Y l 2 V k F G 2 G V e W</formula><formula xml:id="formula_5">L x v H u N 2 z S d s = " &gt; A A A C L X i c b V D L S s N A F J 3 4 r P H R q k s 3 g 0 V w V R I R d C m 4 c S F S w a r Q h D K Z 3 N b B m U m Y u V F L y J e 4 1 b V f 4 0 I Q t / 6 G 0 9 q F V g 9 c O J x z X 5 w k l 8 J i E L x 5 M 7 N z 8 w u L t S V / e W V 1 r d 5 Y 3 7 i 0 W W E 4 d H g m M 3 O d M A t S a O i g Q A n X u Q G m E g l X y e 3 x y L + 6 A 2 N F p i 9 w m E O s 2 E C L v u A M n d R r 1 C O E B y x P h R 5 0 z g C r X q M Z t I I x 6 F 8 S T k i T T N D u r X s L U Z r x Q o F G L p m 1 3 T D I M S 6 Z Q c E l V H 5 U W M g Z v 2 U D 6 D q q m Q I b l + P P K 7 r j l J T 2 M + N K I x 2 r P y d K p q w d q s R 1 K o Y 3 d t o b i f 9 6 q R 0 t n L q O / c O 4 F D o v E D T / P t 4 v J M W M j q K h q T D A U Q 4 d Y d w I 9 z / l N 8 w w j i 5 A 3 4 8 M a L j n m V J M p 2 X E q 2 4 Y l 2 V k F G 2 G V e W</formula><formula xml:id="formula_6">L x v H u N 2 z S d s = " &gt; A A A C L X i c b V D L S s N A F J 3 4 r P H R q k s 3 g 0 V w V R I R d C m 4 c S F S w a r Q h D K Z 3 N b B m U m Y u V F L y J e 4 1 b V f 4 0 I Q t / 6 G 0 9 q F V g 9 c O J x z X 5 w k l 8 J i E L x 5 M 7 N z 8 w u L t S V / e W V 1 r d 5 Y 3 7 i 0 W W E 4 d H g m M 3 O d M A t S a O i g Q A n X u Q G m E g l X y e 3 x y L + 6 A 2 N F p i 9 w m E O s 2 E C L v u A M n d R r 1 C O E B y x P h R 5 0 z g C r X q M Z t I I x 6 F 8 S T k i T T N D u r X s L U Z r x Q o F G L p m 1 3 T D I M S 6 Z Q c E l V H 5 U W M g Z v 2 U D 6 D q q m Q I b l + P P K 7 r j l J T 2 M + N K I x 2 r P y d K p q w d q s R 1 K o Y 3 d t o b i f 9 6 q R 0 t n L q O / c O 4 F D o v E D T / P t 4 v J M W M j q K h q T D A U Q 4 d Y d w I 9 z / l N 8 w w j i 5 A 3 4 8 M a L j n m V J M p 2 X E q 2 4 Y l 2 V k F G 2 G V e W</formula><formula xml:id="formula_7">K i B Q V 7 1 N 2 b L 5 0 J j 1 u T a c U W v C E J Q = " &gt; A A A C K H i c b V B N S 8 N A F N x U r R q / q h 6 9 L A b B U 0 l E 0 G N B E I 8 K V g t N K J v N i 1 2 6 u w m 7 G 6 W E / A 2 v e v b X e B O v / h I 3 N Q d t H V g Y Z t 7 b N 0 y c c 6 a N 7 3 8 6 r a X l l f b q 2 r q 7 s b m 1 v d P Z 3 b v T W a E o 9 G n G M z W I i Q b O J P Q N M x w G u Q I i Y g 7 3 8 e S i 9 u 8 f Q W m W y V s z z S E S 5 E G y l F F i r B S G g p h x n J a X 1 c g f d T y / 6 8 + A F 0 n Q E A 8 1 u B 7 t O u 0 w y W g h Q B r K i d b D w M 9 N V B J l G O V Q u W G h I S d 0 Q h 5 g a K k k A n R U z k J X + M g q C U 4 z Z Z 8 0 e K b + 3 i i J 0 H o q Y j t Z h 9 T z X i 3 + 6 y W 6 / n D u u k n P o 5 L J v D A g 6 c / x t O D Y Z L h u B S d M A T V 8 a g m h i t n 8 m I 6 J I t T Y 7 l w 3 V C D h i W Z C E J m U I a 2 G Q V S W o R L Y C 6 r K t c 0 F 8 z 0 t k r u T b u B 3 g 5 t T r + c 3 H a 6 h A 3 S I j l G A z l A P X a F r 1 E c U 5 e g Z v</formula><formula xml:id="formula_8">K i B Q V 7 1 N 2 b L 5 0 J j 1 u T a c U W v C E J Q = " &gt; A A A C K H i c b V B N S 8 N A F N x U r R q / q h 6 9 L A b B U 0 l E 0 G N B E I 8 K V g t N K J v N i 1 2 6 u w m 7 G 6 W E / A 2 v e v b X e B O v / h I 3 N Q d t H V g Y Z t 7 b N 0 y c c 6 a N 7 3 8 6 r a X l l f b q 2 r q 7 s b m 1 v d P Z 3 b v T W a E o 9 G n G M z W I i Q b O J P Q N M x w G u Q I i Y g 7 3 8 e S i 9 u 8 f Q W m W y V s z z S E S 5 E G y l F F i r B S G g p h x n J a X 1 c g f d T y / 6 8 + A F 0 n Q E A 8 1 u B 7 t O u 0 w y W g h Q B r K i d b D w M 9 N V B J l G O V Q u W G h I S d 0 Q h 5 g a K k k A n R U z k J X + M g q C U 4 z Z Z 8 0 e K b + 3 i i J 0 H o q Y j t Z h 9 T z X i 3 + 6 y W 6 / n D u u k n P o 5 L J v D A g 6 c / x t O D Y Z L h u B S d M A T V 8 a g m h i t n 8 m I 6 J I t T Y 7 l w 3 V C D h i W Z C E J m U I a 2 G Q V S W o R L Y C 6 r K t c 0 F 8 z 0 t k r u T b u B 3 g 5 t T r + c 3 H a 6 h A 3 S I j l G A z l A P X a F r 1 E c U 5 e g Z v</formula><formula xml:id="formula_9">K i B Q V 7 1 N 2 b L 5 0 J j 1 u T a c U W v C E J Q = " &gt; A A A C K H i c b V B N S 8 N A F N x U r R q / q h 6 9 L A b B U 0 l E 0 G N B E I 8 K V g t N K J v N i 1 2 6 u w m 7 G 6 W E / A 2 v e v b X e B O v / h I 3 N Q d t H V g Y Z t 7 b N 0 y c c 6 a N 7 3 8 6 r a X l l f b q 2 r q 7 s b m 1 v d P Z 3 b v T W a E o 9 G n G M z W I i Q b O J P Q N M x w G u Q I i Y g 7 3 8 e S i 9 u 8 f Q W m W y V s z z S E S 5 E G y l F F i r B S G g p h x n J a X 1 c g f d T y / 6 8 + A F 0 n Q E A 8 1 u B 7 t O u 0 w y W g h Q B r K i d b D w M 9 N V B J l G O V Q u W G h I S d 0 Q h 5 g a K k k A n R U z k J X + M g q C U 4 z Z Z 8 0 e K b + 3 i i J 0 H o q Y j t Z h 9 T z X i 3 + 6 y W 6 / n D u u k n P o 5 L J v D A g 6 c / x t O D Y Z L h u B S d M A T V 8 a g m h i t n 8 m I 6 J I t T Y 7 l w 3 V C D h i W Z C E J m U I a 2 G Q V S W o R L Y C 6 r K t c 0 F 8 z 0 t k r u T b u B 3 g 5 t T r + c 3 H a 6 h A 3 S I j l G A z l A P X a F r 1 E c U 5 e g Z v</formula><formula xml:id="formula_10">K i B Q V 7 1 N 2 b L 5 0 J j 1 u T a c U W v C E J Q = " &gt; A A A C K H i c b V B N S 8 N A F N x U r R q / q h 6 9 L A b B U 0 l E 0 G N B E I 8 K V g t N K J v N i 1 2 6 u w m 7 G 6 W E / A 2 v e v b X e B O v / h I 3 N Q d t H V g Y Z</formula><formula xml:id="formula_11">i = BiLSTM(?(x i ), h f i?1 ), i = 1, . . . , l,</formula><p>where ? is a learned word embedding function. We compute the backward hidden states h b i similarly. The text representation is an average of the concatenated hidden states</p><formula xml:id="formula_12">x = 1 l l i=1 [h f i ; h b i ].</formula><p>We map the RGB panorama I to a feature representation F 0 with a pre-trained RESNET18 <ref type="bibr" target="#b13">[14]</ref>.</p><p>LINGUNET performs m levels of convolution and deconvolution operations. We generate a sequence of feature maps F k = CNN k (F k?1 ), k = 1, . . . , m with learned convolutional layers CNN k . We slice the text representation x to m equal-sized slices, and reshape each with a linear projection to a 1 ? 1 filter K k . We convolve each feature map F k with K k to obtain a text-conditioned feature map G k = CONV(K k , F k ). We use m deconvolution operations to generate feature maps of increasing size to create H 1 :</p><formula xml:id="formula_13">H k = DECONV k ([H k+1 ; G k ]), if k = 1, . . . , m ? 1 DECONV k (G k ), if k = m .</formula><p>We compute a single value for each pixel by projecting the channel vector for each pixel using a single-layer perceptron with a ReLU non-linearity. Finally, we compute a probability distribution over the feature map using a SOFTMAX. The predicted location is the mode of the distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Experimental Setup</head><p>The evaluation metrics are described in Section 3.2 and the data in Section 5.</p><p>Learning We use supervised learning. The gold label is a Gaussian smoothed distribution. The coordinate of the maximal value of the distribution is the exact coordinate where Touchdown is placed. We minimize the KL-divergence between the Gaussian and the predicted distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Systems</head><p>We evaluate three non-learning baselines: (a) RANDOM: predict a pixel at random; (b) CENTER: predict  <ref type="table">Table 3</ref>. Linguistic analysis of 25 randomly sampled development examples in TOUCHDOWN and R2R. We annotate each example for the presence and count of each phenomenon. We distinguish statistics for the entire text, navigation, and SDR segments in TOUCHDOWN. c is the number of instructions out of the 25 containing at least one example of the phenomenon; ? is the mean number of times each phenomenon appears in each of the 25 instructions.</p><p>the center pixel; (c) AVERAGE: predict the average pixel, computed over the training set. In addition to a two-level LINGUNET (m = 2), we evaluate three learning baselines: CONCAT, CONCATCONV, and TEXT2CONV. The first two compute a RESNET18 feature map representation of the image and then fuse it with the text representation to compute pixel probabilities. The third uses the text to compute kernels to convolve over the RESNET18 image representation.</p><p>The Supplementary Material provides further details. <ref type="table">Table 4</ref> shows development and test results. The low performance of the non-learning baselines illustrates the challenge of the task. We also experiment with a UNET architecture that is similar to our LINGUNET but has no access to the language. This result illustrates that visual biases exist in the data, but only enable relatively low performance. All the learning systems outperform the non-learning baselines and the UNET, with LINGUNET performing best. <ref type="figure" target="#fig_19">Figure 7</ref> shows pixel-level predictions using LIN-GUNET. The distribution prediction is visualized as a heatmap overlaid on the image. LINGUNET often successfully solves descriptions anchored in objects that are unique in the image, such the fire hydrant at the top image. The lower example is more challenging. While the model correctly reasons that Touchdown is on a light just above the doorway, it fails to find the exact door. Instead, the probability distribution is shared between multiple similar locations, the space above three other doors in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Navigation Baselines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Methods and Setup</head><p>We evaluate three non-learning baselines: (a) STOP: agent stops immediately; (b) RANDOM: Agent samples  <ref type="table">Table 4</ref>. Development and test results on the SDR task. We report accuracy/consistency (A/C) with different thresholds (40, 80, and 120) and mean distance error.</p><p>non-stop actions uniformly until reaching the action horizon; and (c) FREQUENT: agent always takes the most frequent action in the training set (FORWARD). We also evaluate two recent navigation models: (a) GA: gatedattention <ref type="bibr" target="#b5">[6]</ref>; and (b) RCONCAT: a recently introduced model for landmark-based navigation in an environment that uses Street View images <ref type="bibr" target="#b23">[24]</ref>. We represent the input images with RESNET18 features similar to the SDR task.</p><p>We use asynchronous training using multiple clients to generate rollouts on different partitions of the training data. We compute the gradients and updates using HOG-WILD! <ref type="bibr" target="#b26">[27]</ref> and ADAM learning rates <ref type="bibr" target="#b16">[17]</ref>. We use supervised learning by maximizing the log-likelihood of actions in the reference demonstrations.</p><p>The details of the models, learning, and hyperparameters are provided in the Supplementary Material.</p><p>there will be a white/grey van parked on the right side of the road, and right behind the van on the walkway, there is a black fire hydrant with silver top, the touchdown is on the silver part of the fire hydrant. a black doorway with red brick to the right of it, and green brick to the left of it. it has a light just above the doorway, and on that light is where you will find touchdown.  <ref type="table" target="#tab_7">Table 5</ref> shows development and test results for our three valuation metrics (Section 3.1). The STOP, FREQUENT and RANDOM illustrate the complexity of the task. The learned baselines perform better. We observe that RCONCAT outperforms GA across all three metrics. In general though, the performance illustrates the challenge of the task. Appendix F includes additional navigation experiments, including single-modality baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Complete Task Performance</head><p>We use a simple pipeline combination of the best models of the SDR and navigation tasks to complete the full task. Task completion is measured as finding Touchdown. We observe an accuracy of 4.5% for a threshold of 80px. In contrast, human performance is significantly higher. We estimate human performance using our annotation statistics <ref type="bibr" target="#b31">[32]</ref>. To avoid spam and impossible examples, we consider only examples that were successfully validated. We then measure the performance of workers that completed over 30 tasks for these valid examples. This includes 55 workers. Because some examples required multiple tries to validate this set includes tasks that workers failed to execute but were later validated. The mean performance across this set of workers using the set of valid tasks is 92% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Data Distribution and Licensing</head><p>We release the environment graph as panorama IDs and edges, scripts to download the RGB panoramas using the Google API, the collected data, and our code at touchdown.ai. These parts of the data are released with a CC-BY 4.0 license. Retention of downloaded panoramas should follow Google's policies. We also release RESNET18 im-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Conclusion</head><p>We introduce TOUCHDOWN, a dataset for natural language navigation and spatial reasoning using real-life visual observations. We define two tasks that require addressing a diverse set of reasoning and learning challenges. Our linguistically-driven analysis shows the data presents complex spatial reasoning challenges. This illustrates the benefit of using visual input that reflects the type of observations people see in their daily life, and demonstrates the effectiveness of our goal-driven data collection process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Route Generation</head><p>We generate each route by randomly sampling two panoramas in the environment graph, and querying the Google Direction API 7 to obtain a route between them that follows correct road directions. Although the routes follow the direction of allowed traffic, the panoramas might still show moving against the traffic in two-way streets depending on which lane was used for the original panorama collection by Google. The route is segmented into multiple routes with length sampled uniformly between 35 and 45. We do not discard the suffix route segment, which may be shorter. Some final routes had gaps due to our use of the API. If the number of gaps is below three, we heuristically connect the detached parts of the route by adding intermediate panoramas, otherwise we remove the route segment. Each of the route segments is used in a separate instructionwriting task. Because panoramas and route segments are sampled randomly, the majority of route segments stop in the middle of a block, rather than at an intersection. This explicit design decision requires instruction-writers to describe exactly where in the block the follower should stop, which elicits references to a variety of object types, rather than simply referring to the location of an intersection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Data Analysis</head><p>We perform linguistically-driven analysis to two additional navigation datasets: SAIL <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7]</ref> and LANI <ref type="bibr" target="#b24">[25]</ref>, both using simulated environments. Both datasets include paragraphs segmented into single instructions. We performs our analysis at the paragraph level. We use the same categories as in Section 5. <ref type="table" target="#tab_9">Table 6</ref> shows the analysis results. In general, in addition to the more complex visual input, TOUCHDOWN displays similar or increased linguistic diversity compared to LANI and SAIL. LANI contains a similar amount of coreference, egocentric spatial relations, and temporal conditions, and more examples than TOUCH-DOWN of imperatives and directions. SAIL contains a similar number of imperatives, and more examples of counts than TOUCHDOWN. We also visualize some of the common nouns and modifiers observed in our data <ref type="figure">(Figure 8</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SDR Pixel-level Predictions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Models</head><p>We use learned word vectors of size 300. For all models, we use a single-layer, bi-directional recurrent neural network (RNN) with long short-term memory (LSTM) cells <ref type="bibr" target="#b14">[15]</ref> to encode the description into a fixed-size vector representation. The hidden layer in the RNN has 600 unit. We compute the text embedding by averaging the RNN hidden states.</p><p>We provide the model with the complete panorama. We embed the panorama by slicing it into eight images, and projecting each image from a equirectangular projection to a perspective projection. Each of the eight projected images is of size 800?460. We pass each image separately through a RESNET18 <ref type="bibr" target="#b13">[14]</ref> pretrained on ImageNet <ref type="bibr" target="#b29">[30]</ref>, and extract features from the fourth to last layer before classification; each slice's feature map is of size 128 ? 100 ? 58. Finally, the features for the eight image slices are concatenated into a single tensor of size 128 ? 100 ? 464.</p><p>CONCAT We concatenate the text representation along the channel dimension of the image feature map at each feature pixel and apply a multi-layer perceptron (MLP) over each pixel to obtain a real-value score for every pixel in the feature map. The multilayer perceptron includes two fullyconnected layers with biases and ReLu non-linearities on the output of the first layer. The hidden size of each layer is 128. A SOFTMAX layer is applied to generate the final probability distribution over the feature pixels.</p><p>CONCATCONV The network structure is the same as CONCAT, except that after concatenating the text and image features and before applying the MLP, we mix the features across the feature map by applying a single convolution operation with a kernel of size 5 ? 5 and padding of 2. This operation does not change the size of the image and text tensor. We use a the same MLP architecture as in CONCAT on the outputs of the convolution, and compute a distribution over pixels with a SOFTMAX.</p><p>TEXT2CONV Given the text representation and the featurized image, we use a kernel conditioned on the text to convolve over the image. The kernel is computed by projecting the text representation into a vector of size 409,600 using a single learned layer without biases or nonlinearities. This vector is reshaped into a kernel of size 5 ? 5 ? 128 ? 128, and used to convolve over the image features, producing a tensor of the same size as the featurized image. We use a the same MLP architecture as in CONCAT on the outputs of this operation, and compute a distribution over pixels with a SOFTMAX.</p><p>LINGUNET We apply two convolutional layers to the image features to compute F 1 and F 2 . Each uses a learned  kernel of size 5?5 and padding of 2. We split the text representation into two vectors of size 300, and use two separate learned layers to transform each vector into another vector of size 16,384 that is reshaped to 1 ? 1 ? 128 ? 128. The result of this operation on the first half of the text representation is K 1 , and on the second is K 2 . The layers do not contain biases or non-linearities. These two kernels are applied to F 1 and F 2 to compute G 1 and G 2 . Finally, we use two deconvolution operations in sequence on G 1 and G 2 to compute H 1 and H 2 using learned kernels of size 5 ? 5 and padding of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Learning</head><p>We initialize parameters by sampling uniformly from [?0.1, 0.1]. During training, we apply dropout to the word embeddings with probability 0.5. We compute gradient updates using ADAM <ref type="bibr" target="#b16">[17]</ref>, and use a global learning rate of 0.0005 for LINGUNET, and 0.001 for all other models. We use early stopping with patience with a validation set containing 7% of the training data to compute accuracy at a threshold of 80 pixels after each epoch. We begin with a patience of 4, and when the accuracy on the validation set reaches a new maximum, patience resets to 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Evaluation</head><p>We compare the predicted location to the gold location by computing the location of the feature pixel corresponding to the gold location in the same scaling as the predicted probability distribution. We scale the accuracy threshold appropriately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. LINGUNET Architecture Clarifications</head><p>Our LINGUNET implementation for SDR task differs slightly from the original implementation <ref type="bibr" target="#b4">[5]</ref>. We set the stride for both convolution and devonvolution operations to be 1, whereas in the original LINGUNET architecture the stride is set to 2. Experiments with the original implementation show equivalent performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Navigation Experimental Setup Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Models</head><p>At each step, the agent observes the agent context.</p><p>Formally, the agent contexts at time step t is a tuple (x n , I t , ? t , (I 1 , ? 1 , a 1 ), . . . , (I t?1 , ? t?1 a t?1 ) ), wherex n is the navigation instruction, I t is the panorama that is currently observed at heading ? t , and (I 1 , ? 1 , a 1 ), . . . , (I t?1 , ? t?1 a t?1 ) is the sequence of previously observed panoramas, orientations, and selected actions. Given an agent contexts, the navigation model computes action probabilities P (a |s).</p><p>We use learned word vectors of size 32 for all models. We map the instructionx n to a vector x using a singlelayer uni-directional RNN with LSTM cells with 256 hidden units. The instruction representation x is the hidden state of the final token in the instruction.</p><p>We generate RESNET18 features for each 360 ? panorama I t . We center the feature map according agent's heading ? t . We crop a 128 ? 100 ? 100 sized feature map from the center. We pre-compute mean value along the channel dimension for every feature map and save the resulting 100 ? 100 features. This pre-computation allows for faster learning. We use the saved features corresponding to I t and the agent's heading ? t as? t .</p><p>RCONCAT We modify the model of Mirowski et al. <ref type="bibr" target="#b23">[24]</ref> for instruction-driven navigation. We use an RNN to embed the instruction instead of a goal embedding, and do not embed a reward signal. We apply a three-layer convolutional neural network to? t . The first layer uses 32 8 ? 8 kernels with stride 4, and the second layer uses 64 4 ? 4 kernels with stride 4, applying ReLu non-linearities after each convolutional operation. We use a single fully-connected layer including biases of size 256 on the output of the convolutional operations to compute the observation's representation I t . We learn embeddings a of size 16 for each action a. For each time step t, we concatenate the instruction representation x, observation representation I t , and action embedding a t?1 into a vectors t . For the first time step, we use a learned embedding for the previous action. We use a single-layer RNN with 256 LSTM cells on the sequence of time steps. The input at time t iss t and the hidden state is h t . We concatenate a learned time step embedding t ? R 32 with h t , and use a single-layer perceptron with biases and a SOFTMAX operation to compute P (a t |s t ).</p><p>GA We apply a three-layer convolutional neural network to? t . The first layer uses 128 8 ? 8 kernels with stride 4, and the second layer uses 64 4 ? 4 kernels with stride 2, applying ReLu non-linearities after each convolutional operation. We use a single fully-connected layer including biases of size 64 on the output of the convolutional operations to compute the observation's representation I t . We use a single hidden layer with biases followed by a sigmoid operation to map x into a vector g ? R 64 . For each time step t, we apply a gated attention on I t using g along the channel dimension to generate a vector u t . We use a single fully-connected layer with biases and a ReLu non-linearity with u t to compute a vector v t ? R 256 . We use a singlelayer RNN with 256 LSTM cells on the sequence of time steps. The input at time t is v t and the hidden state is h t . We concatenate a learned time step embedding t ? R 32 with h t , and use a single-layer perceptron with biases and a SOFTMAX operation to compute P (a t |s t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Learning</head><p>We train using asynchronous learning with six clients, each using a different split of the training data. We use supervised learning with HOGWILD! <ref type="bibr" target="#b26">[27]</ref> and ADAM <ref type="bibr" target="#b16">[17]</ref>. We generate a sequence of agent contexts and actions {(s i , a i )} N i=1 from the reference demonstrations, and maximize the log-likelihood objective:</p><formula xml:id="formula_14">J = max ? N i=1 ln p ? (a i |s i ) ,</formula><p>where ? is the model parameters.</p><p>Hyperparameters We initialize parameters by sampling uniformly from [?0.1, 0.1]. We set the horizon to 55 during learning, and use an horizon of 50 during testing. We stop training using SPD performance on the development set. We use early stopping with patience, beginning with a patience value of 5 and resetting to 5 every time we observe a new minimum SPD error. The global learning rate is fixed at 0.00025.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional Navigation Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Experiments with RGB Images</head><p>We also experiment with raw RGB images similar to Mirowski et al. <ref type="bibr" target="#b23">[24]</ref>. We project and resize each 360 ? panorama I t to a 60 ? perspective image? t of size 3?84?84, where the center of the panorama is the agent's heading ? t . <ref type="table" target="#tab_10">Table 7</ref> shows the development and test results using RGB images. We observe better performance using RESNET18 features compared to RGB images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Single-modality Experiments</head><p>We study the importance of each of the two modalities, language and vision, for the navigation task. We separately remove the embeddings of the language (NO-TEXT) and visual observations (NO-IMAGE) for both models. <ref type="table">Table 8</ref> shows the results. We observe no meaningful learning in the absence of the vision modality, whereas limited performance is possible without the natural language input. These results show both modalities are necessary, and also indicate that our navigation baselines <ref type="table" target="#tab_7">(Table 5</ref>) benefit relatively little from the text. <ref type="figure">Figure 8</ref>. An illustration of the referential language in our navigation (top) and SDR (bottom) instructions. We ranked all nouns by frequency and removed stop words. We show the top five/eight nouns (most inner circle) for navigation and SDR. For each noun, we show the most common modifiers that prefix it. The size of each segment is not relative to the frequency in the data. touch down will be chillin in front of a sign on your right hand side about half way down this street,before you get to the sign there will be a multi color mural on the right w multiple colors and some writing on it.</p><p>LINGUNET The model fails to correctly predict the location of Touchdown, but is relatively close. The selected pixel is 104px from the correct one. The model focuses on the top of the sign instead of the bottom, potentially because of the more common reference to the top, which is visually distinguished.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>46</head><p>TEXT2CONV The model fails to correctly predict the location of Touchdown, but is relatively close. The selected pixel is 96px from the correct one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>46</head><p>CONCATCONV The model fails to predict the location of Touchdown, instead focusing on a person walking on the left.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>46</head><p>CONCAT The model fails to predict the location of Touchdown, instead of focusing the person walking on the left, the colorful sign mentioned in the description, and a car on the far right. <ref type="figure" target="#fig_13">Figure 12</ref>. All the models fail to correctly identify the location of Touchdown. The predictions of LINGUNET, TEXT2CONV, and CON-CATCONV seem to mix biases in the data with objects mentioned in the description, but fail to resolve the exact spatial description. a row of blue bikes, touchdown is in the fifth bike seat in the row, from the way you came.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>46</head><p>LINGUNET The model correctly identifies that a bike is mentioned, but fails to identify the exact bike or the location on the bike seat. Instead the the distribution is divided between multiple bikes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22</head><p>TEXT2CONV Similar to LINGUNET, the model identifies the reference to bikes, but fails to identify the exact bike. The uncertainty of the model is potentially illustrated by how it distributes the probability mass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22</head><p>CONCATCONV The model correctly predicts the location of Touchdown. While the distribution is spread across multiple bikes observed, the highest probability pixel is close enough (i.e., within 80 pixels) of the correct location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22</head><p>CONCAT Similar to CONCATCONV, the model correctly predicts the location of Touchdown. <ref type="figure" target="#fig_13">Figure 13</ref>. LINGUNET and TEXT2CONV fail to correctly identify the location, although their predicted distribution is focused on the correct set of objects. In contrast, the simpler models, CONCAT and CONCATCONV, correctly predict the location of Touchdown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Example instruction where the annotated navigation (underlined) and SDR (bolded) segments overlap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 Figure 5 .</head><label>15</label><figDesc>Text lengths in TOUCHDOWN and related corpora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Softmax &lt; l a</head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 6 3 j p u E / m 3 W P W M l 0 1 L 8 P i X 0 S Q V 5 w = " &gt; A A A C K n i c b V C 7 S s R A F J 3 4 N r 6 1 t B l c B K s l E U H L B R t L R V e F T Z T J 5 E Y H 5 x F m b t Q l 5 D 9 s t f Z r 7 M T W D 3 F 2 3 U J X D 1 w 4 n H N f n K y U w m E U v Q c T k 1 P T M 7 N z 8 + H C 4 t L y y u r a + r k z l e X Q 5 U Y a e 5 k x B 1 J o 6 K J A C Z e l B a Y y C R f Z 3 e H A v 7 g H 6 4 T R Z 9 g v I V X s R o t C c I Z e u k o Q H r E + N Q U q 9 t h c r 7 a i d j Q E / U v i E W m R E Y 6 v 1 4 K Z J D e 8 U q C R S + Z c L 4 5 K T G t m U X A J T Z h U D k r G 7 9 g N 9 D z V T I F L 6 + H b D d 3 2 S k 4 L Y 3 1 p p E P 1 5 0 T N l H N 9 l f l O x f D W j X s D 8 V 8 v d 4 O F Y 9 e x O E h r o c s K Q f P v 4 0 U l K R o 6 y I X m w g J H 2 f e E c S v 8 / 5 T f M s s 4 + v T C M L G g 4 Y E b p Z j O 6 4 Q 3 v T i t 6 8 Q q 2 o q b J v T J x e M 5 / S X n u + 0 4 a s c n e 6 1 O N M p w j m y S L b J D Y r J P O u S I H J M u 4 c S S J / J M X o L X 4 C 1 4 D z 6 + W y e C 0 c w G + Y X g 8 w u d k a b i &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 3 j p u E / m 3 W P W M l 0 1 L 8 P i X 0 S Q V 5 w = " &gt; A A A C K n i c b V C 7 S s R A F J 3 4 N r 6 1 t B l c B K s l E U H L B R t L R V e F T Z T J 5 E Y H 5 x F m b t Q l 5 D 9 s t f Z r 7 M T W D 3 F 2 3 U J X D 1 w 4 n H N f n K y U w m E U v Q c T k 1 P T M 7 N z 8 + H C 4 t L y y u r a + r k z l e X Q 5 U Y a e 5 k x B 1 J o 6 K J A C Z e l B a Y y C R f Z 3 e H A v 7 g H 6 4 T R Z 9 g v I V X s R o t C c I Z e u k o Q H r E + N Q U q 9 t h c r 7 a i d j Q E / U v i E W m R E Y 6 v 1 4 K Z J D e 8 U q C R S + Z c L 4 5 K T G t m U X A J T Z h U D k r G 7 9 g N 9 D z V T I F L 6 + H b D d 3 2 S k 4 L Y 3 1 p p E P 1 5 0 T N l H N 9 l f l O x f D W j X s D 8 V 8 v d 4 O F Y 9 e x O E h r o c s K Q f P v 4 0 U l K R o 6 y I X m w g J H 2 f e E c S v 8 / 5 T f M s s 4 + v T C M L G g 4 Y E b p Z j O 6 4 Q 3 v T i t 6 8 Q q 2 o q b J v T J x e M 5 / S X n u + 0 4 a s c n e 6 1 O N M p w j m y S L b J D Y r J P O u S I H J M u 4 c S S J / J M X o L X 4 C 1 4 D z 6 + W y e C 0 c w G + Y X g 8 w u d k a b i &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 3 j p u E / m 3 W P W M l 0 1 L 8 P i X 0 S Q V 5 w = " &gt; A A A C K n i c b V C 7 S s R A F J 3 4 N r 6 1 t B l c B K s l E U H L B R t L R V e F T Z T J 5 E Y H 5 x F m b t Q l 5 D 9 s t f Z r 7 M T W D 3 F 2 3 U J X D 1 w 4 n H N f n K y U w m E U v Q c T k 1 P T M 7 N z 8 + H C 4 t L y y u r a + r k z l e X Q 5 U Y a e 5 k x B 1 J o 6 K J A C Z e l B a Y y C R f Z 3 e H A v 7 g H 6 4 T R Z 9 g v I V X s R o t C c I Z e u k o Q H r E + N Q U q 9 t h c r 7 a i d j Q E / U v i E W m R E Y 6 v 1 4 K Z J D e 8 U q C R S + Z c L 4 5 K T G t m U X A J T Z h U D k r G 7 9 g N 9 D z V T I F L 6 + H b D d 3 2 S k 4 L Y 3 1 p p E P 1 5 0 T N l H N 9 l f l O x f D W j X s D 8 V 8 v d 4 O F Y 9 e x O E h r o c s K Q f P v 4 0 U l K R o 6 y I X m w g J H 2 f e E c S v 8 / 5 T f M s s 4 + v T C M L G g 4 Y E b p Z j O 6 4 Q 3 v T i t 6 8 Q q 2 o q b J v T J x e M 5 / S X n u + 0 4 a s c n e 6 1 O N M p w j m y S L b J D Y r J P O u S I H J M u 4 c S S J / J M X o L X 4 C 1 4 D z 6 + W y e C 0 c w G + Y X g 8 w u d k a b i &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 3 j p u E / m 3 W P W M l 0 1 L 8 P i X 0 S Q V 5 w = " &gt; A A A C K n i c b V C 7 S s R A F J 3 4 N r 6 1 t B l c B K s l E U H L B R t L R V e F T Z T J 5 E Y H 5 x F m b t Q l 5 D 9 s t f Z r 7 M T W D 3 F 2 3 U J X D 1 w 4 n H N f n K y U w m E U v Q c T k 1 P T M 7 N z 8 + H C 4 t L y y u r a + r k z l e X Q 5 U Y a e 5 k x B 1 J o 6 K J A C Z e l B a Y y C R f Z 3 e H A v 7 g H 6 4 T R Z 9 g v I V X s R o t C c I Z e u k o Q H r E + N Q U q 9 t h c r 7 a i d j Q E / U v i E W m R E Y 6 v 1 4 K Z J D e 8 U q C R S + Z c L 4 5 K T G t m U X A J T Z h U D k r G 7 9 g N 9 D z V T I F L 6 + H b D d 3 2 S k 4 L Y 3 1 p p E P 1 5 0 T N l H N 9 l f l O x f D W j X s D 8 V 8 v d 4 O F Y 9 e x O E h r o c s K Q f P v 4 0 U l K R o 6 y I X m w g J H 2 f e E c S v 8 / 5 T f M s s 4 + v T C M L G g 4 Y E b p Z j O 6 4 Q 3 v T i t 6 8 Q q 2 o q b J v T J x e M 5 / S X n u + 0 4 a s c n e 6 1 O N M p w j m y S L b J D Y r J P O u S I H J M u 4 c S S J / J M X o L X 4 C 1 4 D z 6 + W y e C 0 c w G + Y X g 8 w u d k a b i &lt; / l a t e x i t &gt; LingUNet &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H G k Y r D n U 4 w 9 h N J l r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>7 5 M L p n P 6 S y 7 1 W G L T C 8 / 3 m U T D J s E a 2 y D b Z J S E 5 I E f k h L R J h 3 B S k E f y R J 6 9 F + / V e / c + v l t n v M n M J v k F 7 / M L g 7 e n Q Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H G k Y r D n U 4 w 9 h N J l r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>7 5 M L p n P 6 S y 7 1 W G L T C 8 / 3 m U T D J s E a 2 y D b Z J S E 5 I E f k h L R J h 3 B S k E f y R J 6 9 F + / V e / c + v l t n v M n M J v k F 7 / M L g 7 e n Q Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H G k Y r D n U 4 w 9 h N J l r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>7 5 M L p n P 6 S y 7 1 W G L T C 8 / 3 m U T D J s E a 2 y D b Z J S E 5 I E f k h L R J h 3 B S k E f y R J 6 9 F + / V e / c + v l t n v M n M J v k F 7 / M L g 7 e n Q Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H G k Y r D n U 4 w 9 h N J l r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>7 5 M L p n P 6 S y 7 1 W G L T C 8 / 3 m U T D J s E a 2 y D b Z J S E 5 I E f k h L R J h 3 B S k E f y R J 6 9 F + / V e / c + v l t n v M n M J v k F 7 / M L g 7 e n Q Q = = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>F 0 &lt;</head><label>0</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>a B X 5 8 1 5 d z 6 c z 5 / R l t P s 7 K M / c L 6 + A e j f p W 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>a B X 5 8 1 5 d z 6 c z 5 / R l t P s 7 K M / c L 6 + A e j f p W 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>a B X 5 8 1 5 d z 6 c z 5 / R l t P s 7 K M / c L 6 + A e j f p W 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>t 7 b N 0 y c c 6 a N 7 3 8 6 r a X l l f b q 2 r q 7 s b m 1 v d P Z 3 b v T W a E o 9 G n G M z W I i Q b O J P Q N M x w G u Q I i Y g 7 3 8 e S i 9 u 8 f Q W m W y V s z z S E S 5 E G y l F F i r B S G g p h x n J a X 1 c g f d T y / 6 8 + A F 0 n Q E A 8 1 u B 7 t O u 0 w y W g h Q B r K i d b D w M 9 N V B J l G O V Q u W G h I S d 0 Q h 5 g a K k k A n R U z k J X + M g q C U 4 z Z Z 8 0 e K b + 3 i i J 0 H o q Y j t Z h 9 T z X i 3 + 6 y W 6 / n D u u k n P o 5 L J v D A g 6 c / x t O D Y Z L h u B S d M A T V 8 a g m h i t n 8 m I 6 J I t T Y 7 l w 3 V C D h i W Z C E J m U I a 2 G Q V S W o R L Y C 6 r K t c 0 F 8 z 0 t k r u T b u B 3 g 5 t T r + c 3 H a 6 h A 3 S I j l G A z l A P X a F r 1 E c U 5 e g Z v a B X 5 8 1 5 d z 6 c z 5 / R l t P s 7 K M / c L 6 + A e j f p W 4 = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>F 1 &lt;</head><label>1</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " 2 V d 1 v j q 3 3 i j D m G T K L V f 1 E e N d R o E = " &gt; A A A C K H i c b V B N S w M x F M x W r X X 9 1 q O X Y B E 8 l Y 0 I e h Q E 8 a h g q 9 B d S j b 7 1 o Y m 2 S X J K m X Z v + F V z / 4 a b 9 K r v 8 R s 2 4 N W B w L D z H t 5 w 8 S 5 4 M Y G w c R r L C 2 v N F d b a / 7 6 x u b W 9 s 7 u X s 9 k h W b Q Z Z n I 9 E N M D Q i u o G u 5 F f C Q a 6 A y F n A f j y 5 r / / 4 J t O G Z u r P j H C J J H x V P O a P W S W E o q R 3 G a X l V D c h g p x 1 0 g i n w X 0 L m p I 3 m u B n s e s 0 w y V g h Q V k m q D F 9 E u Q 2 K q m 2 n A m o / L A w k F M 2 o o / Q d 1 R R C S Y q p 6 E r f O S U B K e Z d k 9 Z P F V / b p R U G j O W s Z u s Q 5 p F r x b / 9 R J T f 7 h w 3 a b n U c l V X l h Q b H Y 8 L Q S 2 G a 5 b w Q n X w K w Y O 0 K Z 5 i 4 / Z k O q K b O u O 9 8 P N S h 4 Z p m U V C V l y K o + i c o y 1 B K 3 S V X 5 r j m y 2 N N f 0 j v p k K B D b k / b F 8 G 8 w x Y 6 Q I f o G B F 0 h i 7 Q N b p B X c R Q j l 7 Q K 3 r z 3 r 0 P 7 9 O b z E Y b 3 n x n H / 2 C 9 / U N 6 p i l b w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2V d 1 v j q 3 3 i j D m G T K L V f 1 E e N d R o E = " &gt; A A A C K H i c b V B N S w M x F M x W r X X 9 1 q O X Y B E 8 l Y 0 I e h Q E 8 a h g q 9 B d S j b 7 1 o Y m 2 S X J K m X Z v + F V z / 4 a b 9 K r v 8 R s 2 4 N W B w L D z H t 5 w 8 S 5 4 M Y G w c R r L C 2 v N F d b a / 7 6 x u b W 9 s 7 u X s 9 k h W b Q Z Z n I 9 E N M D Q i u o G u 5 F f C Q a 6 A y F n A f j y 5 r / / 4 J t O G Z u r P j H C J J H x V P O a P W S W E o q R 3 G a X l V D c h g p x 1 0 g i n w X 0 L m p I 3 m u B n s e s 0 w y V g h Q V k m q D F 9 E u Q 2 K q m 2 n A m o / L A w k F M 2 o o / Q d 1 R R C S Y q p 6 E r f O S U B K e Z d k 9 Z P F V / b p R U G j O W s Z u s Q 5 p F r x b / 9 R J T f 7 h w 3 a b n U c l V X l h Q b H Y 8 L Q S 2 G a 5 b w Q n X w K w Y O 0 K Z 5 i 4 / Z k O q K b Ou O 9 8 P N S h 4 Z p m U V C V l y K o + i c o y 1 B K 3 S V X 5 r j m y 2 N N f 0 j v p k K B D b k / b F 8 G 8 w x Y 6 Q I f o G B F 0 h i 7 Q N b p B X c R Q j l 7 Q K 3 r z 3 r 0 P 7 9 O b z E Y b 3 n x n H / 2 C 9 / U N 6 p i l b w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 V d 1 v j q 3 3 i j D m G T K L V f 1 E e N d R o E = " &gt; A A A C K H i c b V B N S w M x F M x W r X X 9 1 q O X Y B E 8 l Y 0 I e h Q E 8 a h g q 9 B d S j b 7 1 o Y m 2 S X J K m X Z v + F V z / 4 a b 9 K r v 8 R s 2 4 N W B w L D z H t 5 w 8 S 5 4 M Y G w c R r L C 2 v N F d b a / 7 6 x u b W 9 s 7 u X s 9 k h W b Q Z Z n I 9 E N M D Q i u o G u 5 F f C Q a 6 A y F n A f j y 5 r / / 4 J t O G Z u r P j H C J J H x V P O a P W S W E o q R 3 G a X l V D c h g p x 1 0 g i n w X 0 L m p I 3 m u B n s e s 0 w y V g h Q V k m q D F 9 E u Q 2 K q m 2 n A m o / L A w k F M 2 o o / Q d 1 R R C S Y q p 6 E r f O S U B K e Z d k 9 Z P F V / b p R U G j O W s Z u s Q 5 p F r x b / 9 R J T f 7 h w 3 a b n U c l V X l h Q b H Y 8 L Q S 2 G a 5 b w Q n X w K w Y O 0 K Z 5 i 4 / Z k O q K b O u O 9 8 P N S h 4 Z p m U V C V l y K o + i c o y 1 B K 3 S V X 5 r j m y 2 N N f 0 j v p k K B D b k / b F 8 G 8 w x Y 6 Q I f o G B F 0 h i 7 Q N b p B X c R Q j l 7 Q K 3 r z 3 r 0 P 7 9 O b z E Y b 3 n x n H / 2 C 9 / U N 6 p i l b w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 V d 1 v j q 3 3 i j D m G T K L V f 1 E e N d R o E = " &gt; A A A C K H i c b V B N S w M x F M x W r X X 9 1 q O X Y B E 8 l Y 0 I e h Q E 8 a h g q 9 B d S j b 7 1 o Y m 2 S X J K m X Z v + F V z / 4 a b 9 K r v 8 R s 2 4 N W B w L D z H t 5 w 8 S 5 4 M Y G w c R r L C 2 v N F d b a / 7 6 x u b W 9 s 7 u X s 9 k h W b Q Z Z n I 9 E N M D Q i u o G u 5 F f C Q a 6 A y F n A f j y 5 r / / 4 J t O G Z u r P j H C J J H x V P O a P W S W E o q R 3 G a X l V D c h g p x 1 0 g i n w X 0 L m p I 3 m u B n s e s 0 w y V g h Q V k m q D F 9 E u Q 2 K q m 2 n A m o / L A w k F M 2 o o / Q d 1 R R C S Y q p 6 E r f O S U B K e Z d k 9 Z P F V / b p R U G j O W s Z u s Q 5 p F r x b / 9 R J T f 7 h w 3 a b n U c l V X l h Q b H Y 8 L Q S 2 G a 5 b w Q n X w K w Y O 0 K Z 5 i 4 / Z k O q K b O u O 9 8 P N S h 4 Z p m U V C V l y K o + i c o y 1 B K 3 S V X 5 r j m y 2 N N f 0 j v p k K B D b k / b F 8 G 8 w x Y 6 Q I f o G B F 0 h i 7 Q N b p B X c R Q j l 7 Q K 3 r z 3 r 0 P 7 9 O b z E Y b 3 n x n H / 2 C 9 / U N 6 p i l b w = = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>F 2 &lt;</head><label>2</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " u r S 1 Z Y Y a A Q t B Z F 5 0 Q 4 d 4 Y B H B N T I = " &gt; A A A C K H i c b V B N S 8 N A F N z 4 W e N X 1 a O X x S B 4 K o k I e i w I 4 l H B 1 k I T y m b z o o u 7 m 7 C 7 U U r I 3 / C q Z 3 + N N + n V X + K m z U F b B x a G m f f 2 D R P n n G n j + x N n a X l l d W 2 9 t e F u b m 3 v 7 L b 3 9 v s 6 K x S F H s 1 4 p g Y x 0 c C Z h J 5 h h s M g V 0 B E z O E + f r q s / f t n U J p l 8 s 6 M c 4 g E e Z A s Z Z Q Y K 4 W h I O Y x T s u r a n Q 6 a n t + x 5 8 C L 5 K g I R 5 q c D P a c 9 b C J K O F A G k o J 1 o P A z 8 3 U U m U Y Z R D 5 Y a F h p z Q J / I A Q 0 s l E a C j c h q 6 w s d W S X C a K f u k w V P 1 9 0 Z J h N Z j E d v J O q S e 9 2 r x X y / R 9 Y d z 1 0 1 6 E Z V M 5 o U B S W f H 0 4 J j k + G 6 F Z w w B d T w s S W E K m b z Y / p I F K H G d u e 6 o Q I J L z Q T g s i k D G k 1 D K K y D J X A X l B V r m 0 u m O 9 p k f R P O 4 H f C W 7 P v K 7 f d N h C h + g I n a A A n a M u u k Y 3 q I c o y t E r e k P v z o f z 6 X w 5 k 9 n o k t P s H K A / c L 5 / A O x R p X A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u r S 1 Z Y Y a A Q t B Z F 5 0 Q 4 d 4 Y B H B N T I = " &gt; A A A C K H i c b V B N S 8 N A F N z 4 W e N X 1 a O X x S B 4 K o k I e i w I 4 l H B 1 k I T y m b z o o u 7 m 7 C 7 U U r I 3 / C q Z 3 + N N + n V X + K m z U F b B x a G m f f 2 D R P n n G n j + x N n a X l l d W 2 9 t e F u b m 3 v 7 L b 3 9 v s 6 K x S F H s 1 4 p g Y x 0 c C Z h J 5 h h s M g V 0 B E z O E + f r q s / f t n U J p l 8 s 6 M c 4 g E e Z A s Z Z Q Y K 4 W h I O Y x T s u r a n Q 6 a n t + x 5 8 C L 5 K g I R 5 q c D P a c 9 b C J K O F A G k o J 1 o P A z 8 3 U U m U Y Z R D 5 Y a F h p z Q J / I A Q 0 s l E a C j c h q 6 w s d W S X C a K f u k w V P 1 9 0 Z J h N Z j E d v J O q S e 9 2 r x X y / R 9 Y d z 1 0 1 6 E Z V M 5 o U B S W f H 0 4 J j k + G 6 F Z w w B d T w s S W E K m b z Y / p I F K H G d u e 6 o Q I J L z Q T g s i k D G k 1 D K K y D J X A X l B V r m 0 u m O 9 p k f R P O 4 H f C W 7 P v K 7 f d N h C h + g I n a A A n a M u u k Y 3 q I c o y t E r e k P v z o f z 6 X w 5 k 9 n o k t P s H K A / c L 5 / A O x R p X A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u r S 1 Z Y Y a A Q t B Z F 5 0 Q 4 d 4 Y B H B N T I = " &gt; A A A C K H i c b V B N S 8 N A F N z 4 W e N X 1 a O X x S B 4 K o k I e i w I 4 l H B 1 k I T y m b z o o u 7 m 7 C 7 U U r I 3 / C q Z 3 + N N + n V X + K m z U F b B x a G m f f 2 D R P n n G n j + x N n a X l l d W 2 9 t e F u b m 3 v 7 L b 3 9 v s 6 K x S F H s 1 4 p g Y x 0 c C Z h J 5 h h s M g V 0 B E z O E + f r q s / f t n U J p l 8 s 6 M c 4 g E e Z A s Z Z Q Y K 4 W h I O Y x T s u r a n Q 6 a n t + x 5 8 C L 5 K g I R 5 q c D P a c 9 b C J K O F A G k o J 1 o P A z 8 3 U U m U Y Z R D 5 Y a F h p z Q J / I A Q 0 s l E a C j c h q 6 w s d W S X C a K f u k w V P 1 9 0 Z J h N Z j E d v J O q S e 9 2 r x X y / R 9 Y d z 1 0 1 6 E Z V M 5 o U B S W f H 0 4 J j k + G 6 F Z w w B d T w s S W E K m b z Y / p I F K H G d u e 6 o Q I J L z Q T g s i k D G k 1 D K K y D J X A X l B V r m 0 u m O 9 p k f R P O 4 H f C W 7 P v K 7 f d N h C h + g I n a A A n a M u u k Y 3 q I c o y t E r e k P v z o f z 6 X w 5 k 9 n o k t P s H K A / c L 5 / A O x R p X A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u r S 1 Z Y Y a A Q t B Z F 5 0 Q 4 d 4 Y B H B N T I = " &gt; A A A C K H i c b V B N S 8 N A F N z 4 W e N X 1 a O X x S B 4 K o k I e i w I 4 l H B 1 k I T y m b z o o u 7 m 7 C 7 U U r I 3 / C q Z 3 + N N + n V X + K m z U F b B x a G m f f 2 D R P n n G n j + x N n a X l l d W 2 9 t e F u b m 3 v 7 L b 3 9 v s 6 K x S F H s 1 4 p g Y x 0 c C Z h J 5 h h s M g V 0 B E z O E + f r q s / f t n U J p l 8 s 6 M c 4 g E e Z A s Z Z Q Y K 4 W h I O Y x T s u r a n Q 6 a n t + x 5 8 C L 5 K g I R 5 q c D P a c 9 b C J K O F A G k o J 1 o P A z 8 3 U U m U Y Z R D 5 Y a F h p z Q J / I A Q 0 s l E a C j c h q 6 w s d W S X C a K f u k w V P 1 9 0 Z J h N Z j E d v J O q S e 9 2 r x X y / R 9 Y d z 1 0 1 6 E Z V M 5 o U B S W f H 0 4 J j k + G 6 F Z w w B d T w s S W E K m b z Y / p I F K H G d u e 6 o Q I J L z Q T g s i k D G k 1 D K K y D J X A X l B V r m 0 u m O 9 p k f R P O 4 H f C W 7 P v K 7 f d N h C h + g I n a A A n a M u u k Y 3 q I c o y t E r e k P v z o f z 6 X w 5 k 9 n o k t P s H K A / c L 5 / A O x R p X A = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>G 2 &lt;</head><label>2</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " / P P w Q n S y q T E W T L t + U W B I Z k p v Y y I = " &gt; A A A C K H i c b V B N S 8 N A F N z 4 W e N X 1 a O X x S B 4 K o k I e i x 4 0 K O C r Y U m l M 3 m R R d 3 N 2 F 3 o 5 S Q v + F V z / 4 a b 9 K r v 8 R N m 4 O 2 D i w M M + / t G y b O O d P G 9 y f O 0 v L K 6 t p 6 a 8 P d 3 N r e 2 W 3 v 7 f d 1 V i g K P Z r x T A 1 i o o E z C T 3 D D I d B r o C I m M N 9 / H R Z + / f P o D T L 5 J 0 Z 5 x A J 8 i B Z y i g x V g p D Q c x j n J Z X 1 e h 0 1 P b 8 j j 8 F X i R B Q z z U 4 G a 0 5 6 y F S U Y L A d J Q T r Q e B n 5 u o p I o w y i H y g 0 L D T m h T + Q B h p Z K I k B H 5 T R 0 h Y + t k u A 0 U / Z J g 6 f q 7 4 2 S C K 3 H I r a T d U g 9 7 9 X i v 1 6 i 6 w / n r p v 0 I i q Z z A s D k s 6 O p w X H J s N 1 K z h h C q j h Y 0 s I V c z m x / S R K E K N 7 c 5 1 Q w U S X m g m B J F J G d J q G E R l G S q B v a C q X N t c M N / T I u m f d g K / E 9 y e e V 2 / 6 b C F D t E R O k E B O k d d d I 1 u U A 9 R l K N X 9 I b e n Q / n 0 / l y J r P R J a f Z O U B / 4 H z / A O 4 N p X E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / P P w Q n S y q T E W T L t + U W B I Z k p v Y y I = " &gt; A A A C K H i c b V B N S 8 N A F N z 4 W e N X 1 a O X x S B 4 K o k I e i x 4 0 K O C r Y U m l M 3 m R R d 3 N 2 F 3 o 5 S Q v + F V z / 4 a b 9 K r v 8 R N m 4 O 2 D i w M M + / t G y b O O d P G 9 y f O 0 v L K 6 t p 6 a 8 P d 3 N r e 2 W 3 v 7 f d 1 V i g K P Z r x T A 1 i o o E z C T 3 D D I d B r o C I m M N 9 / H R Z + / f P o D T L 5 J 0 Z 5 x A J 8 i B Z y i g x V g p D Q c x j n J Z X 1 e h 0 1 P b 8 j j 8 F X i R B Q z z U 4 G a 0 5 6 y F S U Y L A d J Q T r Q e B n 5 u o p I o w y i H y g 0 L D T m h T + Q B h p Z K I k B H 5 T R 0 h Y + t k u A 0 U / Z J g 6 f q 7 4 2 S C K 3 H I r a T d U g 9 7 9 X i v 1 6 i 6 w / n r p v 0 I i q Z z A s D k s 6 O p w X H J s N 1 K z h h C q j h Y 0 s I V c z m x / S R K E K N 7 c 5 1 Q w U S X m g m B J F J G d J q G E R l G S q B v a C q X N t c M N / T I u m f d g K / E 9 y e e V 2 / 6 b C F D t E R O k E B O k d d d I 1 u U A 9 R l K N X 9 I b e n Q / n 0 / l y J r P R J a f Z O U B / 4 H z / A O 4 N p X E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / P P w Q n S y q T E W T L t + U W B I Z k p v Y y I = " &gt; A A A C K H i c b V B N S 8 N A F N z 4 W e N X 1 a O X x S B 4 K o k I e i x 4 0 K O C r Y U m l M 3 m R R d 3 N 2 F 3 o 5 S Q v + F V z / 4 a b 9 K r v 8 R N m 4 O 2 D i w M M + / t G y b O O d P G 9 y f O 0 v L K 6 t p 6 a 8 P d 3 N r e 2 W 3 v 7 f d 1 V i g K P Z r x T A 1 i o o E z C T 3 D D I d B r o C I m M N 9 / H R Z + / f P o D T L 5 J 0 Z 5 x A J 8 i B Z y i g x V g p D Q c x j n J Z X 1 e h 0 1 P b 8 j j 8 F X i R B Q z z U 4 G a 0 5 6 y F S U Y L A d J Q T r Q e B n 5 u o p I o w y i H y g 0 L D T m h T + Q B h p Z K I k B H 5 T R 0 h Y + t k u A 0 U / Z J g 6 f q 7 4 2 S C K 3 H I r a T d U g 9 7 9 X i v 1 6 i 6 w / n r p v 0 I i q Z z A s D k s 6 O p w X H J s N 1 K z h h C q j h Y 0 s I V c z m x / S R K E K N 7 c 5 1 Q w U S X m g m B J F J G d J q G E R l G S q B v a C q X N t c M N / T I u m f d g K / E 9 y e e V 2 / 6 b C F D t E R O k E B O k d d d I 1 u U A 9 R l K N X 9 I b e n Q / n 0 / l y J r P R J a f Z O U B / 4 H z / A O 4 N p X E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / P P w Q n S y q T E W T L t + U W B I Z k p v Y y I = " &gt; A A A C K H i c b V B N S 8 N A F N z 4 W e N X 1 a O X x S B 4 K o k I e i x 4 0 K O C r Y U m l M 3 m R R d 3 N 2 F 3 o 5 S Q v + F V z / 4 a b 9 K r v 8 R N m 4 O 2 D i w M M + / t G y b O O d P G 9 y f O 0 v L K 6 t p 6 a 8 P d 3 N r e 2 W 3 v 7 f d 1 V i g K P Z r x T A 1 i o o E z C T 3 D D I d B r o C I m M N 9 / H R Z + / f P o D T L 5 J 0 Z 5 x A J 8 i B Z y i g x V g p D Q c x j n J Z X 1 e h 0 1 P b 8 j j 8 F X i R B Q z z U 4 G a 0 5 6 y F S U Y L A d J Q T r Q e B n 5 u o p I o w y i H y g 0 L D T m h T + Q B h p Z K I k B H 5 T R 0 h Y + t k u A 0 U / Z J g 6 f q 7 4 2 S C K 3 H I r a T d U g 9 7 9 X i v 1 6 i 6 w / n r p v 0 I i q Z z A s D k s 6 O p w X H J s N 1 K z h h C q j h Y 0 s I V c z m x / S R K E K N 7 c 5 1 Q w U S X m g m B J F J G d J q G E R l G S q B v a C q X N t c M N / T I u m f d g K / E 9 y e e V 2 / 6 b C F D t E R O k E B O k d d d I 1 u U A 9 R l K N X 9 I b e n Q / n 0 / l y J r P R J a f Z O U B / 4 H z / A O 4 N p X E = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>G 1 &lt;</head><label>1</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " a i 7 / g 5 L 8 U P D b t a X / L T 5 9 o J d W T v 4 = " &gt; A A A C K H i c b V B N S w M x F M x W r X X 9 1 q O X Y B E 8 l Y 0 I e h Q 8 6 F H B V q G 7 l G z 2 r Q 1 N s k u S V c q y f 8 O r n v 0 1 3 q R X f 4 n Z t g e t D g S G m f f y h o l z w Y 0 N g o n X W F p e a a 6 2 1 v z 1 j c 2 t 7 Z 3 d v Z 7 J C s 2 g y z K R 6 Y e Y G h B c Q d d y K + A h 1 0 B l L O A + H l 3 W / v 0 T a M M z d W f H O U S S P i q e c k a t k 8 J Q U j u M 0 / K q G p D B T j v o B F P g v 4 T M S R v N c T P Y 9 Z p h k r F C g r J M U G P 6 J M h t V F J t O R N Q + W F h I K d s R B + h 7 6 i i E k x U T k N X + M g p C U 4 z 7 Z 6 y e K r + 3 C i p N G Y s Y z d Z h z S L X i 3 + 6 y W m / n D h u k 3 P o 5 K r v L C g 2 O x 4 W g h s M 1 y 3 g h O u g V k x d o Q y z V 1 + z I Z U U 2 Z d d 7 4 f a l D w z D I p q U r K k F V 9 E p V l q C V u k 6 r y X X N k s a e / p H f S I U G H 3 J 6 2 L 4 J 5 h y 1 0 g A 7 R M S L o D F 2 g a 3 S D u o i h H L 2 g V / T m v X s f 3 q c 3 m Y 0 2 v P n O P v o F 7 + s b 7 F S l c A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a i 7 / g 5 L 8 U P D b t a X / L T 5 9 o J d W Tv 4 = " &gt; A A A C K H i c b V B N S w M x F M x W r X X 9 1 q O X Y B E 8 l Y 0 I e h Q 8 6 F H B V q G 7 l G z 2 r Q 1 N s k u S V c q y f 8 O r n v 0 1 3 q R X f 4 n Z t g e t D g S G m f f y h o l z w Y 0 N g o n X W F p e a a 6 2 1 v z 1 j c 2 t 7 Z 3 d v Z 7 J C s 2 g y z K R 6 Y e Y G h B c Q d d y K + A h 1 0 B l L O A + H l 3 W / v 0 T a M M z d W f H O U S S P i q e c k a t k 8 J Q U j u M 0 / K q G p D B T j v o B F P g v 4 T M S R v N c T P Y 9 Z p h k r F C g r J M U G P 6 J M h t V F J t O R N Q + W F h I K d s R B + h 7 6 i i E k x U T k N X + M g p C U 4 z 7 Z 6 y e K r + 3 C i p N G Y s Y z d Zh z S L X i 3 + 6 y W m / n D h u k 3 P o 5 K r v L C g 2 O x 4 W g h s M 1 y 3 g h O u g V k x d o Q y z V 1 + z I Z U U 2 Z d d 7 4 f a l D w z D I p q U r K k F V 9 E p V l q C V u k 6 r y X X N k s a e / p H f S I U G H 3 J 6 2 L 4 J 5 h y 1 0 g A 7 R M S L o D F 2 g a 3 S D u o i h H L 2 g V / T m v X s f 3 q c 3 m Y 0 2 v P n O P v o F 7 + s b 7 F S l c A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a i 7 / g 5 L 8 U P D b t a X / L T 5 9 o J d W T v 4 = " &gt; A A A C K H i c b V B N S w M x F M x W r X X 9 1 q O X Y B E 8 l Y 0 I e h Q 8 6 F H B V q G 7 l G z 2 r Q 1 N s k u S V c q y f 8 O r n v 0 1 3 q R X f 4 n Z t g e t D g S G m f f y h o l z w Y 0 N g o n X W F p e a a 6 2 1 v z 1 j c 2 t 7 Z 3 d v Z 7 J C s 2 g y z K R 6 Y e Y G h B c Q d d y K + A h 1 0 B l L O A + H l 3 W / v 0 T a M M z d W f H O U S S P i q e c k a t k 8 J Q U j u M 0 / K q G p D B T j v o B F P g v 4 T M S R v N c T P Y 9 Z p h k r F C g r J M U G P 6 J M h t V F J t O R N Q + W F h I K d s R B + h 7 6 i i E k x U T k N X + M g p C U 4 z 7 Z 6 y e K r + 3 C i p N G Y s Y z d Z h z S L X i 3 + 6 y W m / n D h u k 3 P o 5 K r v L C g 2 O x 4 W g h s M 1 y 3 g h O u g V k x d o Q y z V 1 + z I Z U U 2 Z d d 7 4 f a l D w z D I p q U r K k F V 9 E p V l q C V u k 6 r y X X N k s a e / p H f S I U G H 3 J 6 2 L 4 J 5 h y 1 0 g A 7 R M S L o D F 2 g a 3 S D u o i h H L 2 g V / T m v X s f 3 q c 3 m Y 0 2 v P n O P v o F 7 + s b 7 F S l c A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a i 7 / g 5 L 8 U P D b t a X / L T 5 9 o J d W T v 4 = " &gt; A A A C K H i c b V B N S w M x F M x W r X X 9 1 q O X Y B E 8 l Y 0 I e h Q 8 6 F H B V q G 7 l G z 2 r Q 1 N s k u S V c q y f 8 O r n v 0 1 3 q R X f 4 n Z t g e t D g S G m f f y h o l z w Y 0 N g o n X W F p e a a 6 2 1 v z 1 j c 2 t 7 Z 3 d v Z 7 J C s 2 g y z K R 6 Y e Y G h B c Q d d y K + A h 1 0 B l L O A + H l 3 W / v 0 T a M M z d W f H O U S S P i q e c k a t k 8 J Q U j u M 0 / K q G p D B T j v o B F P g v 4 T M S R v N c T P Y 9 Z p h k r F C g r J M U G P 6 J M h t V F J t O R N Q + W F h I K d s R B + h 7 6 i i E k x U T k N X + M g p C U 4 z 7 Z 6 y e K r + 3 C i p N G Y s Y z d Z h z S L X i 3 + 6 y W m / n D h u k 3 P o 5 K r v L C g 2 O x 4 W g h s M 1 y 3 g h O u g V k x d o Q y z V 1 + z I Z U U 2 Z d d 7 4 f a l D w z D I p q U r K k F V 9 E p V l q C V u k 6 r y X X N k s a e / p H f S I U G H 3 J 6 2 L 4 J 5 h y 1 0 g A 7 R M S L o D F 2 g a 3 S D u o i h H L 2 g V / T m v X s f 3 q c 3 m Y 0 2 v P n O P v o F 7 + s b 7 F S l c A = = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>H 1 &lt;</head><label>1</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " b K 6 S y s y o 0 z W n L 8 d t S f O h y h h e d f I = " &gt; A A A C K H i c b V B N S w M x F M z 6 7 f r V 6 t F L s A i e y k Y E P R a 8 9 F j B t k J 3 K d n s W w 0 m 2 S X J K m X Z v + F V z / 4 a b + L V X 2 K 2 7 U F b B w L D z H t 5 w 8 S 5 4 M Y G w Z e 3 s r q 2 v r G 5 t e 3 v 7 O 7 t H z S a h w O T F Z p B n 2 U i 0 3 c x N S C 4 g r 7 l V s B d r o H K W M A w f r y u / e E T a M M z d W s n O U S S 3 i u e c k a t k 8 J Q U v s Q p 2 W 3 G p N x o x W 0 g y n w M i F z 0 k J z 9 M Z N b y N M M l Z I U J Y J a s y I B L m N S q o t Z w I q P y w M 5 J Q 9 0 n s Y O a q o B B O V 0 9 A V P n V K g t N M u 6 c s n q q / N 0 o q j Z n I 2 E 3 W I c 2 i V 4 v / e o m p P 1 y 4 b t O r q O Q q L y w o N j u e F g L b D N e t 4 I R r Y F Z M H K F M c 5 c f s w e q K b O u O 9 8 P N S h 4 Z p m U V C V l y K o R i c o y 1 B K 3 S F X 5 r j m y 2 N M y G Z y 3 S d A m N x e t T j D v c A s d o x N 0 h g i 6 R B 3 U R T 3 U R w z l 6 A W 9 o j f v 3 f v w P r 2 v 2 e i K N 9 8 5 Q n / g f f 8 A 7 h C l c Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b K 6 S y s y o 0 z W n L 8 d t S f O h y h h e d f I = " &gt; A A A C K H i c b V B N S w M x F M z 6 7 f r V 6 t F L s A i e y k Y E P R a 8 9 F j B t k J 3 K d n s W w 0 m 2 S X J K m X Z v + F V z / 4 a b + L V X 2 K 2 7 U F b B w L D z H t 5 w 8 S 5 4 M Y G w Z e 3 s r q 2 v r G 5 t e 3 v 7 O 7 t H z S a h w O T F Z p B n 2 U i 0 3 c x N S C 4 g r 7 l V s B d r o H K W M A w f r y u / e E T a M M z d W s n O U S S 3 i u e c k a t k 8 J Q U v s Q p 2 W 3 G p N x o x W 0 g y n w M i F z 0 k J z 9 M Z N b y N M M l Z I U J Y J a s y I B L m N S q o t Z w I q P y w M 5 J Q 9 0 n s Y O a q o B B O V 0 9 A V P n V K g t N M u 6 c s n q q / N 0 o q j Z n I 2 E 3 W I c 2 i V 4 v / e o m p P 1 y 4 b t O r q O Q q L y w o N j u e F g L b D N e t 4 I R r Y F Z M H K F M c 5 c f s w e q K b O u O 9 8 P N S h 4 Z p m U V C V l y K o R i c o y 1 B K 3 S F X 5 r j m y 2 N M y G Z y 3 S d A m N x e t T j D v c A s d o x N 0 h g i 6 R B 3 U R T 3 U R w z l 6 A W 9 o j f v 3 f v w P r 2 v 2 e i K N 9 8 5 Q n / g f f 8 A 7 h C l c Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b K 6 S y s y o 0 z W n L 8 d t S f O h y h h e d f I = " &gt; A A A C K H i c b V B N S w M x F M z 6 7 f r V 6 t F L s A i e y k Y E P R a 8 9 F j B t k J 3 K d n s W w 0 m 2 S X J K m X Z v + F V z / 4 a b + L V X 2 K 2 7 U F b B w L D z H t 5 w 8 S 5 4 M Y G w Z e 3 s r q 2 v r G 5 t e 3 v 7 O 7 t H z S a h w O T F Z p B n 2 U i 0 3 c x N S C 4 g r 7 l V s B d r o H K W M A w f r y u / e E T a M M z d W s n O U S S 3 i u e c k a t k 8 J Q U v s Q p 2 W 3 G p N x o x W 0 g y n w M i F z 0 k J z 9 M Z N b y N M M l Z I U J Y J a s y I B L m N S q o t Z w I q P y w M 5 J Q 9 0 n s Y O a q o B B O V 0 9 A V P n V K g t N M u 6 c s n q q / N 0 o q j Z n I 2 E 3 W I c 2 i V 4 v / e o m p P 1 y 4 b t O r q O Q q L y w o N j u e F g L b D N e t 4 I R r Y F Z M H K F M c 5 c f s w e q K b O u O 9 8 P N S h 4 Z p m U V C V l y K o R i c o y 1 B K 3 S F X 5 r j m y 2 N M y G Z y 3 S d A m N x e t T j D v c A s d o x N 0 h g i 6 R B 3 U R T 3 U R w z l 6 A W 9 o j f v 3 f v w P r 2 v 2 e i K N 9 8 5 Q n / g f f 8 A 7 h C l c Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b K 6 S y s y o 0 z W n L 8 d t S f O h y h h e d f I = " &gt; A A A C K H i c b V B N S w M x F M z 6 7 f r V 6 t F L s A i e y k Y E P R a 8 9 F j B t k J 3 K d n s W w 0 m 2 S X J K m X Z v + F V z / 4 a b + L V X 2 K 2 7 U F b B w L D z H t 5 w 8 S 5 4 M Y G w Z e 3 s r q 2 v r G 5 t e 3 v 7 O 7 t H z S a h w O T F Z p B n 2 U i 0 3 c x N S C 4 g r 7 l V s B d r o H K W M A w f r y u / e E T a M M z d W s n O U S S 3 i u e c k a t k 8 J Q U v s Q p 2 W 3 G p N x o x W 0 g y n w M i F z 0 k J z 9 M Z N b y N M M l Z I U J Y J a s y I B L m N S q o t Z w I q P y w M 5 J Q 9 0 n s Y O a q o B B O V 0 9 A V P n V K g t N M u 6 c s n q q / N 0 o q j Z n I 2 E 3 W I c 2 i V 4 v / e o m p P 1 y 4 b t O r q O Q q L y w o N j u e F g L b D N e t 4 I R r Y F Z M H K F M c 5 c f s w e q K b O u O 9 8 P N S h 4 Z p m U V C V l y K o R i c o y 1 B K 3 S F X 5 r j m y 2 N M y G Z y 3 S d A m N x e t T j D v c A s d o x N 0 h g i 6 R B 3 U R T 3 U R w z l 6 A W 9 o j f v 3 f v w P r 2 v 2 e i K N 9 8 5 Q n / g f f 8 A 7 h C l c Q = = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>H 2 &lt; 2 .Figure 6 .</head><label>226</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " z t W h a i n N o 3 W l r Y s f C P e u t 4 a p u t I = " &gt; A A A C K H i c b V B N S 8 N A F N z 4 W e N X q 0 c v i 0 X w V B I R 9 F j w 0 m M F 2 w p N K J v N S 7 u 4 u w m 7 G 6 W E / A 2 v e v b X e J N e / S V u 2 h 6 0 d W B h m H l v 3 z B R x p k 2 n j d z N j a 3 t n d 2 a 3 v u / s H h 0 X G 9 c d L X a a 4 o 9 G j K U / U Y E Q 2 c S e g Z Z j g 8 Z g q I i D g M o q e 7 y h 8 8 g 9 I s l Q 9 m m k E o y F i y h F F i r B Q E g p h J l B S d c n Q 1 q j e 9 l j c H X i f + k j T R E t 1 R w 9 k J 4 p T m A q S h n G g 9 9 L 3 M h A V R h l E O p R v k G j J C n 8 g Y h p Z K I k C H x T x 0 i S + s E u M k V f Z J g + f q 7 4 2 C C K 2 n I r K T V U i 9 6 l X i v 1 6 s q w 9 X r p v k N i y Y z H I D k i 6 O J z n H J s V V K z h m C q j h U 0 s I V c z m x 3 R C F K H G d u e 6 g Q I J L z Q V g s i 4 C G g 5 9 M O i C J T A T b 8 s X d u c v 9 r T O u l f t X y v 5 d 9 f N 9 v e s s M a O k P n 6 B L 5 6 A a 1 U Q d 1 U Q 9 R l K F X 9 I b e n Q / n 0 / l y Z o v R D W e 5 c 4 r + w P n + A e / J p X I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z t W h a i n N o 3 W l r Y s f C P e u t 4 a p u t I = " &gt; A A A C K H i c b V B N S 8 N A F N z 4 W e N X q 0 c v i 0 X w V B I R 9 F j w 0 m M F 2 w p N K J v N S 7 u 4 u w m 7 G 6 W E / A 2 v e v b X e J N e / S V u 2 h 6 0 d W B h m H l v 3 z B R x p k 2 n j d z N j a 3 t n d 2 a 3 v u / s H h 0 X G 9 c d L X a a 4 o 9 G j K U / U Y E Q 2 c S e g Z Z j g 8 Z g q I i D g M o q e 7 y h 8 8 g 9 I s l Q 9 m m k E o y F i y h F F i r B Q E g p h J l B S d c n Q 1 q j e 9 l j c H X i f + k j T R E t 1 R w 9 k J 4 p T m A q S h n G g 9 9 L 3 M h A V R h l E O p R v k G j J C n 8 g Y h p Z K I k C H x T x 0 i S + s E u M k V f Z J g + f q 7 4 2 C C K 2 n I r K T V U i 9 6 l X i v 1 6 s q w 9 X r p v k N i y Y z H I D k i 6 O J z n H J s V V K z h m C q j h U 0 s I V c z m x 3 R C F K H G d u e 6 g Q I J L z Q V g s i 4 C G g 5 9 M O i C J T A T b 8 s X d u c v 9 r T O u l f t X y v 5 d 9 f N 9 v e s s M a O k P n 6 B L 5 6 A a 1 U Q d 1 U Q 9 R l K F X 9 I b e n Q / n 0 / l y Z o v R D W e 5 c 4 r + w P n + A e / J p X I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z t W h a i n N o 3 W l r Y s f C P e u t 4 a p u t I = " &gt; A A A C K H i c b V B N S 8 N A F N z 4 W e N X q 0 c v i 0 X w V B I R 9 F j w 0 m M F 2 w p N K J v N S 7 u 4 u w m 7 G 6 W E / A 2 v e v b X e J N e / S V u 2 h 6 0 d W B h m H l v 3 z B R x p k 2 n j d z N j a 3 t n d 2 a 3 v u / s H h 0 X G 9 c d L X a a 4 o 9 G j K U / U Y E Q 2 c S e g Z Z j g 8 Z g q I i D g M o q e 7 y h 8 8 g 9 I s l Q 9 m m k E o y F i y h F F i r B Q E g p h J l B S d c n Q 1 q j e 9 l j c H X i f + k j T R E t 1 R w 9 k J 4 p T m A q S h n G g 9 9 L 3 M h A V R h l E O p R v k G j J C n 8 g Y h p Z K I k C H x T x 0 i S + s E u M k V f Z J g + f q 7 4 2 C C K 2 n I r K T V U i 9 6 l X i v 1 6 s q w 9 X r p v k N i y Y z H I D k i 6 O J z n H J s V V K z h m C q j h U 0 s I V c z m x 3 R C F K H G d u e 6 g Q I J L z Q V g s i 4 C G g 5 9 M O i C J T A T b 8 s X d u c v 9 r T O u l f t X y v 5 d 9 f N 9 v e s s M a O k P n 6 B L 5 6 A a 1 U Q d 1 U Q 9 R l K F X 9 I b e n Q / n 0 / l y Z o v R D W e 5 c 4 r + w P n + A e / J p X I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z t W h a i n N o 3 W l r Y s f C P e u t 4 a p u t I = " &gt; A A A C K H i c b V B N S 8 N A F N z 4 W e N X q 0 c v i 0 X w V B I R 9 F j w 0 m M F 2 w p N K J v N S 7 u 4 u w m 7 G 6 W E / A 2 v e v b X e J N e / S V u 2 h 6 0 d W B h m H l v 3 z B R x p k 2 n j d z N j a 3 t n d 2 a 3 v u / s H h 0 X G 9 c d L X a a 4 o 9 G j K U / U Y E Q 2 c S e g Z Z j g 8 Z g q I i D g M o q e 7 y h 8 8 g 9 I s l Q 9 m m k E o y F i y h F F i r B Q E g p h J l B S d c n Q 1 q j e 9 l j c H X i f + k j T R E t 1 R w 9 k J 4 p T m A q S h n G g 9 9 L 3 M h A V R h l E O p R v k G j J C n 8 g Y h p Z K I k C H x T x 0 i S + s E u M k V f Z J g + f q 7 4 2 C C K 2 n I r K T V U i 9 6 l X i v 1 6 s q w 9 X r p v k N i y Y z H I D k i 6 O J z n H J s V V K z h m C q j h U 0 s I V c z m x 3 R C F K H G d u e 6 g Q I J L z Q V g s i 4 C G g 5 9 M O i C J T A T b 8 s X d u c v 9 r T O u l f t X y v 5 d 9 f N 9 v e s s M a O k P n 6 B L 5 6 A a 1 U Q d 1 U Q 9 R l K F X 9 I b e n Q / n 0 / l y Z o v R D W e 5 c 4 r + w P n + A e / J p X I = &lt; / l a t e x i t &gt; Panorama I Gold Distribution I g Predicted Distribution P Representation u the bike is resting on. ..Touchdown is on top the pillar MLP LingUNet architecture with two layers (m = 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 7 .</head><label>7</label><figDesc>SDR pixel-level predictions with LINGUNET. Red-overlaid pixels indicate the Gaussian smoothed target location. Bright green overlay indicates the model's predicted probability distribution over pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figures 9 -</head><label>9</label><figDesc><ref type="bibr" target="#b13">14</ref> show SDR pixel-level predictions for comparing the four models we used: LINGUNET, CONCAT, CONCATCONV, and CONCAT. Each figure shows the SDR description to resolve followed by the model outputs. We measure accuracy at a threshold of 80 pixels. Red-overlaid pixels visualize the Gaussian smoothed annotated target location. Green-overlaid pixels visualize the model's probability distribution over pixels.7 https://developers.google.com/maps/documentation/directions/start D. SDR Experimental Setup Details D.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>Data statistics of TOUCHDOWN, compared to related cor-</cell></row><row><cell>pora. For TOUCHDOWN, we report statistics for the complete task,</cell></row><row><cell>navigation only, and SDR only. Vocabulary size and text length are</cell></row><row><cell>computed on the combined training and development sets. SAIL</cell></row><row><cell>and LANI statistics are computed using paragraph data.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>. . . You'll pass three trashcans on your left . . . . . . a brownish colored brick building with a black fence around it. . . . . . there are two tiny green signs you can see in the distance . . . . . . up ahead there is some flag poles on your right hand side. . . Enter the next intersection and stop . . . Turn left. Continue forward . . . You should see a small bridge ahead . . .</figDesc><table><row><cell>Phenomenon</cell><cell></cell><cell>R2R</cell><cell cols="2">Overall</cell><cell cols="2">TOUCHDOWN Navigation</cell><cell></cell><cell>SDR</cell><cell>Example from TOUCHDOWN</cell></row><row><cell></cell><cell>c</cell><cell>?</cell><cell>c</cell><cell>?</cell><cell>c</cell><cell>?</cell><cell>c</cell><cell>?</cell><cell></cell></row><row><cell>Reference to unique entity</cell><cell cols="2">25 3.7</cell><cell cols="3">25 10.7 25</cell><cell>9.2</cell><cell cols="2">25 3.2</cell><cell></cell></row><row><cell>Coreference</cell><cell>8</cell><cell>0.5</cell><cell>22</cell><cell>2.4</cell><cell>15</cell><cell>1.1</cell><cell cols="2">22 1.5</cell><cell></cell></row><row><cell>Comparison</cell><cell>1</cell><cell>0.0</cell><cell>6</cell><cell>0.3</cell><cell>3</cell><cell>0.1</cell><cell>5</cell><cell>0.2</cell><cell>. . . The bear is in the middle of the closest tire.</cell></row><row><cell>Sequencing</cell><cell>4</cell><cell>0.2</cell><cell>22</cell><cell>1.9</cell><cell>21</cell><cell>1.6</cell><cell>9</cell><cell>0.4</cell><cell>. . . Turn left at the next intersection . . .</cell></row><row><cell>Count</cell><cell>4</cell><cell>0.2</cell><cell>11</cell><cell>0.5</cell><cell>9</cell><cell>0.4</cell><cell>8</cell><cell>0.3</cell><cell></cell></row><row><cell>Allocentric spatial relation</cell><cell>5</cell><cell>0.2</cell><cell>25</cell><cell>2.9</cell><cell>17</cell><cell>1.2</cell><cell cols="2">25 2.2</cell><cell>. . . There is a fire hydrant, the bear is on top</cell></row><row><cell>Egocentric spatial relation</cell><cell cols="2">20 1.2</cell><cell>25</cell><cell>4.0</cell><cell>23</cell><cell>3.6</cell><cell cols="2">19 1.1</cell><cell></cell></row><row><cell cols="10">Imperative . . . Direction 25 4.0 25 5.3 25 5.2 4 0.2 22 2.8 24 3.7 24 3.7 1 0.0 . . . Temporal condition 7 0.4 21 1.9 21 1.9 2 0.1 . . . Follow the road until you see a school on your right. . .</cell></row><row><cell>State verification</cell><cell>2</cell><cell>0.1</cell><cell>21</cell><cell>1.8</cell><cell>18</cell><cell>1.5</cell><cell cols="2">16 0.8</cell><cell>. . .</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>/ 7.60 29.36 / 10.02 32.60 / 11.42 783 LINGUNET 24.81 / 7.73 32.83 / 13.00 36.44 / 15.01 729 / 8.21 30.40 / 11.73 34.13 / 13.32 747 LINGUNET 26.11 / 8.80 34.59 / 14.57 37.81 / 16.11 708</figDesc><table><row><cell>Method</cell><cell>A/C@40px</cell><cell>A/C@80px</cell><cell cols="2">A/C@120px Dist</cell></row><row><cell cols="2">Development Results</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RANDOM CENTER AVERAGE</cell><cell>0.18 / 0.00 0.55 / 0.07 1.88 / 0.07</cell><cell>0.59 / 0.00 1.62 / 0.07 4.22 / 0.29</cell><cell>1.28 / 0.00 3.26 / 0.36 7.14 / 0.79</cell><cell>1185 777 762</cell></row><row><cell>UNET</cell><cell cols="2">10.86 / 2.69 13.94 / 3.31</cell><cell>16.69 / 3.91</cell><cell>957</cell></row><row><cell cols="3">CONCAT CONCATCONV 13.56 / 3.24 18.00 / 4.58 13.70 / 3.22 17.85 / 4.46 TEXT2CONV 24.03 Test Results</cell><cell>21.16 / 5.47 21.42 / 5.71</cell><cell>917 918</cell></row><row><cell>RANDOM CENTER AVERAGE</cell><cell>0.21 / 0.00 0.31 / 0.00 2.43 / 0.07</cell><cell>0.78 / 0.00 1.61 / 0.21 5.21 / 0.57</cell><cell>1.89 / 0.00 3.93 / 0.57 7.96 / 1.06</cell><cell>1179 759 744</cell></row><row><cell>TEXT2CONV</cell><cell>24.82</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Development and test navigation results. age features of the RGB panoramas through a request form. The complete license is available with the data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Linguistic analysis of 25 randomly sampled development examples in TOUCHDOWN, SAIL, and LANI.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Development and test navigation results using raw RGB images.</figDesc><table><row><cell>Method</cell><cell cols="2">TC SPD</cell><cell>SED</cell></row><row><cell cols="3">Development Results</cell><cell></cell></row><row><cell>RCONCAT GA</cell><cell>6.8 6.5</cell><cell cols="2">23.4 24.0 0.064 0.066</cell></row><row><cell>Test Results</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RCONCAT GA + SUP</cell><cell>9.0 7.9</cell><cell cols="2">22.6 23.4 0.076 0.086</cell></row><row><cell>Method</cell><cell></cell><cell>TC</cell><cell>SPD</cell><cell>SED</cell></row><row><cell>Development Results</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">RCONCAT NO-TEXT RCONCAT NO-IMAGE</cell><cell>24.48 35.68</cell><cell>7.26 0.22</cell><cell>0.07 0.001</cell></row><row><cell cols="5">GA NO-TEXT GA NO-IMAGE Table 8. Single-modality development results. 25.7 6.8 0.07 50.1 0.1 0.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://developers.google.com/maps/documentation/streetview/intro 1 arXiv:1811.12354v7 [cs.CV] 16 May 2020</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This is roughly the size of Touchdown. The number is not directly comparable to the SDR accuracy measures due to different scaling.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Several paths were discarded due to updates in Street View data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by a Google Faculty Award, NSF award CAREER-1750499, NSF Graduate Research Fellowship DGE-1650441, and the generosity of Eric and Wendy Schmidt by recommendation of the Schmidt Futures program. We wish to thank Jason Baldridge for his extensive help and advice, and Valts Blukis and the anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On evaluation of embodied navigation agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<idno>abs/1807.06757</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">van den Hengel. Vision-and-Language Navigation: Interpreting visuallygrounded navigation instructions in real environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards a Dataset for Human Computer Communication via Grounded Language Acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Workshop on Symbiotic Cognitive Systems</title>
		<meeting>the AAAI Workshop on Symbiotic Cognitive Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mapping Navigation Instructions to Continuous Control Actions with Position Visitation Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blukis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Knepper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Robot Learning</title>
		<meeting>the Conference on Robot Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gated-Attention Architectures for Task-Oriented Language Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Sathyendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Pasumarthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajagopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to interpret natural language navigation instructions from observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Microsoft COCO Captions: Data Collection and Evaluation Server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Embodied Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03367</idno>
		<title level="m">Talk the Walk: Navigating New York City through Grounded Dialogue</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Berant. Weakly supervised semantic parsing with abstract examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Latcinnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1809" to="1819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">IQA: Visual Question Answering in Interactive Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Refer-ItGame: Referring to Objects in Photographs of Natural Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05474</idno>
		<title level="m">AI2-THOR: An Interactive 3D Environment for Visual AI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Walk the Talk: Connecting Language, Knowledge, Action in Route Instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Macmahon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stankiewics</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generation and Comprehension of Unambiguous Object Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Joint Model of Language and Perception for Grounded Attribute Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to Navigate in Cities without a Map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blukis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niklasson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shatkhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mapping Instructions and Visual Observations to Actions with Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1004" to="1015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning Deep Representations of Fine-Grained Visual Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>U-Net</surname></persName>
		</author>
		<idno>abs/1505.04597</idno>
		<title level="m">Convolutional Networks for Biomedical Image Segmentation. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Corpus of Natural Language for Visual Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="217" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A corpus for reasoning about natural language grounded in photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<idno>abs/1811.00491</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bennnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07357</idno>
		<title level="m">CHALET: Cornell House Agent Learning Environment</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bringing Semantics into Focus Using Visual Abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3009" to="3016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
