<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MM-DFN: MULTIMODAL DYNAMIC FUSION NETWORK FOR EMOTION RECOGNITION IN CONVERSATIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dou</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ping An Life Insurance Company of China, Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ping An Life Insurance Company of China, Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingwei</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianxin</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ping An Life Insurance Company of China, Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Mo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ping An Life Insurance Company of China, Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MM-DFN: MULTIMODAL DYNAMIC FUSION NETWORK FOR EMOTION RECOGNITION IN CONVERSATIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-emotion recognition</term>
					<term>emotion recognition in conversations</term>
					<term>multimodal fusion</term>
					<term>dialogue systems</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotion Recognition in Conversations (ERC) has considerable prospects for developing empathetic machines. For multimodal ERC, it is vital to understand context and fuse modality information in conversations. Recent graph-based fusion methods generally aggregate multimodal information by exploring unimodal and cross-modal interactions in a graph. However, they accumulate redundant information at each layer, limiting the context understanding between modalities. In this paper, we propose a novel Multimodal Dynamic Fusion Network (MM-DFN) to recognize emotions by fully understanding multimodal conversational context. Specifically, we design a new graph-based dynamic fusion module to fuse multimodal context features in a conversation. The module reduces redundancy and enhances complementarity between modalities by capturing the dynamics of contextual information in different semantic spaces. Extensive experiments on two public benchmark datasets demonstrate the effectiveness and superiority of the proposed model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Emotion Recognition in Conversations (ERC) aims to detect emotions in each utterance of the conversation. It has considerable prospects for developing empathetic machines <ref type="bibr" target="#b0">[1]</ref>. This paper studies ERC under a multimodal setting, i.e., acoustic, visual, and textual modalities.</p><p>A conversation often contains rich contextual clues <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, which are essential for identifying emotions. The key success factors of multimodal ERC are accurate context understanding and multimodal fusion. Previous context-dependent works <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> model conversations as sequence or graph structures to explore contextual clues within a single modality. Although these methods can be naturally extended multimodal paradigms by performing early/late fusion such as <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, it is difficult to capture contextual interactions between modalities, which limits the utilization of multiple modalities. Besides, some carefully-designed hybrid fusion methods <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> Corresponding author. Email: HUDOU470@pingan.com.cn focus on the alignment and interaction between modalities in isolated or sequential utterances. These methods ignore complex interactions between utterances, resulting in leveraging context information in conversations insufficiently.</p><p>Recent remarkable works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> model unimodal and cross-modal interactions in a graph structure, which provides complementarity between modalities for tracking emotions. However, these graph-based fusion methods aggregate contextual information in a specific semantic space at each layer, gradually accumulating redundant information. It limits context understanding between modalities. The contextual information continuously aggregated can be regarded as specific views where each view can have its individual representation space and dynamics. We believe that modeling these dynamics of contextual information in different semantic spaces can reduce redundancy and enhance complementarity, accordingly boosting context understanding between modalities.</p><p>In this paper, we propose a novel Multimodal Dynamic Fusion Network (MM-DFN) to recognize utterance-level emotion by sufficiently understanding multimodal conversational context. Firstly, we utilize a modality encoder to track speaker states and context in each modality. Secondly, inspired by <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, we improve the graph convolutional layer <ref type="bibr" target="#b16">[17]</ref> with gating mechanisms and design a new Graphbased Dynamic Fusion (GDF) module to fuse multimodal context information. The module utilizes graph convolution operation to aggregation context information of both interand intra-modality in a specific semantic space at each layer. Meanwhile, the gating mechanism is used to learn the intrinsic sequential patterns of contextual information in adjacent semantic space. The GDF module can control information flow between layers, reducing redundancy and promoting the complementarity between modalities. The stack of GDFs can naturally fuse multimodal context features by embedding them into a dynamic semantic space. Finally, an emotion classifier is used to predict the emotion label of the utterance.</p><p>We conduct a series of experiments on two public benchmark datasets, i.e., IEMOCAP and MELD. Results consistently demonstrate that MM-DFN significantly outperforms comparison methods. The main contributions are summarized as follows: 1) We propose a novel MM-DFN to facilitate multimodal context understanding for ERC. 2) We design a new  <ref type="figure">Fig. 1</ref>. The architecture of the proposed MM-DFN. Given input multimodal features, modality encoder first captures features of context and speaker in each modality. Then, in each conversation, we construct the fully connected graph in each modality, and connect nodes corresponding to the same utterance between different modalities. Based on the graph, graph-based dynamic fusion modules are stacked to fuse multimodal context features, dynamically and sequentially. Finally, based on the concatenation of features, an emotion classifier is applied to identify emotion label of each utterance.</p><p>graph-based dynamic fusion module to fuse multimodal conversational context. This module can reduce redundancy and enhance complementarity between modalities. 3) Extensive experiments on two benchmark datasets demonstrate the effectiveness and superiority of the proposed model 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formally, given a conversation</head><formula xml:id="formula_0">U = [u 1 , ..., u N ], u i = {u a i , u v i , u t i }, where N is the number of utterances. u a i , u v i , u t i</formula><p>denote the raw feature representation of u i from the acoustic, visual, and textual modality, respectively. There are M speakers P = {p 1 , ..., p M }(M ? 2). Each utterance u i is spoken by the speaker p ?(ui) , where ? maps the index of the utterance into the corresponding speaker. Moreover, we define U ? to represent the set of utterances spoken by the party p ? .</p><formula xml:id="formula_1">U ? = {u i |u i ? U and u i spoken by p ? , ?i ? [1, N ]}, ? ? [1, M ].</formula><p>The goal of multimodal ERC is to predict the emotion label y i for each utterance u i from pre-defined emotions Y.</p><p>In this section, we propose a novel Multimodal Dynamic Fusion Network (MM-DFN) to fully understand the multimodal conversational context for ERC, as shown in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Modality Encoder</head><p>To capture context features for the textual modality, we apply a bi-directional gated recurrent unit (BiGRU); for the acoustic and visual modalities, we apply a fully connected network. The context embedding can be computed as:</p><formula xml:id="formula_2">c ? i = W ? c u ? i + b ? c , ? ? {a, v}, c t i , h c i = ? ?? ? GRU c (u t i , h c i?1 ),<label>(1)</label></formula><p>where ? ?? ? GRU c is a BiGRU to obtain context embeddings and h c i is the hidden vector.</p><formula xml:id="formula_3">W a c , W v c , b a c , b v c are trainable param- eters.</formula><p>Considering the impact of speakers in a conversation, <ref type="bibr" target="#b0">1</ref> The code is available at https://github.com/zerohd4869/MM-DFN we also employ a shared-parameter BiGRU to encode different contextual information from multiple speakers:</p><formula xml:id="formula_4">s ? i , h s ?,j = ? ?? ? GRU s (u ? i , h s ?,j?1 ), j ? [1, |U ? |], ? ? {a, v, t},<label>(2)</label></formula><p>where ? ?? ? GRU s indicates a BiGRU to obtain speaker embeddings. h s ?,j is the j-th hidden state of the party p ? . ? = ?(u i ). U ? refers to all utterances of p ? in a conversation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Graph-based Dynamic Fusion Modules</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Graph Construction</head><p>Following <ref type="bibr" target="#b12">[13]</ref>, we build an undirected graph to represent a conversation, denoted as G = (V, E). V refers to a set of nodes. Each utterance can be represented by three nodes for differentiating acoustic, visual, and textual modalities. Given N utterances, there are 3N nodes in the graph. We add both context embedding and speaker embedding to initialize the embedding of nodes in the graph:</p><formula xml:id="formula_5">x ? i = c ? i + ? ? s ? i , ? ? {a, v, t},<label>(3)</label></formula><p>where ? a , ? v , ? t are trade-off hyper-parameters. E refers to a set of edges, which are built based on two rules. The first rule is that any two nodes of the same modality in the same conversation are connected. The second rule is that each node is connected with nodes corresponding to the same utterance but from different modalities. Following <ref type="bibr" target="#b17">[18]</ref>, edge weights are computed as:</p><formula xml:id="formula_6">A ij = 1 ? arccos(sim(xi,xj )) ?</formula><p>, where sim(?) is cosine similarity function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Dynamic Fusion Module</head><p>Based on the graph, we improve <ref type="bibr" target="#b16">[17]</ref> with gating mechanisms to fuse multimodal context features in the conversation. We utilize graph convolution operation to aggregate context information of both inter-and intra-modality in a specific semantic space at each layer. Meanwhile, inspired by <ref type="bibr" target="#b14">[15]</ref>, we leverage gating mechanisms to learn intrinsic sequential patterns of contextual information in different semantic spaces. The updating process using gating mechanisms is defined as:</p><formula xml:id="formula_7">? (k) ? = ?(W g ? ? [g (k?1) , H (k?1) ] + b g ? ), ? = {u, f, o}, C (k) = tanh(W g C ? [g (k?1) , H (k?1) ] + b g C ), C (k) = ? (k) f C (k?1) + ? (k) u C (k) , g (k) = ? (k) o tanh(C (k) ),<label>(4)</label></formula><p>where ?</p><formula xml:id="formula_8">(k) u , ? (k) f , ? (k)</formula><p>o refer to the update gate, the forget gate, and the output gate in the k-th layer, respectively. g (0) is initialized with zero. W g ? , b g ? are learnable parameters. ?(?) is a sigmoid function.C (k) stores contextual information of previous layers. The update gate ? reads selectively for passing into a graph convolution operation. Following <ref type="bibr" target="#b15">[16]</ref>, the modified convolution operation can be defined as:</p><formula xml:id="formula_9">H (k) = ReLU ((1 ? ?)PH (k?1) + ?H (0) )((1 ? ? k?1 )I n + ? k?1 W (k?1) ) ,<label>(5)</label></formula><p>whereP =D ?1/2?D?1/2 is the graph convolution matrix with the renormalization trick. ?, ? k are two hyperparameters. ? k = log( ? k + 1). ? is also a hyperparameter. W (k) is the weight matrix. H (0) is initialized with X a , X v , X t . I n is an identity mapping matrix. Then, the output of k-th layer can be computed as,</p><formula xml:id="formula_10">H (k) = H (k) + g (k) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Emotion Classifier</head><p>After the stack of K layers, representations of three modalities for each utterance i can be refined as o a i , o v i , o t i . Finally, a classifier is used to predict the emotion of each utterance:</p><formula xml:id="formula_11">y i = Softmax(W z [x a i ; x v i ; x t i ; o a i ; o v i ; o t i ] + b z ),<label>(6)</label></formula><p>where W z and b z are trainable parameters. We apply crossentropy loss along with L2-regularization to train the model:</p><formula xml:id="formula_12">L = ? 1 L l=1 ? (l) L i=1 ? (i) j=1 y l i,j log(? l i,j ) + ? ? 2 ,<label>(7)</label></formula><p>where L is the total number of samples in the training set. ? (i) is the number of utterances in sample i. y l i,j and? l i,j denote the one-hot vector and probability vector for emotion class j of utterance i of sample l, respectively. ? refers to all trainable parameters. ? is the L2-regularization weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>IEMOCAP <ref type="bibr" target="#b18">[19]</ref> contains dyadic conversation videos between pairs of ten unique speakers. It includes 7,433 utterances and 151 dialogues. Each utterance is annotated with one of six emotion labels. We follow the previous studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref> that use the first four sessions for training, use the last session for testing, and randomly extract 10% of the training dialogues as validation split. MELD <ref type="bibr" target="#b1">[2]</ref> contains multi-party conversation videos collected from Friends TV series, where two or more speakers are involved in a conversation. It contains 1,433 conversations, 13,708 utterances and 304 different speakers. Each utterance is annotated with one of seven emotion labels. For a fair comparison, we conduct experiments using the predefined train/validation/test splits in MELD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparison Methods</head><p>TFN <ref type="bibr" target="#b8">[9]</ref> and LMF [10] make non-temporal multimodal fusion by tensor product. MFN <ref type="bibr" target="#b10">[11]</ref> synchronizes multimodal sequences using a multi-view gated memory. bc-LSTM <ref type="bibr" target="#b5">[6]</ref> leverages an utterance-level LSTM to capture multimodal features. ICON <ref type="bibr" target="#b6">[7]</ref>, an extension of CMN <ref type="bibr" target="#b19">[20]</ref>, provides conversational features from modalities by multi-hop memories. DialogueRNN <ref type="bibr" target="#b3">[4]</ref> introduces a recurrent network to track speaker states and context during the conversation. DialogueCRN <ref type="bibr" target="#b2">[3]</ref> designs multi-turn reasoning modules to understand conversational context. DialogueGCN <ref type="bibr" target="#b4">[5]</ref> utilizes graph structures to combine contextual dependencies. MMGCN <ref type="bibr" target="#b12">[13]</ref> uses a graph-based fusion module to capture intra-and inter-modality contextual features. All baselines are reproduced under the same environment, except <ref type="bibr" target="#b6">[7]</ref>, which is only applicable for dyadic conversation and the results are from the original paper. Because <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> are designed for unimodal ERC, a early concatenation fusion is introduced to capture multimodal features in their implementations. Implementation Details. Following <ref type="bibr" target="#b12">[13]</ref>, raw utterancelevel features of acoustic, visual, and textual modality are extracted by TextCNN <ref type="bibr" target="#b20">[21]</ref>, OpenSmile <ref type="bibr" target="#b21">[22]</ref>, and DenseNet <ref type="bibr" target="#b22">[23]</ref>, respectively. We use focal loss <ref type="bibr" target="#b23">[24]</ref> for training due to the class imbalance. The number of layers K are 16 and 32 for IEMOCAP and MELD. ? is set to 0.2 and ? is set to 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experimental Results and Analysis</head><p>Overall Results and Ablation Study. The overall results are reported in <ref type="table">Table 1</ref>. MM-DFN consistently obtains the best performance over the comparison methods on both datasets, which shows the superiority of our model. <ref type="table" target="#tab_3">Table 2</ref> shows ablation studies by removing key components of the proposed model. When removing either the graph-based dynamic fusion (GDF) module or speaker embedding (Speaker), the results decline significantly on both datasets. When further removing the context embedding (Context), the results decrease further. It shows the effectiveness of the three components.</p><p>Comparison with Different Fusion Modules. After the modality encoder, we replace GDF with the following six fusion modules: Concat/Gate Fusion, Tensor/Memory Fusion <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, Early/Late Fusion + GCN <ref type="bibr" target="#b12">[13]</ref>, and Graphbased Fusion (GF) <ref type="bibr" target="#b12">[13]</ref>. From <ref type="table">Table 3</ref>, GF and GDF outperform all fusion modules in the first block since the two  <ref type="table">Table 1</ref>. Results under the multimodal setting (A+V+T). We present the overall performance of Acc and w-F1, which mean the overall accuracy score and weighted-average F1 score, respectively. We also report F1 score per class, except two classes (i.   <ref type="table">Table 3</ref>. Results against different fusion modules. We report w-F1 score for both datasets.</p><p>graph-based fusion modules sufficiently capture intra-and inter-modality interactions in conversations, which provides complementarity between modalities. GDF achieves better performance, reducing redundancy and promoting the complementarity between modalities, which shows the superiority of multimodal fusion. Besides, for GF and GDF, we analyze the impact of inter-and intra-modality edges in the graph for fusion. Intra-/Inter-Modal refers to building edges according to the first/second rule. Ignoring any rules can hurt performance in GF and GDF, which shows that modeling contextual interactions of both inter-and intra-modality, can better utilize the complementarity between modalities. Compared with GF, GDF obtains a better performance in all variants. It shows that GDF can reduce both inter-and intra-modality redundancies and fuse multimodal context better.</p><p>Comparison under Different Modality Settings. <ref type="table">Table 4</ref> shows the results of MM-DFN and the GF-based variant under different modality settings. As expected, bimodal and trimodal models outperform the corresponding unimodal mod-  <ref type="table">Table 4</ref>. Results of graph-based fusion methods under different modality settings. Fusion modules are not used under unimodal types. We report w-F1 score for both datasets. els on both datasets. Under unimodal types, textual modality performs better than acoustic and visual. Under bimodal types, GDF outperforms GF consistently. It again confirms the superiority of GDF. Meanwhile, under acoustic and textual modalities (A+T), both GF and GDF achieve the best performance over other bimodal types, which indicates a stronger complementarity between rich textual semantics and affective audio features. GDF can reduce redundancy as well as enhance complementarity between modalities and thus obtain better results. Moreover, under acoustic and visual modalities (A+V), GDF outperforms GF by a large margin. This phenomenon reflects that the acoustic and visual features have high entanglement and redundancy, limiting the performance of GF. Our GDF encourages disentangling and reduces redundancy by controlling information flow between modalities, accordingly obtaining better fusion representations.</p><formula xml:id="formula_13">Modality IEMOCAP MELD GF GDF GF GDF A / V /</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>This paper proposes a Multimodal Dynamic Fusion Network (MM-DFN) to fully understand conversational context for multimodal ERC task. A graph-based dynamic fusion (GDF) module is designed to fuse multimodal features in a conversation. The stack of GDFs learns dynamics of contextual information in different semantic spaces, successfully reducing redundancy and enhancing complementarity between modalities. Extensive experiments on two benchmark datasets demonstrate the effectiveness and superiority of MM-DFN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>u</head><label></label><figDesc>controls what part of the contextual information is written to the memory, while the forget gate ?(k) f decides what redundant information in C (k) is deleted. The output gate ? (k) o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Input T V A ? Context Embedding Speaker Embedding Graph-based Dynamic Fusion Modules Emotion Classifier Modality Encoder Classifier + Output</head><label></label><figDesc></figDesc><table><row><cell>Visual nodes</cell></row><row><cell>Acoustic nodes</cell></row><row><cell>Textual nodes</cell></row><row><cell>Intra-modality edges</cell></row><row><cell>Inter-modality edges</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>e. Fear and Disgust) on MELD, whose results are not statistically significant due to the smaller number of training samples. Best results are highlighted in bold. * represents statistical significance over state-of-the-art scores under the paired-t test (p &lt; 0.05).</figDesc><table><row><cell>Methods</cell><cell>IEMOCAP</cell><cell>MELD</cell></row><row><cell>MM-DFN</cell><cell>68.18</cell><cell>59.46</cell></row><row><cell>-w/o GDF -w Speaker -w Context</cell><cell>63.80</cell><cell>58.50</cell></row><row><cell>-w GDF -w/o Speaker -w Context</cell><cell>66.89</cell><cell>58.45</cell></row><row><cell>-w/o GDF -w/o Speaker -w Context</cell><cell>62.90</cell><cell>58.50</cell></row><row><cell>-w/o GDF -w/o Speaker -w/o Context</cell><cell>54.81</cell><cell>58.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Ablation results of MM-DFN. We report w-F1 score for both datasets.</figDesc><table><row><cell>Fusion Modules</cell><cell>IEMOCAP</cell><cell>MELD</cell></row><row><cell>Concat / Gate Fusion</cell><cell cols="2">63.80 / 64.30 58.50 / 57.87</cell></row><row><cell>Tensor / Memory Fusion</cell><cell cols="2">61.05 / 65.51 58.54 / 58.48</cell></row><row><cell>Early / Late Fusion + GCN</cell><cell cols="2">64.19 / 65.34 58.69 / 58.43</cell></row><row><cell>Graph-based Fusion (GF)</cell><cell>67.02</cell><cell>58.54</cell></row><row><cell>-w/o Inter-Modal -w Intra-Modal</cell><cell>66.91</cell><cell>58.53</cell></row><row><cell>-w Inter-Modal -w/o Intra-Modal</cell><cell>66.11</cell><cell>58.29</cell></row><row><cell>Graph-based Dynamic Fusion (GDF)</cell><cell>68.18</cell><cell>59.46</cell></row><row><cell>-w/o Inter-Modal -w Intra-Modal</cell><cell>67.82</cell><cell>59.15</cell></row><row><cell>-w Inter-Modal -w/o Intra-Modal</cell><cell>66.22</cell><cell>58.31</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey on empathetic dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linh</forename><surname>Khanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">Z</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="50" to="70" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MELD: A multimodal multi-party dataset for emotion recognition in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dou</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Huai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP, 2021</title>
		<imprint>
			<biblScope unit="page" from="7042" to="7052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dialoguernn: An attentive RNN for emotion detection in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6818" to="6825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP, 2019</title>
		<imprint>
			<biblScope unit="page" from="154" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Context-dependent sentiment analysis in usergenerated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ICON: interactive conversational memory network for multimodal emotion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in EMNLP</title>
		<imprint>
			<biblScope unit="page" from="2594" to="2604" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CONSK-GCN: conversational semanticand knowledge-oriented graph convolutional network for multimodal emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shogo</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longbiao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME. 2021</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1103" to="1114" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient low-rank multimodal fusion with modality-specific factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bharadhwaj Lakshminarasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2247" to="2256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Memory fusion network for multi-view sequential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Mazumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in AAAI</title>
		<imprint>
			<biblScope unit="page" from="5634" to="5641" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning what and when to drop: Adaptive multimodal and contextual dynamics for emotion recognition in conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqiang</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia, 2021</title>
		<imprint>
			<biblScope unit="page" from="1064" to="1073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MMGCN: multimodal fusion via deep graph convolution network for emotion recognition in conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP, 2021</title>
		<imprint>
			<biblScope unit="page" from="5666" to="5675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal emotion recognition with capsule graph convolutional based representation fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longbiao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6339" to="6343" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fusing document, collection and label graph-based representations with word embeddings for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Skianis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fragkiskos</forename><forename type="middle">D</forename><surname>Malliaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TextGraphs@NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">IEMOCAP: interactive emotional dyadic motion capture database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lang. Resour. Evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="335" to="359" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conversational memory network for emotion recognition in dyadic dialogue videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2122" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1746" to="1751" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bj?rn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1062" to="1087" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
