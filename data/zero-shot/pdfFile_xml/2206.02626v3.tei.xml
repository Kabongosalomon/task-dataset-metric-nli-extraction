<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Infinite Recommendation Networks: A Data-Centric Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noveen</forename><surname>Sachdeva</surname></persName>
							<email>nosachde@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego ? Meta AI ?</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preet</forename><surname>Mehak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego ? Meta AI ?</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhaliwal</surname></persName>
							<email>mdhaliwal@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego ? Meta AI ?</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
							<email>carolejeanwu@meta.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego ? Meta AI ?</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
							<email>jmcauley@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego ? Meta AI ?</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Infinite Recommendation Networks: A Data-Centric Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We leverage the Neural Tangent Kernel and its equivalence to training infinitelywide neural networks to devise ?-AE: an autoencoder with infinitely-wide bottleneck layers. The outcome is a highly expressive yet simplistic recommendation model with a single hyper-parameter and a closed-form solution. Leveraging ?-AE's simplicity, we also develop DISTILL-CF for synthesizing tiny, high-fidelity data summaries which distill the most important knowledge from the extremely large and sparse user-item interaction matrix for efficient and accurate subsequent data-usage like model training, inference, architecture search, etc. This takes a data-centric approach to recommendation, where we aim to improve the quality of logged user-feedback data for subsequent modeling, independent of the learning algorithm. We particularly utilize the concept of differentiable Gumbel-sampling to handle the inherent data heterogeneity, sparsity, and semi-structuredness, while being scalable to datasets with hundreds of millions of user-item interactions. Both of our proposed approaches significantly outperform their respective state-of-theart and when used together, we observe 96 ? 105% of ?-AE's performance on the full dataset with as little as 0.1% of the original dataset size, leading us to explore the counter-intuitive question: Is more data what you need for better recommendation?</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Neural Tangent Kernel (NTK) has recently advanced the theoretical understanding of how neural networks learn <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref>. Notably, performing Kernelized Ridge Regression (KRR) with the NTK has been shown to be equivalent to training infinitely-wide neural networks for an infinite number of SGD steps. Owing to KRR's closed-form solution, these networks can be trained in a fast and efficient manner whilst not sacrificing expressivity. While the application of infinite neural networks is being explored in various learning problems <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b2">3]</ref>, detailed comparative analyses demonstrate that deep, finite-width networks tend to perform significantly better than their infinite-width counterparts for a variety of standard computer-vision tasks <ref type="bibr" target="#b30">[31]</ref>.</p><p>On the contrary, for recommendation tasks, there always has been a debate of linear vs. non-linear networks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b64">65]</ref>, along with the importance of increasing the width vs. depth of the network <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">39]</ref>. At a high level, the general conclusion is that a well-tuned, wide and linear network can outperform shallow and deep neural networks for recommendation <ref type="bibr" target="#b49">[50]</ref>. We extend this debate by introducing our model ?-AE, an autoencoder with infinitely wide bottleneck layers and examine its behavior on the recommendation task. Our evaluation demonstrates significantly improved results over state-of-the-art (SoTA) models across various datasets and evaluation metrics. <ref type="bibr">36th</ref> Conference on Neural Information Processing Systems (NeurIPS 2022).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2206.02626v3 [cs.IR] 12 Oct 2022</head><p>A rising challenge in recommender systems research has been the increased cost of training models on massive datasets which can involve billions of user-item interaction logs, in terms of time, computational resources, as well as the downstream carbon footprint. Moreover, despite anonymization efforts, privacy risks have been associated with publicly released user data <ref type="bibr" target="#b37">[38]</ref>. To this end, we further explore recommendation from a data-centric viewpoint <ref type="bibr" target="#b59">[60]</ref>, which we loosely define as: Definition 1.1. (Data-centric AI) Systematic methods for improving the collected data's quality, thereby shifting the focus from merely acquiring large quantities of data; implicitly helping in a learning algorithm's generalization, scalability, and eco-sustainability.</p><p>Previous work on data-centric techniques generally involve constructing terse data summaries of large datasets, and has focused on domains with continuous, real-valued features such as images <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b40">41]</ref>, which are arguably more amenable to data optimization approaches. Due to the heterogeneity, sparsity, and semi-structuredness of recommendation data, such methods are not directly applicable. Common approaches for scaling down such recommendation datasets typically include heuristics such as random, head-user, or k-core sampling. More principled approaches include coreset construction <ref type="bibr" target="#b56">[57]</ref> that focus on optimizing for picking the set of data-points that are the most representative from a given dataset, and are generally shown to out-perform various heuristic sampling strategies. However, synthesizing informative summaries for recommendation datasets still remains a challenge.</p><p>Consequently, we propose DISTILL-CF, a data distillation framework for collaborative filtering (CF) datasets that utilizes ?-AE in a bilevel optimization objective to create highly compressed, informative, and generic data summaries. DISTILL-CF employs efficient multi-step differentiable Gumbel-sampling <ref type="bibr" target="#b22">[23]</ref> at each step of the optimization to encompass the heterogeneity, sparsity, and semi-structuredness of recommendation data. We further provide an analysis of the denoising effect observed when training the model on the synthesized versus the full dataset.</p><p>To summarize, in this paper, we make the following contributions:</p><p>? We develop ?-AE: an infinite-width autoencoder for recommendation, that aims to reconstruct the incomplete preferences in a user's item consumption sequence. We demonstrate its efficacy on four datasets, and demonstrate that ?-AE outperforms complicated SoTA models with only a single fully-connected layer, closed-form optimization, and a single hyper-parameter. We believe our work to be the first to demonstrate that an infinite-width network can outperform their finite-width SoTA counterparts for practical scenarios like recommendation. ? We subsequently develop DISTILL-CF: a novel data distillation framework that can synthesize tiny yet accurate data summaries for a variety of modeling applications. We empirically demonstrate similar performance of models trained on the full dataset versus training the same models on 2 ? 3 orders smaller data summaries synthesized by DISTILL-CF. Notably, DISTILL-CF and ?-AE are complementary for each other's practicality, as ?-AE's closed-form solution enables DISTILL-CF to scale to datasets with hundreds of millions of interactions; whereas, DISTILL-CF's succinct data summaries help improving ?-AE's restrictive training complexity, and achieving SoTA performance when trained on these tiny data summaries. ? Finally, we also note that DISTILL-CF has a strong data denoising effect, validated with a counter-intuitive observation -if there is noise in the original data, models trained on less data synthesized by DISTILL-CF can be better than the same model trained on the entire original dataset. Such observations, along with the strong data compression results attained by DISTILL-CF, reinforce our data-centric viewpoint to recommendation, encouraging the community to think more about the quality of collected data, rather than its quantity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Autoencoders for recommendation. Recent approaches to implicit feedback recommendation involve building models that can re-construct an incomplete user preference list using autoencoders <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b32">33]</ref>. The CDAE model <ref type="bibr" target="#b63">[64]</ref> first introduced this idea and used a standard denoising autoencoder for recommending new items to users. MVAE <ref type="bibr" target="#b31">[32]</ref> later extended this idea to use variational autoencoders, and provided a principled approach to perform variational inference for this model architecture. More recently, EASE <ref type="bibr" target="#b58">[59]</ref> proposed using a shallow autoencoder and estimates only an item-item similarity matrix by performing ordinary least squares regression on the relevance matrix, resulting in closed-form optimization.</p><p>Infinite neural networks. The Neural Tangent Kernel (NTK) <ref type="bibr" target="#b19">[20]</ref> has gained significant attention because of its equivalence to training infinitely-wide neural networks by performing KRR with the NTK. Recent work also demonstrated the double-descent risk curve <ref type="bibr" target="#b3">[4]</ref> that extends the classical regime of train vs. test error for overparameterized neural networks, and plays a crucial role in the generalization capabilities of such infinite neural networks. However, despite this emerging promise of utilizing NTK for learning problems, detailed comparative analyses <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b1">2]</ref> for computer vision tasks demonstrate that finite-width networks are still significantly more accurate than infinite-width ones. Looking at recommendation systems, <ref type="bibr" target="#b64">[65]</ref> performed a theoretical comparison between Matrix Factorization (MF) and Neural MF <ref type="bibr" target="#b17">[18]</ref> by studying their expressivity in the infinite-width limit, comparing the NTK of both of these algorithms. Notably, their settings involved the typical (user-ID, item-ID) input to the recommendation model, and observed results that were equivalent to a random predictor. <ref type="bibr" target="#b47">[48]</ref> performed a similar study that performed matrix completion using the NTK of a single layer fully-connected neural network, but assumed meaningful feature-priors available beforehand.</p><p>Data sampling &amp; distillation. Data sampling is ubiquitous -sampling negatives while contrastive learning <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b24">25]</ref>, sampling large datasets for fast experimentation <ref type="bibr" target="#b56">[57]</ref>, sampling for evaluating expensive metrics <ref type="bibr" target="#b29">[30]</ref>, etc. In this paper, we primarily focus on sampling at the dataset level, principled approaches of which can be categorized into: (1) coreset construction methods that aim to pick the most useful datapoints for subsequent model training <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b25">26]</ref>. These methods typically assume the availability of a submodular set-function f : V ? R + ? V ? X for a given dataset X (see <ref type="bibr" target="#b5">[6]</ref> for a systematic review on submodularity), and use this set-function f as a proxy to guide the search for the most informative subset; and (2) dataset distillation: instead of picking the most informative data-points, dataset distillation techniques aim to synthesize data-points that can accurately distill the knowledge from the entire dataset into a small, synthetic data summary. Originally proposed in <ref type="bibr" target="#b61">[62]</ref>, the authors built upon the notions of gradient-based hyper-parameter optimization <ref type="bibr" target="#b33">[34]</ref> to synthesize representative images for model training. Subsequently, a series of works <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> propose various subtle modifications to the original framework, for improving the sample-complexities of models trained on data synthesized using their algorithms. Note that such distillation techniques focused on continuous data like images, which are trivial to optimize in the original data-distillation framework. More recently, <ref type="bibr" target="#b23">[24]</ref> proposed a distillation technique for synthesizing fake graphs, but also assumed to have innate node representations available beforehand, prohibiting their method's application for recommendation data.</p><p>3 ?-AE: Infinite-width Autoencoders for Recommendation Notation. Given a recommendation dataset D := {(user i , item i , relevance i )} n i=1 consisting of n user-item interactions defined over a set of users U, set of items I, and operating over a binary relevance score (implicit feedback); we aim to best model user preferences for item recommendation. The given dataset D can also be viewed as an interaction matrix, X ? R |U |?|I| where each entry X u,i either represents the observed relevance for item i by user u or is missing. Note that X is typically extremely sparse, i.e., n |U | ? |I|. More formally, we define the problem of recommendation as accurate likelihood modeling of P(i | u, D) ?u ? U, ?i ? I.</p><p>Model. ?-AE takes an autoencoder approach to recommendation, where the all of the bottleneck layers are infinitely-wide. Firstly, to make the original bi-variate problem of which item to recommend to which user more amenable for autoencoders, we make a simplifying assumption that a user can be represented only by their historic interactions with items, i.e., the much larger set of users lie in the linear span of items. This gives us two kinds of modeling advantages: (1) we no longer have to find a unique latent representation of users; and (2) allows ?-AE to be trivially applicable for any user not in U. More specifically, for a given user u, we represent it by the sparse, bag-of-words vector of historical interactions with items X u ? R |I| , which is simply the u th row in X. We then employ the Neural Tangent Kernel (NTK) <ref type="bibr" target="#b19">[20]</ref> of an autoencoder that takes the bag-of-items representation of users as input and aims to reconstruct it. Due to the infinite-width correspondence <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2]</ref>, performing Kernelized Ridge Regression (KRR) with the estimated NTK is equivalent to training an infinitely-wide autoencoder for an infinite number of SGD steps. More formally, given the NTK, K : R |I| ? R |I| ? R over an RKHS H of a single-layer autoencoder with an activation function ? (see <ref type="bibr" target="#b46">[47]</ref> for the NTK derivation of a fully-connected neural network), we reduce the recommendation problem to KRR as follows:</p><formula xml:id="formula_0">arg min [?j ] |U | j=1 u?U f (X u | ?) ? X u 2 2 + ? ? f 2 H s.t. f (X u | ?) = |U | j=1 ? j ? K(X u , X uj ) ; K(X u , X v ) =?(X T u X v ) +? (X T u X v ) ? X T u X v (1)</formula><p>Where ? is a fixed regularization hyper-parameter, ? := [? 1 ; ? 2 ? ? ? ; ? |U | ] ? R |U |?|I| are the set of dual parameters we are interested in estimating,? represents the dual activation of ? <ref type="bibr" target="#b13">[14]</ref>, and? represents its derivative. Defining a gramian matrix K ? R |U |?|U | where each element can intuitively be seen as the similarity of two users, i.e., K i,j := K(X ui , X uj ), the optimization problem defined in Equation (1) has a closed-form solution given by? = (K + ?I) ?1 ? X. We can subsequently perform inference for any novel user as follows:P(? | u, D) = softmax(f (X u |?)). We also provide ?-AE's training and inference pseudo-codes in Appendix A, Algorithms 1 and 2.</p><p>Scalability. We carefully examine the computational cost of ?-AE's training and inference. Starting with training, ?-AE has the following computationally-expensive steps: (1) computing the gramian matrix K; and (2) performing its inversion. The overall training time complexity thus comes out to be O(|U| 2 ? |I| + |U| 2.376 ) if we use the Coppersmith-Winograd algorithm <ref type="bibr" target="#b11">[12]</ref> for matrix inversion, whereas the memory complexity is O(|U| ? |I| + |U| 2 ) for storing the data matrix X and the gramian matrix K. As for inference for a single user, both the time and memory requirements are O(|U| ? |I|). Observing these computational complexities, we note that ?-AE can be difficult to scale-up to larger datasets na?vely. To this effect, we address ?-AE's scalability challenges in DISTILL-CF (Section 4), by leveraging a simple observation from support vector machines: not all datapoints (users in our case) are important for model learning. Additionally, in practice, we can perform all of these matrix operations on accelerators like GPU/TPUs and achieve a much higher throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISTILL-CF</head><p>Motivation. Representative sampling of large datasets has numerous downstream applications. Consequently, in this section we develop DISTILL-CF: a data distillation framework to synthesize small, high-fidelity data summaries for collaborative filtering (CF) data. We design DISTILL-CF with the following rationales: (1) mitigating the scalability challenges in ?-AE by training it only on the terse data summaries generated by DISTILL-CF; (2) improving the sample complexity of existing, finite-width recommendation models; (3) addressing the privacy risks of releasing user feedback data by releasing their synthetic data summaries instead; and (4) abating the downstream CO 2 emissions of frequent, large-scale recommendation model training by estimating their parameters only on much smaller data summaries synthesized by DISTILL-CF.</p><p>Challenges. Existing work in data distillation has focused on continuous domain data such as images <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b65">66]</ref>, and are not directly applicable to heterogeneous and semi-structured domains such as recommender systems and graphs. This problem is further exacerbated since the data for these tasks is severely sparse. For example, in recommendation scenarios, a vast majority of users interact with very few items <ref type="bibr" target="#b21">[22]</ref>. Likewise, the nodes in a number of graph-based datasets tend to have connections with very small set of nodes <ref type="bibr" target="#b67">[68]</ref>. We will later show how our DISTILL-CF framework is elegantly able to circumvent both these issues while being accurate, and scalable to large datasets.</p><p>Methodology. Given a recommendation dataset D, we aim to synthesize a smaller, support dataset D s that can match the performance of recommendation models ? : U ? I ? R when trained on D versus D s . We take inspiration from <ref type="bibr" target="#b39">[40]</ref>, which is also a data distillation technique albeit for images. Formally, given a recommendation model ?, a held-out validation set D val , and a differentiable loss function l : R ? R ? R that measures the correctness of a prediction with the actual user-item relevance, the data distillation task can be viewed as the following bilevel optimization problem:</p><formula xml:id="formula_1">arg min D s E (u,i,r)?D val [l (? * D s (u, i), r)] ; s.t. ? * D s := arg min ? E (u,i,r)?D s [l(?(u, i), r)] (2)</formula><p>This optimization problem has an outer loop which searches for the most informative support dataset D s given a fixed recommendation model ? * , whereas the inner loop aims to find the optimal recommendation model for a fixed support set. In DISTILL-CF, we use ?-AE as the model of choice at each step of the inner loop for two reasons: (1) as outlined in Section 3, ?-AE has a closed-form solution with a single hyper-parameter ?, making the inner-loop extremely efficient; and (2) due to the infinite-width correspondence <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2]</ref>, ?-AE is equivalent to training an infinitely-wide autoencoder on D s , thereby not compromising on performance.</p><p>For the outer loop, we focus only on synthesizing fake users (given a fixed user budget ?) through a learnable matrix X s ? R ??|I| which stores the interactions for each fake user in the support dataset. However, to handle the discrete nature of the recommendation problem, instead of directly optimizing for X s , DISTILL-CF instead learns a continuous prior for each user-item pair, denoting the importance of sampling that interaction in our support set (similar to the notion of propensity <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b57">58]</ref>). We then sampleX s ? X s to get our final, discrete support set.</p><p>Instead of keeping this sampling operation post-hoc, i.e., after solving for the optimization problem in Equation <ref type="formula">(2)</ref>; we perform differentiable Gumbel-sampling <ref type="bibr" target="#b22">[23]</ref> on each row (user) in X s at every optimization step, thereby ensuring search only over sparse and discrete support sets. A notable property of recommendation data is that each user can interact with a variable number of items (this distribution is typically long-tailed due to Zipf's law <ref type="bibr" target="#b62">[63]</ref>). We circumvent this dynamic user sparsity issue by taking multiple Gumbel-samples for each user, with replacement. This implicitly gives DISTILL-CF the freedom to control the user and item popularity distributions in the generated data summaryX s by adjusting the entropy in the prior-matrix X s .</p><p>Having controlled for the discrete and dynamic nature of recommendation data by the multi-step Gumbel-sampling trick, we further focus on maintaining the sparsity of the synthesized data. To do so, in addition to the outer-loop reconstruction loss, we add an L1-penalty overX s to promote and explicitly control its sparsity <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">Chapter 3]</ref>. Furthermore, tuning the number of Gumbel samples we take for each fake user, gives us more control over the sparsity in our generated data summary. More formally, the final optimization objective in DISTILL-CF can be written as:</p><formula xml:id="formula_2">arg min X s E u?U X u ? log(K XuX s ? ?) + (1 ? X u ) ? log(1 ? K XuX s ? ?) + ? 2 ? ||X s || 1 s.t. ? = (KX sX s + ?I) ?1 ?X s ;X s = ? ? i=1 Gumbel ? (softmax(X s ))<label>(3)</label></formula><p>Where, ? 2 represents an appropriate regularization hyper-parameter for minimizing the L1-norm of the sampled support setX s , K XY represents the gramian matrix for the NTK of ?-AE over X and Y as inputs, ? represents the temperature hyper-parameter for Gumbel-sampling, ? denotes the number of samples to take for each fake user in X s , and ? represents an appropriate activation function which clips all values over 1 to be exactly 1, thereby keepingX s binary. We use hard-tanh in our experiments. We also provide DISTILL-CF's pseudo-code in Appendix A, Algorithm 3.</p><p>Scalability. We now analyze the time and memory requirements for optimizing Equation <ref type="formula" target="#formula_2">(3)</ref>. The inner loop's major component clearly shares the same complexity as ?-AE. However, since the parameters of ?-AE (? in Equation <ref type="formula">(1)</ref>) are now being estimated over the much smaller support set X s , the time complexity reduces to O(? 2 ? |I|) and memory to O(? ? |I|), where ? typically only lies in the range of hundreds for competitive performance. On the other hand, for performing multistep Gumbel-sampling for each synthetic user, the memory complexity of a na?ve implementation would be O(? ? ? ? |I|) since AutoGrad stores all intermediary variables for backward computation. However, since the gradient of each Gumbel-sampling step is independent of other sampling steps and can be computed individually, using jax.custom_vjp, we reduced its memory complexity to O(? ? |I|), adding nothing to the overall inner-loop complexity.</p><p>For the outer loop, we optimize the logistic reconstruction loss using SGD where we randomly sample a batch of b users from U and update X s directly. In totality, for an ? number of outer-loop iterations, the time complexity to run DISTILL-CF is O(? ? (? 2 + b + b ? ?) ? |I|), and O(b ? ? + (? + b) ? |I|) for memory. In our experiments, we note convergence in only hundreds of outer-loop iterations, making DISTILL-CF scalable even for the largest of the publicly available datasets and practically useful.  <ref type="table" target="#tab_4">Table 2</ref>. Bold values represent the best in a given row, and underlined represent the second-best. Results for ?-AE on the Netflix dataset (marked with a *) consist of random user-sampling with a max budget of 25K ? 5.4% users, and results for DISTILL-CF + ?-AE have a user-budget of 500 for all datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Setup. We use four recommendation datasets with varying sizes and sparsity characteristics. A brief set of data statistics can be found in Appendix B.3, <ref type="table" target="#tab_4">Table 2</ref>. For each user in the dataset, we randomly split their interaction history into 80/10/10% train-test-validation splits. Following recent warnings against unrealistic dense preprocessing of recommender system datasets <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b56">57]</ref>, we only prune users that have fewer than 3 interactions to enforce at least one interaction per user in the train, test, and validation sets. No such preprocessing is followed for items.</p><p>Competitor methods &amp; evaluation metrics. We compare ?-AE with various baseline and SoTA competitors as surveyed in recent comparative analyses <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref>. More details on their architectures can be found in Appendix B.1. We evaluate all models on a variety of pertinent ranking metrics, namely AUC, HitRate (HR@k), Normalized Discounted Cumulative Gain (nDCG@k), and Propensity-scored Precision (PSP@k), each focusing on different components of the algorithm performance. A notable addition to our list of metrics compared to the literature is the PSP metric <ref type="bibr" target="#b21">[22]</ref>, which we adapt to the recommendation use case as an indicator of performance on tail items. The exact definition of all of these metrics can be found in Appendix B.5.</p><p>Training details. We implement both ?-AE and DISTILL-CF using JAX <ref type="bibr" target="#b7">[8]</ref>   Does ?-AE outperform existing methods? We compare the performance of ?-AE with various baseline and competitor methods in <ref type="table" target="#tab_0">Table 1</ref>. We also include the results of training ?-AE on data synthesized by DISTILL-CF with an additional constraint of having a budget of only ? = 500 synthetic users. For the sake of reference, for our largest dataset (Netflix), this equates to a mere 0.1% of the total users. There are a few prominent observations from the results in <ref type="table" target="#tab_0">Table 1</ref>. First, ?-AE significantly outperforms SoTA recommendation algorithms despite having only a single fully-connected layer, and also being much simpler to train and implement. Second, we note that ?-AE trained on just 500 users generated by DISTILL-CF is able to attain 96 ? 105% of ?-AE's performance on the full dataset while also outperforming all competitors trained on the full dataset.</p><p>How sample efficient is ?-AE? Having noted from <ref type="table" target="#tab_0">Table 1</ref> that ?-AE is able to outperform all SoTA competitors with as little as 0.1% of the original users, we now aim to better understand ?-AE's sample complexity, i.e., the amount of training data ?-AE needs in order to perform accurate recommendation. In addition to DISTILL-CF, we use the following popular heuristics for down-sampling (more details in Appendix B.2): interaction random negative sampling (RNS); user RNS; head user sampling; and a coreset construction technique, SVP-CF user <ref type="bibr" target="#b56">[57]</ref>. We then train ?-AE on sampled data for different sampling budgets, while evaluating on the original test-set. We plot the performance for all datasets computed over the HR@10 and PSP@10 metrics in <ref type="figure">Figure 1</ref>.</p><p>We observe that while all heuristic sampling strategies tend to be closely bound to the identity line with a slight preference to user RNS, ?-AE when trained on data synthesized by DISTILL-CF tends to quickly saturate in terms of performance when the user budget is increased, even on the log-scale. This indicates DISTILL-CF's superiority in generating terse data summaries for ?-AE, thereby allowing it to get SoTA performance on the largest datasets with as little as 500 users. Results for the remaining metrics can be found in Appendix B.6, <ref type="figure" target="#fig_2">Figure 15</ref>.</p><p>How transferable are the data summaries synthesized by DISTILL-CF? In order to best evaluate the quality and universality of data summaries synthesized by DISTILL-CF, we train and evaluate EASE <ref type="bibr" target="#b58">[59]</ref> on data synthesized by DISTILL-CF. Note that the inner loop of DISTILL-CF still consists of ?-AE, but we nevertheless train and evaluate EASE to test the synthesized data's universality. We re-use the heuristic sampling strategies from the previous experiment for comparison with DISTILL-CF. From the results in <ref type="figure">Figure 2</ref>, we observe similar scaling laws as ?-AE's for the heuristic samplers as well as DISTILL-CF. The semantically similar results for MVAE <ref type="bibr" target="#b31">[32]</ref> are presented in Appendix B.6, <ref type="figure" target="#fig_8">Figure 10</ref> for completeness. This behaviour validates the re-usability of data summaries generated by DISTILL-CF, because they transfer well to SoTA finite-width models, which were not involved in DISTILL-CF's user synthesis optimization.</p><p>How robust are DISTILL-CF and ?-AE to noise? User feedback data is often noisy due to various biases (see <ref type="bibr" target="#b9">[10]</ref> for a detailed review). Furthermore, due to the significant number of logged interactions in these datasets, recommender systems are often trained on down-sampled data in practice. Despite this, to the best of our knowledge, there is no prior work that explicitly studies the interplay between noise in the data and how sampling it affects downstream model performance.</p><p>Consequently, we simulate a simple experiment: we add x% noise in the original train-set ? sample the noisy training data ? train recommendation models on the sampled data ? evaluate their performance on the original, noise-free test-set. For the noise model, we randomly flip x% of the total number of items in the corpus for each user. In <ref type="figure">Figure 3</ref>, we compare the drop in HR@10 the EASE model suffers for different sampling strategies when different levels of noise are added to the MovieLens-1M dataset <ref type="bibr" target="#b14">[15]</ref>. We make a few main observations: (1) unsurprisingly, sampling noisy data compounds the performance losses of learning algorithms; (2) DISTILL-CF has the best noise:sampling:performance trade-off compared to other sampling strategies, with an increasing performance gap relative to other samplers as we inject more noise into the original data; and <ref type="formula" target="#formula_2">(3)</ref> as we down-sample noisy data more aggressively, head user sampling improves relative to other samplers, simply because these head users are the least affected by our noise injection procedure.</p><p>Furthermore, to better understand ?-AE's denoising capabilities, we repeat the aforementioned noise-injection experiment but now train ?-AE on down-sampled, noisy data. In <ref type="figure">Figure 4</ref>, we track the change in ?-AE's performance as a function of the number of users sampled, and the amount of noise injected before sampling. We also add the semantically equivalent results for the EASE model for reference. Firstly, we note that the full-data performance-gap between ?-AE and EASE significantly increases when there is more noise in the data, demonstrating ?-AE's robustness to noise, even when its not specifically optimized for it. Furthermore, looking at the 5% noise injection scenario, we notice two counter-intuitive observations: (1) training EASE on tiny data summaries synthesized by DISTILL-CF is better than training it on the full data; and (2) solely looking at data synthesized by DISTILL-CF for EASE, we notice the best performance when we have a lower user sampling budget. Both of these observations call for more investigation of a data-centric viewpoint to recommendation, i.e., focusing more on the quality of collected data rather than its quantity. Applications to continual learning. Continual learning (see <ref type="bibr" target="#b44">[45]</ref> for a detailed review) is an important area for recommender systems, because these systems are typically updated at regular intervals. A continual learning scenario involves data that is split into multiple periods, with the predictive task being: given data until the i th period, maximize algorithm performance for prediction on the (i + 1) th period. ADER <ref type="bibr" target="#b34">[35]</ref> is a SoTA continual learning model for recommender systems, that injects the most informative user sequences from the last period to combat the catastrophic forgetting problem <ref type="bibr" target="#b51">[52]</ref>. An intuitive application for DISTILL-CF is to synthesize succinct data summaries of the last period and inject these instead. To compare these approaches, we simulate a continual learning scenario by splitting the MovieLens-1M dataset into 17 equal sized epochs, and perform experiments with MVAE <ref type="bibr" target="#b31">[32]</ref> for each period. Note that in DISTILL-CF, we still use ?-AE to synthesize data summaries (inner loop). We also compare with two baselines: (1) Joint: concatenate the data from all periods before the current; and (2) Individual: use the data only from the current period. As we can see from <ref type="figure" target="#fig_2">Figure 5</ref>, DISTILL-CF consistently outperforms ADER and the baselines, again demonstrating its ability to generate high-fidelity data summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion &amp; Future Work</head><p>In this work, we proposed two complementary ideas: ?-AE, an infinite-width autoencoder for modeling recommendation data, and DISTILL-CF for creating tiny, high-fidelity data summaries of massive datasets for subsequent model training. To our knowledge, our work is the first to employ and demonstrate that infinite-width neural networks can beat complicated SoTA models on recommendation tasks. Further, the data summaries synthesized through DISTILL-CF outperform generic samplers and demonstrate further performance gains for ?-AE as well as finite-width SoTA models despite being trained on orders of magnitude less data.</p><p>Both our proposed methods are closely linked with one another: ?-AE's closed-loop formulation is especially crucial in the practicality of DISTILL-CF, whereas DISTILL-CF's ability to distill the entire dataset's knowledge into small summaries helps ?-AE to scale to large datasets. Moreover, the Gumbel sampling trick enables us to adapt data distillation techniques designed for continuous, real-valued, dense domains to heterogeneous, semi-structured, and sparse domains like recommender systems and graphs. We additionally explore the strong denoising effect observed with DISTILL-CF, noting that in the case of noisy data, models trained on considerably less data synthesized by DISTILL-CF perform better than the same model trained on the entire original dataset. These observations lead us to contemplate a much larger, looming question: Is more data what you need for recommendation?</p><p>Our results call for further investigation on the data-centric viewpoint of recommendation.</p><p>The findings of our paper open up numerous promising research directions. First, building such closed-form, easy-to-implement infinite networks is beneficial for various downstream practical applications like search, sequential recommendation, or CTR prediction. Further, the anonymization achieved by synthesizing fake data summaries is crucial for mitigating the privacy risks associated with confidential or PII datasets. Another direction is analyzing the environmental impact and reduction in carbon footprint as our experiments show that models can achieve similar performance gains when trained on much less data. </p><formula xml:id="formula_3">K u,v ? K(X u , X v ) ?u ? U, v ? U 4: ? ? (K + ?I) ?1 ? X 5: return ? Algorithm 2 ?-AE inference Input: User set U ; dataset X ? R |U |?|I| ; NTK K : R |I| ?R |I| ? R; dual params. ? ? R |U |?|I| ; inference user historyX u ? R |I| Output: Prediction? ? R |I| 1: procedure PREDICT(U , X,X u , K, ?) 2: K ? [0] |U | Zero Initialization 3: K v ? K(X u , X v ) ?v ? U 4:? ? sof tmax(K ? ?) 5:</formula><p>return? Algorithm 3 Data synthesis using DISTILL-CF</p><formula xml:id="formula_4">Input: User set U ; dataset X ? R |U |?|I| ; NTK K : R |I| ? R |I| ? R ; support user size ? ? R ; gumbel softmax temperature ? ? R ; reg. const. ? 2 ? R ; SGD batch-size b, step-size ? ? R Output: Synthesized data X s ? R ??|I| 1: procedure SAMPLE(n, U, X) 2: U ? U</formula><p>Randomly sample n users from U 3:</p><p>X ? X u ?u ? U Retrieve corresponding rows from X 4:</p><formula xml:id="formula_5">return U , X 5: procedure SYNTHESIZE(U , X, K) 6: U s , X s ? SAMPLE(?, U, X)</formula><p>Sample support data <ref type="bibr">7:</ref> for steps = 0 . . . ? do 8:</p><formula xml:id="formula_6">X s ? ? [ ? i=1 gumbel ? (sof tmax(X s ))] 9: ? s ? FIT(U s ,X s , K)</formula><p>Fit ?-AE on support data 10:</p><formula xml:id="formula_7">U b , X b ? SAMPLE(b, U, X) 11:X ? [0] b?|I| 12:X u ? PREDICT(U s ,X s , X b u , K, ? s ) ?u ? U b</formula><p>Predict for all sampled users <ref type="bibr" target="#b12">13</ref>:</p><formula xml:id="formula_8">L ? X b ? log(X) + (1 ? X b ) ? log(1 ?X) + ? 2 ? ||X s || 1</formula><p>Re-construction loss <ref type="bibr" target="#b13">14</ref>:</p><formula xml:id="formula_9">X s ? X s ? ? ? ?L ?X s SGD on X s 15:</formula><p>return X s B Appendix: Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Baselines &amp; Competitor Methods</head><p>We provide a high-level overview of the competitor models used in our experiments:</p><p>? PopRec: This implicit-feedback baseline simply recommends the most popular items in the catalog irrespective of the user. Popularity of an item is estimated by their empirical frequency in the logged train-set. ? Bias-only: This baseline learns scalar user and item biases for all users and item in the train-set, optimized by solving a least-squares regression problem between the predicted and observed relevance. More formally, given a user u and an item i, the relevance is predicted a? r u,i = ? + ? u + ? i , where ? ? R is a global offset bias, and ? u , ? i ? R are the user and item specific biases respectively. This model doesn't consider any cross user-item interactions, and hence lacks expressivity. ? MF: Building on top of the bias-only baseline, the Matrix Factorization algorithm tries to represent the users and items in a shared latent space, modeling their relevance by the dotproduct of their representations. More formally, given a user u and an item i, the relevance is predicted asr u,i = ? + ? u + ? i + (? u ? ? i ), where ?, ? u , ? i are global, user, and item biases respectively, and ? u , ? i ? R d represent the learned user and item representations. The biases and latent representations in this model are estimated by optimizing for the Bayesian Personalized Ranking (BPR) loss <ref type="bibr" target="#b48">[49]</ref>.</p><p>? NeuMF <ref type="bibr" target="#b17">[18]</ref>: As a neural extension to MF, Neural Matrix Factorization aims to replace the linear cross-interaction between the user and item representations with an arbitrarily complex, non-linear neural network. More formally, given a user u and an item i, the relevance is predicted</p><formula xml:id="formula_10">asr u,i = ? + ? u + ? i + ?(? u , ? i ),</formula><p>where ?, ? u , ? i are global, user, and item biases respectively, ? u , ? i ? R d represent the learned user and item representations, and ? :</p><formula xml:id="formula_11">R d ? R d ? R is a neural network.</formula><p>The parameters for this model are again optimized using the BPR loss.</p><p>? MVAE <ref type="bibr" target="#b31">[32]</ref>: This method proposed using variational auto-encoders for the task of collaborative filtering. Their main contribution was to provide a principled approach to perform variational inference for the task of collaborative filtering.</p><p>? LightGCN <ref type="bibr" target="#b16">[17]</ref>: This simplistic Graph Convolution Network (GCN) framework removes all the steps in a typical GCN <ref type="bibr" target="#b27">[28]</ref>, only keeping a linear neighbourhood aggregation step. This light model demonstrated promising results for the collaborative filtering scenario, despite its simple architecture. We use the official public implementation 3 for our experiments.</p><p>? EASE <ref type="bibr" target="#b58">[59]</ref>: This linear model proposed doing ordinary least squares regression by estimating an item-item similarity matrix, that can be viewed as a zero-depth auto-encoder. Performing regression gives them the benefit of having a closed-form solution. Despite its simplicity, EASE has been shown to out-perform most of the deep non-linear neural networks for the task of collaborative filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Sampling strategies</head><p>Given a recommendation dataset D := {(user i , item i , relevance i )} n i=1 consisting of n user-item interactions defined over a set of users U, set of items I, and operating over a binary relevance score (implicit feedback); we aim to make a p% sub-sample of D, defined in terms of number of interactions. Below are the different sampling strategies we used in comparison with DISTILL-CF:</p><p>? Interaction-RNS: Randomly sample p% interactions from D.</p><p>? User-RNS: Randomly sample a user u ? U, and add all of its interactions into the sampled set.</p><p>Keep repeating this procedure until the size of sampled set is less than p?n 100 . ? Head user: Sample the user u from U with the most number of interactions, and add all of its interactions into the sampled set. Remove u from U. Keep repeating this procedure until the size of sampled set is less than p?n 100 . ? SVP-CF user <ref type="bibr" target="#b56">[57]</ref>: This coreset mining technique proceeds by first training a proxy model on D for e epochs. SVP-CF then modifies the forgetting events approach <ref type="bibr" target="#b60">[61]</ref>, and counts the inverse AUC for each user in U, averaged over all e epochs. Just like head-user sampling, we now iterate over users in the order of their forgetting count, and keep sampling users until we exceed our sampling budget of p?n 100 interactions. We use the bias-only model as the proxy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Data statistics &amp; hyper-parameter configurations</head><p>We present a brief summary of statistics of the datasets used in our experiments in <ref type="table" target="#tab_4">Table 2</ref>, and list all the hyper-parameter configurations tried for ?-AE, DISTILL-CF, and other baselines in <ref type="table" target="#tab_2">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Additional training details</head><p>We now discuss additional training details about DISTILL-CF that could not be presented in the main text due to space constraints. Firstly, we make use of a validation-set, and evaluate the performance of the ?-AE model trained in DISTILL-CF's inner-loop to perform hyper-parameter search, as well as early exit. Note that we don't validate the inner-loop's ? at every outer-loop iteration, but keep changing it on-the-fly at each validation cycle. We notice this trick gives us a much faster convergence compared to keeping ? fixed for the entire distillation procedure, and validating for it like other hyper-parameters.</p><p>We also discuss the Gumbel sampling procedure described in Equation <ref type="formula" target="#formula_2">(3)</ref>, in more detail. Given the sampling prior matrix X s , that intuitively denotes the importance of sampling a specific user-item interaction, we intend to sampleX s which will finally be used for downstream model applications.</p><p>Note that for each row (user) in X s , we need multiple, but variable number of samples to conform to the Zipfian law for user and item popularity. This requirement in itself rejects the possibility to use top-K sampling which will sample the same number of items for each row. Furthermore, to keepX s ? X s sampling part of the optimization procedure, we need to compute the gradients of the logistic objective in Equation <ref type="formula" target="#formula_2">(3)</ref> with respect to X s , and hence need the entire process to be differentiable. This requirement prohibits the usage of other popular strategies like Nucleus sampling <ref type="bibr" target="#b18">[19]</ref>, which is non-differentiable. To workaround all the requirements, we devise a multi-step Gumbel sampling strategy where for each row (user) we take a fixed number of Gumbel samples (?), with replacement, followed by taking a union of all the sampled user-item interactions. Note that the union operation ensures that due to sampling with replacement, if a user-item interaction is sampled multiple times, we sample it only once. Hence, the number of sampled interactions is strictly upper-bounded by ? ? |I|. To be precise, the sampling procedure is formalized below:</p><formula xml:id="formula_12">X s u,i = ? ? ? ? exp( log(X s u,i )+gu,i ? ) j?I exp( log(X s u,j )+gu,j ? ) ? ? s.t. g u,i ? Gumbel(? = 0, ? = 1) ?u ? U, i ? I</formula><p>Where ? represents an appropriate function which clamps all values between 0 and 1. In our experiments, we use hard-tanh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Evaluation metrics</head><p>We now present formal definitions of all the ranking metrics used in this study:</p><p>? AUC: Intuitively defined as a threshold independent classification performance measure, AUC can also be interpreted as the expected probability of a recommender system ranking a positive item over a negative item for any given user. More formally, given a user u from the user set U with its set of positive interactions I + u ? I with a similarly defined set of negative interactions I ? u = I\I + u , the AUC for a relevance predictor ?(u, i) is defined as:</p><formula xml:id="formula_13">AUC(?) := E u?U E i + ?I + u E i ? ?I ? u ?(u, i + ) &gt; ?(u, i ? )</formula><p>? HitRate (HR@k): Another name for the recall metric, this metric estimates how many positive items are predicted in a top-k recommendation list. More formally, given recommendation list? Y u ? I K ?u ? U and the set of positive interactions I + u ? I ?u ? U:</p><formula xml:id="formula_14">HR@k := E u?U |? u ? I + u | |I + u |</formula><p>? Normalized Discounted Cumulative Gain (nDCG@k): Unlike HR@k which gives equal importance to all items in the recommendation list, the nDCG@k metric instead gives a higher importance to items predicted higher in the recommendation list and performs logarithmic discounting further down. More formally, given sorted recommendation lists? u ? I K ?u ? U and the set of positive interactions I + u ? I ?u ? U:</p><formula xml:id="formula_15">nDCG@k := E u?U DCG u IDCG u ; DCG u := k i=1? i u ? I + u log 2 (i + 1) ; IDCG u := |I + u | i=1 1 log 2 (i + 1)</formula><p>? Propensity-scored Precision (PSP@k): Originally introduced in <ref type="bibr" target="#b21">[22]</ref> for extreme classification scenarios <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37]</ref>, the PSP@k metric intuitively accounts for missing labels (items in the case of recommendation) by dividing the true relevance of an item (binary) with a propensity correction term. More formally, given recommendation lists? u ? I K ?u ? U, the set of positive interactions I + u ? I ?u ? U, and a propensity model ? : I ? R:</p><formula xml:id="formula_16">PSP@k := E u?U uPSP u mPSP u ; uPSP u := 1 k ? k i=1? i u ? I + u ?(? i u ) ; mPSP u := i?I + u 1 ?(i)</formula><p>For ?, we adapt the propensity model proposed in <ref type="bibr" target="#b21">[22]</ref> for the case of recommendation as: ?(i) ? E u?U P(r u,i = 1|r * u,i = 1) = 1 1 + C ? e ?A?log(ni+B) ; C = (logN ? 1) ? (B + 1) A Where, N represents the total number of interactions in the dataset, and n i represents the empirical frequency of item i in the dataset. We use A = 0.55 and B = 1.5 for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Additional experiments</head><p>How does depth affect ?-AE? To better understand the effect of depth on an infinitely-wide auto-encoder's performance for recommendation, we extend ?-AE to multiple layers and note its downstream performance change in <ref type="figure">Figure 6</ref>. The prominent observation is that models tend to get worse as they get deeper, with generally good performance in the range of 1 ? 2 layers, which also has been common practical knowledge even for finite-width recommender systems. <ref type="bibr" target="#b0">1</ref>   <ref type="figure">Figure 6</ref>: Performance of ?-AE with varying depths. The y-axis represents the normalized metric i.e. performance relative to the best depth for a given metric.</p><p>How does ?-AE perform on cold users &amp; cold items? Cold-start has been one of the hardest problems in recommender systems -how to best model users or items that have very little training data available? Even though ?-AE doesn't have any extra modeling for these scenarios, we try to better understand the performance of ?-AE over users' and items' coldness spectrum. In <ref type="figure" target="#fig_6">Figure 7</ref>, we quantize different users and items based on their coldness (computed by their empirical occurrence in the train-set) into equisized buckets and measure different models' performance only on the binned users or items. We note ?-AE's dominance over other competitors especially over the tail, head-users; and torso, head-items. Avg. HR@100 Avg. HR@100 Avg. HR@100  How anonymized is the data synthesized by DISTILL-CF? Having evaluated the fidelity of distills generated using DISTILL-CF, we now focus on understanding its anonymity and syntheticity. For the generic data down-sampling case, the algorithm presented in <ref type="bibr" target="#b37">[38]</ref> works well to de-anonymize the Netflix prize dataset. The algorithm assumes a complete, non-PII dataset D along with an incomplete, noisy version of the same dataset D , but also has the sensitive PII available. We simulate a similar setup but extend to datasets other than Netflix, by following a simple down-sampling and noise addition procedure: given a sampling strategy s, down-sample D and add x% noise by randomly flipping x% of the total items for each user to generate D . We then use our implementation of the algorithm proposed in <ref type="bibr" target="#b37">[38]</ref> to match the corresponding users in the original, noise-free dataset D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ML-1M</head><p>However, if instead of a down-sampled dataset D , a data distill of D (using DISTILL-CF), let's sa? D, is made publicly available. The task of de-anonymization can no longer be carried out by simply matching user histories from D to D, since D is no longer available. The only solution now is to predict the missing items in D . Note that this this task is easier than the usual recommendation problem, as the user histories to complete in D do exist in some incoherent way in the data distill D, and is more similar to train-set prediction. To test this out, we formulate a simple experiment: given a data distillD, an incomplete, noisy subset D with PII information, and also hypothetically the number of missing items for each user in D -how accurately can we predict the exact set of missing items in D using an ?-AE model trained onD.</p><p>We perform experiments for both the cases of data-sampling and data-distillation. In <ref type="figure">Figure 8</ref>, we measure the % of users de-anonymized using the aforementioned procedures. We interestingly note no level of de-anonymization with the data-distill, even if there's no noise in D . We also note the expected observation for the data-sampling case: less users are de-anonymized when there's more noise in D . In <ref type="figure">Figure 9</ref>, we now control the amount of data revealed in D . We again note the same observation: even with 90% of the correct data from D revealed in D with 0% of noise, we still note a very tiny 0.86% of user de-anonymization with data-distillation, whereas 96.43% with data-sampling for the Douban dataset.</p><p>How does DISTILL-CF compare to data augmentation approaches? We compare the quality of data synthesized by DISTILL-CF with generative models proposed for data augmentation. One such SoTA method is AR-CF <ref type="bibr" target="#b8">[9]</ref>, which leverages two conditional GANs <ref type="bibr" target="#b35">[36]</ref> to generate fake users and fake items. For our experiment, we focus only on AR-CF's user generation sub-network and consequently train the EASE <ref type="bibr" target="#b58">[59]</ref> model only on these synthesized users, while testing on the original test-set for the MovieLens-1M dataset. We plot the results in <ref type="figure">Figure 11</ref>, comparing the amount of users synthesized according to different strategies and plot the HR@10 of the correspondingly trained model. The results signify that training models only on data synthesized by data augmentation models is impractical, as these users have only been optimized for being realistic, whereas the users synthesized by DISTILL-CF are optimized to be informative for model training. The same observation tends to hold true for the case of images as well <ref type="bibr" target="#b66">[67]</ref>.</p><p>Additional experiments on the generality of data summaries synthesized by DISTILL-CF.</p><p>Continuing the results in <ref type="figure">Figure 2</ref>, we now train and evaluate MVAE <ref type="bibr" target="#b31">[32]</ref> on data synthesized by DISTILL-CF. Note that the inner loop of DISTILL-CF still consists of ?-AE, but we nevertheless train and evaluate MVAE to test the synthesized data's universality. We re-use the heuristic sampling strategies from <ref type="figure">Figure 2</ref> for comparison with DISTILL-CF. From the results in <ref type="figure" target="#fig_8">Figure 10</ref>, we observe similar scaling laws as EASE for the heuristic samplers as well as DISTILL-CF. A notable exception is interaction RNS, that scales like ?-AE. Another interesting observation to note is that training MVAE on the full data performs slightly worse than training MVAE on the same amount of data synthesized by DISTILL-CF. This behaviour validates the re-usability of data summaries generated by DISTILL-CF, because they transfer well to SoTA finite-width models, which were not involved in DISTILL-CF's user synthesis procedure. Additional experiments on Continual learning. Continuing the results in <ref type="figure" target="#fig_2">Figure 5</ref>, we plot the results stratified per period for the MovieLens-1M dataset in <ref type="figure" target="#fig_0">Figure 12</ref>. The results are a little noisy, but we can observe that exemplar data distilled with DISTILL-CF has better performance for a majority of the data periods. Note that we use the official public implementation 4 of ADER. Additional plots for the sample complexity of ?-AE. In addition to <ref type="figure">Figure 1</ref> in the main paper, we visualize the sample complexity of ?-AE for all datasets and all metrics in <ref type="figure">Figure 13</ref>. We notice similar trends for all metrics across datasets.</p><p>Additional plots on the robustness of DISTILL-CF &amp; ?-AE to noise. In addition to <ref type="figure">Figure 3</ref> and <ref type="figure">Figure 4</ref> in the main paper, we plot results for the EASE model trained on data sampled by different sampling strategies, when there's varying levels of noise in the original data. We plot this for the MovieLens-1M dataset and all metrics in <ref type="figure">Figure 13</ref>. We notice similar trends for all metrics across datasets. We also plot the sample complexity results for EASE and ?-AE over the MovieLens-1M dataset and all metrics in <ref type="figure" target="#fig_2">Figure 15</ref>. We observe similar trends across metrics. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Performance of ?-AE with the amount of users (log-scale) sampled according to different sampling strategies over the HR@10 and PSP@10 metrics. Results for the Netflix dataset have been clipped due to memory constraints. Results for the remaining metrics can be found in Appendix B.6,Figure 13. Performance of the EASE model trained on different amounts of users (log-scale) sampled by different samplers on the ML-1M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Performance drop of the EASE model trained on data sampled by different sampling strategies when there is varying levels of noise in the data. Performance drop is relative to training on the full, noise-free ML-1M dataset. Results for the remaining metrics can be found in Appendix B.+ EASE User-RNS + ?-AE Distill-CF + EASE Distill-CF + ?-AE Performance of ?-AE on data sampled by DISTILL-CF and User-RNS when there is noise in the data. Results for EASE have been added for reference. All results are on the ML-1M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>DISTILL-CF for continual learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Performance comparison of ?-AE with SoTA finite-width models stratified over the coldness of users and items. The y-axis represents the average HR@100 for users/items in a particular quanta. All user/item bins are equisized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Amount of noise added in D vs. % of users de-anonymized. Amount of data revealed in D vs. % of users de-anonymized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Performance of the MVAE model trained on different amounts of users (log-scale) sampled by different samplers on the ML-1M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Performance of EASE on varying amounts of data sampled/synthesized using various strategies for the MovieLens-1M dataset. Per-period evaluation of the MVAE model on various continual learning strategies as discussed in Section 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :Figure 15 :</head><label>1315</label><figDesc>Performance of ?-AE with the amount of users sampled according to different sampling strategies over different metrics. Each column represents a single dataset, and each row represents an evaluation metric. Performance of ?-AE on data sampled by DISTILL-CF and User-RNS when there's noise in the data. Results for EASE have been added for reference. Each column represents a specific level of noise in the original data, and each row represents an evaluation metric. All results are on the MovieLens-1M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of ?-AE with different methods on various datasets. All metrics are better when higher. Brief set of data statistics can be found in Appendix B.3,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>along with the Neural-Tangents package<ref type="bibr" target="#b43">[44]</ref> for the relevant NTK computations.<ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2</ref> We re-use the official implementation of LightGCN, and implement the remaining competitors ourselves. To ensure a fair comparison, we conduct a hyper-parameter search for all competitors on the validation set. More details on the hyper-parameters for ?-AE, DISTILL-CF, and all competitors can be found in Appendix B.3,</figDesc><table /><note>. All of our experiments are performed on a single RTX 3090 GPU, with a random-seed initialization of 42. Additional training details about DISTILL-CF can be found in Appendix B.4.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Discussed throughout the setup sub-section in Section 5, and provided detailed instructions in A Appendix: Pseudo-codes Algorithm 1 ?-AE model training Input: User set U ; dataset X ? R |U |?|I| ; NTK K : R |I| ? R |I| ? R ; regularization const.</figDesc><table><row><cell cols="3">? ? R Output: Dual parameters ? ? R |U |?|I|</cell></row><row><cell cols="2">1: procedure FIT(U , X, K)</cell><cell></cell></row><row><cell>2: 3:</cell><cell>K ? [0] |U |?|U |</cell><cell>Zero Initialization</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Brief set of statistics of the datasets used in this paper.</figDesc><table><row><cell>Dataset</cell><cell cols="4"># Users # Items # Interactions Sparsity</cell></row><row><cell>Amazon Magazine [42]</cell><cell>3k</cell><cell>1.3k</cell><cell>12k</cell><cell>99.7%</cell></row><row><cell>ML-1M [15]</cell><cell>6k</cell><cell>3.7k</cell><cell>1M</cell><cell>95.6%</cell></row><row><cell>Douban [69]</cell><cell>2.6k</cell><cell>34k</cell><cell>1.2M</cell><cell>98.7%</cell></row><row><cell>Netflix [5]</cell><cell>476k</cell><cell>17k</cell><cell>100M</cell><cell>98.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>List of all the hyper-parameters grid-searched for ?-AE, DISTILL-CF, and baselines.</figDesc><table><row><cell>Hyper-Parameter</cell><cell>Model</cell><cell>Amz. Magazine</cell><cell>ML-1M</cell><cell>Douban</cell><cell>Netflix</cell></row><row><cell></cell><cell>MF</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Latent size</cell><cell>NeuMF LightGCN</cell><cell></cell><cell cols="2">{4, 8, 16, 32, 50, 64, 128}</cell><cell></cell></row><row><cell></cell><cell>MVAE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>MF</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell># Layers</cell><cell>NeuMF LightGCN</cell><cell></cell><cell>{1, 2, 3}</cell><cell></cell><cell></cell></row><row><cell></cell><cell>MVAE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>?-AE</cell><cell></cell><cell>{1}</cell><cell></cell><cell></cell></row><row><cell></cell><cell>MF</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Learning rate</cell><cell>NeuMF LightGCN</cell><cell></cell><cell>{0.001, 0.006, 0.01}</cell><cell></cell><cell>{0.006}</cell></row><row><cell></cell><cell>MVAE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>DISTILL-CF</cell><cell></cell><cell>{0.04}</cell><cell></cell><cell></cell></row><row><cell></cell><cell>MF</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dropout</cell><cell>NeuMF LightGCN</cell><cell></cell><cell>{0.0, 0.3, 0.5}</cell><cell></cell><cell></cell></row><row><cell></cell><cell>MVAE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>EASE</cell><cell></cell><cell cols="2">{1, 10, 100, 1K, 10K}</cell><cell></cell></row><row><cell>?</cell><cell>?-AE</cell><cell></cell><cell cols="2">{0.0, 1.0, 5.0, 20.0 50.0, 100.0}</cell><cell></cell></row><row><cell></cell><cell>DISTILL-CF</cell><cell></cell><cell cols="2">{1e-5, 1e-3, 0.1, 1.0, 5.0, 50.0}</cell><cell></cell></row><row><cell>?2</cell><cell>DISTILL-CF</cell><cell></cell><cell cols="2">{1e-3, 10.0} avg. # of interactions per user</cell><cell></cell></row><row><cell>?</cell><cell>DISTILL-CF</cell><cell></cell><cell cols="2">{0.3, 0.5, 0.7, 5.0}</cell><cell></cell></row><row><cell>?</cell><cell>DISTILL-CF</cell><cell>{50, 100, 200}</cell><cell cols="3">{200, 500, 700} {500, 1K, 2K} {500, 700}</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our implementation for ?-AE is available at https://github.com/noveens/infinite_ae_cf 2 Our implementation for DISTILL-CF is available at https://github.com/noveens/distill_cf</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/gusye1234/LightGCN-PyTorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/doublemul/ADER available with the MIT license.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Top-n recommendation algorithms: A quest for the state-of-the-art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vito</forename><surname>Walter Anelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Bellog?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Di Noia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Pomo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.01155</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On exact computation with an infinitely wide neural net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruosong</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Harnessing the power of infinitely wide deep nets on small-data tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingli</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Reconciling modern machine learning practice and the bias-variance trade-off</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumik</forename><surname>Mandal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11118</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The netflix prize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Lanning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.00132</idno>
		<title level="m">Submodularity in machine learning and artificial intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coresets via bilevel optimization for continual learning and streaming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zal?n</forename><surname>Borsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojmir</forename><surname>Mutny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="14879" to="14890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">James</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skye</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">JAX: composable transformations of Python+NumPy programs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ar-cf: Augmenting virtual users and items in collaborative filtering for addressing cold-start problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoo</forename><surname>Dong-Kyu Chae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duen Horng</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Wook</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1251" to="1260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bias and debias in recommender system: A survey and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hande</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<idno>abs/2010.03240</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ispir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st workshop on deep learning for recommender systems</title>
		<meeting>the 1st workshop on deep learning for recommender systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Matrix multiplication via arithmetic progressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Coppersmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the nineteenth annual ACM symposium on Theory of computing</title>
		<meeting>the nineteenth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Are we really making much progress? a worrying analysis of recent neural recommendation approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dacrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jannach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems, RecSys &apos;19</title>
		<meeting>the 13th ACM Conference on Recommender Systems, RecSys &apos;19</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Daniely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<title level="m">The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The elements of statistical learning: data mining, inference, and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jerome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web, WWW &apos;17</title>
		<meeting>the 26th International Conference on World Wide Web, WWW &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Hongler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Slice: Scalable linear extreme classifiers trained on 100 million labels for related searches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chunduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Conference on Web Search and Data Mining</title>
		<meeting>the 12th ACM Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extreme multi-label loss functions for recommendation, tagging, ranking and other missing label applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph condensation for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hard negative mixing for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bulent</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noe</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21798" to="21809" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Regularized submodular maximization at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moran</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Karbasi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grad-match: Gradient matching based data subset selection for efficient deep model training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnateja</forename><surname>Killamsetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Durga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Iyer</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations, ICLR &apos;17</title>
		<meeting>the 5th International Conference on Learning Representations, ICLR &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Linear, or non-linear, that is the question!</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeyong</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeri</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongwhan</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeon-Chang</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noseong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Wook</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Fifteenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="517" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On sampled metrics for item recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Finite versus infinite neural networks: an empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="15156" to="15172" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Variational autoencoders for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference, WWW &apos;18</title>
		<meeting>the 2018 World Wide Web Conference, WWW &apos;18</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning disentangled representations for recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gradient-based hyperparameter optimization through reversible learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2113" to="2122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ader: Adaptively distilled exemplar replay towards continual learning for session-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boi</forename><surname>Faltings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourteenth ACM Conference on Recommender Systems</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="408" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Eclare: Extreme classification with label graph correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The ACM International World Wide Web Conference</title>
		<meeting>The ACM International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust de-anonymization of large sparse datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Symposium on Security and Privacy</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="111" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning recommendation model for personalization and recommendation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Jun Michael</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongsoo</forename><surname>Sundaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udit</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carole-Jean</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alisson</forename><forename type="middle">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Azzolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Dzhulgakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilia</forename><surname>Mallevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghai</forename><surname>Cherniavskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuraman</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ansha</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Kondratenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smelyanskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dataset meta-learning from kernel ridge-regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dataset distillation with infinitely wide convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Justifying recommendations using distantly-labeled reviews and fine-grained aspects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Jeffrey Pennington, and Jascha Sohl-dickstein. Bayesian deep convolutional networks with many channels are gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Abolafia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural tangents: Fast and easy infinite neural networks in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Continual lifelong learning with neural networks: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>German</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Part</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="54" to="71" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Parabel: Partitioned label trees for extreme classification with application to dynamic search advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harsola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International World Wide Web Conference</title>
		<meeting>the International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Lecture 5: Ntk origin and derivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adityanarayanan</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Simple, fast, and flexible framework for matrix completion with infinite width neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adityanarayanan</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Stefanakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page">2115064119</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bpr: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI &apos;09</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI &apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering vs. matrix factorization revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Steffen Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourteenth ACM conference on recommender systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="240" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Contrastive learning with hard negative samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">David</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Experience replay for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Wayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Training data subset selection for regression with controlled generalization error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Durga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>De</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sequential variational autoencoders for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noveen</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Manco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ettore</forename><surname>Ritacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Pudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Web Search and Data Mining, WSDM &apos;19</title>
		<meeting>the ACM International Conference on Web Search and Data Mining, WSDM &apos;19</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">How useful are reviews for recommendation? a critical review and potential improvements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noveen</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;20</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;20</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Off-policy bandits with deficient support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noveen</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On sampling collaborative filtering datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noveen</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, WSDM &apos;22</title>
		<meeting>the Fifteenth ACM International Conference on Web Search and Data Mining, WSDM &apos;22<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="842" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Recommendations as treatments: Debiasing learning and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chandak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Embarrassingly shallow autoencoders for sparse data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Steck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3251" to="3257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Andrew ng: Farewell, big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliza</forename><surname>Strickland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spectrum</title>
		<imprint>
			<date type="published" when="2022-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">An empirical study of example forgetting during deep neural network learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Combes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10959</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Dataset distillation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Developing a recommendation benchmark for mlperf training and inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Raimond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/2003.07336</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Collaborative denoising auto-encoders for top-n recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth ACM international conference on web search and data mining</title>
		<meeting>the ninth ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Rethinking neural vs. matrix-factorization collaborative filtering: the theoretical perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanwei</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evren</forename><surname>Korpeoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sushant</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kannan</forename><surname>Achan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11514" to="11524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Dataset condensation with differentiable siamese augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12674" to="12685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Dataset condensation with gradient matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konda</forename><surname>Reddy Mopuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Cold brew: Distilling graph node representations with incomplete or missing neighborhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumeet</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Katariya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Subbian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Dtcdr: A framework for dual-target cross-domain recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaochao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanfeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1533" to="1542" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
