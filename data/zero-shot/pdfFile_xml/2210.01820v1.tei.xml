<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MOAT: ALTERNATING MOBILE CONVOLUTION AND ATTENTION BRINGS STRONG VISION MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoding</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MOAT: ALTERNATING MOBILE CONVOLUTION AND ATTENTION BRINGS STRONG VISION MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents MOAT, a family of neural networks that build on top of MObile convolution (i.e., inverted residual blocks) and ATtention. Unlike the current works that stack separate mobile convolution and transformer blocks, we effectively merge them into a MOAT block. Starting with a standard Transformer block, we replace its multi-layer perceptron with a mobile convolution block, and further reorder it before the self-attention operation. The mobile convolution block not only enhances the network representation capacity, but also produces better downsampled features. Our conceptually simple MOAT networks are surprisingly effective, achieving 89.1% top-1 accuracy on ImageNet-1K with ImageNet-22K pretraining. Additionally, MOAT can be seamlessly applied to downstream tasks that require large resolution inputs by simply converting the global attention to window attention. Thanks to the mobile convolution that effectively exchanges local information between pixels (and thus cross-windows), MOAT does not need the extra window-shifting mechanism. As a result, on COCO object detection, MOAT achieves 59.2% AP box with 227M model parameters (single-scale inference, and hard NMS), and on ADE20K semantic segmentation, MOAT attains 57.6% mIoU with 496M model parameters (single-scale inference). Finally, the tiny-MOAT family, obtained by simply reducing the channel sizes, also surprisingly outperforms several mobile-specific transformer-based models on ImageNet. We hope our simple yet effective MOAT will inspire more seamless integration of convolution and self-attention. Code is made publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The vision community has witnessed the prevalence of self-attention <ref type="bibr" target="#b3">(Bahdanau et al., 2015)</ref> and Transformers <ref type="bibr" target="#b74">(Vaswani et al., 2017)</ref>. The success of Transformers in natural language processing motivates the creation of their variants for vision recognition. The Vision Transformer (ViT) <ref type="bibr">(Dosovitskiy et al., 2021)</ref> has great representation capacity with global receptive field. However, it requires pretraining on a large-scale proprietary dataset <ref type="bibr" target="#b64">(Sun et al., 2017)</ref>. Its unsatisfying performance, when trained with a small number of images, calls for the need of better training recipes <ref type="bibr" target="#b70">(Touvron et al., 2021a;</ref><ref type="bibr" target="#b63">Steiner et al., 2021)</ref> or architectural designs <ref type="bibr">(Liu et al., 2021;</ref><ref type="bibr">Graham et al., 2021)</ref>. On the other hand, ConvNet has been the dominant network choice since the advent of AlexNet <ref type="bibr" target="#b45">(Krizhevsky et al., 2012)</ref> in 2012. Vision researchers have condensed the years of network design experience into multiple principles, and have started to incorporate them to vision transformers. For example, there are some works adopting the ConvNet's hierarchical structure to extract multi-scale features for vision transformers <ref type="bibr">(Liu et al., 2021;</ref><ref type="bibr">Fan et al., 2021;</ref><ref type="bibr" target="#b79">Wang et al., 2022)</ref>, and others proposing to integrate the translation equivariance of convolution into transformers <ref type="bibr">(Graham et al., 2021;</ref><ref type="bibr">d'Ascoli et al., 2021;</ref><ref type="bibr">Xiao et al., 2021)</ref>.</p><p>Along the same direction of combining the best from Transformers and ConvNets, CoAtNet <ref type="bibr">(Dai et al., 2021)</ref> and MobileViT <ref type="bibr" target="#b53">(Mehta &amp; Rastegari, 2022a)</ref> demonstrate outstanding performance by stacking Mobile Convolution (MBConv) blocks (i.e., inverted residual blocks <ref type="bibr" target="#b59">(Sandler et al., 2018))</ref> and Transformer blocks (i.e., a self-attention layer and a Multi-Layer Perceptron (MLP)). However, both works focus on the macro-level network design. They consider MBConv and Transformer blocks as individual separate ones, and systematically study the effect of stacking them to strike a better balance between the remarkable efficiency of MBConv and strong capacity of Transformer.</p><p>In this work, on the contrary, we study the micro-level building block design by taking a deeper look at the combination of MBConv and Transformer blocks. We make two key observations after a careful examination of those blocks. First, the MLP module in Transformer block is similar to MBConv, as both adopt the inverted bottleneck design. However, MBConv is a more powerful operation by employing one extra 3 ? 3 depthwise convolution (to encode local interaction between pixels), and more activation <ref type="bibr" target="#b32">(Hendrycks &amp; Gimpel, 2016)</ref> and normalization <ref type="bibr" target="#b42">(Ioffe &amp; Szegedy, 2015)</ref> are employed between convolutions. Second, to extract multi-scale features using Transformer blocks, one may apply the average-pooling (with stride 2) to input features before the self-attention layer. However, the pooling operation reduces the representation capacity of self-attention. Our observations motivate us to propose a novel MObile convolution with ATtention (MOAT) block, which efficiently combines MBConv and Transformer blocks. The proposed MOAT block modifies the Transformer block by first replacing its MLP with a MBConv block, and then reversing the order of attention and MBConv. The replacement of MLP with MBConv brings more representation capacity to the network, and reversing the order (MBConv comes before self-attention) delegates the downsampling duty to the strided depthwise convolution within the MBConv, learning a better downsampling kernel.</p><p>We further develop a family of MOAT models by stacking and increasing the channels of network blocks. Surprisingly, our extremely simple design results in a remarkable impact. On the challenging ImageNet-1K classification benchmark <ref type="bibr" target="#b58">(Russakovsky et al., 2015)</ref>, our model (190M parameters) achieves 86.7% top-1 accuracy without extra data. When further pretraining on ImageNet-22K, our best model (483M parameters) attains 89.1% top-1 accuracy, setting a new state-of-the-art.</p><p>Additionally, MOAT can be seamlessly deployed to downstream tasks that require large resolution inputs by simply converting the global attention to non-overlapping local window attention. Thanks to the MBConv that effectively exchanges local information between pixels (enabling cross-window propagation), MOAT does not need the extra window-shifting mechanism <ref type="bibr">(Liu et al., 2021)</ref>. As a result, on COCO object detection <ref type="bibr" target="#b47">(Lin et al., 2014)</ref> and ADE20K semantic segmentation <ref type="bibr" target="#b95">(Zhou et al., 2019)</ref>, MOAT shows superior performances. Specifically, on COCO object detection, our best model (227M parameters), achieves 59.2% AP box with single-scale inference and hard NMS, setting a new state-of-the-art in the regime of model size 200M with Cascade Mask R-CNN <ref type="bibr" target="#b7">(Cai &amp; Vasconcelos, 2018;</ref>. On ADE20K semantic segmentation, our best model (496M parameters), adopting DeepLabv3+ , attains 57.6% mIoU with single-scale inference, also setting a new state-of-the-art in the regime of models using input size 641 ? 641.</p><p>Finally, to explore the scalability of MOAT models, we simply scale down the models by reducing the channel sizes (without any other change), resulting in the tiny-MOAT family, which also surprisingly outperforms mobile-specific transformer-based models, such as Mobile-Former <ref type="bibr" target="#b13">(Chen et al., 2022b)</ref> and MobileViTs <ref type="bibr" target="#b53">(Mehta &amp; Rastegari, 2022a;</ref><ref type="bibr">b)</ref>. Specifically, in the regime of model parameters 5M, 10M, and 20M, our tiny MOAT outperforms the concurrent MobileViTv2 <ref type="bibr" target="#b54">(Mehta &amp; Rastegari, 2022b</ref>) by 1.1%, 1.3%, and 2.0% top-1 accuracy on ImageNet-1K classification benchmark.</p><p>In summary, our method advocates the design principle of simplicity. Without inventing extra complicated operations, the proposed MOAT block effectively merges the strengths of both mobile convolution and self-attention into one block by a careful redesign. Despite its conceptual simplicity, impressive results have been obtained on multiple core vision recognition tasks. We hope our study will inspire future research on seamless integration of convolution and self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHOD</head><p>Herein, we review the Mobile Convolution (MBConv) <ref type="bibr" target="#b59">(Sandler et al., 2018)</ref> and Transformer <ref type="bibr" target="#b74">(Vaswani et al., 2017)</ref> blocks before introducing the proposed MOAT block. We then present MOAT, a family of neural networks, targeting at different trade-offs between accuracy and model complexity.  <ref type="bibr" target="#b59">(Sandler et al., 2018)</ref> employs the inverted bottleneck design with depthwise convolution and squeeze-and-excitation <ref type="bibr" target="#b38">(Hu et al., 2018)</ref> applied to the expanded features. (b) The Transformer block <ref type="bibr" target="#b74">(Vaswani et al., 2017)</ref> consists of a self-attention module and a MLP module. (c) The proposed MOAT block effectively combines them. The illustration assumes the input tensor has channels c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MOBILE CONVOLUTION AND TRANSFORMER BLOCKS</head><p>MBConv block. Also known as the inverted residual block, the Mobile Convolution (MBConv) (Sandler et al., 2018) block ( <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>) is an effective building block that has been widely used in mobile models <ref type="bibr" target="#b35">(Howard et al., 2019;</ref><ref type="bibr" target="#b53">Mehta &amp; Rastegari, 2022a)</ref> or efficient models <ref type="bibr" target="#b68">(Tan &amp; Le, 2019;</ref><ref type="bibr">Dai et al., 2021)</ref>. Unlike the bottleneck block in ResNet <ref type="bibr" target="#b29">(He et al., 2016a)</ref>, the MBConv block employs the design of an "inverted bottleneck", together with the efficient depthwise convolution <ref type="bibr" target="#b36">(Howard et al., 2017)</ref>. Specifically, a 1 ? 1 convolution is first applied to expand the input channels by a factor of 4. Then, a 3 ? 3 depthwise convolution is used to effectively capture the local spatial interactions between pixels. Finally, the features are projected back to the original channel size via a 1 ? 1 convolution, enabling a residual connection <ref type="bibr" target="#b29">(He et al., 2016a</ref>). An optional Squeeze-and-Excitation (SE) <ref type="bibr" target="#b38">(Hu et al., 2018)</ref> module (which uses the global information to re-weight the channel activation) may also be used after the depthwise convolution, following MobileNetV3 <ref type="bibr" target="#b35">(Howard et al., 2019)</ref>. Note that one could tune the channel expansion ratio and depthwise convolution kernel size for better performance. We fix them throughout the experiments for simplicity. Formally, given an input tensor x ? R H?W ?C (H, W, C are its height, width, and channels), the MBConv block is represented as follows:</p><formula xml:id="formula_0">MBConv(x) = x + (N 2 ? S ? D ? N 1 )(BN(x)), (1) N 1 (x) = GeLU(BN(Conv(x))), (2) D(x) = GeLU(BN(DepthConv(x))), (3) S(x) = ?(MLP(GAP(x)) ? x, (4) N 2 (x) = Conv(x),<label>(5)</label></formula><p>where BN, GeLU, GAP, and MLP stand for Batch Normalization <ref type="bibr" target="#b42">(Ioffe &amp; Szegedy, 2015)</ref>, Gaussian error Linear Unit <ref type="bibr" target="#b32">(Hendrycks &amp; Gimpel, 2016)</ref>, Global Average Pooling, and Multi-Layer Perceptron (with reduction ratio 4 and hard-swish <ref type="bibr" target="#b56">(Ramachandran et al., 2017)</ref>), respectively. The MBConv block consists of four main functions: N 1 , D, S, and N 2 , which correspond to the 1 ? 1 convolution for channel expansion (by 4?), 3 ? 3 depthwise convolution, squeeze-and-excitation <ref type="bibr" target="#b38">(Hu et al., 2018)</ref> (? is the sigmoid function), and 1 ? 1 convolution for channel projection (by 4?), respectively.</p><p>Transformer block. The Transformer <ref type="bibr" target="#b74">(Vaswani et al., 2017</ref>) block ( <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>) is a powerful building block that effectively captures the global information via the data-dependent self-attention operation. It consists of two main operations: self-attention and MLP. The self-attention operation computes the attention map based on the pairwise similarity between every pair of pixels in the input tensor, thus enabling the model's receptive field to encompass the entire spatial domain. Additionally, the attention map dynamically depends on the input, enlarging the model's representation capacity (unlike the convolution kernels, which are data-independent). The MLP operation contains two 1 ? 1 convolutions, where the first one expands the channels (by 4?), the second one shrinks back the channels, and GeLU non-linearity is used in-between.</p><p>Formally, given an input tensor x ? R H?W ?C , the Transformer block is represented as follows:</p><formula xml:id="formula_1">Transformer(x) = x + (M 2 ? M 1 ? Attn)(LN(x)), (6) M 1 (x) = GeLU(Conv(LN(x))), (7) M 2 (x) = Conv(x),<label>(8)</label></formula><p>where LN and Attn denote the Layer Normalization <ref type="bibr" target="#b2">(Ba et al., 2016)</ref>, and self-attention <ref type="bibr" target="#b74">(Vaswani et al., 2017)</ref>. The self-attention operation also includes a residual connection <ref type="bibr" target="#b29">(He et al., 2016a)</ref>, which is not shown in the equations for simplicity, while the MLP operation is represented by two functions M 1 and M 2 , which correspond to the 1 ? 1 convolution for channel expansion (by 4?) and 1 ? 1 convolution for channel projection, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MOBILE CONVOLUTION WITH ATTENTION (MOAT) BLOCK</head><p>Comparing MBConv and Transformer blocks. Before getting into the architecture of our MOAT block, it is worthwhile to compare the MBConv <ref type="bibr" target="#b59">(Sandler et al., 2018)</ref> and Transformer <ref type="bibr" target="#b74">(Vaswani et al., 2017)</ref> blocks, which helps to understand our design motivations. Specifically, we make the following key observations.</p><p>First, both MBConv and Transformer blocks advocate the "inverted bottleneck" design, where the channels of input tensors are expanded and then projected by 1 ? 1 convolutions. However, MBConv additionally employs a 3 ? 3 depthwise convolution between those two 1 ? 1 convolutions, and there are both batch normalization and GeLU activation between the convolutions.</p><p>Second, to capture the global information, the MBConv block may employ a Squeeze-and-Excitation (SE) module, while the Transformer block adopts the self-attention operation. Note that the SE module squeezes the spatial information via a global average pooling, while the self-attention module maintains the tensor's spatial resolution.</p><p>Third, the downsampling operation is performed at different places within the block. To downsample the features, the standard MBConv block uses the strided depthwise convolution, while the Transformer block, deployed in the modern hybrid model CoAtNet <ref type="bibr">(Dai et al., 2021)</ref>, adopts an average-pooling operation before the self-attention.</p><p>MOAT block. Given the above observations, we now attempt to design a new block that effectively merges the best from both MBConv and Transformer blocks. We begin with the powerful Transformer block, and gradually refine over it.</p><p>Based on the first observation, both MBConv and Transformer blocks employ the "inverted bottleneck" design. Since depthwise convolution could effectively encode local interaction between pixels, which is crucial for modeling the translation equivariance in ConvNets, we thus start to add the depthwise convolution to Transformer's MLP module. However, we did not observe any performance improvement until we also added the extra normalization and activations between convolutions.</p><p>For the second observation, we simply do not add the SE module to the MBConv block. The self-attention operation is kept to capture the global information.</p><p>We found the third observation critical. The downsampling operation (average-pooling) right before the self-attention operation in Transformer block slightly reduces its representation capacity. On the other hand, the MBConv block is well-designed for the downsampling operation with the strided depthwise convolution, which effectively learns the downsampling convolution kernel for each input channel. Therefore, we further reorder the "inverted bottleneck" (containing depthwise convolution) before the self-attention operation, delegating the downsampling operation to depthwise convolution. In this way, we need no extra downsampling layer like average-pooling in CoAtNet <ref type="bibr">(Dai et al., 2021)</ref>, or patch-embedding layers in Swin <ref type="bibr">(Liu et al., 2021)</ref> and ConvNeXt . Finally, it results in our MObile convolution with ATtention (MOAT) block, as illustrated in <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>.</p><p>Formally, given an input tensor x ? R H?W ?C , the MOAT block is represented as follows:</p><formula xml:id="formula_2">MOAT(x) = x + (Attn ? N 2 ? D ? N 1 )(BN(x)),<label>(9)</label></formula><p>where MBConv (w/o SE) contains functions N 1 (Eq. 2), D (Eq. 3), and N 2 (Eq. 5), and Attn denotes the self-attention operation. The MOAT block then simply consists of MBConv (w/o SE) and the self-attention operation, successfully combining the best from the MBConv block and Transformer block into one (which we will show empirically).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">META ARCHITECTURE</head><p>Macro-level network design. After developing the MOAT block, we then study how to effectively stack them to form our base model. We adopt the same strategy as the existing works <ref type="bibr">(Liu et al., 2021;</ref><ref type="bibr" target="#b78">Wang et al., 2021b;</ref><ref type="bibr">Graham et al., 2021;</ref><ref type="bibr">Xiao et al., 2021;</ref><ref type="bibr">Dai et al., 2021;</ref><ref type="bibr" target="#b53">Mehta &amp; Rastegari, 2022a)</ref>. Specifically, we summarize several key findings from those works, and use them as design principles of our meta architecture.</p><p>? Employing convolutions in the early stages improves the performance and training convergence of Transformer models <ref type="bibr">(Wu et al., 2021;</ref><ref type="bibr">Graham et al., 2021;</ref><ref type="bibr">Xiao et al., 2021)</ref>.</p><p>? The Mobile Convolution (MBConv) <ref type="bibr" target="#b59">(Sandler et al., 2018)</ref> blocks are also effective building blocks in the hybrid Conv-Transformer models <ref type="bibr">(Dai et al., 2021;</ref><ref type="bibr" target="#b53">Mehta &amp; Rastegari, 2022a)</ref>.</p><p>? Extracting multi-scale backbone features benefits the downstream tasks, such as detection and segmentation <ref type="bibr">(Liu et al., 2021;</ref><ref type="bibr" target="#b78">Wang et al., 2021b;</ref><ref type="bibr">Fan et al., 2021;</ref><ref type="bibr">Heo et al., 2021)</ref>.</p><p>As a result, our meta architecture consists of the convolutional stem, MBConv blocks, and MOAT blocks. Additionally, through the ablation study in the appendix, we found the layer layout proposed by CoAtNet-1 <ref type="bibr">(Dai et al., 2021)</ref> effective. We thus follow their layer layout, resulting in our base model MOAT-1. To form the MOAT model family, we then scale down or up MOAT-1 in the dimensions of number of blocks and number of channels, as shown in Tab. 1. We only scale the number of blocks in the third and fourth stages (out of five stages). The downsampling operation is performed in the first block of each stage. Note that our base model MOAT-1 and CoAtNet-1 share the same layer layout and channel sizes. However, we take a different scaling strategy: our MOAT is scaled up (or down) by alternatively increasing the depth and expanding the width between variants.  </p><formula xml:id="formula_3">MOAT-0 MOAT-1 MOAT-2 MOAT-3 MOAT-4 tiny-MOAT-{0,1,2,3} B C B C B C B C B C B C</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL RESULTS</head><p>In this section, we show that MOAT variants are effective on the ImageNet-1K <ref type="bibr" target="#b58">(Russakovsky et al., 2015)</ref> image classification. We then deploy them to other recognition tasks, including COCO object detection <ref type="bibr" target="#b47">(Lin et al., 2014)</ref>, instance segmentation <ref type="bibr" target="#b28">(Hariharan et al., 2014)</ref>, and ADE20K <ref type="bibr" target="#b95">(Zhou et al., 2019)</ref> semantic segmentation. MOAT can be seamlessly applied to downstream tasks. For small resolution inputs, we directly fine-tune the global attention, while for large resolution inputs, we simply convert the global attention to non-overlapping local window attention without using extra window-shifting mechanism. The detailed experiment setup could be found in the appendix.  ImageNet Image Classification. In Tab. 2, we include the current state-of-art methods in the categories of ConvNets, ViTs and Hybrid models. At similar model costs (parameters or FLOPs), our MOAT models consistently outperform all of them. Specifically, with the ImageNet-1K data only and input size 224, for light-weight models, our MOAT-0 significantly outperforms ConvNeXt-T , Swin-T , and CoAtNet-0 <ref type="bibr">(Dai et al., 2021</ref>) by 1.2%, 2.0%, and 1.7%, respectively. For large-scale models using input size 384, MOAT-3 is able to surpass ConvNeXt-L, CoAtNet-3 by 1.0% and 0.7%, respectively. With the ImageNet-22K pretraining and input size 384, the prior arts ConvNeXt-L, Swin-L, and CoAtNet-3 already show strong performances (87.5%, 87.3% and 87.6%), while our MOAT-3 achieves the score of 88.2%, outperforming them by 0.7%, 0.9%, and 0.6%, respectively. For ImageNet-1K and input size 224, we plot the performances vs. parameters and FLOPs in <ref type="figure">Fig. 2 and Fig. 3</ref>, respectively. For ImageNet-22K pretraining and input size 384, we plot the performances vs. parameters and FLOPs in <ref type="figure">Fig. 4 and Fig. 5</ref>, respectively. In the figures, MOAT clearly demonstrates the best performance in all computation regimes. Finally, our largest model MOAT-4, with ImageNet-22K and input size 512, further attains 89.1% accuracy.</p><p>COCO Detection. Tab. 3 summarizes the COCO object detection (box) and instance segmentation (mask) results. Our MOAT backbones significantly outperform the baseline methods, including Swin <ref type="bibr">(Liu et al., 2021)</ref> and ConvNeXt  across different model sizes. Specifically, our MOAT-0 outperforms Swin-T and ConvNeXt-T by 5.4% and 5.5% AP box (3.7% and 3.7% AP mask ). Our MOAT-1 surpasses Swin-S and ConvNeXt-S by 5.9% and 5.8% AP box (4.3% and 4.0% AP mask ). Our MOAT-2, with 110M parameters, is still 5.5% and 4.5% AP box (3.5% and 2.4% AP mask ) better than Swin-B and ConvNeXt-B. Finally, our MOAT-3, using 227M parameters, achieves 59.2% AP box (50.3% AP mask ), setting a new state-of-the-art in the regime of model size 200M that is built on top of Cascade Mask R-CNN <ref type="bibr" target="#b7">(Cai &amp; Vasconcelos, 2018;</ref>.  tiny-MOAT on ImageNet. We simply scale down the channels of MOAT-0 to obtain the tiny-MOAT family without any specific adaptions. In the left of Tab. 5, with the similar model parameters, tiny-MOAT-0/1/2 surpass the Mobile-Former counterparts by 6.8%, 5.5%, and 4.3%, respectively. In the right of Tab. 5, our tiny-MOAT also shows stronger performances than MobileViT <ref type="bibr" target="#b53">(Mehta &amp; Rastegari, 2022a)</ref>. Even compared with the concurrent work MobileViTv2 <ref type="bibr" target="#b54">(Mehta &amp; Rastegari, 2022b)</ref>, tiny-MOAT-1/2/3 surpass their counterparts by 1.1%, 1.3%, and 2.0%, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ABLATION STUDIES ON IMAGENET</head><p>MOAT block design. In Tab. 6, we ablate the MOAT block design, which only affects the last two stages of MOAT, and we keep everything else the same (e.g., training recipes). We start from the Transformer block, consisting of Attn (self-attention) and MLP, which already attains a strong top-1 accuracy (82.6%). Directly inserting a 3 ? 3 depthwise convolution in the MLP degrades the performance by 0.9%. If we additionally insert batch normalization and GeLU between convolutions (i.e., replace MLP with MBConv, but no Squeeze-and-Excitation), the performance is improved to 82.9%. Finally, placing MBConv before Attn reaches the performance of 83.3%. Additionally, our MOAT block brings more improvements (from 1.2% up to 2.6% gains) in the tiny model regime.</p><p>Order of MBConv and Attn in MOAT block. Our MOAT block design reverses the order of Attention (Attn) and Mobile Convolution (MBConv), delegating the downsampling duty to the strided depthwise convolution within the MBConv. However, the dowsampling can be still performed in the MBConv with the original order (i.e., Attn + MBConv). Since the operations, Attn and MBConv, are interlaced, the key difference then comes from the first block in each stage, where the Attn is operated on the (1) spatially downsampled and/or (2) channel expanded features. To conduct the study, we employ different blocks in the MOAT variants, using "Attn + MLP", "Attn + MBConv", or "MBConv + Attn". For the "Attn + MBConv" block, we further ablate the place (Attn vs. MBConv), where we apply the spatial downsampling and channel expansion operations.</p><p>In Tab. 7, we observe the following results. First, replacing the MLP with MBConv improves the performance by 0.3% and 0.7% for MOAT-0 and tiny-MOAT-2. Second, if we perform both spatial downsampling and channel expansion at the MBConv block, the performance is further improved by 0.5% and 0.9% for MOAT-0 and tiny-MOAT-2, showing that MBConv learns better downsampled features. However, this design is equivalent to shifting the first Attn layer to its previous stage, reducing the representation capacity of the current stage. More concretely, only the last stage will be affected, since one layer is shifted. Third, to enhance the representation capacity, reversing the order of Attn and MBConv allows us to keep the first Attn layer in the same stage. This design further improves the performance by 0.7% and 1.2% for MOAT-0 and tiny-MOAT-2. Fourth, to compensate for the shifting effect, we could also employ another 1 ? 1 convolution to expand the channels at the first Attn layer (then, MBConv only performs the spatial downsampling). However, this design performs similarly to our MOAT block design, but uses more parameters and FLOPs.</p><p>Downsampling layer. For the MOAT block design, we have discussed the reodering of MBConv and Attention. As a result, we do not need the downsampling layer like average-pooling in CoAtNet <ref type="bibr">(Dai et al., 2021)</ref> or patch-embedding layer (i.e., 2 ? 2 convolution with stride 2) in Swin <ref type="bibr">(Liu et al., 2021)</ref> and ConvNeXt . As shown in Tab. 8, using patch-embedding layer indeed improves over the average-pooling scheme by 0.2% accuracy, but it takes more cost of model parameters. On the other hand, our MOAT design (i.e., delegating the downsampling to the MBConv block) shows the best performance with the least cost of parameters and comparable FLOPs.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Transformers <ref type="bibr" target="#b74">(Vaswani et al., 2017)</ref> were recently introduced to the vision community <ref type="bibr" target="#b80">(Wang et al., 2018;</ref><ref type="bibr" target="#b57">Ramachandran et al., 2019;</ref><ref type="bibr" target="#b37">Hu et al., 2019)</ref> and demonstrated remarkable performance on vision recognition tasks <ref type="bibr" target="#b8">(Carion et al., 2020;</ref><ref type="bibr" target="#b81">Zhu et al., 2021;</ref><ref type="bibr" target="#b77">Wang et al., 2021a;</ref><ref type="bibr">Arnab et al., 2021;</ref><ref type="bibr">Liu et al., 2021;</ref><ref type="bibr">Cheng et al., 2021;</ref><ref type="bibr" target="#b90">Yu et al., 2022a;</ref><ref type="bibr">Kim et al., 2022;</ref><ref type="bibr">Cheng et al., 2022;</ref><ref type="bibr" target="#b91">Yu et al., 2022b)</ref>, thanks to their ability to efficiently encode long-range interaction via the attention mechanism <ref type="bibr" target="#b3">(Bahdanau et al., 2015)</ref>. Particularly, ViT <ref type="bibr">(Dosovitskiy et al., 2021)</ref> obtains impressive results on ImageNet <ref type="bibr" target="#b58">(Russakovsky et al., 2015)</ref> by applying the vanilla Transformer with the novel large stride patch embedding, after pretraining on the proprietary large-scale JFT dataset <ref type="bibr" target="#b64">(Sun et al., 2017)</ref>. There have been several works aiming to improve the vision transformers, either with better training strategies <ref type="bibr" target="#b70">(Touvron et al., 2021a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b63">Steiner et al., 2021;</ref><ref type="bibr">Zhai et al., 2022;</ref><ref type="bibr" target="#b72">Touvron et al., 2022)</ref> or with efficient local-attention modules <ref type="bibr" target="#b41">(Huang et al., 2019;</ref><ref type="bibr" target="#b34">Ho et al., 2019;</ref><ref type="bibr">Liu et al., 2021;</ref><ref type="bibr">Chu et al., 2021;</ref><ref type="bibr" target="#b96">Yang et al., 2021;</ref><ref type="bibr" target="#b81">Yu et al., 2021;</ref><ref type="bibr">Dong et al., 2022;</ref><ref type="bibr">Tu et al., 2022)</ref>.</p><p>Since the debut of AlexNet <ref type="bibr" target="#b45">(Krizhevsky et al., 2012)</ref>, the vision community has witnessed a rapid improvement on the ImageNet benchmark using different types of ConvNets, including (but not limited to) VGGNet <ref type="bibr" target="#b61">(Simonyan &amp; Zisserman, 2015)</ref>, Inceptions <ref type="bibr" target="#b42">Ioffe &amp; Szegedy, 2015;</ref><ref type="bibr" target="#b66">Szegedy et al., 2016;</ref><ref type="bibr" target="#b18">2017)</ref>, ResNets <ref type="bibr" target="#b29">(He et al., 2016a;</ref><ref type="bibr">b)</ref>, ResNeXt <ref type="bibr" target="#b84">(Xie et al., 2017)</ref>, DenseNet <ref type="bibr" target="#b40">(Huang et al., 2017)</ref>, SENet <ref type="bibr" target="#b38">(Hu et al., 2018)</ref>, MobileNets <ref type="bibr" target="#b36">(Howard et al., 2017;</ref><ref type="bibr" target="#b59">Sandler et al., 2018;</ref><ref type="bibr" target="#b35">Howard et al., 2019)</ref>, EfficientNets <ref type="bibr" target="#b68">(Tan &amp; Le, 2019;</ref>, and ConvNeXt  each focusing on different aspects of accuracy and efficiency. The ubiquity of ConvNets in computer vision could be attributed to their built-in inductive biases.</p><p>Given the success of Transformers and ConvNets, another line of research is to explore how to effectively combine them. Swin <ref type="bibr">(Liu et al., 2021;</ref>, PVT <ref type="bibr" target="#b78">(Wang et al., 2021b;</ref>, MViT <ref type="bibr">(Fan et al., 2021;</ref><ref type="bibr" target="#b79">Li et al., 2022)</ref>, and PiT <ref type="bibr">(Heo et al., 2021)</ref> adopt the ConvNet hierarchical structure to extract multi-scale features for Transformers. SASA <ref type="bibr" target="#b57">(Ramachandran et al., 2019)</ref>, AA-ResNet , Axial-ResNet  and BoTNet <ref type="bibr">(Srinivas et al., 2021)</ref> incorporate the attention modules to ResNets. CvT <ref type="bibr">(Wu et al., 2021)</ref>, LeViT <ref type="bibr">(Graham et al., 2021)</ref>, Visformer <ref type="bibr" target="#b81">(Chen et al., 2021)</ref>, and ViT C <ref type="bibr">(Xiao et al., 2021)</ref> replace ViT's patch embedding with strided convolutions. ViTAE <ref type="bibr">(Xu et al., 2021)</ref> adopts parallel attention modules and convolutional layers. LVT <ref type="bibr">(Yang et al., 2022)</ref> introduces local self-attention into the convolution. Recently, CoAtNet <ref type="bibr">(Dai et al., 2021)</ref> and MobileViT <ref type="bibr" target="#b53">(Mehta &amp; Rastegari, 2022a)</ref> propose hybrid models that build on top of the efficient Mobile Convolution <ref type="bibr" target="#b59">(Sandler et al., 2018)</ref> and Transformer block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we have introduced MOAT model variants that effectively and seamlessly combine the mobile convolution blocks and Transformer blocks. Extensive experiments on multiple computer vision benchmarks, including ImageNet, COCO, and ADE20K, demonstrate the superior performance of employing MOAT as the network backbone. We hope MOAT could inspire the community to explore more efficient ways to integrate convolutions and self-attention operations together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>In the appendix, we provide more details for both our model and experiments.</p><p>? In section A.1, we provide MOAT implementation details.</p><p>? In section A.2, we provide ImageNet experiment details.</p><p>? In section A.3, we provide COCO detection experiment details.</p><p>? In section A.4, we proivde ADE20K semantic segmentation experiment details.</p><p>? In section A.5, we add COCO panoptic segmentation experiments.</p><p>? In section A.6, we provide ablation studies on MOAT macro-level design.</p><p>? In section A.7, we discuss limitations of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 MOAT IMPLEMENTATION DETAILS</head><p>In the MOTA networks, we employ kernel size 3 for both convolutions and depthwise convolutions. We use the multi-head self attention <ref type="bibr" target="#b74">(Vaswani et al., 2017)</ref>, where each attention head has channels 32.</p><p>For the MBConv and MOAT blocks, we use expansion ratio 4. The SE module <ref type="bibr" target="#b38">(Hu et al., 2018)</ref> in the MBConv blocks (i.e., 2nd and 3rd stages) adopt reduction ratio 4 (relative to the input channels).</p><p>Our MOAT block includes the relative positional embedding <ref type="bibr" target="#b60">(Shaw et al., 2018;</ref><ref type="bibr">Dai et al., 2021)</ref> for ImageNet. However, the downstream tasks usually take a larger input resolution than ImageNet, demanding for a special adaptation (e.g., bilinear interpolation of pretrained positional embedding). For simplicity, we remove the positional embedding, when running MOAT on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 IMAGENET IMAGE CLASSIFICATION</head><p>The ImageNet-1K dataset <ref type="bibr" target="#b58">(Russakovsky et al., 2015)</ref> contains 1.2M training images with 1000 classes. We report top-1 accuracy on the ImageNet-1K validation set, using the last checkpoint. We also experiment with pretraining on the larger ImageNet-22K dataset, and then fine-tuning on the ImageNet-1K. We closely follow the prior works <ref type="bibr">(Dai et al., 2021;</ref> and provide more details below. In Tab. 9, we compare our MOAT with more state-of-the-art models.</p><p>Experimental setup. We train MOAT models on ImageNet-1K with resolution 224 for 300 epochs. If pretraining on the larger ImageNet-22K, we use resolution 224 and 90 epochs. Afterwards, the models are fine-tuned on ImageNet-1K for 30 epochs. During fine-tuning, we also experiment with larger resolutions (e.g., 384 and 512). We employ the typical regularization methods during training, such as label smoothing <ref type="bibr" target="#b66">(Szegedy et al., 2016)</ref>, <ref type="bibr">RandAugment (Cubuk et al., 2020)</ref>, MixUp <ref type="bibr" target="#b94">(Zhang et al., 2017)</ref>, stochastic depth <ref type="bibr" target="#b39">(Huang et al., 2016)</ref>, and Adam (Kingma &amp; Ba, 2015) with decoupled weight decay (i.e., AdamW <ref type="bibr" target="#b52">(Loshchilov &amp; Hutter, 2019)</ref>  <ref type="bibr" target="#b7">(Cai &amp; Vasconcelos, 2018;</ref> on the COCO 2017 dataset <ref type="bibr" target="#b47">(Lin et al., 2014)</ref> with our MOAT architectures. The dataset contains 118K training and 5K validation samples. We use the official TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> implementation of Cascade Mask R-CNN by TF-Vision Model Garden <ref type="bibr">(Yu et al., 2020)</ref>. Our training setting closely follows the prior works <ref type="bibr">Tu et al., 2022)</ref>, except that we use batch size 64 and initial learning rate 0.0001. To adapt the MOAT models to high-resolution inputs, we partition the features into non-overlapping windows for the self-attention computations with the window size set to 14 for the second last stage, and use global attention for the last stage. As a result of this window partition, the input size must be divisible by 14. The TF-Vision Model Garden codebase further requires the input size to be square (with padding) and divisible by 64. Hence, we choose 1344 as the input size, similar to the size used in the baseline methods (i.e., longest side is no more than 1333). We use Feature Pyramid Network <ref type="bibr" target="#b48">(Lin et al., 2017)</ref> to integrate features from different levels.  <ref type="bibr" target="#b71">(Touvron et al., 2021b)</ref> 384 2 68M 48.0B 85.0 -DeepViT-L <ref type="bibr" target="#b96">(Zhou et al., 2021)</ref> 224 2 55M 12.5B 83.1 -PVT-Large <ref type="bibr" target="#b78">(Wang et al., 2021b)</ref> 224 2 61.4M 9.8B 81.7 -HaloNet-H4 <ref type="bibr">(Vaswani et al., 2021)</ref> 384 2 85M -85.6 -HaloNet-H5 <ref type="bibr">(Vaswani et al., 2021)</ref> 512  <ref type="bibr">(Bello, 2021)</ref> 320 2 --84.9 -T2T-ViT-24 <ref type="bibr" target="#b81">(Yuan et al., 2021)</ref> 224 2 64.1M 15.0B 82.6 -CvT-21 <ref type="bibr">(Wu et al., 2021)</ref> 384 2 32M 24.9B 83.3 -PVTv2-B5 <ref type="bibr" target="#b79">(Wang et al., 2022)</ref> 224 2 82M 11.8B 83.8 -MaxViT-XL <ref type="bibr">(Tu et al., 2022)</ref> 512     <ref type="bibr" target="#b47">(Lin et al., 2014)</ref> using Panoptic-DeepLab <ref type="bibr" target="#b15">(Cheng et al., 2020)</ref> with the official codebase <ref type="bibr" target="#b81">(Weber et al., 2021)</ref>. We fine-tune the global attention on downstream segmentation tasks for MOAT. We adopt the same training strategies for MOAT and its counterparts.   <ref type="bibr" target="#b15">(Cheng et al., 2020)</ref>, and results for SWideRNet is cited from , while results for ConvNeXt and MOAT are obtained using the official code-base <ref type="bibr" target="#b81">(Weber et al., 2021)</ref> with the same training recipe. All models are trained and evaluated with input images resized to 641 ? 641, and thus FLOPs are also measured w.r.t. size 641 ? 641. ?: use ImageNet-22K pretrained weights. Ablation studies on MOAT-based model. In Tab. 14, we ablate the stage-wise design by using either MBConv or MOAT block in stage 2 to stage 5. The first stage is the convolutional stem, containing two 3 ? 3 convolutions. We use the layer layout of MOAT-0. As shown in the table, the pure MOAT-based model (i.e., using MOAT blocks for all four stages) achieves the best performance of 83.6%, which however uses the most FLOPs. Our MOAT model design (i.e., use MOAT block in the last two stages) attains the better trade-off between accuracy and model complexity. Ablation studies on MOAT meta architecture. We perform ablation studies on the metaarchitecture by varying the number of blocks per stage. For simplicity, we only vary the block numbers in the third and fourth stages, while keeping the block numbers in the other stages unchanged. Note that the first stage corresponds to the convolutional stem. The studies with MOAT-1 meta architecture are shown in Tab. 15. In the end, we choose the layout {2, 2, 6, 14, 2} because it has the best performance and lower parameter cost. Interestingly, our discovery echoes the layer layout proposed by CoAtNet <ref type="bibr">(Dai et al., 2021)</ref>. We visualize the architecture of MOAT-1 in <ref type="figure" target="#fig_2">Fig. 6</ref>. <ref type="table" target="#tab_0">Table 15</ref>: Ablation studies of MOAT meta-architecture design on ImageNet-1K, using MOAT-1 and input size 224. We control the first, second and last stages to have two blocks, and vary the block numbers of the third and fourth stages.</p><p>number of blocks in five stages params (M) FLOPs (B) top-1 acc.</p><p>(2, 2, 2, 16, 2) 43.7 8.9 84.1 <ref type="bibr">(2,</ref><ref type="bibr">2,</ref><ref type="bibr">4,</ref><ref type="bibr">15,</ref><ref type="bibr">2)</ref> 42.6 9.0 84.2 (2, 2, 6, 14, 2) 41.6 9.1 84.2 <ref type="bibr">(2,</ref><ref type="bibr">2,</ref><ref type="bibr">8,</ref><ref type="bibr">13,</ref><ref type="bibr">2)</ref> 40.6 9.2 84.1 (2, 2, 10, 12, 2) 39.5 9.  A.7 LIMITATIONS Currently, the scaling rule of MOAT model variants are hand-designed. We, therefore, expect the architecture could be further improved by the breakthroughs in neural architecture search or network pruning (attaining faster inference speed while maintaining a similar accuracy).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Block comparison. (a) The MBConv block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :Figure 4 :Figure 5 :</head><label>2345</label><figDesc>Parameters vs. accuracy using ImageNet-1K only with input size 224. FLOPs vs. accuracy using ImageNet-1K only with input size 224. Parameters vs. accuracy using ImageNet-22K and ImageNet-1K with input size 384. FLOPs vs. accuracy using ImageNet-22K and ImageNet-1K with input size 384.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Architecture of MOAT-1, including the convolutional stem, MBConv, and MOAT blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>MOAT variants differ in the number of blocks B and number of channels C in each stage.</figDesc><table /><note>block stride</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance on ImageNet-1K. 1K only: Using ImageNet-1K only. 22K + 1K: ImageNet-22K pretraining and ImageNet-1K fine-tuning.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>model</cell><cell></cell><cell></cell><cell cols="2">eval size params FLOPs ImageNet-1K top-1 accuracy</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1K only</cell><cell>22K+1K</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">EfficientNetV2-L (Tan &amp; Le, 2021)</cell><cell>480 2</cell><cell>120M</cell><cell>53B</cell><cell>85.7</cell><cell>-</cell></row><row><cell></cell><cell cols="2">ConvNets</cell><cell cols="3">EfficientNetV2-XL (Tan &amp; Le, 2021) ConvNeXt-T (Liu et al., 2022b)</cell><cell>480 2 224 2</cell><cell>208M 29M</cell><cell>94B 4.5B</cell><cell>-82.1</cell><cell>87.3 82.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ConvNeXt-L (Liu et al., 2022b)</cell><cell></cell><cell>384 2</cell><cell>198M</cell><cell>101.0B</cell><cell>85.5</cell><cell>87.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ConvNeXt-XL (Liu et al., 2022b)</cell><cell></cell><cell>384 2</cell><cell>350M</cell><cell>179.0B</cell><cell>-</cell><cell>87.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">PVT-Large (Wang et al., 2021b)</cell><cell></cell><cell>224 2</cell><cell>61.4M</cell><cell>9.8B</cell><cell>81.7</cell><cell>-</cell></row><row><cell></cell><cell>ViTs</cell><cell></cell><cell cols="2">Swin-T (Liu et al., 2021) Swin-L (Liu et al., 2021)</cell><cell></cell><cell>224 2 384 2</cell><cell>28M 197M</cell><cell>4.5B 103.9B</cell><cell>81.3 -</cell><cell>-87.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SwinV2-L (Liu et al., 2021)</cell><cell></cell><cell>384 2</cell><cell>197M</cell><cell>115.4B</cell><cell>-</cell><cell>87.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">MViTv2-H (Li et al., 2022)</cell><cell></cell><cell>512 2</cell><cell>667M</cell><cell>763.5B</cell><cell>-</cell><cell>88.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">PVTv2-B5 (Wang et al., 2022)</cell><cell></cell><cell>224 2</cell><cell>82M</cell><cell>11.8B</cell><cell>83.8</cell><cell>-</cell></row><row><cell></cell><cell cols="2">Hybrid</cell><cell cols="2">MaxViT-XL (Tu et al., 2022) CoAtNet-0 (Dai et al., 2021)</cell><cell></cell><cell>512 2 224 2</cell><cell>475M 25M</cell><cell>535.2B 4.2B</cell><cell>-81.6</cell><cell>88.7 -</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CoAtNet-3 (Dai et al., 2021)</cell><cell></cell><cell>384 2</cell><cell>168M</cell><cell>107.4B</cell><cell>85.8</cell><cell>87.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CoAtNet-4 (Dai et al., 2021)</cell><cell></cell><cell>512 2</cell><cell>275M</cell><cell>360.9B</cell><cell>-</cell><cell>88.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MOAT-0</cell><cell></cell><cell></cell><cell>224 2</cell><cell>27.8M</cell><cell>5.7B</cell><cell>83.3</cell><cell>83.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MOAT-1</cell><cell></cell><cell></cell><cell>224 2</cell><cell>41.6M</cell><cell>9.1B</cell><cell>84.2</cell><cell>84.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MOAT-2</cell><cell></cell><cell></cell><cell>224 2</cell><cell>73.4M</cell><cell>17.2B</cell><cell>84.7</cell><cell>86.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MOAT-3</cell><cell></cell><cell></cell><cell>224 2</cell><cell>190.0M</cell><cell>44.9B</cell><cell>85.3</cell><cell>86.8</cell></row><row><cell></cell><cell cols="2">Hybrid (ours)</cell><cell>MOAT-0 MOAT-1 MOAT-2</cell><cell></cell><cell></cell><cell>384 2 384 2 384 2</cell><cell>27.8M 41.6M 73.4M</cell><cell>18.2B 29.6B 54.3B</cell><cell>84.6 85.9 86.2</cell><cell>85.7 87.0 87.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MOAT-3</cell><cell></cell><cell></cell><cell>384 2</cell><cell>190.0M 141.2B</cell><cell>86.5</cell><cell>88.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MOAT-1</cell><cell></cell><cell></cell><cell>512 2</cell><cell>41.6M</cell><cell>58.7B</cell><cell>86.2</cell><cell>87.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MOAT-2</cell><cell></cell><cell></cell><cell>512 2</cell><cell>73.4M</cell><cell>104.6B</cell><cell>86.5</cell><cell>87.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MOAT-3</cell><cell></cell><cell></cell><cell>512 2</cell><cell>190.0M 271.0B</cell><cell>86.7</cell><cell>88.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MOAT-4</cell><cell></cell><cell></cell><cell>512 2</cell><cell>483.2M 648.5B</cell><cell>-</cell><cell>89.1</cell></row><row><cell></cell><cell>86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>accuracy</cell><cell>83 84</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Top-1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">MOAT CoAtNet</cell><cell></cell></row><row><cell></cell><cell>82</cell><cell></cell><cell></cell><cell cols="2">ConvNeXt</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Swin</cell><cell></cell><cell></cell></row><row><cell></cell><cell>81</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Parameters (M)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Object detection and instance segmentation on the COCO 2017 val set. We employ Cascade Mask-RCNN, and single-scale inference (hard NMS). ?: use ImageNet-22K pretrained weights.ADE20K Semantic Segmentation. In Tab. 4, when using input size 513 2 , MOAT consistently outperforms the ConvNeXt counterparts. MOAT-0 surpasses ConvNeXt-T by 3.0% mIoU. Moreover, MOAT-2, with ImageNet-22k pretraining, surpasses ConvNeXt-B by 3.1%. The larger MOAT-3 and MOAT-4 further outperform ConvNeXt-L and ConvNeXt-XL by 4.9% and 5.4%, respectively. Finally, when using input size 641 2 , our MOAT-4 achieves the performance of 57.6% mIoU, setting a new state-of-the-art in the regime of models using input size 641 2 .</figDesc><table><row><cell>backbone</cell><cell>input size</cell><cell cols="4">params FLOPs AP box AP box 50</cell><cell>AP box 75</cell><cell cols="2">AP mask AP mask 50</cell><cell>AP mask 75</cell></row><row><cell>Swin-T</cell><cell>1280 ? 800</cell><cell>86M</cell><cell>745B</cell><cell>50.5</cell><cell>69.3</cell><cell>54.9</cell><cell>43.7</cell><cell>66.6</cell><cell>47.1</cell></row><row><cell>Swin-S</cell><cell>1280 ? 800</cell><cell>107M</cell><cell>838B</cell><cell>51.8</cell><cell>70.4</cell><cell>56.3</cell><cell>44.7</cell><cell>67.9</cell><cell>48.5</cell></row><row><cell>Swin-B ?</cell><cell>1280 ? 800</cell><cell>145M</cell><cell>982B</cell><cell>53.0</cell><cell>71.8</cell><cell>57.5</cell><cell>45.8</cell><cell>69.4</cell><cell>49.7</cell></row><row><cell>Swin-L ?</cell><cell>1280 ? 800</cell><cell>254M</cell><cell>1382B</cell><cell>53.9</cell><cell>72.4</cell><cell>58.8</cell><cell>46.7</cell><cell>70.1</cell><cell>50.8</cell></row><row><cell>ConvNeXt-T</cell><cell>1280 ? 800</cell><cell>86M</cell><cell>741B</cell><cell>50.4</cell><cell>69.1</cell><cell>54.8</cell><cell>43.7</cell><cell>66.5</cell><cell>47.3</cell></row><row><cell>ConvNeXt-S</cell><cell>1280 ? 800</cell><cell>108M</cell><cell>827B</cell><cell>51.9</cell><cell>70.8</cell><cell>56.5</cell><cell>45.0</cell><cell>68.4</cell><cell>49.1</cell></row><row><cell cols="2">ConvNeXt-B ? 1280 ? 800</cell><cell>146M</cell><cell>964B</cell><cell>54.0</cell><cell>73.1</cell><cell>58.8</cell><cell>46.9</cell><cell>70.6</cell><cell>51.3</cell></row><row><cell>ConvNeXt-L ?</cell><cell>1280 ? 800</cell><cell>255M</cell><cell>1354B</cell><cell>54.8</cell><cell>73.8</cell><cell>59.8</cell><cell>47.6</cell><cell>71.3</cell><cell>51.7</cell></row><row><cell>MOAT-0</cell><cell>1344 ? 1344</cell><cell>65M</cell><cell>799B</cell><cell>55.9</cell><cell>73.9</cell><cell>60.9</cell><cell>47.4</cell><cell>70.9</cell><cell>52.1</cell></row><row><cell>MOAT-1</cell><cell>1344 ? 1344</cell><cell>79M</cell><cell>921B</cell><cell>57.7</cell><cell>76.0</cell><cell>63.4</cell><cell>49.0</cell><cell>73.4</cell><cell>53.2</cell></row><row><cell>MOAT-2 ?</cell><cell cols="2">1344 ? 1344 110M</cell><cell>1217B</cell><cell>58.5</cell><cell>76.6</cell><cell>64.3</cell><cell>49.3</cell><cell>73.9</cell><cell>53.9</cell></row><row><cell>MOAT-3 ?</cell><cell cols="2">1344 ? 1344 227M</cell><cell>2216B</cell><cell>59.2</cell><cell>77.8</cell><cell>64.9</cell><cell>50.3</cell><cell>74.8</cell><cell>55.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Semantic segmentation on ADE20K val set. We employ DeepLabv3+ (single-scale inference). Results for ConvNeXt and MOAT are obtained using the official code-base<ref type="bibr" target="#b81">(Weber et al., 2021)</ref> with the same training recipe. ?: use ImageNet-22K pretrained weights.</figDesc><table><row><cell></cell><cell>backbone</cell><cell>input params</cell><cell>FLOPs</cell><cell>mIoU (%)</cell></row><row><cell>backbone ConvNeXt-T ConvNeXt-S ConvNeXt-B ? ConvNeXt-L ? ConvNeXt-XL ? 513 2 364.0M 446.2B input params FLOPs mIoU (%) 513 2 34.2M 47.6B 45.8 513 2 55.8M 70.8B 47.8 513 2 95.8M 119.5B 50.5 513 2 208.3M 256.4B 51.0 51.8</cell><cell cols="2">MOAT-0 MOAT-1 MOAT-2 ? 513 2 513 2 513 2 MOAT-3 ? 513 2 198.4M 33.3M 47.0M 80.5M MOAT-4 ? 513 2 496.3M MOAT-2 ? 641 2 80.5M MOAT-3 ? 641 2 198.4M</cell><cell>61.3B 85.4B 144.3B 331.5B 779.9B 242.0B 554.7B</cell><cell>48.8 51.8 53.6 55.9 57.2 54.7 56.5</cell></row><row><cell></cell><cell cols="3">MOAT-4 ? 641 2 496.3M 1273.5B</cell><cell>57.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performances of tiny-MOAT family on ImageNet-1K.</figDesc><table><row><cell>input size 224 2</cell><cell cols="3">params FLOPs top-1 acc.</cell><cell>input size 256 2</cell><cell cols="3">params FLOPs top-1 acc.</cell></row><row><cell cols="2">Mobile-Former-52M Mobile-Former-96M Mobile-Former-214M Mobile-Former-508M 14.0M 3.5M 4.6M 9.4M</cell><cell>0.05B 0.1B 0.2B 0.5B</cell><cell>68.7 72.8 76.7 79.3</cell><cell cols="2">MobileViT-XS MobileViT-S MobileViTv2-1.0 MobileViTv2-1.5 10.6M 2.3M 5.6M 4.9M</cell><cell>0.7B 2.0B 1.8B 4.0B</cell><cell>74.8 78.4 78.1 80.4</cell></row><row><cell>tiny-MOAT-0</cell><cell>3.4M</cell><cell>0.8B</cell><cell>75.5</cell><cell cols="2">MobileViTv2-2.0 18.5M</cell><cell>7.5B</cell><cell>81.2</cell></row><row><cell>tiny-MOAT-1 tiny-MOAT-2</cell><cell>5.1M 9.8M</cell><cell>1.2B 2.3B</cell><cell>78.3 81.0</cell><cell>tiny-MOAT-1 tiny-MOAT-2</cell><cell>5.1M 9.8M</cell><cell>1.6B 3.0B</cell><cell>79.2 81.7</cell></row><row><cell>tiny-MOAT-3</cell><cell>19.5M</cell><cell>4.5B</cell><cell>82.7</cell><cell>tiny-MOAT-3</cell><cell>19.5M</cell><cell>6.0B</cell><cell>83.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation studies of MOAT block design on ImageNet-1K with input size 224.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>model</cell><cell cols="4">block composition params FLOPs top-1 acc.</cell></row><row><cell>model</cell><cell>block composition Attn + MLP</cell><cell cols="3">params FLOPs top-1 acc. 28.0M 5.4B 82.6</cell><cell>tiny-MOAT-2</cell><cell>Attn + MLP MBConv + Attn</cell><cell>9.8M 9.8M</cell><cell>2.2B 2.3B</cell><cell>79.8 81.0</cell></row><row><cell>MOAT-0</cell><cell>Attn + MLP (w/ depth. conv) Attn + MBConv</cell><cell>28.2M 28.2M</cell><cell>5.4B 5.4B</cell><cell>81.7 82.9</cell><cell>tiny-MOAT-1</cell><cell>Attn + MLP MBConv + Attn</cell><cell>5.1M 5.1M</cell><cell>1.1B 1.2B</cell><cell>76.2 78.3</cell></row><row><cell></cell><cell>MBConv + Attn</cell><cell>27.8M</cell><cell>5.7B</cell><cell>83.3</cell><cell>tiny-MOAT-0</cell><cell>Attn + MLP MBConv + Attn</cell><cell>3.3M 3.4M</cell><cell>0.8B 0.8B</cell><cell>72.9 75.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Ablation studies of the order of MBConv and Attention (Attn) on ImageNet-1K with input 224. We also ablate the place, where we apply the spatial downsampling and channel expansion.</figDesc><table><row><cell>model</cell><cell cols="6">block composition spatial downsampling channel expansion params (M) FLOPs (B) top-1 acc.</cell></row><row><cell></cell><cell>Attn + MLP</cell><cell>Attn</cell><cell>Attn</cell><cell>28.0</cell><cell>5.4</cell><cell>82.6</cell></row><row><cell></cell><cell>Attn + MBConv</cell><cell>Attn</cell><cell>Attn</cell><cell>28.2</cell><cell>5.4</cell><cell>82.9</cell></row><row><cell>MOAT-0</cell><cell>Attn + MBConv</cell><cell>MBConv</cell><cell>MBConv</cell><cell>25.6</cell><cell>5.8</cell><cell>83.1</cell></row><row><cell></cell><cell>MBConv + Attn</cell><cell>MBConv</cell><cell>MBConv</cell><cell>27.8</cell><cell>5.7</cell><cell>83.3</cell></row><row><cell></cell><cell>Attn + MBConv</cell><cell>MBConv</cell><cell>Attn</cell><cell>29.3</cell><cell>7.1</cell><cell>83.2</cell></row><row><cell></cell><cell>Attn + MLP</cell><cell>Attn</cell><cell>Attn</cell><cell>9.8</cell><cell>2.2</cell><cell>79.8</cell></row><row><cell></cell><cell>Attn + MBConv</cell><cell>Attn</cell><cell>Attn</cell><cell>9.9</cell><cell>2.2</cell><cell>80.5</cell></row><row><cell>tiny-MOAT-2</cell><cell>Attn + MBConv</cell><cell>MBConv</cell><cell>MBConv</cell><cell>9.0</cell><cell>2.3</cell><cell>80.7</cell></row><row><cell></cell><cell>MBConv + Attn</cell><cell>MBConv</cell><cell>MBConv</cell><cell>9.8</cell><cell>2.3</cell><cell>81.0</cell></row><row><cell></cell><cell>Attn + MBConv</cell><cell>MBConv</cell><cell>Attn</cell><cell>10.3</cell><cell>2.8</cell><cell>81.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Ablation studies of the downsampling layer design on ImageNet</figDesc><table><row><cell>-1K, using MOAT-0 and input size</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>). See Tab. 10 and Tab. 11 for detailed hyper-parameters. All the experiments are performed on TPUv4. MOAT-{0,1,2} are trained with 16 cores while MOAT-3 with 32 cores. We report the training time of MOAT model variants in Tab. 12. A.3 COCO OBJECT DETECTION AND INSTANCE SEGMENTATION Experimental setup. We train Cascade Mask R-CNN</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Performance on ImageNet-1K with more state-of-the-art models are included. 1K only: Using ImageNet-1K only. 22K + 1K: ImageNet-22K pretraining and ImageNet-1K fine-tuning.</figDesc><table><row><cell></cell><cell>model</cell><cell cols="2">eval size params</cell><cell cols="3">FLOPs ImageNet-1K top-1 accuracy</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1K only</cell><cell>22K+1K</cell></row><row><cell></cell><cell>RegNetY-16G (Radosavovic et al., 2020)</cell><cell>224 2</cell><cell>84M</cell><cell>16.0B</cell><cell>82.9</cell><cell>-</cell></row><row><cell></cell><cell>NFNet-F5 (Brock et al., 2021)</cell><cell>544 2</cell><cell>377M</cell><cell>289.8B</cell><cell>86.0</cell><cell>-</cell></row><row><cell></cell><cell>EfficientNetV2-L (Tan &amp; Le, 2021)</cell><cell>480 2</cell><cell>120M</cell><cell>53B</cell><cell>85.7</cell><cell>-</cell></row><row><cell></cell><cell>EfficientNetV2-XL (Tan &amp; Le, 2021)</cell><cell>480 2</cell><cell>208M</cell><cell>94B</cell><cell>-</cell><cell>87.3</cell></row><row><cell>ConvNets</cell><cell>ConvNeXt-T (Liu et al., 2022b)</cell><cell>224 2</cell><cell>29M</cell><cell>4.5B</cell><cell>82.1</cell><cell>82.9</cell></row><row><cell></cell><cell>ConvNeXt-S (Liu et al., 2022b)</cell><cell>224 2</cell><cell>50M</cell><cell>8.7B</cell><cell>83.1</cell><cell>84.6</cell></row><row><cell></cell><cell>ConvNeXt-B (Liu et al., 2022b)</cell><cell>224 2</cell><cell>89M</cell><cell>15.4B</cell><cell>83.8</cell><cell>85.8</cell></row><row><cell></cell><cell>ConvNeXt-L (Liu et al., 2022b)</cell><cell>384 2</cell><cell>198M</cell><cell>101.0B</cell><cell>85.5</cell><cell>87.5</cell></row><row><cell></cell><cell>ConvNeXt-XL (Liu et al., 2022b)</cell><cell>384 2</cell><cell>350M</cell><cell>179.0B</cell><cell>85.5</cell><cell>87.8</cell></row><row><cell></cell><cell>DeiT-B (Touvron et al., 2021a)</cell><cell>384 2</cell><cell>86M</cell><cell>55.4B</cell><cell>83.1</cell><cell>-</cell></row><row><cell></cell><cell>CaiT-S-36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViTs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>We experiment with the proposed MOAT models on ADE20K semantic segmentation dataset<ref type="bibr" target="#b95">(Zhou et al., 2019)</ref> using DeepLabv3+<ref type="bibr" target="#b18">2017)</ref>. We fine-tune the global attention for MOAT. The same training strategies are used for all backbone variants. Specifically, for training hyper-parameters, we train the model with 32 TPU cores for 180k iterations, with batch size 64, Adam (Kingma &amp; Ba, 2015) optimizer, and a poly schedule learning rate starting at 0.0001. For data augmentations, the inputs images are resized and padded to either 513 ? 513 or 641 ? 641, with random cropping, flipping, and color jittering<ref type="bibr" target="#b20">(Cubuk et al., 2019)</ref>. No test-time augmentation is used during inference.</figDesc><table><row><cell></cell><cell></cell><cell>2</cell><cell>475M</cell><cell>535.2B</cell><cell>-</cell><cell>88.7</cell></row><row><cell></cell><cell>CoAtNet-0 (Dai et al., 2021)</cell><cell>224 2</cell><cell>25M</cell><cell>4.2B</cell><cell>81.6</cell><cell>-</cell></row><row><cell></cell><cell>CoAtNet-1 (Dai et al., 2021)</cell><cell>224 2</cell><cell>42M</cell><cell>8.4B</cell><cell>83.3</cell><cell>-</cell></row><row><cell></cell><cell>CoAtNet-2 (Dai et al., 2021)</cell><cell>224 2</cell><cell>75M</cell><cell>15.7B</cell><cell>84.1</cell><cell>-</cell></row><row><cell></cell><cell>CoAtNet-3 (Dai et al., 2021)</cell><cell>384 2</cell><cell>168M</cell><cell>107.4B</cell><cell>85.8</cell><cell>87.6</cell></row><row><cell></cell><cell>CoAtNet-4 (Dai et al., 2021)</cell><cell>512 2</cell><cell>275M</cell><cell>360.9B</cell><cell>-</cell><cell>88.6</cell></row><row><cell></cell><cell>MOAT-0</cell><cell>224 2</cell><cell>27.8M</cell><cell>5.7B</cell><cell>83.3</cell><cell>83.6</cell></row><row><cell></cell><cell>MOAT-1</cell><cell>224 2</cell><cell>41.6M</cell><cell>9.1B</cell><cell>84.2</cell><cell>84.9</cell></row><row><cell></cell><cell>MOAT-2</cell><cell>224 2</cell><cell>73.4M</cell><cell>17.2B</cell><cell>84.7</cell><cell>86.0</cell></row><row><cell></cell><cell>MOAT-3</cell><cell>224 2</cell><cell>190.0M</cell><cell>44.9B</cell><cell>85.3</cell><cell>86.8</cell></row><row><cell>Hybrid (ours)</cell><cell>MOAT-0 MOAT-1 MOAT-2</cell><cell>384 2 384 2 384 2</cell><cell>27.8M 41.6M 73.4M</cell><cell>18.2B 29.6B 54.3B</cell><cell>84.6 85.9 86.2</cell><cell>85.7 87.0 87.5</cell></row><row><cell></cell><cell>MOAT-3</cell><cell>384 2</cell><cell cols="2">190.0M 141.2B</cell><cell>86.5</cell><cell>88.2</cell></row><row><cell></cell><cell>MOAT-1</cell><cell>512 2</cell><cell>41.6M</cell><cell>58.7B</cell><cell>86.2</cell><cell>87.2</cell></row><row><cell></cell><cell>MOAT-2</cell><cell>512 2</cell><cell>73.4M</cell><cell>104.6B</cell><cell>86.5</cell><cell>87.7</cell></row><row><cell></cell><cell>MOAT-3</cell><cell>512 2</cell><cell cols="2">190.0M 271.0B</cell><cell>86.7</cell><cell>88.4</cell></row><row><cell></cell><cell>MOAT-4</cell><cell>512 2</cell><cell cols="2">483.2M 648.5B</cell><cell>-</cell><cell>89.1</cell></row><row><cell cols="2">A.4 ADE20K SEMANTIC SEGMENTATION</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Experimental setup.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>MOAT ImageNet hyper-parameter settings.</figDesc><table><row><cell></cell><cell cols="2">ImageNet-1K</cell><cell cols="2">ImageNet-22K</cell></row><row><cell>hyper-parameter</cell><cell>1K pre-training</cell><cell>1K ? 1K fine-tuning</cell><cell cols="2">22K pre-training fine-tuning 22K ? 1K</cell></row><row><cell></cell><cell cols="2">(MOAT-0/1/2/3)</cell><cell cols="2">(MOAT-0/1/2/3)</cell></row><row><cell>stochastic depth rate</cell><cell>0.2 / 0.3 / 0.5 / 0.7</cell><cell>0.2 / 0.3 / 0.5 / 0.9</cell><cell>0.1 / 0.2 / 0.3 / 0.6</cell><cell>0.1 / 0.2 / 0.3 / 0.6</cell></row><row><cell>center crop</cell><cell>true</cell><cell>false</cell><cell>true</cell><cell>false</cell></row><row><cell>randaugment</cell><cell>2, 15</cell><cell>2, 15/15/15/20</cell><cell>2, 5</cell><cell>2, 5</cell></row><row><cell>mixup alpha</cell><cell>0.8</cell><cell>0.8</cell><cell>none</cell><cell>none</cell></row><row><cell>loss type</cell><cell>softmax</cell><cell>softmax</cell><cell>sigmoid</cell><cell>softmax</cell></row><row><cell>label smoothing</cell><cell>0.1</cell><cell>0.1</cell><cell>0.0001</cell><cell>0.1</cell></row><row><cell>train epochs</cell><cell>300</cell><cell>30</cell><cell>90</cell><cell>30</cell></row><row><cell>train batch size</cell><cell>4096</cell><cell>512</cell><cell>4096</cell><cell>1024</cell></row><row><cell>optimizer type</cell><cell>AdamW</cell><cell>AdamW</cell><cell>AdamW</cell><cell>AdamW</cell></row><row><cell>peak learning rate</cell><cell>3e-3</cell><cell>5e-5</cell><cell>1e-3</cell><cell>5e-5</cell></row><row><cell>min learning rate</cell><cell>1e-5</cell><cell>5e-5</cell><cell>1e-5</cell><cell>5e-5</cell></row><row><cell>warm-up</cell><cell>10K steps</cell><cell>none</cell><cell>5 epochs</cell><cell>none</cell></row><row><cell>lr decay schedule</cell><cell>cosine</cell><cell>none</cell><cell>linear</cell><cell>none</cell></row><row><cell>weight decay rate</cell><cell>0.05</cell><cell>1e-8</cell><cell>0.01</cell><cell>1e-8</cell></row><row><cell>gradient clip</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>EMA decay rate</cell><cell>0.9999</cell><cell>0.9999</cell><cell>None</cell><cell>0.9999</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>tiny-MOAT ImageNet hyper-parameter settings. : use EMA decay rate 0.9999 for tiny-MOAT-3.</figDesc><table><row><cell></cell><cell cols="2">ImageNet-1K</cell></row><row><cell>hyper-parameter</cell><cell cols="2">1K input size 224 input size 256 1K</cell></row><row><cell></cell><cell cols="2">(tiny-MOAT-0/1/2/3)</cell></row><row><cell>stochastic depth rate</cell><cell>0.0 / 0.0 / 0.0 / 0.1</cell><cell>0.0 / 0.0 / 0.0 / 0.1</cell></row><row><cell>center crop</cell><cell>true</cell><cell>true</cell></row><row><cell>randaugment</cell><cell>2, 15</cell><cell>2, 15</cell></row><row><cell>mixup alpha</cell><cell>0.8</cell><cell>0.8</cell></row><row><cell>loss type</cell><cell>softmax</cell><cell>softmax</cell></row><row><cell>label smoothing</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>train epochs</cell><cell>300</cell><cell>300</cell></row><row><cell>train batch size</cell><cell>4096</cell><cell>4096</cell></row><row><cell>optimizer type</cell><cell>AdamW</cell><cell>AdamW</cell></row><row><cell>peak learning rate</cell><cell>3e-3</cell><cell>3e-3</cell></row><row><cell>min learning rate</cell><cell>1e-5</cell><cell>1e-5</cell></row><row><cell>warm-up</cell><cell>10K steps</cell><cell>10K steps</cell></row><row><cell>lr decay schedule</cell><cell>cosine</cell><cell>cosine</cell></row><row><cell>weight decay rate</cell><cell>0.05</cell><cell>0.05</cell></row><row><cell>gradient clip</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>EMA decay rate</cell><cell>None</cell><cell>None</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Training time measured in hours. We use 16 TPUv4 cores for training MOAT-{0,1,2} and 32 TPUv4 cores for MOAT-3. MOAT is training efficient: for ImageNet-22k pretraining, MOAT takes no more than 2.05 days, while for ImageNet-1k pretraining, MOAT takes &lt; 1 day. We also evaluate the proposed MOAT architectures on the challenging COCO panoptic segmentation dataset</figDesc><table><row><cell>dataset</cell><cell>model</cell><cell>pre-training</cell><cell cols="2">fine-tuning</cell></row><row><cell></cell><cell cols="4">input size 224 ? 224 224 ? 224 384 ? 384</cell></row><row><cell>ImageNet-1K</cell><cell>MOAT-0 MOAT-1</cell><cell>6.5h 9.8h</cell><cell>--</cell><cell>2.8h 4.4h</cell></row><row><cell></cell><cell>MOAT-2</cell><cell>13.9h</cell><cell>-</cell><cell>6.1h</cell></row><row><cell></cell><cell>MOAT-3</cell><cell>16.0h</cell><cell>-</cell><cell>7.9h</cell></row><row><cell></cell><cell cols="4">input size 224 ? 224 224 ? 224 384 ? 384</cell></row><row><cell>ImageNet-22K</cell><cell>MOAT-0 MOAT-1</cell><cell>20.1h 30.0h</cell><cell>0.9h 1.3h</cell><cell>2.5h 3.9h</cell></row><row><cell></cell><cell>MOAT-2</cell><cell>42.6h</cell><cell>1.8h</cell><cell>5.4h</cell></row><row><cell></cell><cell>MOAT-3</cell><cell>49.2h</cell><cell>2.2h</cell><cell>7.0h</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Specifically, for training hyper-parameters, we train the model with 32 TPU cores for 200k iterations with the first 2k for warm-up stage. We use batch size 64, Adam<ref type="bibr" target="#b44">(Kingma &amp; Ba, 2015)</ref> optimizer, and a poly schedule learning rate starting at 0.0005. For data augmentations, the inputs images are resized and padded to 641 ? 641, with random cropping, flipping, and color jittering<ref type="bibr" target="#b20">(Cubuk et al., 2019)</ref>. No test-time augmentation is used during inference.</figDesc><table><row><cell>Main results. The results are summarized in Tab. 13, where MOAT consistently outperforms other</cell></row><row><cell>backbones. Specifically, our MOAT-0 surpasses ConvNeXt-T significantly by 4.3% PQ. In the large</cell></row><row><cell>model regime, MOAT-3 surpasses ConvNeXt-L by 3.5%. Our MOAT-4 achieves the performance of</cell></row><row><cell>46.7% PQ, outperforming the heavy backbone SWideRNet (Chen et al., 2020) by 2.3%.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 13 :</head><label>13</label><figDesc>Panoptic segmentation on COCO val set. The results are obtained by applying different backbones with Panoptic-DeepLab, using single-scale inference (i.e., no test-time augmentation). Results for MobileNet, ResNet, and Xception are cited from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 14 :</head><label>14</label><figDesc>Ablation studies of MOAT-based model on ImageNet-1K, using MOAT-0 layer layout and input size 224. We change the block type (MBConv vs. MOAT block) from stage 2 to stage 5. The first stage is fixed to use the convolutional stem.</figDesc><table><row><cell>stage-2</cell><cell>stage-3</cell><cell>stage-4</cell><cell>stage-5</cell><cell cols="3">params (M) FLOPs (B) top-1 acc.</cell></row><row><cell>MOAT</cell><cell>MOAT</cell><cell>MOAT</cell><cell>MOAT</cell><cell>28.2</cell><cell>11.9</cell><cell>83.6</cell></row><row><cell>MBConv</cell><cell>MOAT</cell><cell>MOAT</cell><cell>MOAT</cell><cell>28.1</cell><cell>6.9</cell><cell>83.5</cell></row><row><cell cols="2">MBConv MBConv</cell><cell>MOAT</cell><cell>MOAT</cell><cell>27.8</cell><cell>5.7</cell><cell>83.3</cell></row><row><cell cols="3">MBConv MBConv MBConv</cell><cell>MOAT</cell><cell>25.7</cell><cell>4.7</cell><cell>82.2</cell></row><row><cell cols="4">MBConv MBConv MBConv MBConv</cell><cell>23.4</cell><cell>4.5</cell><cell>82.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lambdanetworks: Modeling long-range interactions without attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Scaling wide residual networks for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11675</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simple single-scale vision transformer for object localization and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mobile-former: Bridging mobilenet and transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visformer: The vision-friendly transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengsu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Maskedattention mask transformer for universal image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">TubeFormer-DeepLab: Video Mask Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Seok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>So Kweon, and Liang-Chieh Chen</editor>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mvitv2: Improved multiscale vision transformers for classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Swin transformer v2: Scaling up capacity and resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mobilevit: light-weight, general-purpose, and mobilefriendly vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Separable self-attention for mobile vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.02680</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mo-bilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">How to train your vit? data, augmentation, and regularization in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10270</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Inception-v4, inceptionresnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Efficientnetv2: Smaller models and faster training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">DeiT III: Revenge of the ViT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.07118</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Maxvit: Multi-axis vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxiao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Pvtv2: Improved baselines with pyramid vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Visual Media</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09748</idno>
		<title level="m">DeepLab2: A TensorFlow Library for Deep Labeling</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Vitae: Vision transformer advanced by exploring intrinsic inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Lite vision transformer with enhanced self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Rashwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyoun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://github.com/tensorflow/models,2020" />
	</analytic>
	<monogr>
		<title level="j">TensorFlow Model Garden</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Glance-and-gaze vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingda</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Cmt-deeplab: Clustering mask transformers for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Alan Yuille, and Liang-Chieh Chen. k-means Mask Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Scaling vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="321" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11886</idno>
		<title level="m">Qibin Hou, and Jiashi Feng. Deepvit: Towards deeper vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
