<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SIMULTANEOUS PERTURBATION METHOD FOR MULTI-TASK WEIGHT OPTIMIZATION IN ONE-SHOT META-LEARNING A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Boiarov</surname></persName>
							<email>andrei.boiarov@sit.team</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostiantyn</forename><surname>Khabarlak</surname></persName>
							<email>habarlack@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Yastrebov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Hungary</roleName><forename type="first">Otp</forename><forename type="middle">Bank</forename><surname>Budapest</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">SIT Rolos Schaffhausen</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Dnipro University of Technology</orgName>
								<address>
									<country key="UA">Ukraine</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SIMULTANEOUS PERTURBATION METHOD FOR MULTI-TASK WEIGHT OPTIMIZATION IN ONE-SHOT META-LEARNING A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Meta-Learning ? Multi-Task Learning ? One-Shot Learning ? Stochastic Approximation ? Deep Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Meta-learning methods aim to build learning algorithms capable of quickly adapting to new tasks in low-data regime. One of the most difficult benchmarks of such algorithms is a one-shot learning problem. In this setting many algorithms face uncertainties associated with limited amount of training samples, which may result in overfitting. This problem can be resolved by providing additional information to the model. One of the most efficient ways to do this is multi-task learning. In this paper we investigate the modification of a standard meta-learning pipeline. The proposed method simultaneously utilizes information from several meta-training tasks in a common loss function. The impact of these tasks in the loss function is controlled by a per task weight. Proper optimization of the weights can have big influence on training and the final quality of the model. We propose and investigate the use of methods from the family of Simultaneous Perturbation Stochastic Approximation (SPSA) for optimization of meta-train tasks weights. We also demonstrate superiority of stochastic approximation in comparison to gradient-based method. The proposed Multi-Task Modification can be applied to almost all meta-learning methods. We study applications of this modification on Model-Agnostic Meta-Learning and Prototypical Network algorithms on CIFAR-FS, FC100, miniImageNet and tieredImageNet one-shot learning benchmarks. During these experiments Multi-Task Modification has demonstrated improvement over original methods. SPSA-Tracking algorithm first adapted in this paper for multi-task weight optimization shows the largest accuracy boost that is competitive to the state-of-the-art meta-learning methods. Our code is available online * .</p><p>A PREPRINT we have adapted SPSA for Tracking method for multi-task weight optimization in the one-shot meta-learning setting for the first time. To summarize, our main contributions are as follows:</p><p>? We have successfully applied our MTM to the important optimization-based method MAML [1] and the important metric-based method Prototypical Networks <ref type="bibr" target="#b4">[5]</ref>. For both of these methods, the proposed approach achieved an improvement over the original method on the main one-shot learning benchmarks.</p><p>? We have formulated novel SPSA for Tracking method (SPSA-Track) as a multi-task weights optimizer in MTM for meta-learning. This method demonstrated a significant performance boosting on average.</p><p>? We have shown robustness of SPSA-based approaches on multiple benchmarks. Also, they appear to outperform gradient-based methods as a multi-task weights optimizer.</p><p>? We have demonstrated that combining MTM SPSA-Track with modern backbone model achieves a significant boost in accuracy. Such a performance improvement puts our result among the best in the field.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the most important hallmarks of human intelligence is the ability to learn new concepts from few examples. However, despite significant progress in deep learning in a variety of fields in recent years limitations of deep learning approaches still exist in many practical applications where labeled data is intrinsically rare or expensive. For machine learning systems it is difficult to adapt to new concepts fast with very little supervision. That raises attention to the challenging few-shot learning problem and its most difficult setting: one-shot learning. One of the promising areas of research in recent years was to tackle this problem using the approach of meta-learning or "learning to learn" <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Meta-learning process is performed on a family of tasks set on disjoint meta-training and meta-testing sets. Each task includes only a limited amount of training data which requires fast adaptability of the meta-learner. The performance of the meta-learner is evaluated on meta-testing tasks. In this paper we have modified Multi-Task Modification (MTM) approach for meta-learning from <ref type="bibr" target="#b3">[4]</ref> where this algorithm was investigated on Omniglot dataset, which is a less challenging setting than those discussed in this paper. Multi-task weight optimization plays the key role in MTM. Therefore, we consider members of the family of Simultaneous Perturbation Stochastic Approximation (SPSA) methods as optimizers of multi-task weights and compare them with a gradient-based approach. To the best of our knowledge, 2 Related Works Few-Shot Learning. Meta-learning attempts to acquire general knowledge of a target domain by learning many tasks that lie within it <ref type="bibr" target="#b0">[1]</ref>. Few-shot learning is widely used as one of the main benchmarks for meta-learning approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7]</ref>. In one-shot learning setting, training set consists of only one sample per task. It is expected that by training on similar yet different tasks the model will learn common features that will still be relevant to unseen tasks and, thus, acquire general understanding of the field. Few-shot learning models are typically divided into 2 broad categories based on how the problem is modelled: optimization-based and metric-based.</p><p>The class of optimization-based few-shot learning algorithms uses explicit optimization for fast adaptation to new tasks. Model-Agnostic Meta-Learning (MAML) <ref type="bibr" target="#b0">[1]</ref> attempted to find network weights that are able to quickly adapt to new tasks through an optimization procedure. Many refinements to the MAML algorithm have been proposed since its inception, for instance, Latent Embedding Optimization (LEO) <ref type="bibr" target="#b2">[3]</ref> tried to improve generalization by learning in a lower-dimensional latent embedding space. R2D2 <ref type="bibr" target="#b7">[8]</ref> and MetaOptNet <ref type="bibr" target="#b1">[2]</ref> improved accuracy by using Ridge Regression and SVM as classifiers.</p><p>Metric-based approaches are a class of methods for few-shot learning problems that aim to learn a discriminative embedding transferable to a target task. Metric learning has a long history of research and various applications <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Siamese Convolutional Neural Networks <ref type="bibr" target="#b10">[11]</ref> was the first metric-based method for one-shot image classification that learned task-agnostic feature embeddings. Matching Networks <ref type="bibr" target="#b5">[6]</ref> enhanced this approach by using two different networks and episodic training. Prototypical Networks (ProtoNet) <ref type="bibr" target="#b4">[5]</ref> introduced the idea of learning class representation by using mean features embeddings. Two other important approaches were suggested in Relational Networks <ref type="bibr" target="#b11">[12]</ref> that introduced an architecture to model the similarity of embeddings and Task Dependent Adaptive Metric (TADAM) <ref type="bibr" target="#b12">[13]</ref> that switched to learning task-specific features. Recently, Deep Subspace Networks (DSN) <ref type="bibr" target="#b13">[14]</ref> have been used to predict class labels by calculating the distance between a query point and its projections onto discriminative subspaces formed by the support sets for each class.</p><p>Transductive inference as an approach to the few-shot learning problem was the subject of research in several recent papers <ref type="bibr" target="#b14">[15]</ref>. In this setting a classifier model performs class predictions jointly for all the unlabeled query examples of a single few-shot learning task, instead of making predictions for one sample at a time as in inductive methods. As an alternative to the meta-learning framework, several recent studies investigated performance of standard end-to-end pre-trained classifiers on few-shot tasks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15]</ref>. However, in this work we focus on improving the meta-training phase and consider only the inductive methods that use meta-learning pipeline without pre-training. For this reason, we selected MAML and ProtoNet because papers describing these methods are among the most cited in the field. Thus, we expect that results of applying multi-task meta-learning modification should generalize when used for the modification of other one-shot learning algorithms following these restrictions.</p><p>Multi-Task Learning. Multi-task learning is an approach where a model is trained to give predictions for multiple tasks jointly. Multi-tasking can be considered as an implicit data augmentation and regularization technique assisting the network to focus on more relevant features in the input <ref type="bibr" target="#b16">[17]</ref>. However, proper task weighting might be crucial to improving model performance. Kendall et al. <ref type="bibr" target="#b17">[18]</ref> have introduced a multi-task loss function that relies on maximizing the Gaussian likelihood with task-dependent uncertainty. The proposed single model has outperformed separate models for each task. In <ref type="bibr" target="#b3">[4]</ref>, multi-tasking approach has been applied for a few-shot character recognition problem, which resulted in an improvement over the baseline model. A close connection in terms of optimization task formulation between multi-task learning and gradient-based meta-learning was established in <ref type="bibr" target="#b18">[19]</ref>.</p><p>Stochastic Approximation. The stochastic approximation algorithm was developed for solving the optimization problem by Kiefer and Wolfowitz <ref type="bibr" target="#b19">[20]</ref>. Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm <ref type="bibr" target="#b20">[21]</ref> uses only two observations at each iteration which recursively generates estimates along random directions. In highdimensional setting SPSA has the same order of convergence rate as Kiefer-Wolfowitz approach while requiring significantly fewer measurements of a function. When an unknown but bounded disturbance corrupts the observed data, the quality of methods based on stochastic gradient decreases. However, SPSA-like algorithms demonstrate a high level of resistance to such disturbances <ref type="bibr" target="#b21">[22]</ref>. Stochastic approximation algorithms are successfully used in various machine learning problems <ref type="bibr" target="#b21">[22]</ref>.</p><p>3 Multi-Task Meta-Learning Modification</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">One-Shot Learning Problem Definition</head><p>We consider one-shot learning problem as a special case of few-shot learning. According to the few-shot learning problem formulation, we need to train a classifier that can quickly adapt to new unseen classes using only few labeled examples of classes. To cast this problem as meta-learning problem, Vinyals et al. <ref type="bibr" target="#b5">[6]</ref> proposed the pipeline where elements of each class were randomly divided into support set and query set. A set of classes in training phase does not overlap with a set of classes in testing phase. In the meta-learning pipeline, training and testing processes consist of a series of episodes where each episode ? t includes one or more tasks and each task t i consists of support and query sets for several classes.</p><p>Consider labeled dataset {(x 1 , y 1 ), . . . , (x CN , y CN )} with C classes and N examples per each class, where x i ? R d is the input data vector and y i ? {1, . . . , C} is the class label. Let N S be the number of examples in the support set for each class, N Q be the number of examples in the query set, N S + N Q = N , N C ? C be the number of classes in a task that are randomly selected from all set of C classes. Corresponding few-shot learning classification procedure is called N S -shot N C -way classification. In one-shot setting, N S = 1.</p><formula xml:id="formula_0">Let episode ? t : (t 1 , . . . , t M ) consist of M tasks. Each task t i involves per task support set S ti = {(x i , y i )} N S N C i=1 and query set Q ti = {(x i , y i )} N Q N C i=1</formula><p>containing elements of all N C classes included in the task t i of per class support and query sets, respectively. Parameter M ? N can vary in different algorithms. For each task t i in an episode, L ?,ti (D) denotes the value of loss function on D, where ? is the vector of learnable parameters of model ? ? (convolutional neural network in this paper) and D can be support, query, or another dataset.</p><p>Model-Agnostic Meta-Learning. MAML uses loss function which is a two-step training procedure: 1) adaptation step (inner-loop) where network parameters are adapted to the specific task t i :</p><formula xml:id="formula_1">? i = ? ? ?? ? L ?,ti (S ti );<label>(1)</label></formula><p>2) meta-gradient update (outer-loop) step where parameters ? are updated by backpropagating through the adaptation procedure:</p><formula xml:id="formula_2">? ? ? ? ?? ? M i=1 L ? i ,ti (Q ti ),<label>(2)</label></formula><p>where ?, ? are adaptation and meta-learning rates correspondingly.</p><p>Prototypical Networks. In ProtoNet algorithm model ? ? computes embedding of input data. Each class k is represented by prototype vector c k ti which is calculated as a mean vector of the corresponding support set. The Prototypical Networks model ? ? is trained via stochastic gradient descent (SGD) by minimizing loss function for train task t i , where d(?, ?) is Euclidean distance function:</p><formula xml:id="formula_3">L ?,ti (Q ti ) = 1 N C N C k=1 1 N Q xj ?Q k t i ? log exp(?d(? ? (x), c k ti )) k exp(?d(? ? (x), c k ti ))</formula><p>.</p><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Task Meta-Learning Loss Function</head><p>Many few-shot learning methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2]</ref> work with M = 1 (i.e. with one task in a training episode ? t ). Other approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref> utilize a batch of tasks with M &gt; 1 per episode but consider the contribution of each task equally. In this paper we develop the idea of adaptive use of information from multiple tasks simultaneously via the multi-task approach for meta-learning proposed in <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1:</head><p>Multi-task meta-learning modification pipeline for a training episode ? t described in Algorithm 1.</p><p>Multi-task learning methods for deep neural networks can be divided into two main areas: soft and hard parameter sharing of hidden layers of neural network <ref type="bibr" target="#b16">[17]</ref>. In accordance with <ref type="bibr" target="#b3">[4]</ref>, we use hard parameter sharing for all hidden layers of a convolutional network. Thus, one neural network is used for all tasks, and the presence of several tasks is reflected only in the loss function. For this purpose <ref type="bibr" target="#b3">[4]</ref> proposed an adapted approach discussed in <ref type="bibr" target="#b17">[18]</ref> which used task-depended (homoscedastic) uncertainty as a basis for weighting losses in a multi-task learning problem.</p><p>Corresponding multi-task meta-learning loss function takes the following form:</p><formula xml:id="formula_4">L M T ?t (? t , {Q ti } M i=1 ) = M i=1 1 (? (i) t ) 2 L ?,ti (Q ti ) + M i=1 log(? (i) t ) 2 ,<label>(4)</label></formula><p>where weights ? t = (?</p><formula xml:id="formula_5">(1) t , . . . , ? (M ) t</formula><p>) are hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Task Weights Optimization</head><p>As shown in <ref type="bibr" target="#b17">[18]</ref>, model performance is extremely sensitive to multi-task weights selection and tuning of ? t is critical for the success of a multi-task learning. On the other hand, searching for these optimal weights is expensive and increasingly difficult for a large model with numerous tasks.</p><p>In this work we focus on developing and investigating performance of optimization methods for hyperparameters ? t in the loss function <ref type="bibr" target="#b3">(4)</ref>. Nowadays gradient optimization approach is a natural choice in many deep learning algorithms <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b17">18]</ref>. Therefore, we consider an approach with embedding weights optimization in the backpropagation procedure similar to the one described in <ref type="bibr" target="#b17">[18]</ref>. This procedure is described in Algorithm 1. Let ? t and ? t denote estimates at iteration t of multi-task weights and model parameters, respectively. This multi-task meta-learning modification pipeline for a training episode ? t is visualized in <ref type="figure">Figure 1</ref>.</p><p>In this paper gradient-based and stochastic approximation methods are considered and compared as the multi-task weights optimizer.</p><p>When solving a meta-learning problem, algorithms face model uncertainties associated with a critical issue of limited amount of training samples per task, which may result in overfitting <ref type="bibr" target="#b24">[25]</ref>. This phenomenon is especially pronounced in the extreme case of one-shot learning. Therefore, meta-learning algorithms must be able to effectively adapt to these uncertainties <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. We approach this issue from optimization point and propose to use methods from the family of Simultaneous Perturbation Stochastic Approximation (SPSA) algorithms as a multi-task weights optimizer due to their robustness and successful application in various machine learning and control problems with uncertainties <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Gradient-Based Methods. As stated earlier, one of the proposed gradient-based approaches is to incorporate multitask weights in a computational graph of meta-learning algorithm (MAML and ProtoNet in this work). In that case Algorithm 1 Training for episode ? t : (t 1 , . . . , t M )</p><formula xml:id="formula_6">Input: N S , N Q , N C , ? t?1 , ? t?1 Output: Updated parameters estimates ? t , ? t 1: for i in {1, . . . , M } do 2:</formula><p>Sample N C random classes <ref type="bibr">3:</ref> Sample random elements in S ti and Q ti 4:</p><p>Compute task loss function L ?,ti (Q ti ) 5: end for</p><formula xml:id="formula_7">6: Compute L M T ?t ( ? t?1 , {Q ti } M i=1</formula><p>) via (4) 7: Use multi-task weights optimizer to update ? t 8: Update parameters ? t via SGD by L M T ?t backpropagation procedure is used to compute weights updates and a gradient-based optimizer is shared with the whole model. This multi-task modification of meta-learning is denoted by MTM Backprop. We also explored first-order optimization approaches as a separate multi-task weights optimizer described in Algorithm 1. Adam optimization method <ref type="bibr" target="#b29">[30]</ref> was selected based on the results of experiments and due to multiple mentions in the deep learning literature. This approach is named MTM Inner First-Order by analogy with inner loop optimization step in MAML (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">SPSA for Tracking method</head><p>When solving a meta-learning problem, algorithms face model uncertainties associated with a critical issue of limited amount of training samples per task, which may result in overfitting <ref type="bibr" target="#b24">[25]</ref>. This phenomenon is especially pronounced in the extreme case of one-shot learning. Therefore, meta-learning algorithms must be able to effectively adapt to these uncertainties <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. We approach this issue from optimization point and propose to use methods from the family of SPSA algorithms as a multi-task weights optimizer due to their robustness and successful application in various machine learning and control problems with uncertainties <ref type="bibr" target="#b21">[22]</ref>. In this subsection, we propose the SPSA for Tracking algorithm firstly adapted for multi-task weights optimization.</p><p>In order to cast the SPSA-based approach as a multi-task weights optimizer, we reformulate the problem of finding estimates of hyperparameters ? t in the multi-task loss function (4) as a non-stationary optimization problem suitable for a stochastic approximation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b3">4]</ref>. In order to do this, we introduce an observation model for the training episode ? t that takes into account uncertainties arising in meta-learning:</p><formula xml:id="formula_8">L t (? t ) = L M T ?t (? t , {Q ti } M i=1 ) + ? t ,<label>(5)</label></formula><p>where ? t is an additive external noise caused by uncertainties arising from the limited amount of training samples per task. Therefore, it is necessary to find an algorithm that produces an estimate ? t of an unknown vector ? t that minimizes mean-risk functional of objective functions (5) based on observations L 1 , L 2 , . . . , L t from training episodes ? 1 , ? 2 , . . . , ? t .</p><p>To the best of our knowledge, we propose to use SPSA for Tracking approach [31] as a multi-task weights optimizer for the first time. This algorithm simultaneously uses observation from the current and previous iterations which allows it to be more stable under parameters drift conditions. Let ? t ? R d be a vector consisting of independent random variables with Bernoulli distribution, ? 0 a vector with the initial values, and {? t } and {? t } sequences of positive numbers. Then SPSA for tracking multi-task weights optimizer (MTM SPSA-Track) constructs the following estimates of multi-task weights:</p><formula xml:id="formula_9">? ? ? ? ? ? ? ? ? ? ? ? ? L 2t = L 2t ( ? 2t?2 + ? n ? t ), L 2t?1 = L 2t?1 ( ? 2t?2 ? ? t ? t ) ? 2t?1 = ? 2t?2 ? 2t = ? 2t?1 ? ? t ? t L2t?L2t?1 2?t .<label>(6)</label></formula><p>Algorithm <ref type="formula" target="#formula_9">(6)</ref> has theoretical convergence guarantees expressed in terms of an upper bound of residuals between the estimates and the theoretical optimal solution <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Multi-Task Modification for Class-Based Weights</head><p>In Algorithm 1, N C classes in task t i is sampled randomly. However, if additional meta-information about classes is available, the data can be sampled from some coarse classes. For this, we define the modified multi-task loss function (4) applicable to coarse classes as well as to usual finer-grained classes. A per-task loss function L ?,ti can be rewritten as a sum of terms corresponding to classes:</p><formula xml:id="formula_10">L ?,ti (Q ti ) = C k=1 L ?,ti,k (Q ti ),</formula><p>where k is a class, k ? {1, . . . , K}, K is the number of coarse classes (or K = C), L ?,ti,k (Q ti ) is the algorithmspecific partial loss computed for the corresponding class which is equal to zero if the class k is not present in the task t i . Then we modify multi-task loss function (4) by introducing weights ? (k) C for each class and combining like terms from M training tasks:</p><formula xml:id="formula_11">L M T ?t (? t , {Q ti } M i=1 ) = C k=1 1 (? (k) C ) 2 M i=1 L ?,ti,k (Q ti ) + log(? (k) C ) 2 .<label>(7)</label></formula><p>SPSA-based multi-task weights optimizer with loss function <ref type="formula" target="#formula_11">(7)</ref> is denoted as MTM SPSA-Coarse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conducted experiments on four datasets -CIFAR-FS <ref type="bibr" target="#b7">[8]</ref>, FC100 <ref type="bibr" target="#b12">[13]</ref>, miniImageNet <ref type="bibr" target="#b5">[6]</ref> and tieredImageNet <ref type="bibr" target="#b31">[32]</ref>. These datasets were selected as they became standard benchmarks over the last couple of years <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b6">7]</ref>. Experiments were performed on Rolos platform * with a A100 vGPU, 48 vCPUs at 2.30GHz, 128GiB RAM. <ref type="table" target="#tab_0">Tables 1, 2 and 3</ref> summarize results of these experiments.</p><p>The CIFAR-FS and FC100 datasets are derived from CIFAR-100 dataset having 100 classes with each class consisting of 600 images of size 32 ? 32. In the CIFAR-FS dataset classes are randomly divided into groups of 64, 16 and 20 for training, validation, and testing, respectively, while in the FC100 datasets 20 superclasses of CIFAR-100 are split into groups of 12, 4 and 4. The miniImageNet and tieredImageNet datasets are derived from ILSVRC-2012 dataset having 1000 categories. The miniImageNet dataset consists of 100 classes randomly chosen from ILSVRC-2012 split into groups of 64, 16 and 20 as proposed in <ref type="bibr" target="#b32">[33]</ref> which is a standard convention. The tieredImageNet dataset consists of 608 categories from ILSVRC-2012 grouped into 34 supercategories that are then split into groups of 20, 6 and 8. In both cases 600 images of 84 ? 84 pixels in size are sampled for each class. Adam was used as the meta-optimizer with learning rate ? = 10 ?3 and adaptation step size ? = 0.01 as in <ref type="bibr" target="#b1">(2)</ref>. We used 5 adaptation steps during training and 10 during testing for all datasets. We selected meta-batch size of M = 4 and trained the model for 300 epochs, each containing 100 tasks, unless specified otherwise. Following <ref type="bibr" target="#b32">[33]</ref> and the original paper, 15 samples per class were taken for evaluation. For the MTM the baseline model was trained for extra 40 epochs.</p><p>In the case of ProtoNet we used 64-64-64-64 feature extraction backbone ? ? as suggested in the original paper <ref type="bibr" target="#b4">[5]</ref>. We followed the standard practice <ref type="bibr" target="#b33">[34]</ref> for the meta-learning setup using SGD with Nesterov momentum of 0.9 and weight decay of 0.0005 as an optimizer. The learning rate was initially set to 0.1 and then decreased according to the strategy from <ref type="bibr" target="#b33">[34]</ref>. During meta-training, we used the first 20 epochs for pre-training the model by using the original ProtoNet method and implementing the Multi-Task Modifications only for the last 40 epochs.</p><p>We designed several experiment settings to research the relative advantage of using the multi-task loss function (4) and SPSA-based optimization over original methods <ref type="table" target="#tab_0">(Table 1</ref>) and over gradient-based method ( <ref type="table" target="#tab_1">Table 2)</ref> where multi-task weights in the loss function are optimized jointly with the network parameters ?. In experiments we mainly used M = 4 tasks per training episode ? t (other values of M are indicated explicitly) and the best model for testing was selected on the validation set. For SPSA and SPSA-Track we set ? n = 0.25/n 1 6 , ? n = 15/n 1 24 in (6) as per the theoretical result from <ref type="bibr" target="#b21">[22]</ref>. During the experiments we found that L 2 -normalization of multi-task weights in MTM SPSA and MTM SPSA-Track improves the stability of training. Results of all experiments are formulated in terms of average few-shot classification accuracy after 1000 testing iterations. We used the miniImageNet dataset to compare our method with the prior work. Since the majority of recent approaches use more advanced convolutional neural networks with higher embedding dimension such as residual networks (ResNet) <ref type="bibr" target="#b23">[24]</ref> as feature extraction backbones, we implemented the original ProtoNet with ResNet-12 backbone provided in <ref type="bibr" target="#b1">[2]</ref> to compare against the results of other methods with backbones from the ResNet family. We did not include approaches that were developed for semi-supervised and transductive learning settings in this comparison since such approaches use the statistics of query examples or statistics across the one-shot tasks. We also excluded methods that use non-episodic pre-training as mentioned in Section 2. <ref type="table" target="#tab_2">Table 3</ref> shows that, when used with a comparable backbone, our multi-task meta-learning modification of ProtoNet with stochastic approximation increases one-shot classification accuracy to the level of significantly more advanced methods and is competitive against state-of-the-art meta-learning approaches. These results are presented with 95 % confidence intervals.</p><p>5 Ablation Study 5.1 Improvements of MTM with SPSA and SPSA-Track.</p><p>We have conducted experiments for four datasets most widely used in the field of one-shot learning as shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>On CIFAR-FS, we have improved on original methods up to 2.0%, with the largest improvement by 2-way MAML MTM SPSA-Track. On FC100 benchmark, the largest improvement of 1.9% has been achieved with the novel MTM SPSA-Track method on MAML in 5-way scenario. On tieredImageNet, we got improvements up to 1.6% for MAML in 2-way scenario with SPSA. The top performing methods include MTM SPSA and MTM SPSA-Track for both MAML and ProtoNet. MTM SPSA-Track gives the greatest boost in 5-way settings. The last dataset we considered was miniImageNet, which is the most widely used benchmark for few-shot learning. Here we have achieved significant improvements up to 2.6% with MAML MTM SPSA-Track leading and MAML MTM SPSA following. Similar results are observed for ProtoNet. The results presented in <ref type="table" target="#tab_0">Table 1</ref> show that proposed MTM with SPSA and SPSA-Track methods outperform the original approaches on all four benchmarks. Novel SPSA-Track demonstrates the largest improvement over the baseline in most cases.</p><p>The experiment results shown in <ref type="table" target="#tab_2">Table 3</ref> suggest that applying our method to ProtoNet with a more modern ResNet-12 backbone gives performance improvement of 3.72% for MTM SPSA, 4.81% for MTM SPSA-Track, making MTM SPSA-Track better performing in this scenario as well. Such a performance improvement puts our result among the best in the field. Applying MTM SPSA-Track to ProtoNet with number of tasks M = 2 gives a performance improvement  <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref> ResNet-18 49.61 ? 0.92 % Chen et al. <ref type="bibr" target="#b15">[16]</ref> ResNet-18 51.87 ? 0.77 % Relation Networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref> ResNet-18 52.48 ? 0.86 % Matching Networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref> ResNet-18 52.91 ? 0.88 % RAP-ProtoNet <ref type="bibr" target="#b34">[35]</ref> ResNet-10 53.64 ? 0.60 % ProtoNet (reproduced) <ref type="bibr" target="#b4">[5]</ref> ResNet-12 56.52 ? 0.45 % Gidaris et al. <ref type="bibr" target="#b33">[34]</ref> ResNet-15 55.45 ? 0.89 % SNAIL <ref type="bibr" target="#b22">[23]</ref> ResNet-15 55.71 ? 0.99 % Bauer et al. <ref type="bibr" target="#b35">[36]</ref> ResNet-34 56.30 ? 0.40 % AdaResNet <ref type="bibr" target="#b36">[37]</ref> ResNet-12 56.88 ? 0.62 % TADAM <ref type="bibr" target="#b12">[13]</ref> ResNet-12 58.50 ? 0.30 % Shot-Free <ref type="bibr" target="#b37">[38]</ref> ResNet-12 59.04 ? n/a % CAML <ref type="bibr" target="#b38">[39]</ref> ResNet-12 59.23 ? 0.99 % Wang et al. <ref type="bibr" target="#b18">[19]</ref> ResNet-12 59.84 ? 0.22 % MTL <ref type="bibr" target="#b39">[40]</ref> ResNet-12 61.20 ? 1.80 % vFSL <ref type="bibr" target="#b40">[41]</ref> ResNet-12 61.23 ? 0.26 % MetaOptNet <ref type="bibr" target="#b1">[2]</ref> ResNet-12 62.64 ? 0.61 % DSN <ref type="bibr" target="#b13">[14]</ref> ResNet 42% that is competitive against state-of-the-art methods. The fact that this improvement has been achieved by modifying the loss function only and the fact that MTM SPSA and MTM SPSA-Track can be applied to almost any of the meta-learning methods in the table makes our result even more significant. It is worth noting that in the case of meta-learning methods like MAML that originally use meta-batch with several tasks, our Multi-Task Modification requires practically no additional computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Gradient-Based vs SPSA-Based Optimization.</head><p>We explored the comparison between gradient-based and SPSA-based approaches of multi-task weight optimization. As can be seen from <ref type="table" target="#tab_1">Table 2</ref>, SPSA-based approaches give superior results in all experiments, so we can conclude that zero-order optimizers are better suited for the proposed multi-task modification in one-shot setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">SPSA Multi-Task Weights Behavior</head><p>In MTM SPSA and MTM SPSA-Track experiments multi-task weights are renormalized during the optimization procedure after each epoch. The corresponding dynamics of SPSA-Track multi-task weights during training in miniImageNet 1-shot 2-way experiment is depicted in <ref type="figure" target="#fig_0">Figure 2</ref> (a) for MAML and (b) for ProtoNet. Weights in ProtoNet experiment demonstrate larger fluctuation, yet in both cases there is a visible trend of multi-task weights splitting despite the difference in meta-learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Task Data Sampling</head><p>We used data sampling provided by Torchmeta library <ref type="bibr" target="#b41">[42]</ref> to supply a unified approach to the problem. It follows the usual procedure where classes are sampled from the set of candidates and then an appropriate number of examples are selected per class. This procedure is randomized for every task and is independent from the choice of algorithm or its multi-task modification.</p><p>In MTM SPSA-Coarse experiment setting the tasks are sampled in the same way, however, information about coarse classes is passed to the MTM SPSA algorithm. We have modified MTM SPSA, so that instead of having M = 4 multi-task weights (one per task), we use 20 (one per CIFAR-100 coarse class). Consequently, instead of weighting tasks, we now weight instances of the corresponding coarse classes. A tie between a particular coarse class and the corresponding weight is established via modified multi-task loss function <ref type="bibr" target="#b6">(7)</ref>. At the end of a training episode the weights are updated only for the coarse classes that are present in the episode's meta-batch.</p><p>As can be seen from <ref type="figure" target="#fig_1">Figure 3</ref> (a), in case of MAML the coarse classes weights ? (k) C dynamic during training varies significantly between different classes, which contrasts with ProtoNet, where weights follow the general dynamics established in <ref type="bibr" target="#b3">[4]</ref> as shown in <ref type="figure" target="#fig_1">Figure 3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we have developed a new multi-task meta-learning modification with stochastic approximation for one-shot learning. The application of this approach to optimization-based method MAML and metric-based method ProtoNet was investigated on four most widely used one-shot learning benchmarks. In all experiments our algorithm showed significant improvements over baseline methods. In addition, in most cases SPSA-based approach was better than gradient method. We presented novel SPSA for Tracking algorithm as a multi-task weights optimizer which has demonstrated the largest performance boost on average. For future work, we aim to apply the described approach to state-of-the-art few-shot learning algorithms and to reinforcement learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>MTM SPSA-Track multi-task weights dynamic during training with weights normalization on miniImageNet 1-shot 2-way: (a) MAML, (b) ProtoNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>MTM SPSA-Coarse class weights dynamic during training on CIFAR-FS 1-shot 5-way: (a) MAML, (b) ProtoNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Multi-Task Modification results on CIFAR-FS, FC100, miniImageNet and tieredImageNet (1-shot setting). Improvements are shown in bold.As mentioned in Section 2, the experiments with Multi-Task Modification were performed using MAML and Prototypical Networks. For MAML algorithm we used the neural network ? ? defined in the original paper<ref type="bibr" target="#b0">[1]</ref> with 32-32-32-32 configuration where a-b-c-d denotes a 4-layer convolutional neural network with a, b, c, d filters in convolutional layers.</figDesc><table><row><cell></cell><cell cols="2">CIFAR-FS</cell><cell cols="2">FC100</cell><cell cols="2">miniImageNet</cell><cell cols="2">tieredImageNet</cell></row><row><cell>Configuration</cell><cell>2-way</cell><cell>5-way</cell><cell>2-way</cell><cell>5-way</cell><cell>2-way</cell><cell>5-way</cell><cell>2-way</cell><cell>5-way</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">MAML</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Reproduced</cell><cell cols="8">74.8 % 54.5 % 66.0 % 36.4 % 73.2 % 45.9 % 73.3 % 47.9 %</cell></row><row><cell>MTM SPSA</cell><cell cols="8">76.4 % 54.7 % 67.0 % 36.4 % 74.9 % 48.0 % 74.9 % 47.8 %</cell></row><row><cell cols="9">MTM SPSA-Track 76.8 % 54.6 % 66.8 % 38.3 % 75.8 % 46.5 % 73.8 % 48.3 %</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ProtoNet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Reproduced</cell><cell cols="8">77.8 % 58.9 % 65.0 % 35.7 % 74.2 % 50.0 % 72.9 % 49.4 %</cell></row><row><cell>MTM SPSA</cell><cell cols="8">79.1 % 59.7 % 65.1 % 36.0 % 74.7 % 50.2 % 73.6 % 49.5 %</cell></row><row><cell cols="9">MTM SPSA-Track 78.2 % 59.8 % 65.3 % 36.1 % 74.8 % 50.8 % 73.1 % 50.0 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of gradient-based (Backprop), SPSA and SPSA for Tracking multi-task weights optimizers on 1-shot, 5-way experiments.</figDesc><table><row><cell>Algorithm</cell><cell>CIFAR-FS</cell><cell>FC100</cell><cell>miniImageNet</cell><cell>tieredImageNet</cell></row><row><cell></cell><cell></cell><cell>MAML</cell><cell></cell><cell></cell></row><row><cell>Backprop</cell><cell>53.1 %</cell><cell>37.6 %</cell><cell>47.4 %</cell><cell>46.7 %</cell></row><row><cell>SPSA</cell><cell>54.7 %</cell><cell>36.4 %</cell><cell>48.0 %</cell><cell>47.8 %</cell></row><row><cell>SPSA-Track</cell><cell>54.6 %</cell><cell>38.3 %</cell><cell>46.5 %</cell><cell>48.3 %</cell></row><row><cell></cell><cell></cell><cell>ProtoNet</cell><cell></cell><cell></cell></row><row><cell>Backprop</cell><cell>59.4 %</cell><cell>35.5 %</cell><cell>50.4 %</cell><cell>49.2 %</cell></row><row><cell>SPSA</cell><cell>59.7 %</cell><cell>36.0 %</cell><cell>50.2 %</cell><cell>49.5 %</cell></row><row><cell>SPSA-Track</cell><cell>59.8 %</cell><cell>36.1 %</cell><cell>50.8 %</cell><cell>50.0 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison to prior work on miniImageNet meta-test split. Bold values are the accuracy no less than 1 % compared with the highest one.</figDesc><table><row><cell>Algorithm</cell><cell>Backbone</cell><cell>1-shot 5-way</cell></row><row><cell>MAML</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* https://rolos.com/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Sygnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simultaneous perturbation stochastic approximation for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Boiarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Granichin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Granichina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 European Control Conference (ECC)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="350" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: A good embedding is all you need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="266" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large scale landmark recognition via deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Boiarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Tyantov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;19: Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A metric learning reality check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="681" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TADAM: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodr?guez L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive subspaces for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guneet</forename><surname>Singh Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
	</analytic>
	<monogr>
		<title level="m">deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bridging multi-task learning and meta-learning: Towards efficient training and effective adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10991" to="11002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stochastic estimation of the maximum of a regression function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wolfowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="462" to="466" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multivariate stochastic approximation using a simultaneous perturbation gradient approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">C</forename><surname>Spall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="332" to="341" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Randomized Algorithms in Automatic Control and Data Mining, volume 67 of Intelligent Systems Reference Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Granichin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeev</forename><surname>Volkovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dvora</forename><surname>Toledano-Kitai</surname></persName>
		</author>
		<idno>978-3-642-54785-0</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Uncertainty in model-agnostic meta-learning using variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Toan</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probabilistic model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A review of uncertainty quantification in deep learning: Techniques, applications and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moloud</forename><surname>Abdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farhad</forename><surname>Pourpanah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadiq</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Rezazadegan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">Rajendra</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Makarenkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="243" to="297" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">On using simultaneous perturbation stochastic approximation for learning to rank, and the empirical optimality of LambdaRank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burges</surname></persName>
		</author>
		<idno>MST-TR-2007-115</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Microsoft Research</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stochastic approximation algorithm with randomization at the input for unsupervised parameters estimation of gaussian mixture model with sparse parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Boiarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">N</forename><surname>Granichin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autom. and Remote Cont</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1403" to="1418" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simultaneous perturbation stochastic approximation for tracking under unknown but bounded disturbances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Granichin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Amelina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1653" to="1658" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reinforced attention for few-shot learning and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="913" to="923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Discriminative k-shot learning using probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateo</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Bart?omiej?wi?tkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00326</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rapid adaptation with conditionally shifted neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="3664" to="3673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Few-shot learning with embedded class models and shot-free meta training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to learn with conditional class dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farshid</forename><surname>Varno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Matwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
	<note>Tat-Seng Chua, and Bernt Schiele</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Variational few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Torchmeta: A metalearning library for pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>W?rfl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandana</forename><surname>Samiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">Paul</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06576</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
