<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object Pose Estimation using Mid-level Visual Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Nejatishahidin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooya</forename><surname>Fayyazsanavi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Ko?ecka</surname></persName>
						</author>
						<title level="a" type="main">Object Pose Estimation using Mid-level Visual Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work proposes a novel pose estimation model for object categories that can be effectively transferred to previously unseen environments. The deep convolutional network models (CNN) for pose estimation are typically trained and evaluated on datasets specifically curated for object detection, pose estimation, or 3D reconstruction, which requires large amounts of training data. In this work, we propose a model for pose estimation that can be trained with small amount of data and is built on the top of generic mid-level representations [33] (e.g. surface normal estimation and re-shading). These representations are trained on a large dataset without requiring pose and object annotations. Later on, the predictions are refined with a small CNN neural network that exploits object masks and silhouette retrieval. The presented approach achieves superior performance on the Pix3D dataset [26] and shows nearly 35% improvement over the existing models when only 25% of the training data is available. We show that the approach is favorable when it comes to generalization and transfer to novel environments. Towards this end, we introduce a new pose estimation benchmark for commonly encountered furniture categories on challenging Active Vision Dataset <ref type="bibr" target="#b0">[1]</ref> and evaluated the models trained on the Pix3D dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Detecting objects and their 3D poses are an integral part of spatial 3D perception relevant to semantic simultaneous localization and mapping approaches <ref type="bibr" target="#b21">[22]</ref> and target-driven navigation. The state of the art deep learning approaches have marked notable advancements by training pose estimation models on large datasets with standard ResNet <ref type="bibr" target="#b5">[6]</ref> backbone, requiring a large amount of training data and costly pose annotations. The resulting models did not generalize well to the the same instance in different environments.</p><p>The proposed approach uses RGB images only and estimates the pose from a single view. The premise of the approach is to develop a method that can benefit from the availability of 3D CAD models, can be seamlessly integrated with the state-of-the-art object detection models, and will generalize well to novel environments. Instead of training the entire model end-to-end from pixels to pose predictions, we train a lightweight convolutional neural network on the top of generic mid-level representation features (e.g. normal estimation and re-shading features) that have been pre-trained in indoor environments. The initial predictions of the model are further refined using object masks and silhouette retrieval. The appeal of using mid-level representations features <ref type="bibr" target="#b22">[23]</ref> for this task is their re-usability and effectiveness for training visuomotor policies for exploration, navigation to target, as well as local planning and ability to transfer well to novel These authors contributed equally to this work. George Mason University, e-mail:nnejatis,pfayyazs,kosecka@gmu.edu <ref type="figure">Fig. 1</ref>: Active Vision Dataset <ref type="bibr" target="#b0">[1]</ref> pose estimation benchmark challenges; many objects are highly occluded (e.g. the first two rows), large lighting variations, glassy objects (the third row), and truncated viewpoints (last row). These challenges affect both computation of mid-level representations and pose estimation. environments. In summary, the contributions of the proposed approach are summarized as follows:</p><p>? Novel object pose estimation model build on the top of generic mid-level representation feature maps <ref type="bibr" target="#b32">[33]</ref> (surface normals, and re-shading feature maps) that have been shown to be effective for other perceptual tasks. ? Effective refinement stage aided by object detection masks and silhouette retrieval achieving superior performance on the state-of-the-art Pix3D dataset <ref type="bibr" target="#b25">[26]</ref>. ? Competitive performance in low training data regime achieving 35% improvement over the existing models when there's only 25% of training data is available. ? New pose estimation benchmark for commonly encountered furniture categories on Active Vision Dataset <ref type="bibr" target="#b0">[1]</ref> and zero-shot pose estimation baseline for these realworld scans of indoor environments captured by the robot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>There is a large body of work on 3D object pose estimation. The existing techniques vary depending on the sensing modality, focus on the object instances or categories, and availability of 3D models. With the success of deep convolutional neural networks (DCNN) for object recognition and detection, many works focused on estimating the pose (azimuth and elevation or full 6D pose) of object categories, separately or jointly with the object detection DCNNs by adding another branch to the network. In <ref type="bibr" target="#b15">[16]</ref> a 3D pose regressor is learned for each object category, while in <ref type="bibr" target="#b16">[17]</ref> a discrete-continuous formulation for the pose prediction is introduced, with the input being the cropped object bounding boxes. Authors in <ref type="bibr" target="#b31">[32]</ref> decouple the pose estimation task into multiple components such as predicting pixel-wise object labels, estimating the object's center and distance from the camera to recover the translation, and estimating the rotation, while <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b10">[11]</ref> both extend the SSD <ref type="bibr" target="#b14">[15]</ref> object detector to predict azimuth and elevation or the 6-DoF pose respectively. These methods learn the pose estimation and recognition directly from image pixels, and require a large number of training examples with pose annotations that are challenging to obtain for many categories. This is the case also for keypoint-based methods, which typically work better in the presence of occlusions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b9">10]</ref>. Notable progress has been made in pose estimation for object instances from images, where 3D textured instance models were available during the training stage <ref type="bibr" target="#b8">[9]</ref>. Authors in <ref type="bibr" target="#b11">[12]</ref> exploit non-textured models where given the predicted pose and shape, the object is rendered and compared to 2D instance segmentation and trained end-to-end on the small number of categories. These approaches require 3D pose annotations in images during training <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b7">8]</ref>. Approaches that resort to keypoint based representations and use CAD models require annotations of 3D keypoints on textured CAD models. Authors in <ref type="bibr" target="#b13">[14]</ref> generate a synthetic dataset provides additional supervision during training. They learn to predict the 2D image locations of the projected vertices or projections of object's 3D bounding box <ref type="bibr" target="#b26">[27]</ref> before a PnP algorithm estimates the pose.</p><p>At last, there are approaches that use point clouds or depth data to tackle the pose estimation problem. Examples of these include methods that exploit effective 3D shape representations of 3D objects <ref type="bibr" target="#b27">[28]</ref>, methods that learn how to align the sparse point clouds with the CAD models <ref type="bibr" target="#b1">[2]</ref> or deep voting based methods <ref type="bibr" target="#b24">[25]</ref> that use point clouds both in training and testing. These works utilize repositories of 3D shape models <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b2">3]</ref> and/or video or 3D datasets with pose annotations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Our work is most closely related to the approach proposed in <ref type="bibr" target="#b25">[26]</ref>, where 2.5D sketches are learned as intermediate representations for 3D reconstruction and pose prediction. There the authors introduced a new benchmark for pose estimation with instance (instead of category) level annotations and adopted the 2.5D sketch representation <ref type="bibr" target="#b28">[29]</ref> as intermediate object representations for prediction of 3D structure and object pose. We take a departure from this training paradigm and show that effective pose estimation can be built on the top of feature maps that are part of generic perceptual skill set, also referred to as mid-level representations. The midlevel representations <ref type="bibr" target="#b22">[23]</ref> have been shown in previous works as useful priors for learning visuomotor policies. This work demonstrates their effectiveness for pose estimation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head><p>In this work, we present a novel pose estimation approach using RGB images only. Our work is inspired by approaches that learn 2.5D sketches comprised of surface normal, depth map, and object silhouette as intermediate representations for 3D reconstruction and pose estimation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26]</ref>. Pix3D dataset comprised of several furniture categories, with instance-level pose and keypoint annotations along with depth, normal, and silhouette that are required for supervision of training the intermediate representations.</p><p>The effectiveness and often the superior performance of the models trained in an end-to-end or on a single dataset usually comes with the generalization and domain adaptation challenges when applied in novel environments. The fundamental question when it comes to building computer vision systems for robot perception is whether the existence of perceptual priors or representations learned through a set of proxy tasks (e.g. depth estimation, edge detection) can be useful for different downstream tasks. Since a robot's visual perception requires tackling multiple tasks, the ability to share the representations/features that pertain to a particular class of environments is appealing. In this work, we propose to exploit generic mid-level representations <ref type="bibr" target="#b32">[33]</ref> and show their effectiveness for down-stream object pose estimation task. The feature maps and models associated with mid-level representations are learned separately using a large number of images of indoor scenes with the proxy task supervision (e.g. re-shading and normal estimation) and are frozen in our approach. Our model consists of two stages. In the first stage, the initial pose predictions are made with up-sampled mid-level representations features to generate predictions for discretized candidate poses. For the top three pose candidates 1 , we retrieve their discretized pose masks (Rendered D-Masks) and train a small neural network that takes the mid-level features representations gated (multiplied) by the object masks and learn to predict the correct pose of the retrieved mask. In this work, we will focus only on commonly encountered furniture categories and estimation of azimuth and elevation. In the following, we will describe the two stages in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Mid-Level Visual Representations</head><p>The inputs to our model are the feature maps of mid-level representations trained separately in indoor environments on visual proxy tasks in indoor environments <ref type="bibr" target="#b32">[33]</ref>. Out of 25 available networks for different tasks, we found surface normal and re-shading features most effective for the pose classification model, see <ref type="figure" target="#fig_1">Figure 3</ref>. We determined the most useful feature maps experimentally by testing their informative combinations and report results in the experiments section V-C.2 in <ref type="table" target="#tab_1">Table II</ref>. These feature maps provide encoding of the input image, while the network weights are frozen, forming an input to our pose estimation model. We concatenate the 16 ? 16 ? 8 feature maps from models trained on surface normal and re-shading tasks. Based on our experiments, using the actual predicted normal and re-shading images, same as pix3D <ref type="bibr" target="#b25">[26]</ref>, reduces the performance of the model. In the first phase of the model the feature maps are followed by additional convolution layers to get initial azimuth and elevation predictions, see <ref type="figure" target="#fig_0">Figure 2</ref> (left). The predictions are further refined in the second stage using predicted object masks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Masks and Discretized-Masks</head><p>The second stage of our model aims to refine the predictions by using the predicted object masks, mid-level representations features, and rendered masks from CAD models. Since the state-of-the-art mask prediction models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref> are not accurate enough to be solely effective, we generate masks rendered from different viewpoints of the CAD models of the training set and use them to further refine the predictions of the model from the first stage. We called these masks, Discretized-mask (D-mask). The 45 masks from different viewpoints, 9 different azimuths and 5 different elevations (9 ? 5) are stored per instance. In addition, for each image, we used the mask output of the state-of-the-art object detector <ref type="bibr" target="#b4">[5]</ref>, we call it predicted-mask, see <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>C. Model <ref type="figure" target="#fig_0">Figure 2</ref> shows different stages of the approach. In the first stage, the model proposes its top candidates as the probable pose classes. The input to the model is only the surface normals and re-shading feature maps generated from midlevel visual representations, each map is 16 ? 16 ? 8. We concatenate the features to get the input representation (16 ? 16 ? 16) and up-sample the concatenated features to the size of 128 ? 128 ? 8. The up-sampled features are then passed to a convolutional layer. For the classification purpose, we use three fully connected layers, batch normalization, and ReLU, along with the dropout to get the final embedding before Softmax classification. Azimuth and Elevation are estimated separately from the last fully connected layer. In the second stage, the up-sampled feature maps are masked out using the 'predicted-mask'. The top three pose candidates and their D-masks are stacked with the features one at a time. These channels are followed by convolution layers along with batch normalization and three fully connected layers before the binary classification. The model in the second phase learns to determine whether the selected Dmask matches the features and the predicted-mask. Since our method uses only RGB images, in the testing stage, we need to retrieve correct CAD model to use it's D-masks. As a CAD model retrieval pipeline, we used the predicted-mask of the test image and compare with all the predicted-masks of the training data. For this stage, we found out that a simple Template Matching is sufficient to find the most similar CAD model for the pose estimation task, but more elaborate silhouette matching techniques, such as Chamfer matching can be used in practice. Although the CAD model retrieval part is time consuming, it is guided by the knowledge of object category provided by object detector and for many indoor setting applications, this needs to be done once for each object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ACTIVE VISION DATASET POSE LABELING</head><p>The AVD dataset <ref type="bibr" target="#b0">[1]</ref> is a public dataset for active robotic vision tasks. It is comprised of dense scans of real indoor environments and has a total of 17 scenes. Each object is viewed from multiple viewpoints while the robot traverses the environment making it suitable for pose estimation. The pose estimation challenges include occluded objects, truncated images, dark objects, reflections, shiny objects, glass, lighting variations, and novel object instances. <ref type="figure">Figure  1</ref> shows examples of these challenges. To train and evaluate pose estimation on AVD we first provide pose annotation for the main object categories of sofa, table, desk, bed, and chair. We first get the dense 3D dense point-cloud of each scene using each scene RGB and depth images and annotate the 3D bounding boxes for objects using LabelCloud tool <ref type="bibr" target="#b20">[21]</ref>, the example is in <ref type="figure" target="#fig_2">Figure 4c</ref>. We then project corners of 3D bounding-boxes in world coordinate are projected back to the image plane using the transformation matrix from world to camera, <ref type="figure" target="#fig_2">Figure 4d</ref>, base on the following equation: [X c ,Y c , Z c ] T is a point in the camera coordinate frame and X w = [X w ,Y w , Z w ] T is the point in the world coordinate frame. R c w is the rotation matrix from the world to the camera and T c w is the translation between them. The camera intrinsic parameters matrix K is also known. See <ref type="figure" target="#fig_2">Figure  4b</ref> for an example of dense point-cloud reconstruction. To get the pose of an object in camera coordinate frame we used PnP algorithm <ref type="bibr" target="#b12">[13]</ref> between the points in 2D image frame and correspondences in the object coordinate frame. The 3D bounding box can be defined by it's center c = [c x , c y , c x ] which in object coordinate is [0, 0, 0], it's orientation R(? , ? , ?), and it's dimensions D = [d x , d y , d z ] which is annotated in second step. Therefore, the corners in object coordinate are X b</p><formula xml:id="formula_0">X w = [R w c |T w c ]X c</formula><formula xml:id="formula_1">1 = [d x /2, d y /2, d z /2] T , X b 2 = [?d x /2, d y /2, d z /2] T , ..., X b 8 = [?d x /2, ?d y /2, ?d z /2] T .</formula><p>The example object coordinate space is shown in <ref type="figure" target="#fig_2">Figure 4e</ref>. In the last step, the PnP algorithm <ref type="bibr" target="#b12">[13]</ref> is used to estimate rotation and translation of the object's 3D bounding-box from correspondences between the 2D image points and 3D points in the object coordinate frame. The result is shown in <ref type="figure" target="#fig_2">Figure   4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>We train and evaluate the proposed pose estimation approach on the Pix3D dataset <ref type="bibr" target="#b25">[26]</ref> comparing it to the stateof-the-art approaches <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b16">17]</ref> and some baseline methods. We then directly evaluate it without any additional training or fine-tuning on the challenging AVD dataset <ref type="bibr" target="#b0">[1]</ref> to see the generalizability of the approach. Code is also available at https://github.com/N-NEJATISHAHIDIN/ Pose_from_Mid-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training</head><p>We use frozen mid-level-representation features as the input to our model, and set the learning rate to 0.001 with a step size of 3. All the models are trained for 10 epochs with early stopping. The first phase is trained with cross-entropy loss, where azimuth ? and elevation ? are the logits over the number of bins with L ? ,? = L ? + L ? The second phase is trained with binary cross entropy loss (BCE) between the target and the output logits.</p><formula xml:id="formula_2">L mask = ? 1 n n ? i=0 y * i ? log (y i ) + (1 ? y * i ) ? log (1 ? y i )</formula><p>where y * i = 1 when the chosen D-mask is from the ground truth bin. Since Pix3D does not provide a train/test split, we use widely adopted Mesh R-CNN split S1. Split S1 randomly allocates 7539 images for training and 2530 for testing. The model is trained in the category agnostic level, i.e. we have only a single model for all categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Testing</head><p>For every test image, we find the nearest CAD model using template matching from OpenCV. To find the nearest CAD model, we use the predicted-mask of the image and compare it with the training predicted-masks of the images from the same category. To be more robust and scale-invariant, we compare each mask at different scales. See sample results are in <ref type="figure" target="#fig_4">Figure 6</ref>. As discussed in Section III-C, we use the top three pose candidate bins from stage one along with their probability. At testing time, we run binary classification between each of the top three candidate poses D-masks and the predicted mask from the image. If the predicted mask and D mask are in the same bin, stage two output is a positive match. In case of more than one positive output, we choose the bin which maximizes the max ?i?C pose? i P phase1 (i), where C pose is the set of three candidate poses from phase one, th? Y i is the binary classification result for bin i from phase2, and P phase1 (i) is the output probability of the bin from phase1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pose Evaluation</head><p>1) Baseline vs. Ours: To establish the importance of midlevel representations, we performed an ablation experiment in which the same network architecture is used but trained entirely from scratch. We called this network Baseline. Our second baseline, ResNet baseline, uses ResNet backbone features instead of the features of the mid-level representation. The results for phase one and Our whole model are shown in <ref type="table" target="#tab_1">Table I</ref>. The backbone from mid-level features in our model is frozen.</p><p>2) Mid-level representations features input: The 16 ? 8 feature maps of mid-level representations are used as our model input. Inspired by <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26]</ref> we use the surface normals that have been shown to be beneficial for the pose estimation. From other 24 models introduced in <ref type="bibr" target="#b32">[33]</ref>, the re-shading feature map benefits our model most as shown in our experiments in <ref type="table" target="#tab_1">Table II</ref>. <ref type="figure" target="#fig_1">Figure 3</ref> shows sample inputs. We also tried the combinations of 3 feature maps, but they did not improve the model by more than a few 0.1%.</p><p>The second stage of the approach completely relies on the D-Masks. With the even number of bins, the symmetric objects have the same D-masks for multiple orientations. This fact encourages us to use the odd number of bins. Based on our experiments, it's difficult for models to distinguish the pose of the object if it's close to the bin's borders, hence overlapping bins are used as in <ref type="bibr" target="#b16">[17]</ref>. To make the results comparable to other pose classification models, we defined 9 bins with 2.5 degree overlap on each side; which is comparable to 8 bins. Since the range of each bin in both is 45 degree; Similarly, the 5, 13, 25 bins with 9, 1.15, 0.3, degree overlap on each side is comparable with 4, 12, 24 bins. <ref type="table" target="#tab_1">Table IV</ref> shows the model performance for the different number of bins.</p><p>3) Comparison with Available Models: In comparison Mousavian et al. <ref type="bibr" target="#b16">[17]</ref>, our model outperforms by a large margin, which shows the importance of generalizability of the mid-level features for the pose estimation task, <ref type="table" target="#tab_1">Table I</ref>. The model is also compared with the Pix3D dataset <ref type="bibr" target="#b25">[26]</ref> on the chair category. The Pix3D trained reconstruction and pose estimation together only for the untruncated and unoccluded chairs. It is stated that the reconstruction branch helps to improve the pose classification accuracy. Compared to Pix3D, our model is trained category agnostic way, with no need for supervision on the mid-level features. On the contrary, Pix3D needs supervision for normal, depth, and silhouette images and is trained per category. The results are shown in <ref type="table" target="#tab_1">Table III</ref>     <ref type="figure" target="#fig_5">Figure 7</ref> shows some quantitative results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. AVD experiments</head><p>As mentioned earlier, the AVD dataset is a challenging dataset for pose estimation task; most of the images are either truncated or heavily occluded. The generated normal and re-shading features for these images are less accurate in comparison to the Pix3D, see <ref type="figure">Figure 1</ref>. The fact that AVD has been densely sampled from different views, generates object poses that are less probable in the Pix3D, such as the back of the sofa. <ref type="figure" target="#fig_3">Figure 5</ref> is the map of two homes with the   location and orientation of the camera. Since the predicted masks for AVD are of lower quality because of the mentioned challenges, we report the results for the Our phase1 model. The model was trained on Pix3D and tested on AVD. We also evaluated the ResNet baseline model performance on AVD to better show the generalizability of our model. <ref type="table" target="#tab_7">Table V</ref> shows the results. The proposed Ours phase1 outperforms the other model with 10.56%. This shows the effectiveness and generalizability of mid-level features. In comparison to Pix3D, our model's failures include objects in rare poses or cases of heavily occluded objects. <ref type="figure" target="#fig_6">Figure 8</ref> shows some of the results on the AVD dataset.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>We present a novel object pose estimation approach built on the top of generic mid-level representations trained on computer vision proxy tasks of surface normal estimation and reshading. The first stage is competitive with the stateof-the-art approaches that are trained on the Pix3D dataset. The second refinement via learned retrieval stage achieves superior performance compared to the state-of-the-art. We also introduce a new pose estimation benchmark on the Active Vision Dataset and establish several new pose estimation baselines. We currently formulate the problem as classification and estimate only discretized azimuth and elevation angles. We plan to address the full 6 DOF pose estimation in the future. The performance of our approach is notably affected by the quality of the object masks, which deteriorates with challenging viewpoints and occlusions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Network Architecture. First stage: up-sampled and fused mid-level feature maps (not the actual normal and re-shading images) are used to predict azimuth and elevation. Second stage: The object mask and mid-level features are compared with the top three object D-masks corresponding to most likely pose hypotheses from the first stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Visualization of mid-level representations and masks on the Pix3D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>The pose labelling pipeline for main object categories in AVD. Using (a) RGB and depth images of each scene, we reconstructed the (b) dense 3D point-cloud of each scene. (c) The 3D bounding boxes of objects inside these point clouds are annotated using the LabelCloud tool. (f) Poses are generated using the PNP algorithm between (d) the corners projected on the image plane and (e) corners in object coordinate frame. In total, we have labeled 6337 objects pose and 3D bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>The image shows the dense RGB-D sampling from each home in the AVD dataset. The red dots are the locations of the camera, and the blue arrows around each dot are the 12 camera orientations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Shape Retrieval on Pix3D. Template matching compares the predicted-mask of the query image with other predicted-masks in the training data of the same category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>The accuracy results with training on different percentages of the training data. Achieving 35% improvement over the existing models when there's only 25% of training data is available.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Results on the challenging AVD dataset. This model is capable of estimating pose without any training on AVD data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>The azimuth classification accuracy for 9 bins with 2.5 ? overlap. Models are all trained category agnostic.</figDesc><table><row><cell></cell><cell cols="2">Azimuth Elevation</cell></row><row><cell>Normal</cell><cell>68.37</cell><cell>74.47</cell></row><row><cell>Normal+ 2D key-points</cell><cell>68.77</cell><cell>74.38</cell></row><row><cell>Normal+edge occlusion</cell><cell>69.52</cell><cell>75.91</cell></row><row><cell>Normal+ depth euclidean</cell><cell>70.24</cell><cell>76.36</cell></row><row><cell>Normal+ segment unsup25d</cell><cell>70.40</cell><cell>74.42</cell></row><row><cell>Normal+ 3D key points</cell><cell>71.03</cell><cell>76.36</cell></row><row><cell>Normal+reshading</cell><cell>72.21</cell><cell>76.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>The choice of mid-level features and the effect on pose classification task with 9 bins with 2.5 ? overlap. samples for each category with the pose annotation. Some approaches [26] also require the surface normal, silhouette, or depth supervision. The labeling process is costly and timeconsuming. The proposed model outperforms other models by a large margin when using a fraction of training examples.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>The comparison of our model with the Pix3D model. Results are only for the chair category for different number of bins.</figDesc><table><row><cell></cell><cell></cell><cell>Azimuth</cell><cell></cell><cell></cell><cell>Elevation</cell><cell></cell></row><row><cell># bins</cell><cell>5 + 9 ?</cell><cell>9 + 2.5 ?</cell><cell>13 + 1.5 ?</cell><cell>3</cell><cell>5</cell><cell>7</cell></row><row><cell>ours</cell><cell>77.54</cell><cell>73.56</cell><cell>64.82</cell><cell>85.37</cell><cell>76.08</cell><cell>66.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>The average accuracy of the azimuth and elevation for the different number of bins.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V :</head><label>V</label><figDesc>Per category accuracy of two models on AVD dataset. The results are for 9 bins with 2.5 ? overlap.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">90% of the time, the correct pose label is included in the top three pose candidates, which is optimized based on our experiments.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A dataset for developing and benchmarking active vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ammirato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1378" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scan2CAD: Learning CAD Model Alignment in RGBD Scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Avetisyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<idno>arxiv:1512.03012. 2015</idno>
		<ptr target="http://arxiv.org/abs/1512.03012" />
		<title level="m">An Information-Rich 3D Model Repository</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITT vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mesh R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV) (2019)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9784" to="9794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016)</title>
		<imprint>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="386" to="397" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="548" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">BOP: Benchmark for 6D Object Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hodan</surname></persName>
		</author>
		<idno>ArXiv abs/1808.08319</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">SeeThrough: finding chairs in heavily occluded indoor scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hueting</surname></persName>
		</author>
		<idno>CoRR abs/1710.10473</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SSD-6D: Making RGB-based 3D detection and 6D pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3D-RCNN: Instance-Level 3D Object Reconstruction via Render-and-Compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3559" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">EPnP: An Accurate O(n) Solution to the PnP Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep supervision with shape concepts for occlusionaware 3d object parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3D pose regression using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5632" to="5640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Total3DUnderstanding: Joint Layout, Object Pose and Mesh Reconstruction for Indoor Scenes From a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020)</title>
		<imprint>
			<biblScope unit="page" from="52" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">6-dof object pose from semantic keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA</title>
		<imprint>
			<biblScope unit="page" from="2011" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast single shot detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2016 Fourth International Conference on</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="676" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">labelCloud: A Lightweight Domain-Independent Labeling Tool for 3D Object Detection in Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zschech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>K?hl</surname></persName>
		</author>
		<idno>ArXiv abs/2103.04970</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SLAM++: Simultaneous localisation and mapping at the level of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salas-Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Mid-Level Visual Representations Improve Generalization and Sample Efficiency for Learning Visuomotor Policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-Time Seamless Single Shot 6D Object Pose Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="292" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MarrNet: 3D Shape Reconstruction via 2.5D Sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond pascal: A benchmark for 3d object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ObjectNet3D: A Large Scale Database for 3D Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00199</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling Task Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
