<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Closer Look at Local Aggregation Operators in Point Cloud Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
							<email>liuze@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
							<email>hanhu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
							<email>yuecao@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
							<email>xtong@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Closer Look at Local Aggregation Operators in Point Cloud Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D point cloud</term>
					<term>local aggregation operator</term>
					<term>position pooling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances of network architecture for point cloud processing are mainly driven by new designs of local aggregation operators. However, the impact of these operators to network performance is not carefully investigated due to different overall network architecture and implementation details in each solution. Meanwhile, most of operators are only applied in shallow architectures. In this paper, we revisit the representative local aggregation operators and study their performance using the same deep residual architecture. Our investigation reveals that despite the different designs of these operators, all of these operators make surprisingly similar contributions to the network performance under the same network input and feature numbers and result in the state-of-the-art accuracy on standard benchmarks. This finding stimulate us to rethink the necessity of sophisticated design of local aggregation operator for point cloud processing. To this end, we propose a simple local aggregation operator without learnable weights, named Position Pooling (PosPool), which performs similarly or slightly better than existing sophisticated operators. In particular, a simple deep residual network with PosPool layers achieves outstanding performance on all benchmarks, which outperforms the previous state-of-the methods on the challenging PartNet datasets by a large margin (7.4 mIoU). The code is publicly available at https://github.com/zeliu98/CloserLook3D.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rise of 3D scanning devices and technologies, 3D point cloud becomes a popular input for many machine vision tasks, such as autonomous driving, robot navigation, shape matching and recognition, etc. Different from images and videos that are defined on regular grids, the point cloud locates at a set of irregular positions in 3D space, which makes the powerful convolutional neural networks (CNN) and other deep neural networks designed for regular data hard to be applied. Early studies transform the irregular point set into a regular Equal contribution. ? This work is done when Ze Liu is an intern at MSRA. arXiv:2007.01294v1 [cs.CV] 2 Jul 2020 grid by either voxelization or multi-view 2D projections such that the regular CNN can be adopted. However, the conversion process always results in extra computational and memory costs and the risk of information loss.</p><p>Recent methods in point cloud processing develop networks that can directly model the unordered and non-grid 3D point data. These architectures designed for point cloud are composed by two kinds of layers: the point-wise transformation layers and local aggregation layers. While the point-wise transformation layer is applied on features at each point, the local aggregation layer plays a similar role for points as the convolution layer does for image pixels. Specifically, it takes features and relative positions of neighborhood points to a center point as input, and outputs the transformed feature for the center point. To achieve better performance in different point cloud processing tasks, a key task of point cloud network design is to develop effective local aggregation operators.</p><p>Existing local aggregation operators can be roughly categorized into three groups according to the way that they combine the relative positions and point features: point-wise multi-layer perceptions (MLP) based <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref>, pseudo grid feature based <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref> and adaptive weight based <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b13">14]</ref>. The point-wise MLP based methods treat a point feature and its corresponding relative position equally by concatenation. All the concatenated features at neighborhood are then abstracted by a small PointNet <ref type="bibr" target="#b19">[20]</ref> (multiple point-wise transformation layers followed by a MAX pooling layer) to produce the output feature for the center point. The pseudo grid feature based methods first generate pseudo features on pre-defined grid locations, and then learn the parametrized weights on these grid locations like regular convolution layer does. The adaptive weight based methods aggregate neighbor features by weighted average with the weights adaptively determined by relative position of each neighbor.</p><p>Despite the large efforts for aggregation layer design and performance improvements of the resulting network in various point cloud processing tasks, the contributions of the aggreation operator to the network performance have never been carefully investigated and fairly compared. This is mainly due to the different network architectures used in each work, such as the network depth, width, basic building blocks, whether to use skip connection, as well as different implementation of each approach, such as point sampling method, neighborhood computation, and so on. Meanwhile, most of existing aggregation layers are applied in shallow networks, it is unclear whether these designs are still effective as the network depth increases.</p><p>In this paper, we present common experimental settings for studying these operators, selecting a deep residual architecture as the base networks, as well as same implementation details regarding point sampling, local neighborhood selection and etc. We also adopt three widely used datasets, ModelNet40 <ref type="bibr" target="#b36">[37]</ref>, S3DIS <ref type="bibr" target="#b0">[1]</ref> and PartNet <ref type="bibr" target="#b18">[19]</ref> for evaluation, which account for different tasks, scenarios and data scales. Using these common experimental settings, we revisit the performance of each representative operator and make fair comparison between them. We find appropriate settings for some operators under this deep residual architecture are different from that of using shallower and non-residual networks. We also surprisingly find that different representative methods perform similarly well under the same representation capacity on these datasets, if appropriate settings are adopted for each method, although these methods may be invented by different motivations and formulations, in different years.</p><p>These findings also encourage us to rethink the role of local aggregation layers in point cloud modeling: do we really need sophisticated/heavy local aggregation computation? We answer this question by proposing an extremely simple local aggregation operator with no learnable weights: combining a neighbor point feature and its 3-d relative coordinates by element-wise multiplication, followed with an AVG pool layer to abstract information from neighborhood. We name this new operator as position pooling (PosPool), which shows no less or even better accuracy than other highly tuned sophisticated operators on all the three datasets. These results indicate that we may not need sophisticated/heavy operators for local aggregation computation. We also harness a strong baseline for point cloud analysis by a simple deep residual architecture and the proposed position pooling layers, which achieves 53.8 part category mIoU accuracy on the challenging PartNet datasets, significantly outperforming the previous best method by 7.4 mIoU.</p><p>The contributions of this paper are summarized as -A common testbed to fairly evaluate different local aggregation operators.</p><p>-New findings of aggregation operators. Specifically, different operators perform similarly well and all of them can achieve the state-of-the-art accuracy, if appropriate settings are adopted for each operator. Also, appropriate settings in deep residual networks are different from those in shallower networks. We hope these findings could shed new light on network design. -A new local aggregation operator (PosPool) with no learnable weights that performs as effective as existing operators. Combined with a deep residual network, this simple operator achieve state-of-the-art performance on 3 representative benchmarks and outperforms the previous best method by a large margin of 7.4 mIoU on the challenging PartNet datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Projection based Methods project the irregular point cloud onto a regular sampling grid and then apply 2D or 3D CNN over regularly-sampled data for various vision tasks. View-based methods project a 3D point cloud to a set of 2D views from various angles. Then these view images could be processed by 2D CNNs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref>. Voxel-based methods project the 3D points to regular 3D grid, and then standard 3D CNN could be applied <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">37]</ref>. Recently, adaptive voxel-based representations such as K-d trees <ref type="bibr" target="#b9">[10]</ref> or octrees <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33]</ref> have been proposed for reducing the memory and computational cost of 3D CNN. The viewbased and voxel-based representations are also combined <ref type="bibr" target="#b20">[21]</ref> for point cloud analysis. All these methods require preprocessing to convert the input point cloud and may lose the geometry information.</p><p>Global Aggregation Methods process the 3D point cloud via point-wise 1?1 transformation (fully connected) layers followed by a global pooling layer to aggregate information globally from all points <ref type="bibr" target="#b19">[20]</ref>. These methods are the first to directly process the irregular point data. They have no restriction on point number, order and regularity of neighborhoods, and obtain fairly well accuracy on several point cloud analysis tasks. However, the lack of local relationship modeling components hinders the better performance on these tasks. Local Aggregation Methods Recent point cloud architectures are usually composed by 1 ? 1 point-wise transformation layers and local aggregation operators. Different methods are mainly differentiated by their local aggregation layers, which usually adopt the neighboring point features and their relative coordinates as input, and output a transformed center point feature. According to the way they combine point features and relative coordinates, these methods can be roughly categorized into three groups: point-wise MLP based <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref>, pseudo grid feature based <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref>, and adaptive weight based <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b13">14]</ref>, as will be detailed in Section 3. There are also some works use additional edge features (relative relationship between point features) as input <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref>, also commonly referred to as graph based methods. While we have witnessed significant accuracy improvements on benchmarks by new local aggregation operators year-by-year, the actual progress is a bit vague to the community as the comparisons are made on different grounds that the other architecture components and implementations may vary significantly. The effectiveness of designing components in some operators using deeper residual architectures is also unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview of Local Aggregation Operators</head><p>In this section, we present a general formulation for local aggregation operators as well as a categorization of them. General Formulation In general, for each point i, a local aggregation layer first transforms a neighbor point j's feature f j ? R d?1 and its relative location ?p ij = p j ? p i ? R 3?1 into a new feature by a function G(?, ?), and then aggregate all transformed neighborhood features to form point i's output feature by a reduction function R (typically using MAX, AVG or SUM), as</p><formula xml:id="formula_0">g i = R ({G(?p ij , f j )|j ? N (i)}) ,<label>(1)</label></formula><p>where N (i) represents the neighborhood of point i. Alternatively, edge features {f i , ?f ij } (?f ij = f j ? f i ) can be used as input instead of ?p ij <ref type="bibr" target="#b34">[35]</ref>. According to the family to which the transformation function G(?, ?) belongs, existing local aggregation operators can be roughly categorized into three types: point-wise MLP based, pseudo grid feature based, and adaptive weight based. Point-wise MLP based Methods The pioneer work of point-wise MLP based method, PointNet++ <ref type="bibr" target="#b21">[22]</ref>, applies several point-wise transformation (fully connected) layers on a concatenation of relative position and point feature to achieve transformation:</p><p>G(?p ij , f j ) = MLP (concat(?p ij , f j )) .</p><p>There are also variants by using an alternative edge feature {f i , ?f ij } as input <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b12">13]</ref>, or by using a special neighborhood strategy <ref type="bibr" target="#b10">[11]</ref>. The reduction function R(?) is usually set as MAX <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b12">13]</ref>. The multiple point-wise layers after concatenation operation can approximate any continuous function about the relative coordinates and point feature <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>. However, a drawback lies in its large computation complexity, considering the fact that the multiple fully connected (FC) layers are applied to all neighboring points when computing each point's output. Specifically, the FLOPs is O(time) = ((2d + 3) + (h ? 2)d/2) ? d/2 ? nK, for a point cloud with n points, neighborhood size of K, FC layer number of h, and inter-mediate dimension of d/2, when h ? 2. The space complexity is O(space) = ((2d + 3) + (h ? 2)d/2) ? d/2. For h = 1, there exists efficient implementation by computation sharing (see Section 4.2). Pseudo Grid Feature based Methods The pseudo grid feature based methods generate pseudo features on several sampled regular grid points, such that regular convolution methods can be applied. A representative method is KP-Conv <ref type="bibr" target="#b29">[30]</ref>, where equally distributed spherical grid points are sampled and the pseudo features on the k th grid point is computed as</p><formula xml:id="formula_2">f i,k = j?N (i) max(0, 1 ? ?p jk 2 ? ) ? f j .<label>(3)</label></formula><p>The index of each grid point k will have strict mapping with the relative position to center point ?p ik . Hence, a (depth-wise) convolution operator with parametrized weights w k ? R d?1 defined on each grid point can be used to achieve feature transformation:</p><formula xml:id="formula_3">G(?p ik , f i,k ) = w k f i,k .<label>(4)</label></formula><p>Different pseudo grid feature based methods mainly differ each other by the definition of grid points <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27]</ref> or index order <ref type="bibr" target="#b13">[14]</ref>. When depth-wise convolution is used, the space and time complexity are O(space) = dM and O(time) = ndKM , respectively, where M is the number of grid points. Adaptive Weight based Methods The adaptive weight based methods define convolution filters over arbitrary relative positions, and hence can compute aggregation weights on all neighbor points:</p><formula xml:id="formula_4">G(?p ij , f j ) = H (?p ij ) f j ,<label>(5)</label></formula><p>where H is typically implemented by several point-wise transformation (fully connected) layers <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5]</ref>; is an element-wise multiplication operator; R is typically set as SUM. Some methods adopt more position related variables <ref type="bibr" target="#b15">[16]</ref>, point density <ref type="bibr" target="#b35">[36]</ref>, or edge features <ref type="bibr" target="#b31">[32]</ref> as the input to compute adaptive weights. More sophisticated function other than fully connected (FC) layers are also used, for example, </p><formula xml:id="formula_5">1 x 16C 1 x 8C 1 x 4C N' x C o / N' x C o / N' x C o / ? ? ? ? ? 1x1 conv 1 x 2C Fig. 1.</formula><p>A common deep residual architecture used to evaluate different local aggregation operators. In evaluation, we adjust the model complexity by changing architecture depth (or block repeating factor Nr), base width C and bottleneck ratio ?. Note the point numbers drawn in this figure is an approximation to indicate the rough complexity but not an accurate number. Actually, the points on each stage are generated by a subsampling method <ref type="bibr" target="#b28">[29]</ref> using a fixed grid size and the point number on different point cloud instances can vary</p><p>Taylor approximation <ref type="bibr" target="#b13">[14]</ref> and an additional SoftMax function to normalize aggregation weights over neighborhood <ref type="bibr" target="#b31">[32]</ref>. Please see Appendix A6 for detailed analysis of the space and time complexity for the above 3 operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Benchmarking Local Aggregation Operators in Common Deep Architecture</head><p>While most local aggregation operators described in Section 3 are reported using specific shallow architectures, it is unknown whether their designing components perform also sweet using a deep residual architecture. In addition, these operators usually use different backbone architectures and different implementation details, making a fair comparison between them difficult. In this section, we first present a deep residual architecture, as well as implementation details regarding point sampling and neighborhood selection. Then we evaluate the designing components of representative operators using common architectures, implementation details and benchmarks. The appropriate settings within each method type using the common deep residual architectures are suggested and discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Common Experimental Settings</head><p>Architecture To investigate different local aggregation operators on a same, deep and modern ground, we select a 5-stage deep residual network, similar to the standard ResNet model <ref type="bibr" target="#b6">[7]</ref> in image analysis. Residual architectures have been widely adopted in different fields to facilitate the training of deep networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref>. However, in the point cloud field, until recently, there are some works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13]</ref> starting to use deep residual architectures, probably because the unnecessary use of deep networks on several small scale benchmarks. Nevertheless, our investigation shows that on larger scale and more challenging datasets such as PartNet <ref type="bibr" target="#b18">[19]</ref>, deep residual architectures can bring significantly better performance, for example, with either local aggregation operator type described in Section 3, the deep residual architectures can surpass previous best methods by more than 3 mIoU. On smaller scale datasets such as ModelNet40, they also seldom hurt the performance. The deep residual architecture would be a reasonable choice for practitioners working on point cloud analysis. <ref type="figure">Fig. 1</ref> shows the residual architecture used in this paper. It consists 5 stages of different point resolution, with each stage stacked by several bottleneck residual blocks. Each bottleneck residual block is composed successively by a 1 ? 1 pointwise transformation layer, a local aggregation layer, and another 1 ? 1 pointwise transformation layer. At the block connecting two stages, a stridded local aggregation layer is applied where the local neighborhood is selected at a higher resolution and the output adopts a lower resolution. Batch normalization and ReLU layers are applied after each 1 ? 1 layer to facilitate training. For head networks, we use a 4-layer classifier and a U-Net style encoder-decoder <ref type="bibr" target="#b23">[24]</ref> for classification and semantic segmentation, respectively.</p><p>In evaluation of a local aggregation operator, we use this operator to instantiate all local aggregation layers in the architecture. We also consider different model capacity by varying network depth (block repeating factor N r ), width (C) and bottleneck ratio (?).</p><p>Point Sampling and Neighborhoods. To generate point sets for different resolution levels, we follow <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> to use a subsampling method with different grid sizes to generate point sets in different resolution stages. Specifically, the whole 3D space is divided by grids and one point is randomly sampled to represent a grid if multiple points appear in the grid. This method can alleviate the varying density problem <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. Given a base grid size at the highest resolution of Res1, the grid size for different resolutions are multiplied by 2? stage-by-stage. The base grid size for different datasets are detailed in Section 6.</p><p>To generate a point neighborhood, we follow the ball radius method <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>, which in general result in more balanced density than the location or feature kNN methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b34">35]</ref>. The ball radius is set as 2.5? of the base grid size. <ref type="table">Table 1</ref>. The performance of baseline operators, sweet spots of point-wise MLP based, pseudo grid feature based and adaptive weight based operators, and the proposed PosPool operators on three benchmark datasets. Baseline * denotes Eq. <ref type="bibr" target="#b5">(6)</ref> and baseline ? (AVG/MAX) denotes Eq. (7) AVG/MAX, respectively. PosPool and PosPool* denote the operators in Eq. <ref type="formula" target="#formula_9">(8)</ref> and <ref type="formula" target="#formula_0">(10)</ref>, respectively. (S) after each method denotes a smaller configuration of this method (Nr = 1, ? = 2 and C = 36), which is about 16? more efficient than the regular configuration (the other row) of Nr = 1, ? = 2 and C = 144. Previous best performing methods on three benchmarks in literature are shown in the first block of this Datasets We consider three datasets with varying scales of training data, task outputs (classification and semantic segmentation) and scenarios (CAD models and real scenes): ModelNet40 <ref type="bibr" target="#b36">[37]</ref>, S3DIS <ref type="bibr" target="#b0">[1]</ref> and PartNet <ref type="bibr" target="#b18">[19]</ref>. More details about datasets are described in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance of Two Baseline Operators</head><p>For point cloud modeling, the architectures without local aggregation operators also perform well to some extent, e.g. PointNet <ref type="bibr" target="#b19">[20]</ref>. To investigate what local aggregation operators perform beyond, we present two baseline functions to replace the local aggregation operators described in Section 3:</p><formula xml:id="formula_6">gi = fi,<label>(6)</label></formula><formula xml:id="formula_7">gi = R ({fj|j ? N (i)}) .<label>(7)</label></formula><p>The former is an identity function, without encoding neighborhood points. The latter is an AVG/MAX pool layer without regarding their relative positions. <ref type="table">Table 1</ref> shows the accuracy of these two baseline operators using the common architecture in <ref type="figure">Fig. 1</ref> on three benchmarks. It can be seen that these baseline In the following, we will revisit different designing components in the pointwise MLP based methods and the adaptive weight based methods using the common deep residual architecture in <ref type="figure">Fig. 1</ref>. For the pseudo grid feature methods, we choose a representative operator, KPConv <ref type="bibr" target="#b29">[30]</ref>, with depth-wise convolution kernel and its default grid settings (M = 15) for comparison. There are not much hyper-settings for it and we will omit the detailed tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Study on Point-wise MLP based Method</head><p>We start the investigation of this type of methods from a representative method, PointNet++ <ref type="bibr" target="#b21">[22]</ref>. We first reproduce this method using its own specific overall architecture and with other implementation details the same as ours. <ref type="table" target="#tab_1">Table 2</ref> (denoted as PointNet++ * ) shows our reproduction is fairly well, which achieves slightly better accuracy than that reported by the authors <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b18">19]</ref> on ModelNet40 and PartNet.</p><p>We re-investigate several design components for this type of methods using the deep architecture in <ref type="figure">Fig. 1</ref>, including the number of fully connected (FC) layers in an MLP, the choice of input features and the reduction function. <ref type="table" target="#tab_1">Table 2</ref> shows the ablation study on these aspects, with architecture hyper-parameters as: block repeat factor N r = 1, base width C = 144 and bottleneck ratio ? = 8.</p><p>We can draw the following conclusions:</p><p>-Number of FC layers. In literature of this method type, 3 layers are usually used by default to approximate complex functions. Surprisingly, in our experiments, using 1 FC layer without non-linearity significantly outperforms that using 2 or 3 FC layers on S3DIS, and it is also competitive on Model-Net40 and PartNet. We hypothesize that the fitting ability by multiple FC layers applied on the concatenation of point feature and relative position may be partly realized by the point-wise transformation layers (the first and the last layers in a residual block) applied on point feature alone. Less FC layers also ease optimization. Using 1 FC layer is also favorable considering the efficiency issue: the computation can be significantly reduced when 1 FC layer is adopted, through computing sharing as explained below. -Input Features. The relative position and edge feature perform similarly on ModelNet40 and PartNet, and combining them has no additional gains. However, on S3DIS datasets, combining both significantly outperforms the variants using each alone. -Reduction function. MAX pooling performs the best, which is in accord with that in literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An efficient implementation when 1 FC layer is used. Denote the weight matrix of this only FC layer as</head><formula xml:id="formula_8">W = [W 1 , W 2 ] ? R d?(d+3) where W 1 ? R d?3 and W 2 ? R d?d . We have G = W ? concat(?p ij , f j ) = W 1 ?p ij + W 2 f j .</formula><p>Noting the computation of the second term W 2 f j can be shared when point j appears in different neighborhoods, the computation complexity of this operator is significantly reduced from (d + 3)ndK to nd 2 + 3ndK. Sweet spots for point-wise MLP methods. Regarding both the efficacy and efficiency, the sweet spot settings are applying 1 FC layer to an input combination of relative position and edge features. <ref type="table" target="#tab_1">Table 2</ref> also shows that using ? = 2 for this method can approach or surpass the state-of-the-art on all three datasets. <ref type="table">Table 3</ref> shows the ablations over several designing components within this method type, including the number of fully connected (FC) layers, choice of input features, the reduction function and whether to do weight normalization. We adopt architecture hyper-parameters as: block repeat factor N r = 1, base width C = 144, and bottleneck ratio ? = 8. We can draw the following conclusions:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Study on Adaptive Weight based Method</head><p>-Number of FC layers. Using 1 FC layer performs noticeably better than that using 2 or 3 layers on S3DIS, and is comparable on ModelNet40 and PartNet. -Input features. Using relative positions alone performs best on all datasets.</p><p>The accuracy slightly drops with additional position features <ref type="bibr" target="#b15">[16]</ref>. The edge features harm the performance, probably because it hinders the effective learning of adaptive weights from relative positions. -Reduction function. MAX and AVG functions perform slightly better than SUM function, probably because the MAX and AVG functions are more insensitive to varying neighbor size. We use AVG function by default. -SoftMax normalization. The accuracy significantly drops by SoftMax normalization, probably because the positive weights after normalization let kernels act as low-pass filters and may cause the over-smoothing problem <ref type="bibr" target="#b12">[13]</ref>. <ref type="table">Table 3</ref>. Evaluating different settings of the adaptive weight based methods. dp * denotes the 9-dimensional position vector as in <ref type="bibr" target="#b15">[16]</ref>. "Sweet spot" denotes balanced settings regarding both efficacy and efficiency. Sweet spots for adaptive weight based methods. The best performance is achieved by applying 1 FC layer without SoftMax normalization on relative positions alone to compute the adaptive weights. This method also approaches or surpasses the state-of-the-art on all three datasets using a deep residual network. <ref type="table">Table 1</ref> indicates that the three local aggregation operator types with appropriate settings all achieve the state-of-the-art performance on three representative datasets using the same deep residual architectures. With 16? less parameters and computations (marked by "S"), they also perform competitive compared with the previous state-of-the-art. The sweet spots of different operators also favor a simplicity principle, that the relative position alone and 1 FC layer perform well in most scenarios. While recent study in point cloud analysis mainly lies in inventing new local aggregation operators, the above results indicate that some of them may worth re-investigation under deeper and residual architectures. These results also stimulate a question: could a much simpler local aggregation operator achieve similar accuracy as the sophisticated ones? In the following section, we will try to answer this question by presenting an extremely simple local aggregation operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PosPool: An Extremely Simple Local Aggregation Operator</head><p>In this section, we present a new local aggregation operator, which is extremely simple with no learnable weights. The new operator is illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. For each neighboring point j, it combines the relative position ?p ij and point feature f j by element-wise multiplication. Considering the dimensional difference between the 3-dimensional ?p ij and d-dimensional f j , the multiplication is applied group-wise that ?p ij 's three scalars [?x ij , ?y ij , ?z ij ] are multiplied to 1/3 channels of f j , respectively, as</p><formula xml:id="formula_9">G(?p ij , f j ) = Concat ?x ij f 0 j ; ?y ij f 1 j ; ?z ij f 2 j ,<label>(8)</label></formula><p>where f 0,1,2 j are the 3 sub-vectors equally split from f j , as f j = f 0 j ; f 1 j ; f 2 j . The operator is named position pooling (PosPool), featured by its property of no learnable weight. It also reserves the permutation/translation invariance property which is favorable for point cloud analysis. A Variant. We also consider a variant of position pooling operator which is slightly more complex, but maintains the no learnable weight property. Instead of using 3-d relative coordinates, we embed the coordinates into a vector with the same dimension as point feature f ij using cosine/sine functions, similar as in <ref type="bibr" target="#b30">[31]</ref>. The embedding is concatenated from d/6 group of 6-dimensional vectors, with the m th 6-d vector representing the cosine/sine functions with a wave length of 1000 6m/d on relative locations x, y, z: Then an element-wise multiplication operation is applied on the embedding E and the point feature f ij :</p><formula xml:id="formula_10">G(?p ij , f j ) = E f ij .<label>(10)</label></formula><p>The resulting operator also does not have any learnable weights, and is set as a variant of position pooling layer. We find this variant performs slightly better than the direct multiplication in Eq. (8) in some scenarios. We will show more variants in Appendix A3. Complexity Analysis The space complexity O(space) = 0, as there are no learnable weights. The time complexity is also small O(time) = ndK. Due to the no learnable weight nature, it may also potentially ease the hardware implementation, which does not require an adaption to different learnt weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Benchmark Settings</head><p>In this section, we detailed the three benchmark datasets with varying scales of training data, task outputs (classification and semantic segmentation) and scenarios (CAD models and real scenes).</p><p>-ModelNet40 [37] is a 3D classification benchmark. This dataset consists of 12,311 meshed CAD models from 40 classes. We follow the official data splitting scheme in <ref type="bibr" target="#b36">[37]</ref> for training/testing. We adopt an input resolution of 5,000 and a base grid size of 2cm. -S3DIS [1] is a real indoor scene segmentation dataset with 6 large scale indoor areas captured from 3 different buildings. 273 million points are annotated and classified into 13 classes. We follow <ref type="bibr" target="#b27">[28]</ref> and use Area-5 as the test scene and all others for training. In both training and test, we segment small sub-clouds in spheres with radius of 2m. In training, the spheres are randomly selected in scenes. In test, we select spheres regularly in the point clouds. We adopt a base grid size of 4cm. -PartNet <ref type="bibr" target="#b18">[19]</ref> is a more recent challenging benchmark for large-scale finegrained part segmentation. This dataset consists of pre-sampled point clouds of 26, 671 3D object models in 24 object categories, with each object containing 18 parts on average. This dataset is officially split into three parts: 70% training, 10% validation, and 20% test sets. We train our model with official training dataset and then conduct the comparison study on the validation set on 17 categories with fine-grained annotation. We also report the best accuracies of different methods on the test set. We use the 10,000 points provided with the datasets as input, and the base grid size is set as 2cm.</p><p>The training/inference settings are detailed in Appendix A1. Note for Part-Net datasets, while in <ref type="bibr" target="#b18">[19]</ref> independent networks are trained for 17 different shapes, we adopt a shared backbone and independent 3 fully connected layers for part segmentation of different categories and train all the categories together, which significantly facilitate the evaluation on this dataset. We note using the shared backbone network achieves similar accuracy than the methods training different shapes independently. performances using varying network hyper-parameters on all datasets, showing its strong stability and adaptability. While the other more sophisticated operators may achieve similar accuracy with the PosPool layers on some datasets or settings, their performance are less stable across scenarios and model capacity. For example, the accuracy of the "AdaptWeight" method will drop significantly on S3DIS when the model capacity is reduced by either the width, depth or bottleneck ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparing Operators with Varying Architecture Capacity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper studies existing local aggregation operators in depth via a carefully designed common testbed that consists of a deep residual architecture and three representative benchmarks. Our investigation illustrates that with appropriate settings, all operators can achieve the state-of-the-art performance on three tasks. Motivated by this finding, we present a new extremely simple operator without learned weights, which performs as good as existing operators with sophisticated design. To understand what the networks with these operators are learnt from input, we visualize the norm of activation map before prediction by different methods (operators), suggesting that different operators tend to offer similar activations for a same input point cloud, as shown in <ref type="figure">Fig 4.</ref> We hope our study and new design can encourage further rethinking and understanding on the role of local aggregation operators and shed new light to future network design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MaxPool</head><p>PointMLP PseudoGrid AdaptWeight PosPool <ref type="figure">Fig. 4</ref>. Activation maps before the final prediction using different methods on PartNet validation shapes, indicating similar high energy area learnt by different methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1 Training/Inference Settings</head><p>In this section, we describe the training and inference settings of each dataset in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ModelNet40</head><p>In training, we adopt the SGD optimizer, with initial learning rate of 0.002, which is decayed by 0.1 1/200 every epoch. The momentum is 0.98 and weight decay is 0.001. The data is augmented with anisotropic random scaling (from 0.6 to 1.4), and gaussian noise of std = 0.002. We train networks for 600 epochs on 4 GPUs with 16 point clouds per GPU.</p><p>In inference, the model of the last epoch is used. We follow a common practice of voting scheme <ref type="bibr">[22,?,16,30]</ref>, which augment each shape 100 times using the same augmentation method in training, and the predicted logits (the values before SoftMax) of these 100 augmented shapes are averaged to produce the final proabalities.</p><p>S3DIS Following <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref>, we use 3 color channels as features. In training, we adopt the SGD optimizer, with initial learning rate is 0.02, which is decayed by 0.1 1/200 every epoch. The momentum is 0.98 and weight decay is 0.001. The data is augmented with anisotropic random scaling (from 0.7 to 1.3), gaussian noise of std = 0.001, random rotations around z-axis, random droping colors with 0.2 probability. The networks are trained for 600 epochs, using 4 GPUs and 8 point clouds per GPU.</p><p>In inference, the model of the last epoch is used. We divide each point cloud into regular overlaped spheres (totally 100). A point may appear in multiple spheres, and its logit (before SoftMax) is set as the average of this point's logits in different spheres.</p><p>PartNet In training, we adopt the AdamW optimizer [?] with learning rate of 0.000625. The momentum is 0.98 and the weight decay is 0.001. The data is augmented with anisotropic random scaling (from 0.8 to 1.2), and gaussian noise of std = 0.001. The networks are trained for 300 epochs on 4 GPUs with 8 point clouds per GPU.</p><p>In inference, the model of the last epoch is used. We adopt the 10-augment voting scheme (using the same augmentation method as in training) to compute each point's probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2 Detailed Experimental Settings for Section 6.2</head><p>In Section 6.2 of the main paper, we evaluate different methods with varying architecture width, depth and bottleneck ratios. In this section, we provide detailed experimental settings as below.</p><p>For the experiments of varying architecture widths (see <ref type="figure" target="#fig_7">Fig. 3</ref> left column of the main paper), we fix the depth and bottleneck ratio as N r + 1 = 1 and ? = 2, respectively. For the experiments of varying architecture depths (see <ref type="figure" target="#fig_7">Fig.  3</ref> middle column of the main paper), we fix the width and bottleneck ratio as C = 36 and ? = 2, respectively. For the experiments of varying architecture bottleneck ratios (see <ref type="figure" target="#fig_7">Fig. 3</ref> right column of the main paper), we fix the width and depth as C = 144 and N r + 1 = 1, respectively.</p><p>For different methods, the designing settings are as follows:</p><p>Angle Angle and Gaussian Inversed Distance The above variants encourage the distant points to have larger amplitudes of encoding scalars. Here we present a variant which encourages close points to have larger amplitudes of encoding scalars, by inverse the distance by a Gaussian function:</p><formula xml:id="formula_11">e 0 =dij = exp ?d 2 ij , e 1 = ?xij dij , e 2 = ?yij dij , e 3 = ?zij dij .<label>(15)</label></formula><p>Angle or Distance Alone We also consider variants which use angle or distance functions alone:</p><formula xml:id="formula_12">e 0 = ?xij dij , e 1 = ?yij dij , e 2 = ?zij dij .<label>(16)</label></formula><formula xml:id="formula_13">e 0 = dij.<label>(17)</label></formula><formula xml:id="formula_14">e 0 =dij.<label>(18)</label></formula><p>Results. <ref type="table" target="#tab_5">Table 4</ref> shows the comparison of different variants using three benchmarks. For PartNet datasets, we report the part category mean IoU on the validation set. While PosPool adopts AVG as the default reduction function, we also report the results when using other reduction function (SUM, MAX). It can be seen: 1) all variants containing full configurations of relative positions perform similarly well. They perform significantly better than the variants using angle or distance alone. 2) Whether more distant points have larger or smaller encoding amplitudes than closer points is insignificant. 3) Using AVG as the reduction function performs comparably well than those using SUM, and slightly better than those using MAX.   varying noise ratios, the proposed PosPool operator performs best, slightly better than AdaptWeight and PointMLP, and significantly better than PseudoGrid and the MaxPool baseline. <ref type="figure">Fig. 6</ref> show the activation maps of the last layer in each stage by using clean data (top row) and noisy data (bottom row, ratio 1%), respectively. While the noisy point features significantly contaminate the activations of some other regular point features in the MaxPool and PseudoGrid methods, the activations of clean points in other methods are less affected by these noisy points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4 The Robustness of Different Operators with Missing/Noisy Points</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A5 More Detailed Results on PartNet</head><p>We first report the part-category mIoU for each category on PartNet. From <ref type="table">Table 5</ref> and <ref type="table">Table 6</ref> we can see that all operators show similar results on each category, which further validates our findings. <ref type="table">Table 7</ref> shows the number of training, validation, test samples. We then show some qualitative results in <ref type="figure">Fig 7.</ref> All the representative methods perform similarly well on most shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A6 Detailed Space and Time complexity Analysis</head><p>In this section, we provide detailed analysis for the space and time complexity of different aggregators presented in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A6.1 Point-wise MLP based Methods</head><p>The detailed architecture is shown in <ref type="figure">Fig 8.</ref> For h = 1, a shared FC is applied on each point with K neighborhoods, and the time cost is (d + 3)dnK and the space cost (parameter number) is (d + 3)d. For h ? 2, the time cost is </p><p>and the space cost is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A6.2 Pseudo Grid Feature based Methods</head><p>Our default settings adopt depth-wise convolution. In depth-wise convolution, a d-dim learnt weight vector is associated to each grid point. Hence, the space cost (parameter number) is d ? M and the time cost is ndKM .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A6.3 Adaptive Weight based Methods</head><p>Adaptive Weight based Methods involve two computation steps. Firstly, a shared MLP is used to compute the aggregation weights for each neighboring point. This step has time cost of 3 ? (d/2)nK + (h ? 2) ? (d/2)(d/2)nK + (d/2)dnK =</p><formula xml:id="formula_16">(3 + d + (h ? 2)d/2) ? d/2 ? nK,<label>(21)</label></formula><p>and space cost of </p><p>Secondly, depth-wise aggregation is conducted, where the time cost is dnK and space cost is 0. The total time cost is (3 + d + (h ? 2)d/2) ? d/2 ? nK + dnK = ((h ? 2)d/2 + d + 5) ? d/2 ? nK and the total space cost is ((h ? 2)d/2 + d + 3) ? d/2. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The space and time complexity of this method are O(space) = ((h ? 2)d/2 + d + 3) ? d/2 and O(time) = ((h ? 2)d/2 + d + 5) ? d/2 ? nK, respectively, when an inter-mediate dimesion of d/2 is used and the number of FC layers h ? 2. When h = 1, the space and computation complexity is much smaller, as O(space) = 3d and O(time) = 5dnK, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of the proposed position pooling (PosPool) operator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>E</head><label></label><figDesc>m (x, y, z) =[sin(100x/1000 6m/d , cos(100x/1000 6m/d ), sin(100y/1000 6m/d , cos(100y/1000 6m/d ), sin(100z/1000 6m/d , cos(100z/1000 6m/d )].(9)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 Fig. 3 .</head><label>33</label><figDesc>shows comparison of different local aggregation operators using architectures with different model capacity on three benchmarks, by varying the network width, depth and bottleneck ratio. Detailed experimental settings are presented in Appendix A2. It can be seen: the PosPool operators achieve top or close-to-top (a) Evaluation on ModelNet40 datasets with different width, depth and bottleneck ratio (b) Evaluation on S3DIS datasets with different width, depth and bottleneck ratio (c) Evaluation on PartNet datasets with different width, depth and bottleneck ratio Accuracy of different methods with varying width (C), depth (Nr + 1) and bottleneck ratio (?) on three benchmark datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig 5</head><label>5</label><figDesc>show the accuracy curves of AdaptWeight, PseudoGrid, MaxPool, PosPool, PointMLP for inputs with different ratios of noise or missing points. All the experiments are executed on the PartNet benchmark. The model for each curve is trained on the clean data of PartNet. Only the testing data at the inference stage includes noise and missing points.As shown inFig 5(left), different local aggregation operators (AdaptWeight, PseudoGrid, PointMLP, PosPool) perform similarly in robustness with varying missing point ratios, all significantly better than the MaxPool baseline. With</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>(d + 3 )</head><label>3</label><figDesc>(d/2)nK + (h ? 2) ? (d/2)(d/2)nK + (d/2)dnK = ((2d + 3) + (h ? 2)d/2) ? d/2 ? nK,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>(d + 3 )</head><label>3</label><figDesc>(d/2) + (h ? 2) ? (d/2)(d/2) + (d/2)d = ((2d + 3) + (h ? 2)d/2) ? d/2. (20)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>3 ?</head><label>3</label><figDesc>(d/2) + (h ? 2) ? (d/2)(d/2) + (d/2)d = (3 + d + (h ? 2)d/2) ? d/2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>table method</head><label>method</label><figDesc>91.4 19.4M 1.8G 51.5 18.4M 7.2G 42.5 44.6 18.5M 6.7G baseline ? (AVG, S) 90.7 1.2M 0.1G 50.3 1.1M 0.5G 39.5 40.6 1.1M 0.4G baseline ? (AVG) 91.4 19.4M 1.8G 51.0 18.4M 7.2G 44.2 45.8 18.5M 6.7G baseline ? (MAX, S) 91.5 1.2M 0.1G 57.4 1.1M 0.5G 39.8 41.2 1.1M 0.4G baseline ? (MAX) 91.8 19.4M 1.8G 58.4 18.4M 7.2G 45.4 47.4 18.5M 6.7G point-wise MLP (S) 92.6 1.7M 0.2G 56.7 1.6M 0.8G 45.3 47.0 1.6M 0.7G point-wise MLP 92.8 26.5M 2.7G 66.2 25.5M 9.8G 48.1 51.5 25.6M 9.1G pseudo grid (S) 92.3 1.2M 0.3G 64.3 1.2M 1.0G 44.2 45.2 1.2M 0.9G pseudo grid 93.0 19.5M 2.0G 65.9 18.5M 9.3G 50.8 53.0 18.5M 8.5G adapt weights (S) 92.1 1.2M 0.2G 61.9 1.2M 0.6G 44.1 46.1 1.2M 0.5G adapt weights 93.0 19.4M 2.3G 66.5 18.4M 7.8G 50.1 53.5 18.5M 7.2G PosPool (S) 92.5 1.2M 0.1G 64.2 1.1M 0.5G 44.6 47.2 1.1M 0.5G PosPool 92.9 19.4M 1.8G 66.5 18.4M 7.3G 50.0 53.4 18.5M 6.8G PosPool</figDesc><table><row><cell></cell><cell>ModelNet40</cell><cell></cell><cell>S3DIS</cell><cell></cell><cell></cell><cell></cell><cell>PartNet</cell><cell></cell></row><row><cell></cell><cell cols="8">acc param FLOP mIoU param FLOP val test param FLOP</cell></row><row><cell>DensePoint [15]</cell><cell>93.2 0.7M 0.7G</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>KPConv [30]</cell><cell cols="5">92.9 15.2M 1.7G 65.7 15.0M 6.5G -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PointCNN [14]</cell><cell cols="8">92.5 0.6M 25.3G 65.4 4.4M 36.7G -46.4 4.4M 23.1G</cell></row><row><cell>baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>** (S) 92.6 1.2M 0.1G 61.3 1.1M 0.5G 46.1 47.2 1.1M 0.5G PosPool* 93.2 19.4M 1.8G 66.7 18.4M 7.3G 50.6 53.8 18.5M 6.8G</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Evaluating different settings of the point-wise MLP method. The option ?, , 2 and 3 denote input features using {?pij, fj}, {fi, ?fij}, {?pij, fi, ?fij}, and {?pij, fi, fj, ?fij}, respectively. "Sweet spot" denotes balanced settings regarding both efficacy and efficiency. The accuracy on PartNet test set is not tested in ablations to avoid the tuning of test set</figDesc><table><row><cell>method</cell><cell>?</cell><cell>?</cell><cell cols="3">input #FC R(?) ModelNet40 S3DIS 2 3</cell><cell>PartNet (val/test)</cell></row><row><cell cols="2">PointNet++ [22] -</cell><cell></cell><cell>3 MAX</cell><cell>90.7</cell><cell>-</cell><cell>-/42.5</cell></row><row><cell cols="2">PointNet++* -</cell><cell></cell><cell>3 MAX</cell><cell>91.6</cell><cell cols="2">55.3 43.1/45.3</cell></row><row><cell>sweet spot</cell><cell>8 2</cell><cell></cell><cell>1 MAX 1 MAX</cell><cell>92.8 92.8</cell><cell cols="2">62.9 48.2/50.8 66.2 48.1/51.2</cell></row><row><cell>FC num</cell><cell>8 8</cell><cell></cell><cell>2 MAX 3 MAX</cell><cell>92.5 92.0</cell><cell>59.5 59.9</cell><cell>47.9/-48.7/-</cell></row><row><cell></cell><cell>8</cell><cell></cell><cell>1 MAX</cell><cell>92.6</cell><cell>59.8</cell><cell>47.1/-</cell></row><row><cell>input</cell><cell>8</cell><cell></cell><cell>1 MAX</cell><cell>92.5</cell><cell>61.4</cell><cell>47.6/-</cell></row><row><cell></cell><cell>8</cell><cell></cell><cell>1 MAX</cell><cell>92.7</cell><cell>51.0</cell><cell>47.9/-</cell></row><row><cell>reduction R(?)</cell><cell>8 8</cell><cell></cell><cell>1 AVG 1 SUM</cell><cell>92.3 92.2</cell><cell>55.1 44.7</cell><cell>46.8/-46.7/-</cell></row></table><note>operators mostly perform marginally worse than the previous best performing methods on the three datasets. The baseline ? operator using a MAX pooling layer even slightly outperforms the previous state-of-the-art with smaller computation FLOPs (47.4 mIoU, 6.7G FLOPs vs. 46.4 mIoU, 23.1G FLOPs).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The accuracy on PartNet test set is not tested for ablations to avoid tuning the test set.</figDesc><table><row><cell cols="2">method ?</cell><cell>input {dp} {df } {dp, df } {dp  *  }</cell><cell cols="3">#FC R(?) S.M. ModelNet S3DIS</cell><cell>PartNet (val)</cell></row><row><cell cols="2">PConv[34] -</cell><cell></cell><cell>2 SUM</cell><cell>-</cell><cell>58.3</cell><cell>-</cell></row><row><cell cols="2">FlexConv[5] -</cell><cell></cell><cell>1 SUM</cell><cell>90.2</cell><cell>56.6</cell><cell>-</cell></row><row><cell cols="2">sweet spot 8</cell><cell></cell><cell>1 AVG</cell><cell>92.7</cell><cell>62.6</cell><cell>50.0</cell></row><row><cell cols="2">sweet spot* 2</cell><cell></cell><cell>1 AVG</cell><cell>93.0</cell><cell>66.5</cell><cell>50.1</cell></row><row><cell>FC num</cell><cell>8 8</cell><cell></cell><cell>2 AVG 3 AVG</cell><cell>92.6 92.5</cell><cell>61.3 58.5</cell><cell>49.9 49.6</cell></row><row><cell></cell><cell>8</cell><cell></cell><cell>1 AVG</cell><cell>85.3</cell><cell>46.6</cell><cell>46.9</cell></row><row><cell>input</cell><cell>8</cell><cell></cell><cell>1 AVG</cell><cell>82.2</cell><cell>55.7</cell><cell>46.4</cell></row><row><cell></cell><cell>8</cell><cell></cell><cell>1 AVG</cell><cell>92.1</cell><cell>57.0</cell><cell>49.1</cell></row><row><cell>reduction</cell><cell>8 8</cell><cell></cell><cell>1 SUM 1 MAX</cell><cell>92.6 92.4</cell><cell>61.7 62.3</cell><cell>49.1 49.7</cell></row><row><cell cols="2">SoftMax 8</cell><cell></cell><cell>1 AVG</cell><cell>91.7</cell><cell>55.9</cell><cell>45.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>and Distance In this variant, we decouple the relative position into distance d ij = ?x 2 ij + ?y 2 ij + ?z 2 ij and angle</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">?xij dij ,</cell><cell cols="2">?yij dij ,</cell><cell>?zij dij</cell><cell>. The encoding</cell></row><row><cell>functions are:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>e 0 = dij, e 1 =</cell><cell>?xij dij</cell><cell>, e 2 =</cell><cell>?yij dij</cell><cell cols="2">, e 3 =</cell><cell>?zij dij</cell><cell>.</cell><cell>(14)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Evaluation of different PosPool variants on three benchmarks. For PartNet datasets, the part category mean IoU on validation set are reported method ? C Nr + 1 R(?) ModelNet40 S3DIS PartNet The detailed architecture for Point-wise MLP based operators.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>AVG</cell><cell>93.0</cell><cell>64.2</cell><cell>48.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4 144</cell><cell>2</cell><cell>AVG</cell><cell>92.8</cell><cell>65.1</cell><cell>50.0</cell></row><row><cell cols="6">?xij, ?yij, ?zij</cell><cell>2 144</cell><cell>2</cell><cell>AVG</cell><cell>93.1</cell><cell>66.6</cell><cell>49.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>SUM</cell><cell>92.8</cell><cell>64.6</cell><cell>48.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>MAX</cell><cell>92.6</cell><cell>61.1</cell><cell>48.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>AVG</cell><cell>92.7</cell><cell>62.2</cell><cell>49.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4 144</cell><cell>2</cell><cell>AVG</cell><cell>93.1</cell><cell>63.6</cell><cell>50.9</cell></row><row><cell></cell><cell></cell><cell cols="2">E m</cell><cell></cell><cell></cell><cell>2 144</cell><cell>2</cell><cell>AVG</cell><cell>93.2</cell><cell>64.9</cell><cell>50.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>SUM</cell><cell>92.8</cell><cell>62.9</cell><cell>49.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>MAX</cell><cell>92.4</cell><cell>62.8</cell><cell>48.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>AVG</cell><cell>93.0</cell><cell>63.4</cell><cell>49.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4 144</cell><cell>2</cell><cell>AVG</cell><cell>93.1</cell><cell>64.0</cell><cell>49.9</cell></row><row><cell cols="6">Second Order</cell><cell>2 144</cell><cell>2</cell><cell>AVG</cell><cell>92.9</cell><cell>65.7</cell><cell>50.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>SUM</cell><cell>92.9</cell><cell>64.0</cell><cell>49.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>MAX</cell><cell>92.7</cell><cell>63.3</cell><cell>48.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>AVG</cell><cell>93.2</cell><cell>63.6</cell><cell>49.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4 144</cell><cell>2</cell><cell>AVG</cell><cell>93.3</cell><cell>64.5</cell><cell>50.0</cell></row><row><cell></cell><cell cols="5">Third Order</cell><cell>2 144</cell><cell>2</cell><cell>AVG</cell><cell>93.4</cell><cell>64.7</cell><cell>51.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>SUM</cell><cell>92.7</cell><cell>64.8</cell><cell>47.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>MAX</cell><cell>92.3</cell><cell>62.2</cell><cell>49.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>AVG</cell><cell>92.8</cell><cell>63.5</cell><cell>49.0</cell></row><row><cell>dij,</cell><cell cols="2">?x ij d ij ,</cell><cell cols="2">?y ij d ij ,</cell><cell>?z ij d ij</cell><cell>4 144 2 144 8 144</cell><cell>2 2 2</cell><cell>AVG AVG SUM</cell><cell>93.2 92.9 92.9</cell><cell>65.3 65.6 64.5</cell><cell>48.3 49.8 49.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>MAX</cell><cell>92.7</cell><cell>62.3</cell><cell>48.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>AVG</cell><cell>93.0</cell><cell>64.2</cell><cell>48.6</cell></row><row><cell>dij,</cell><cell cols="2">?x ij d ij ,</cell><cell cols="2">?y ij d ij ,</cell><cell>?z ij d ij</cell><cell>4 144 2 144 8 144</cell><cell>2 2 2</cell><cell>AVG AVG SUM</cell><cell>93.2 93.0 93.0</cell><cell>64.1 64.8 64.3</cell><cell>49.1 49.3 48.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>MAX</cell><cell>92.9</cell><cell>62.3</cell><cell>49.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>AVG</cell><cell>92.1</cell><cell>62.1</cell><cell>46.6</cell></row><row><cell cols="2">?x ij d ij ,</cell><cell cols="2">?y ij d ij ,</cell><cell cols="2">?z ij d ij</cell><cell>4 144 2 144 8 144</cell><cell>2 2 2</cell><cell>AVG AVG SUM</cell><cell>92.1 92.2 91.9</cell><cell>61.8 62.6 60.9</cell><cell>46.5 47.6 46.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>MAX</cell><cell>92.0</cell><cell>61.2</cell><cell>45.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>AVG</cell><cell>90.9</cell><cell>53.3</cell><cell>43.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4 144</cell><cell>2</cell><cell>AVG</cell><cell>91.2</cell><cell>53.1</cell><cell>43.8</cell></row><row><cell></cell><cell></cell><cell cols="2">dij</cell><cell></cell><cell></cell><cell>2 144</cell><cell>2</cell><cell>AVG</cell><cell>90.9</cell><cell>53.4</cell><cell>44.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>SUM</cell><cell>90.7</cell><cell>55.4</cell><cell>43.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>MAX</cell><cell>91.0</cell><cell>56.2</cell><cell>44.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>AVG</cell><cell>90.6</cell><cell>53.7</cell><cell>43.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4 144</cell><cell>2</cell><cell>AVG</cell><cell>90.5</cell><cell>53.0</cell><cell>42.1</cell></row><row><cell></cell><cell></cell><cell cols="2">dij</cell><cell></cell><cell></cell><cell>2 144</cell><cell>2</cell><cell>AVG</cell><cell>90.7</cell><cell>53.4</cell><cell>43.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>SUM</cell><cell>91.1</cell><cell>55.3</cell><cell>45.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8 144</cell><cell>2</cell><cell>MAX</cell><cell>91.7</cell><cell>55.5</cell><cell>43.4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>-PointMLP. We use {?p ij , f i , ?f ij } as input features, MAX pooling as reduction function and 1 FC layer. -PseudoGrid. We use SUM as reduction function and 15 grid points.</p><p>-AdaptWeight. We use {?p ij } as input features, AVG pooling as reduction function and 1 FC layer. -PosPool. We use AVG pooling as reduction function and the computation follows Eq. 8 in our main paper. -PosPool*. We use AVG pooling as reduction function and the computation follows Eq. 9 in our main paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3 More Variants of PosPool</head><p>In this section, we present more variants for PosPool, which all have no learnable weights. We first present a general formulation of these variants:</p><p>where {e 0 , ..., e g?1 } are g scalar encoding functions w.r.t. the relative position;</p><p>are an equal-sized partition of vector f j . In the following, we will present 7 variants by using different encoding functions {e 0 , ..., e g?1 }. Second Order Instead of directly using the 3-dimensional ?p ij as in the standard formulation of Eq. 8 in the main paper, the second-order variant considers 6 additional encoding scalars by squares and pairwise multiplications of relative coordinates, as e 0 = ?xij, e 1 = ?yij, e 2 = ?zij, e 3 = ?x 2 ij , e 4 = ?y 2 ij , e 5 = ?z 2 ij , e 6 = ?xij?yij, e 7 = ?xij?zij, e 8 = ?yij?zij.</p><p>Third Order The third order variant uses additional third-order multiplications as encoding functions: e 0 = ?xij, e 1 = ?yij, e 2 = ?zij, e 3 = ?x 2 ij , e 4 = ?y 2 ij , e 5 = ?z 2 ij , e 6 = ?xij?yij, e 7 = ?xij?zij, e 8 = ?yij?zij, e 9 = ?xij?y 2 ij , e 10 = ?xij?z 2 ij , e 11 = ?yij?z 2 ij , e 12 = ?x 2 ij ?yij, e 13 = ?x 2 ij ?zij, e 14 = ?y 2 ij ?zij, e 15 = ?x <ref type="bibr" target="#b2">3</ref> ij , e 16 = ?y 3 ij , e 17 = ?z 3 ij .</p><p>Note we omit the encoding function ?x ij ?y ij ?z ij to ensure g = 18 such that f j 's channel number C is divisible by g.     </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint 2D-3D-Semantic Data for Indoor Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">112</biblScope>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gvcnn: Group-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="264" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiresolution tree networks for 3d point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Flex-convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Lensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="105" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view 3d object retrieval with deep embedding network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5526" to="5537" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Monte carlo convolution for learning on non-uniformly sampled point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Vzquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">112</biblScope>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="984" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A-cnn: Annularly convolutional neural networks on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Komarichev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7421" to="7430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling local geometric structure of 3d point clouds using geo-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="998" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03751</idno>
		<title level="m">Can gcns go as deep as cnns? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on xtransformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densepoint: Learning densely contextual representation for efficient point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5239" to="5248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interpolated convolutional networks for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1578" to="1587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for realtime object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3577" to="3586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation. MICCAI p</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">234241</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2088" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic classification of 3d point clouds with multiscale spherical neighborhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Legall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10296" to="10305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2589" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shellnet: Efficient point cloud convolutional neural networks using concentric shells statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Avg Bed Bott Chair Clock Dish Disp Door Ear Fauc Knife Lamp Micro Frid Stora Table Trash Vase</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<title level="m">Table 5. part-category mIoU% on PartNet validation sets. PW, PG, AW, PP, PP * refer to Pseudo Grid, Adapt Weights, PosPool, PosPool * respectively</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Avg Bed Bott Chair Clock Dish Disp Door Ear Fauc Knife Lamp Micro Frid Stora Table Trash Vase</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
