<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">U 2 -Net: Going Deeper with Nested U-Structure for Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
							<email>xuebin@ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
							<email>vincent.zhang@ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
							<email>chuang8@ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
							<email>masood1@ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osmar</forename><forename type="middle">R</forename><surname>Zaiane</surname></persName>
							<email>zaiane@ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">U 2 -Net: Going Deeper with Nested U-Structure for Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we design a simple yet powerful deep network architecture, U 2 -Net, for salient object detection (SOD). The architecture of our U 2 -Net is a two-level nested U-structure. The design has the following advantages: (1) it is able to capture more contextual information from different scales thanks to the mixture of receptive fields of different sizes in our proposed ReSidual U-blocks (RSU), (2) it increases the depth of the whole architecture without significantly increasing the computational cost because of the pooling operations used in these RSU blocks. This architecture enables us to train a deep network from scratch without using backbones from image classification tasks. We instantiate two models of the proposed architecture, U 2 -Net (176.3 MB, 30 FPS on GTX 1080Ti GPU) and U 2 -Net ? (4.7 MB, 40 FPS), to facilitate the usage in different environments. Both models achieve competitive performance on six SOD datasets. The code is available:https: //github.com/NathanUA/U-2-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Salient Object Detection (SOD) aims at segmenting the most visually attractive objects in an image. It is widely used in many fields, such as visual tracking and image segmentation. Recently, with the development of deep convolutional neural networks (CNNs), especially the rise of Fully Convolutional Networks (FCN) <ref type="bibr" target="#b23">[24]</ref> in image segmentation, the salient object detection has been improved significantly. It is natural to ask, what is still missing? Let's take a step back and look at the remaining challenges.</p><p>There is a common pattern in the design of most SOD networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b5">6]</ref>, that is, they focus on making good use of deep features extracted by existing backbones, such as Alexnet <ref type="bibr" target="#b16">[17]</ref>, VGG <ref type="bibr" target="#b34">[35]</ref>, ResNet <ref type="bibr" target="#b11">[12]</ref>, ResNeXt <ref type="bibr" target="#b43">[44]</ref>, DenseNet <ref type="bibr" target="#b14">[15]</ref>, etc. However, these backbones are all originally designed for image classification. They extract features that are representative of semantic meaning rather than local details and global contrast information, which are essential to saliency detection. And they need to be pre- <ref type="figure">Figure 1</ref>. Comparison of model size and performance of our U 2 -Net with other state-of-the-art SOD models. The maxF ? measure is computed on dataset ECSSD <ref type="bibr" target="#b45">[46]</ref>. The red star denotes our U 2 -Net (Ours) (176.3 MB) and the blue star denotes our small version U 2 -Net ? (Ours ? ) (4.7 MB). trained on ImageNet <ref type="bibr" target="#b4">[5]</ref> data which is data-inefficient especially if the target data follows a different distribution than ImageNet.</p><p>This leads to our first question: can we design a new network for SOD, that allows training from scratch and achieves comparable or better performance than those based on existing pre-trained backbones?</p><p>There are a few more issues on the network architectures for SOD. First, they are often overly complicated <ref type="bibr" target="#b57">[58]</ref>. It is partially due to the additional feature aggregation modules that are added to the existing backbones to extract multilevel saliency features from these backbones. Secondly, the existing backbones usually achieve deeper architecture by sacrificing high resolution of feature maps <ref type="bibr" target="#b57">[58]</ref>. To run these deep models with affordable memory and computational cost, the feature maps are down scaled to lower resolution at early stages. For instance, at the early layers of both ResNet and DenseNet <ref type="bibr" target="#b14">[15]</ref>, a convolution with stride of two followed by a maxpooling with stride of two are utilized to reduce the size of the feature maps to one fourth of the input maps. However, high resolution also plays an important role in segmentation besides the deep architecture <ref type="bibr" target="#b20">[21]</ref>.</p><p>Hence, our follow-up question is: can we go deeper while maintaining high resolution feature maps, at a low memory and computation cost?</p><p>Our main contribution is a novel and simple network architecture, called U 2 -Net, that addresses the two questions above. First, U 2 -Net is a two-level nested U-structure that is designed for SOD without using any pre-trained backbones from image classification. It can be trained from scratch to achieve competitive performance. Second, the novel architecture allows the network to go deeper, attain high resolution, without significantly increasing the memory and computation cost. This is achieved by a nested U-structure: on the bottom level, we design a novel ReSidual U-block (RSU), which is able to extract intra-stage multi-scale features without degrading the feature map resolution; on the top level, there is a U-Net like structure, in which each stage is filled by a RSU block. The two-level configuration results in a nested U-structure (see <ref type="figure">Fig. 5</ref>). Our U 2 -Net (176.3 MB) achieves competitive performance against the state-of-theart (SOTA) methods on six public datasets, and runs at realtime (30 FPS, with input size of 320?320?3) on a 1080Ti GPU. To facilitate the usage of our design in computation and memory constrained environments, we provide a small version of our U 2 -Net, called U 2 -Net ? (4.7 MB). The U 2 -Net ? achieves competitive results against most of the SOTA models (see <ref type="figure">Fig. 1</ref>) at 40 FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In recent years, many deep salient object detection networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33]</ref> have been proposed. Compared with traditional methods <ref type="bibr" target="#b1">[2]</ref> based on hand-crafted features like foreground consistency <ref type="bibr" target="#b48">[49]</ref>, hyperspectral information <ref type="bibr" target="#b19">[20]</ref>, superpixels' similarity <ref type="bibr" target="#b54">[55]</ref>, histograms <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25]</ref> and so on, deep salient object detection networks show more competitive performance.</p><p>Multi-level deep feature integration: Recent works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">45]</ref> have shown that features from multiple deep layers are able to generate better results <ref type="bibr" target="#b49">[50]</ref>. Then, many strategies and methods for integrating and aggregating multilevel deep features are developed for SOD. Li et al. (MDF) <ref type="bibr" target="#b17">[18]</ref> propose to feed an image patch around a target pixel to a network and then obtain a feature vector for describing the saliency of this pixel. <ref type="bibr">Zhang</ref>   <ref type="bibr" target="#b41">[42]</ref> improve the saliency detection accuracy by developing a novel Mutual Learning Module for better leveraging the correlation of boundaries and regions. Wu et al. <ref type="bibr" target="#b42">[43]</ref> propose to use Cascaded Partial Decoder (CPD) framework for fast and accurate salient object detection. Deep methods in this category take advantage of the multi-level deep features extracted by backbone networks and greatly raise the bar of salient object detection against traditional methods.</p><p>Multi-scale feature extraction: As mentioned earlier, saliency detection requires both local and global information. A 3 ? 3 filter is good for extracting local features at each layer. However, it is difficult to extract global information by simply enlarging the filter size because it will increase the number of parameters and computation costs dramatically. Many works pay more attention to extracting global context. Wang et al. (SRM) <ref type="bibr" target="#b39">[40]</ref> adapt the pyramid pooling module <ref type="bibr" target="#b56">[57]</ref> to capture global context and propose a multi-stage refinement mechanism for saliency maps refinement. Zhang et al. (PAGRN) <ref type="bibr" target="#b55">[56]</ref> develop a spatial and a channel-wise attention module to obtain the global information of each layer and propose a progressive attention guidance mechanism to refine the saliency maps. Wang et al. (DGRL) <ref type="bibr" target="#b40">[41]</ref> develop an inception-like <ref type="bibr" target="#b35">[36]</ref> contextual weighting module to localize salient objects globally and then use a boundary refinement module to refine the saliency map locally. <ref type="bibr">Liu</ref>   <ref type="bibr" target="#b8">[9]</ref> develop a global perception module and attentive feedback modules to better explore the structure of salient objects. <ref type="bibr">Qin et al. (BASNet)</ref>  <ref type="bibr" target="#b32">[33]</ref> propose a predictrefine model by stacking two differently configured U-Nets  <ref type="bibr" target="#b21">[22]</ref> develop encoderdecoder architecture for salient object detection by introducing a global guidance module for extraction of global localization features and a multi-scale feature aggregation module adapted from pyramid pooling module for fusing global and fine-level features. In these methods, many inspiring modules are proposed to extract multi-scale features from multi-level deep features extracted from existing backbones. Diversified receptive fields and richer multi-scale contextual features introduced by these novel modules significantly improve the performance of salient object detection models.</p><p>In summary, multi-level deep feature integration methods mainly focus on developing better multi-level feature aggregation strategies. On the other hand, methods in the category of multi-scale feature extraction target at designing new modules for extracting both local and global information from features obtained by backbone networks. As we can see, almost all of the aforementioned methods try to make better use of feature maps generated by the existing image classification backbones. Instead of developing and adding more complicated modules and strategies to use these backbones' features, we propose a novel and simple architecture, which directly extracts multi-scale features stage by stage, for salient object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>First, we introduce the design of our proposed residual U-block and then describe the details of the nested Uarchitecture built with this block. The network supervision strategy and the training loss are described at the end of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Residual U-blocks</head><p>Both local and global contextual information are very important for salient object detection and other segmentation tasks. In modern CNN designs, such as VGG, ResNet, DenseNet and so on, small convolutional filters with size of 1?1 or 3?3 are the most frequently used components for feature extraction. They are in favor since they require less storage space and are computationally efficient. <ref type="figure" target="#fig_0">Figures 2</ref>(a)-(c) illustrates typical existing convolution blocks with small receptive fields. The output feature maps of shallow layers only contain local features because the receptive field of 1?1 or 3?3 filters are too small to capture global information. To achieve more global information at high resolution feature maps from shallow layers, the most direct idea is to enlarge the receptive field. <ref type="figure" target="#fig_0">Fig. 2 (d)</ref> shows an inception like block <ref type="bibr" target="#b49">[50]</ref>, which tries to extract both local and non-local features by enlarging the receptive fields using dilated convolutions <ref type="bibr" target="#b2">[3]</ref>. However, conducting multiple dilated convolutions on the input feature map (especially in the early stage) with original resolution requires too much computation and memory resources. To decrease the computation costs, PoolNet <ref type="bibr" target="#b21">[22]</ref> adapt the parallel configura- tion from pyramid pooling modules (PPM) <ref type="bibr" target="#b56">[57]</ref>, which uses small kernel filters on the downsampled feature maps other than the dilated convolutions on the original size feature maps. But fusion of different scale features by direct upsampling and concatenation (or addition) may lead to degradation of high resolution features.</p><p>Inspired by U-Net <ref type="bibr" target="#b33">[34]</ref>, we propose a novel ReSidual Ublock, RSU, to capture intra-stage multi-scale features. The structure of RSU-L(C in , M, C out ) is shown in <ref type="figure" target="#fig_0">Fig. 2</ref></p><formula xml:id="formula_0">(e),</formula><p>where L is the number of layers in the encoder, C in , C out denote input and output channels, and M denotes the number of channels in the internal layers of RSU. Hence, our RSU mainly consists of three components: (i) an input convolution layer, which transforms the input feature map x (H ?W ?C in ) to an intermediate map F 1 (x) with channel of C out . This is a plain convolutional layer for local feature extraction.</p><p>(ii) a U-Net like symmetric encoder-decoder structure with height of L which takes the intermediate feature map F 1 (x) as input and learns to extract and encode the multi-scale contextual information U(F 1 (x)). U represents the U-Net like structure as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>(e). Larger L leads to deeper residual U-block (RSU), more pooling operations, larger range of receptive fields and richer local and global features. Configuring this parameter enables extraction of multi-scale features from input feature maps with arbitrary spatial resolutions. The multi-scale features are extracted from gradually downsampled feature maps and encoded into high resolution feature maps by progressive upsampling, concatenation and convolution. This process mitigates the loss of fine details caused by direct upsampling with large scales. (iii) a residual connection which fuses local features and the multi-scale features by the summation:</p><formula xml:id="formula_1">F 1 (x) + U(F 1 (x)).</formula><p>To better illustrate the intuition behind our design, we compare our residual U-block (RSU) with the original residual block <ref type="bibr" target="#b11">[12]</ref> in <ref type="figure" target="#fig_1">Fig. 3</ref>. The operation in the residual block can be summarized as H(x) = F 2 (F 1 (x))+x, where H(x) denotes the desired mapping of the input features x; F 2 , F 1 stand for the weight layers, which are convolution  <ref type="figure" target="#fig_0">Fig. 2</ref>: the computation costs are calculated based on transferring an input feature map with dimension 320 ? 320 ? 3 to a 320 ? 320?64 output feature map. "PLN", "RES", "DSE", "INC" and "RSU" denote plain convolution block, residual block, dense block, inception block and our residual U-block respectively.</p><p>operations in this setting. The main design difference between RSU and residual block is that RSU replaces the plain, single-stream convolution with a U-Net like structure, and replace the original feature with the local feature transformed by a weight layer:</p><formula xml:id="formula_2">H RSU (x) = U(F 1 (x)) + F 1 (x),</formula><p>where U represents the multi-layer U-structure illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>(e). This design change empowers the network to extract features from multiple scales directly from each residual block. More notably, the computation overhead due to the U-structure is small, since most operations are applied on the downsampled feature maps. This is illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>, where we show the computation cost comparison between RSU and other feature extraction modules in <ref type="figure" target="#fig_0">Fig. 2</ref> (a)-(d). The FLOPs of dense block (DSE), inception block (INC) and RSU all grow quadratically with the number of internal channel M . But RSU has a much smaller coefficient on the quadratic term, leading to an improved efficiency. Its computational overhead compared with plain convolution (PLN) and residual block (RES) blocks, which are both linear w.r.t. M , is not significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture of U 2 -Net</head><p>Stacking multiple U-Net-like structures for different tasks has been explored for a while. , e.g. stacked hourgalss network <ref type="bibr" target="#b30">[31]</ref>, DocUNet <ref type="bibr" target="#b27">[28]</ref>, CU-Net <ref type="bibr" target="#b37">[38]</ref> for pose estimation, etc. These methods usually stack U-Net-like structures sequentially to build cascaded models and can be summarized as "(U?n-Net)", where n is the number of repeated U-Net modules. The issue is that the computation and the memory costs get magnified by n. <ref type="figure">Figure 5</ref>. Illustration of our proposed U 2 -Net architecture. The main architecture is a U-Net like Encoder-Decoder, where each stage consists of our newly proposed residual U-block (RSU). For example, En 1 is based on our RSU block shown in <ref type="figure" target="#fig_0">Fig. 2</ref>(e). Detailed configuration of RSU block of each stage is given in the last two rows of <ref type="table">Table 1</ref>.</p><p>In this paper, we propose a different formulation, U n -Net, of stacking U-structure for salient object detection. Our exponential notation refers to nested U-structure rather than cascaded stacking. Theoretically, the exponent n can be set as an arbitrary positive integer to achieve single-level or multi-level nested U-structure. But architectures with too many nested levels will be too complicated to be implemented and employed in real applications.</p><p>Here, we set n as 2 to build our U 2 -Net. Our U 2 -Net is a two-level nested U-structure shown in <ref type="figure">Fig. 5</ref>. Its top level is a big U-structure consists of 11 stages (cubes in <ref type="figure">Fig. 5</ref>). Each stage is filled by a well configured residual Ublock (RSU) (bottom level U-structure). Hence, the nested U-structure enables the extraction of intra-stage multi-scale features and aggregation of inter-stage multi-level features more efficiently.</p><p>As illustrated in <ref type="figure">Fig.5</ref>, the U 2 -Net mainly consists of three parts: (1) a six stages encoder, (2) a five stages decoder and (3) a saliency map fusion module attached with the decoder stages and the last encoder stage: (i) In encoder stages En 1, En 2, En 3 and En 4, we use residual U-blocks RSU-7, RSU-6, RSU-5 and RSU-4, respectively. As mentioned before, "7", "6", "5" and "4" denote the heights (L) of RSU blocks. The L is usually configured according to the spatial resolution of the input feature maps. For feature maps with large height and width, we use greater L to capture more large scale information. The resolution of feature maps in En 5 and En 6 are relatively low, further downsampling of these feature maps leads to loss of useful context. Hence, in both En 5 and En 6 stages, RSU- <ref type="table">Table 1</ref>. Detailed configurations of different architectures used in ablation study. "PLN", "RES", "DSE", "INC", "PPM" and "RSU" denote plain convolution block, residual block, dense block, inception block, Pyramid Pooling Module and our residual U-block respectively. "NIV U 2 -Net" denotes U-Net with its each stage replaced by a naive U-Net block. "I", "M" <ref type="bibr">En</ref>  4F are used, where "F" means that the RSU is a dilated version, in which we replace the pooling and upsampling operations with dilated convolutions (see <ref type="figure">Fig. 5</ref>). That means all of intermediate feature maps of RSU-4F have the same resolution with its input feature maps.</p><p>(ii) The decoder stages have similar structures to their symmetrical encoder stages with respect to En 6. In De 5, we also use the dilated version residual U-block RSU-4F which is similar to that used in the encoder stages En 5 and En 6.</p><p>Each decoder stage takes the concatenation of the upsampled feature maps from its previous stage and those from its symmetrical encoder stage as the input, see <ref type="figure">Fig. 5</ref>.</p><p>(iii) The last part is the saliency map fusion module which is used to generate saliency probability maps. Similar to HED <ref type="bibr" target="#b44">[45]</ref>, our U 2 -Net first generates six side output saliency probability maps S</p><p>side , S</p><p>side , S</p><p>side , S</p><p>side , S</p><p>side , S In summary, the design of our U 2 -Net allows having deep architecture with rich multi-scale features and relatively low computation and memory costs. In addition, since our U 2 -Net architecture is only built upon our RSU blocks without using any pre-trained backbones adapted from image classification, it is flexible and easy to be adapted to different working environments with insignificant performance loss. In this paper, we provide two instances of our U 2 -Net by using different configurations of filter numbers: a normal version U 2 -Net (176.3 MB) and a relatively smaller version U 2 -Net ? (4.7 MB). Detailed configurations are presented in the last two rows of Table 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Supervision</head><p>In the training process, we use deep supervision similar to HED <ref type="bibr" target="#b44">[45]</ref>. Its effectiveness has been proven in HED and DSS. Our training loss is defined as:</p><formula xml:id="formula_8">L = M m=1 w (m) side (m) side + w f use f use (1)</formula><p>where (m) side (M = 6, as the Sup1, Sup2, ? ? ? , Sup6 in <ref type="figure">Fig.  5</ref>) is the loss of the side output saliency map S (m) side and f use (Sup7 in <ref type="figure">Fig. 5</ref>) is the loss of the final fusion output saliency map S f use . w <ref type="bibr">(m)</ref> side and w f use are the weights of each loss term. For each term , we use the standard binary crossentropy to calculate the loss:</p><formula xml:id="formula_9">= ? (H,W ) (r,c) [P G(r,c) logP S(r,c) + (1 ? P G(r,c) )log(1 ? P S(r,c) )]<label>(2)</label></formula><p>where (r, c) is the pixel coordinates and (H, W ) is image size: height and width. P G(r,c) and P S(r,c) denote the pixel values of the ground truth and the predicted saliency probability map, respectively. The training process tries to minimize the overall loss L of Eq. (1). In the testing process, we choose the fusion output f use as our final saliency map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Training dataset: We train our network on DUTS-TR, which is a part of DUTS dataset <ref type="bibr" target="#b38">[39]</ref>. DUTS-TR contains 10553 images in total. Currently, it is the largest and most frequently used training dataset for salient object detection. We augment this dataset by horizontal flipping to obtain 21106 training images offline.</p><p>Evaluation datasets: Six frequently used benchmark datasets are used to evaluate our method including: DUT-OMRON <ref type="bibr" target="#b46">[47]</ref>, DUTS-TE <ref type="bibr" target="#b38">[39]</ref>, HKU-IS <ref type="bibr" target="#b17">[18]</ref>, ECSSD <ref type="bibr" target="#b45">[46]</ref>, PASCAL-S <ref type="bibr" target="#b18">[19]</ref>, SOD <ref type="bibr" target="#b29">[30]</ref>. DUT-OMRON includes 5168 images, most of which contain one or two structurally complex foreground objects. DUTS dataset consists of two parts: DUTS-TR and DUTS-TE. As mentioned above we use DUTS-TR for training. Hence, DUTS-TE, which contains 5019 images, is selected as one of our evaluation dataset. HKU-IS contains 4447 images with multiple foreground objects. ECSSD contains 1000 structurally complex images and many of them contain large foreground objects. PASCAL-S contains 850 images with complex foreground objects and cluttered background. SOD only contains 300 images. But it is very challenging. Because it was originally designed for image segmentation and many images are low contrast or contain complex foreground objects overlapping with the image boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>The outputs of the deep salient object methods are usually probability maps that have the same spatial resolution with the input images. Each pixel of the predicted saliency maps has a value within the range of 0 and 1 (or [0, 255]). The ground truth are usually binary masks, in which each pixel is either 0 or 1 (or 0 and 255) where 0 indicates the background pixels and 1 indicates the foreground salient object pixels.</p><p>To comprehensively evaluate the quality of those probability maps against the ground truth, six measures including (1) Precision-Recall (PR) curves , (2) maximal Fmeasure (maxF ? ) <ref type="bibr" target="#b0">[1]</ref> , (3) Mean Absolute Error (M AE) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b21">22]</ref>, (4) weighted F-measure (F w ? ) <ref type="bibr" target="#b28">[29]</ref> , (5) structure measure (S m ) <ref type="bibr" target="#b7">[8]</ref> and <ref type="formula" target="#formula_3">(6)</ref> relaxed F-measure of boundary (relaxF b ? ) <ref type="bibr" target="#b32">[33]</ref> are used: (1) PR curve is plotted based on a set of precision-recall pairs. Given a predicted saliency probability map, its precision and recall scores are computed by comparing its thresholded binary mask against the ground truth mask. The precision and recall of a dataset are computed by averaging the precision and recall scores of those saliency maps. By varying the thresholds from 0 to 1, we can obtain a set of average precision-recall pairs of the dataset.</p><p>(2) F-measure F ? is used to comprehensively evaluate both precision and recall as:</p><formula xml:id="formula_10">F ? = (1+? 2 )?P recision?Recall ? 2 ?P recision+Recall .<label>(3)</label></formula><p>We set the ? 2 to 0.3 and report the maximum F ? (maxF ? ) for each dataset similar to previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>(3) MAE is the Mean Absolute Error which denotes the average per-pixel difference between a predicted saliency map and its ground truth mask. It is defined as:</p><formula xml:id="formula_11">M AE = 1 H?W H r=1 W c=1 |P (r, c) ? G(r, c)| (4)</formula><p>where P and G are the probability map of the salient object detection and the corresponding ground truth respectively, (H, W ) and (r, c) are the (height, width) and the pixel coordinates.</p><p>(4) weighted F-measure (F w ? ) <ref type="bibr" target="#b28">[29]</ref> is utilized as a complementary measure to maxF ? for overcoming the possible unfair comparison caused by "interpolation flaw, dependency flaw and equal-importance flaw" <ref type="bibr" target="#b22">[23]</ref>. It is defined as:</p><formula xml:id="formula_12">F w ? = (1 + ? 2 )</formula><p>P recision w ? Recall w ? 2 ? P recision w + Recall w . (5) (5) S-measure (S m ) is used to evaluate the structure similarity of the predicted non-binary saliency map and the ground truth. The S-measure is defined as the weighted sum of region-aware S r and object-aware S o structural similarity:</p><formula xml:id="formula_13">S = (1 ? ?)S r + ?S o .<label>(6)</label></formula><p>where ? is usually set to 0.5. (6) relax boundary F-measure relaxF b ? <ref type="bibr" target="#b6">[7]</ref> is utilized to quantitatively evaluate boundaries' quality of the predicted saliency maps <ref type="bibr" target="#b32">[33]</ref>. Given a saliency probability map P ? [0, 1], its binary mask P bw is obtained by a simple thresholding operation (threshold is set to 0.5). Then, the XOR(P bw , P erd ) operation is conducted to obtain its one pixel wide boundary, where P erd denotes the eroded binary mask <ref type="bibr" target="#b10">[11]</ref> of P bw . The boundaries of ground truth mask are obtained in the same way. The computation of relaxed boundary F-measure relaxF b ? is similar to equation <ref type="formula" target="#formula_6">(3)</ref>. The difference is that relaxP recision b and relaxRecall b other than P recision and Recall are used in equation <ref type="formula" target="#formula_6">(3)</ref>. The definition of relaxed boundary precision (relaxP recision b ) is the fraction of predicted boundary pixels within a range of ? pixels from ground truth boundary pixels. The relaxed boundary recall (relaxRecall b ) is defined as the fraction of ground truth boundary pixels that are within ? pixels of predicted boundary pixels. The slack parameter ? is set to 3 as in the previous work <ref type="bibr" target="#b32">[33]</ref>. Given a dataset, its average relaxF b ? of all predicted saliency maps is reported in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>In the training process, each image is first resized to 320?320 and randomly flipped vertically and cropped to 288?288. We are not using any existing backbones in our network. Hence, we train our network from scratch and all of our convolutional layers are initialized by Xavier <ref type="bibr" target="#b9">[10]</ref>. The loss weights w (m) side and w f use are all set to 1. Adam optimizer <ref type="bibr" target="#b15">[16]</ref> is used to train our network and its hyper parameters are set to default (initial learning rate lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight decay=0). We train the network until the loss converges without using validation set which follows the previous methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b49">50]</ref>. After 600k iterations (with a batch size of 12), the training loss converges and the whole training process takes about 120 hours. During testing, the input images (H ? W ) are resized to 320?320 and fed into the network to obtain the saliency maps. The predicted saliency maps with size of 320?320 are resized back to the original size of the input image (H ? W ). Bilinear interpolation is used in both resizing processes. Our network is implemented based on Pytorch 0.4.0 <ref type="bibr" target="#b31">[32]</ref>. Both training and testing are conducted on an eight-core, 16 threads PC with an AMD Ryzen 1800x 3.5 GHz CPU (32GB RAM) and a GTX 1080ti GPU (11GB memory). We will release our code later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>To verify the effectiveness of our U 2 -Net, ablation studies are conducted on the following three aspects: i) basic blocks, ii) architectures and iii) backbones. All the ablation studies follow the same implementation setup. <ref type="table">Table 2</ref>. Results of ablation study on different blocks, architectures and backbones. "PLN", "RES", "DSE", "INC", "PPM" and "RSU" denote plain convolution block, residual block, dense block, inception block, pyramid pooling module and our residual U-block respectively. "NIV U 2 -Net" denotes U-Net with its each stage replaced by a naive U-Net block. The "Time (ms)" (ms: milliseconds) costs are computed by averaging the inference time costs of images in ECSSD dataset. Values with bold fonts indicate the best two performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Ablation on Blocks</head><p>In the blocks ablation, the goal is to validate the effectiveness of our newly designed residual U-blocks (RSUs). Specifically, we fix the outside Encoder-Decoder architecture of our U 2 -Net and replace its stages with other popular blocks including plain convolution blocks (PLN), residuallike blocks (RSE), dense-like blocks (DSE), inception-like blocks (INC) and pyramid pooling module (PPM) other than RSU block, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref> (a)-(d). Detailed configurations can be found in <ref type="table">Table 1</ref>. <ref type="table">Table 2</ref> shows the quantitative results of the ablation study. As we can see, the performance of baseline U-Net is the worst, while PLN U-Net, RES U-Net, DES U-Net, INC U-Net and PPM U-Net achieve better performance than the baseline U-Net. Because they are either deeper or have the capability of extracting multi-scale features. However, their performance is still inferior to both our full size U 2 -Net and small version U 2 -Net ? . Particularly, our full size U 2 -Net improves the maxF ? about 3.3% and 1.8%, and decreases the M AE over 12.9% and 21.4% against the second best model (in the blocks ablation study) on DUT-OMRON and ECSSD datasets, respectively. Furthermore, our U 2 -Net and U 2 -Net ? increase the maxF ? by 9.8% and 8.8% and decrease the M AE by 34.1% and 27.0%, which are significant improvements, on DUT-OMRON dataset against the baseline U-Net. On ECSSD dataset, although the maxF ? improvements (5.5%, 4.7%) of our U 2 -Net and U 2 -Net ? against the baseline U-Net is slightly less significant than that on DUT-OMRON, the improvements of M AE are much greater (50.0%, 38.0%). Therefore, we believe that our newly designed residual U-block RSU is better then others in this salient object detection task. Besides, there is no significant time costs increasing of our residual U-block (RSU) based U 2 -Net architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Ablation on Architectures</head><p>As we mentioned above, previous methods usually use cascaded ways to stack multiple similar structures for building more expressive models. One of the intuitions behind this idea is that multiple similar structures are able to refine the results gradually while reducing overfitting. Stacked HourglassNet <ref type="bibr" target="#b30">[31]</ref> and CU-Net <ref type="bibr" target="#b36">[37]</ref> are two representative models in this category. Therefore, we adapted the stacked HourglassNet and CU-Net to compare the performance between the cascaded architectures and our nested architectures. As shown in <ref type="table">Table.</ref> 2, both our full size U 2 -Net and small size model U 2 -Net ? outperform these two cascaded models. It is worth noting the both stacked HourglassNet and CU-Net utilizes improved U-Net-like modules as their stacking sub-models. To further demonstrate the effectiveness of our nested architecture, we also illustrate the performance of an U 2 -Net based on naive U-blocks (NIV) other than our newly proposed residual U-blocks. We can see that the NIV U 2 -Net still achieves better performance than these two cascaded models. In addition, the nested architectures are faster than the cascaded ones. In summary, our nested architecture is able to achieve better performance than the cascaded architecture both in terms of accuracy and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Ablation on Backbones</head><p>Different from the previous salient object detection models which use backbones (e.g. VGG, ResNet, etc.) as their encoders, our newly proposed U 2 -Net architecture is backbone free. To validate the backbone free design, we conduct ablation studies on replacing the encoder part of our full size U 2 -Net with different backbones: VGG16 and ResNet50. Practically, we adapt the backbones (VGG-16 and ResNet-50) by adding an extra stage after their last convolutional stages to achieve the same receptive fields with our original U 2 -Net architecture design. As shown in <ref type="table">Table 2</ref>, the models using backbones and our RSUs as decoders achieve better performance than the previous ablations and comparable performance against our small size U 2 -Net. However, they are still inferior to our full size U 2 -Net. Therefore, we believe that our backbones free design is more competitive than backbones-based design in this salient object detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with State-of-the-arts</head><p>We compare our models (full size U 2 -Net, 176.3 MB and small size U 2 -Net ? , 4.7 MB) with 20 state-of-the-art meth-ods including one AlexNet based model: MDF; 10 VGG based models: UCF, Amulet, NLDF, DSS, RAS, PAGRN, BMPM, PiCANet, MLMS, AFNet; one DenseNet based model MSWS; one ResNeXt based model: R 3 Net; and seven ResNet based models: CapSal, SRM, DGRL, Pi-CANetR, CPD, PoolNet, BASNet. For fair comparison, we mainly use the salient object detection results provided by the authors. For the missing results on certain datasets of some methods, we run their released code with their trained models on their suggested environment settings. <ref type="figure" target="#fig_3">Fig. 6</ref> illustrates the precision-recall curves of our models (U 2 -Net, 176.3 MB and U 2 -Net ? , 4.7 MB) and typical stateof-the-art methods on the six datasets. The curves are consistent with the <ref type="table">Table 3</ref> and 4 which demonstrate the stateof-the-art performance of our U 2 -Net on DUT-OMRON, HKU-IS and ECSSD and competitive performance on other datasets. <ref type="table">Table 3</ref> and 4 compares five (six include the model size) evaluation metrics and the model size of our proposed method with others. As we can see, our U 2 -Net achieves the best performance on datasets DUT-OMRON, HKU-IS and ECSSD in terms of almost all of the five evaluation metrics. On DUTS-TE dataset our U 2 -Net achieves the second best overall performance, which is slightly inferior to PoolNet. On PASCAL-S, the performance of our U 2 -Net is slightly inferior to AFNet, CPD and PoolNet. It is worth noting that U 2 -Net achieves the second best performance in terms of the boundary quality evaluation metric relaxF b ? . On SOD dataset, PoolNet performs the best and our U 2 -Net is the second best in terms of the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Quantitative Comparison</head><p>Our U 2 -Net ? is only 4.7 MB, which is currently the smallest model in the field of salient object detection. With much fewer number of parameters against other models, it still achieves surprisingly competitive performance. Although its performance is not as good as our full size U 2 -Net, its small size will facilitate its applications in many computation and memory constrained environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Qualitative Comparison:</head><p>To give an intuitive understanding of the promising performance of our models, we illustrate the sample results of our models and several other state-of-the-art methods in <ref type="figure" target="#fig_5">Fig. 7</ref>. As we can see, our U 2 -Net and U 2 -Net ? are able to handle different types of targets and produce accurate salient object detection results.</p><p>The 1st and 2nd row of <ref type="figure" target="#fig_5">Fig. 7</ref> show the results of small and large objects. As we can observe, our U 2 -Net and U 2 -Net ? are able to produce accurate results on both small and large objects. Other models either prone to miss the small target or produce large object with poor accuracy. The 3rd row shows the results of target touching image borders. Our   <ref type="figure" target="#fig_3">Figure 6</ref>. Precision-Recall curves of our models and other typical state-of-the-art models on six SOD datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUT-OMRON</head><p>U 2 -Net correctly segments all the regions. Although U 2 -Net ? erroneously segments the bottom right hole, it is still much better than other models. The 4th row demonstrates the performance of models in segmenting targets that con-sists of both large and thin structures. As we can see, most of other models extract large regions well while missing the cable-wise thin structure except for AFNet (col (j)). The 5th row shows a tree with relatively clean background of blue sky. It seems easy, but it is actually challenging to most of the models because of the complicated shape of the target. As we can see, our models segment both the trunk and branches well, while others fail in segmenting the complicated tree branch region. Compared with the 5th row, the bench shown in the 6th row is more complex thanks to the hollow structure. Our U 2 -Net produces near perfect result.</p><p>Although the bottom right of the prediction map of U 2 -Net ? is imperfect, its overall performance on this target is much better than other models. Besides, the results of our models are more homogenous with fewer gray areas than models like PoolNet (col (f)), CPD (col (g)), PiCANetR (col (h)) and AFNet (col (j)). The 7th row shows that our models can produce results even finer than the ground truth. Labeling these small holes in the 7th image is burdensome and time-consuming. Hence, these repeated fine structures are usually ignored in the annotation process. Inferring the correct results from these imperfect labeling is challenging. But our models show promising capability in segmenting these fine structures thanks to the well designed architectures for extracting and integrating high resolution local and low resolution global information. The 8th and 9th row are illustrated to show the strong ability of our models in detecting targets with cluttered backgrounds and complicated foreground appearance. The 10th row shows that our models are able to segment multiple targets while capturing the details of the detected targets (see the gap region of the two pieces of sail of each sailboat). In summary, both our full size and small size models are able to handle various scenarios and produce high accuracy salient object detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed a novel deep network: U 2 -Net, for salient object detection. The main architecture of our U 2 -Net is a two-level nested U-structure. The nested Ustructure with our newly designed RSU blocks enables the network to capture richer local and global information from both shallow and deep layers regardless of the resolutions.</p><p>Compared with those SOD models built upon the existing backbones, our U 2 -Net is purely built on the proposed RSU blocks which makes it possible to be trained from scratch and configured to have different model size according to the target environment constraints. We provide a full size U 2 -Net (176.3 MB, 30 FPS) and a smaller size version U 2 -Net ? (4.7 MB, 40 FPS) in this paper. Experimental results on six public salient object detection datasets demonstrate that both models achieve very competitive performance against other 20 state-of-the-art methods in terms of both qualitative and quantitative measures.</p><p>Although our models achieve competitive results against other state-of-the-art methods, faster and smaller models are needed for computation and memory limited devices, such as mobile phones, robots, etc. In the near future, we will explore different techniques and architectures to further improve the speed and decrease the model size. In addition, larger diversified salient object datasets are needed to train more accurate and robust models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of existing convolution blocks and our proposed residual U-block RSU: (a) Plain convolution block PLN, (b) Residual-like block RES, (c) Dense-like block DSE, (d) Inception-like block INC and (e) Our residual U-block RSU. sequentially and a Hybrid loss for boundary-aware salient object detection. Liu et al. (PoolNet)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of the residual block and our RSU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Computation costs (GFLOPS Giga Floating Point Operations) of different blocks shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>En 6 ,</head><label>6</label><figDesc>De 5, De 4, De 3, De 2 and De 1 by a 3 ? 3 convolution layer and a sigmoid function. Then, it upsamples the logits (convolution outputs before sigmoid functions) of the side output saliency maps to the input image size and fuses them with a concatenation operation fol-lowed by a 1?1 convolution layer and a sigmoid function to generate the final saliency probability map S f use (see bottom right of Fig. 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative comparison of the proposed method with seven other SOTA methods: (a) image, (b) GT, (c) Ours, (d) Ours ? , (e) BASNet, (f) PoolNet, (g) CPD, (h) PiCANetR, (i) R 3 Net+, (j) AFNet, (k) DSS+, where '+' indicates the CRF post-processing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>et al. (Amulet) [53] predict saliency maps by aggregating multi-level features into different resolutions. Zhang et al. (UCF) [54] propose to reduce the checkerboard artifacts of deconvolution operators by introducing a reformulated dropout and a hybrid upsampling module. Luo et al. [27] design a saliency detection network (NLDF+) with a 4?5 grid architecture, in which deeper features are progressively integrated with shallower features. Zhang et al. (LFR) [52] predict saliency maps by extracting features from both original input images and their reflection images with a sibling architecture. Hou et al. (DSS+) [13] propose to integrate multi-level features by introducing short connections from deep layers to shallow layers. Chen et al. (RAS) [4] predict and refine saliency maps by iteratively using the side output saliency of a backbone network as the feature attention guidance. Zhang et al. (BMPM) [50] propose to integrate features from shallow and deep layers by a controlled bi-directional passing strategy. Deng et al. (R 3 Net+) [6] alternately incorporate shallow and deep layers' features to refine the predicted saliency maps. Hu et al. (RADF+) [14] propose to detect salient objects by recurrently aggregating multi-level deep features. Wu et al. (MLMS)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>et al. (PiCANet) [23] recurrently capture the local and global pixel-wise contextual attention and predict the saliency map by incorporating it with a U-Net architecture. Zhang et al. (CapSal) [51] design a local and global perception module to extract both local and global information from features extracted by backbone network. Zeng et al. (MSWS) [48] design an attention module to predict the spatial distribution of foreground objects over image regions meanwhile aggregate their features. Feng et al. (AFNet)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Comparison of our method and 20 SOTA methods on DUT-OMRON, DUTS-TE, HKU-IS in terms of model size, maxF ? (?), M AE (?), weighted F w ? (?), structure measure Sm (?) and relax boundary F-measure relaxF b ? (?). Red, Green, and Blue indicate the best, second best and third best performance. Comparison of our method and 20 SOTA methods on ECSSD, PASCAL-S, SOD in terms of model size, maxF ? (?), M AE (?), weighted F w ? (?), structure measure Sm (?) and relax boundary F-measure relaxF b ? (?). Red, Green, and Blue indicate the best, second best and third best performance.</figDesc><table><row><cell cols="3">Method Backbone Size(MB)</cell><cell cols="4">DUT-OMRON (5168) maxF? M AE F w ? Sm</cell><cell>relaxF b ?</cell><cell cols="4">DUTS-TE (5019) maxF? M AE F w ? Sm</cell><cell>relaxF b ?</cell><cell cols="4">HKU-IS (4447) maxF? M AE F w ? Sm</cell><cell>relaxF b ?</cell></row><row><cell>MDFTIP16</cell><cell>AlexNet</cell><cell>112.1</cell><cell>0.694</cell><cell>0.142</cell><cell cols="2">0.565 0.721</cell><cell>0.406</cell><cell>0.729</cell><cell>0.099</cell><cell cols="2">0.543 0.723</cell><cell>0.447</cell><cell>0.860</cell><cell>0.129</cell><cell cols="2">0.564 0.810</cell><cell>0.594</cell></row><row><cell>UCFICCV17</cell><cell>VGG-16</cell><cell>117.9</cell><cell>0.730</cell><cell>0.120</cell><cell cols="2">0.573 0.760</cell><cell>0.480</cell><cell>0.773</cell><cell>0.112</cell><cell cols="2">0.596 0.777</cell><cell>0.518</cell><cell>0.888</cell><cell>0.062</cell><cell cols="2">0.779 0.875</cell><cell>0.679</cell></row><row><cell>AmuletICCV17</cell><cell>VGG-16</cell><cell>132.6</cell><cell>0.743</cell><cell>0.098</cell><cell cols="2">0.626 0.781</cell><cell>0.528</cell><cell>0.778</cell><cell>0.084</cell><cell cols="2">0.658 0.796</cell><cell>0.568</cell><cell>0.897</cell><cell>0.051</cell><cell cols="2">0.817 0.886</cell><cell>0.716</cell></row><row><cell>NLDF+CVPR17</cell><cell>VGG-16</cell><cell>428.0</cell><cell>0.753</cell><cell>0.080</cell><cell cols="2">0.634 0.770</cell><cell>0.514</cell><cell>0.813</cell><cell>0.065</cell><cell cols="2">0.710 0.805</cell><cell>0.591</cell><cell>0.902</cell><cell>0.048</cell><cell cols="2">0.838 0.879</cell><cell>0.694</cell></row><row><cell>DSS+CVPR17</cell><cell>VGG-16</cell><cell>237.0</cell><cell>0.781</cell><cell>0.063</cell><cell cols="2">0.697 0.790</cell><cell>0.559</cell><cell>0.825</cell><cell>0.056</cell><cell cols="2">0.755 0.812</cell><cell>0.606</cell><cell>0.916</cell><cell>0.040</cell><cell cols="2">0.867 0.878</cell><cell>0.706</cell></row><row><cell>RASECCV18</cell><cell>VGG-16</cell><cell>81.0</cell><cell>0.786</cell><cell>0.062</cell><cell cols="2">0.695 0.814</cell><cell>0.615</cell><cell>0.831</cell><cell>0.059</cell><cell cols="2">0.740 0.828</cell><cell>0.656</cell><cell>0.913</cell><cell>0.045</cell><cell cols="2">0.843 0.887</cell><cell>0.748</cell></row><row><cell>PAGRNCVPR18</cell><cell>VGG-19</cell><cell>-</cell><cell>0.771</cell><cell>0.071</cell><cell cols="2">0.622 0.775</cell><cell>0.582</cell><cell>0.854</cell><cell>0.055</cell><cell cols="2">0.724 0.825</cell><cell>0.692</cell><cell>0.918</cell><cell>0.048</cell><cell cols="2">0.820 0.887</cell><cell>0.762</cell></row><row><cell>BMPMCVPR18</cell><cell>VGG-16</cell><cell>-</cell><cell>0.774</cell><cell>0.064</cell><cell cols="2">0.681 0.809</cell><cell>0.612</cell><cell>0.852</cell><cell>0.048</cell><cell cols="2">0.761 0.851</cell><cell>0.699</cell><cell>0.921</cell><cell>0.039</cell><cell cols="2">0.859 0.907</cell><cell>0.773</cell></row><row><cell>PiCANetCVPR18</cell><cell>VGG-16</cell><cell>153.3</cell><cell>0.794</cell><cell>0.068</cell><cell cols="2">0.691 0.826</cell><cell>0.643</cell><cell>0.851</cell><cell>0.054</cell><cell cols="2">0.747 0.851</cell><cell>0.704</cell><cell>0.921</cell><cell>0.042</cell><cell cols="2">0.847 0.906</cell><cell>0.784</cell></row><row><cell>MLMSCVPR19</cell><cell>VGG-16</cell><cell>263.0</cell><cell>0.774</cell><cell>0.064</cell><cell cols="2">0.681 0.809</cell><cell>0.612</cell><cell>0.852</cell><cell>0.048</cell><cell cols="2">0.761 0.851</cell><cell>0.699</cell><cell>0.921</cell><cell>0.039</cell><cell cols="2">0.859 0.907</cell><cell>0.773</cell></row><row><cell>AFNetCVPR19</cell><cell>VGG-16</cell><cell>143.0</cell><cell>0.797</cell><cell>0.057</cell><cell cols="2">0.717 0.826</cell><cell>0.635</cell><cell>0.862</cell><cell>0.046</cell><cell cols="2">0.785 0.855</cell><cell>0.714</cell><cell>0.923</cell><cell>0.036</cell><cell cols="2">0.869 0.905</cell><cell>0.772</cell></row><row><cell cols="2">MSWSCVPR19 Dense-169</cell><cell>48.6</cell><cell>0.718</cell><cell>0.109</cell><cell cols="2">0.527 0.756</cell><cell>0.362</cell><cell>0.767</cell><cell>0.908</cell><cell cols="2">0.586 0.749</cell><cell>0.376</cell><cell>0.856</cell><cell>0.084</cell><cell cols="2">0.685 0.818</cell><cell>0.438</cell></row><row><cell>R 3 Net+IJCAI18</cell><cell>ResNeXt</cell><cell>215.0</cell><cell>0.795</cell><cell>0.063</cell><cell cols="2">0.728 0.817</cell><cell>0.599</cell><cell>0.828</cell><cell>0.058</cell><cell cols="2">0.763 0.817</cell><cell>0.601</cell><cell>0.915</cell><cell>0.036</cell><cell cols="2">0.877 0.895</cell><cell>0.740</cell></row><row><cell cols="2">CapSalCVPR19 ResNet-101</cell><cell>-</cell><cell>0.699</cell><cell>0.101</cell><cell cols="2">0.482 0.674</cell><cell>0.396</cell><cell>0.823</cell><cell>0.072</cell><cell cols="2">0.691 0.808</cell><cell>0.605</cell><cell>0.882</cell><cell>0.062</cell><cell cols="2">0.782 0.850</cell><cell>0.654</cell></row><row><cell cols="2">SRMICCV17 ResNet-50</cell><cell>189.0</cell><cell>0.769</cell><cell>0.069</cell><cell cols="2">0.658 0.798</cell><cell>0.523</cell><cell>0.826</cell><cell>0.058</cell><cell cols="2">0.722 0.824</cell><cell>0.592</cell><cell>0.906</cell><cell>0.046</cell><cell cols="2">0.835 0.887</cell><cell>0.680</cell></row><row><cell cols="2">DGRLCVPR18 ResNet-50</cell><cell>646.1</cell><cell>0.779</cell><cell>0.063</cell><cell cols="2">0.697 0.810</cell><cell>0.584</cell><cell>0.834</cell><cell>0.051</cell><cell cols="2">0.760 0.836</cell><cell>0.656</cell><cell>0.913</cell><cell>0.037</cell><cell cols="2">0.865 0.897</cell><cell>0.744</cell></row><row><cell cols="2">PiCANetRCVPR18 ResNet-50</cell><cell>197.2</cell><cell>0.803</cell><cell>0.065</cell><cell cols="2">0.695 0.832</cell><cell>0.632</cell><cell>0.860</cell><cell>0.050</cell><cell cols="2">0.755 0.859</cell><cell>0.696</cell><cell>0.918</cell><cell>0.043</cell><cell cols="2">0.840 0.904</cell><cell>0.765</cell></row><row><cell cols="2">CPDCVPR19 ResNet-50</cell><cell>183.0</cell><cell>0.797</cell><cell>0.056</cell><cell cols="2">0.719 0.825</cell><cell>0.655</cell><cell>0.865</cell><cell>0.043</cell><cell cols="2">0.795 0.858</cell><cell>0.741</cell><cell>0.925</cell><cell>0.034</cell><cell cols="2">0.875 0.905</cell><cell>0.795</cell></row><row><cell cols="2">PoolNetCVPR19 ResNet-50</cell><cell>273.3</cell><cell>0.808</cell><cell>0.056</cell><cell cols="2">0.729 0.836</cell><cell>0.675</cell><cell>0.880</cell><cell>0.040</cell><cell cols="2">0.807 0.871</cell><cell>0.765</cell><cell>0.932</cell><cell>0.033</cell><cell cols="2">0.881 0.917</cell><cell>0.811</cell></row><row><cell cols="2">BASNetCVPR19 ResNet-34</cell><cell>348.5</cell><cell>0.805</cell><cell>0.056</cell><cell cols="2">0.751 0.836</cell><cell>0.694</cell><cell>0.860</cell><cell cols="2">0.047 0.803</cell><cell>0.853</cell><cell>0.758</cell><cell>0.928</cell><cell>0.032</cell><cell cols="2">0.889 0.909</cell><cell>0.807</cell></row><row><cell>U 2 -Net (Ours)</cell><cell>RSU</cell><cell>176.3</cell><cell>0.823</cell><cell>0.054</cell><cell cols="2">0.757 0.847</cell><cell>0.702</cell><cell>0.873</cell><cell>0.044</cell><cell cols="2">0.804 0.861</cell><cell>0.765</cell><cell>0.935</cell><cell>0.031</cell><cell cols="2">0.890 0.916</cell><cell>0.812</cell></row><row><cell>U 2 -Net  ? (Ours)</cell><cell>RSU</cell><cell>4.7</cell><cell>0.813</cell><cell cols="3">0.060 0.731 0.837</cell><cell>0.676</cell><cell>0.852</cell><cell>0.054</cell><cell cols="2">0.763 0.847</cell><cell>0.723</cell><cell>0.928</cell><cell>0.037</cell><cell cols="2">0.867 0.908</cell><cell>0.794</cell></row><row><cell>Method</cell><cell cols="2">Backbone Size(MB)</cell><cell cols="4">ECSSD (1000) maxF? M AE F w ? Sm</cell><cell>relaxF b ?</cell><cell cols="4">PASCAL-S (850) maxF? M AE F w ? Sm</cell><cell>relaxF b ?</cell><cell cols="2">maxF? M AE</cell><cell>SOD (300) F w ?</cell><cell>Sm</cell><cell>relaxF b ?</cell></row><row><cell>MDFTIP16</cell><cell>AlexNet</cell><cell>112.1</cell><cell>0.832</cell><cell>0.105</cell><cell cols="2">0.705 0.776</cell><cell>0.472</cell><cell>0.759</cell><cell>0.142</cell><cell cols="2">0.589 0.696</cell><cell>0.343</cell><cell>0.746</cell><cell>0.192</cell><cell cols="2">0.508 0.643</cell><cell>0.311</cell></row><row><cell>UCFICCV17</cell><cell>VGG-16</cell><cell>117.9</cell><cell>0.903</cell><cell>0.069</cell><cell cols="2">0.806 0.884</cell><cell>0.669</cell><cell>0.814</cell><cell>0.115</cell><cell cols="2">0.694 0.805</cell><cell>0.493</cell><cell>0.808</cell><cell>0.148</cell><cell cols="2">0.675 0.762</cell><cell>0.471</cell></row><row><cell>AmuletICCV17</cell><cell>VGG-16</cell><cell>132.6</cell><cell>0.915</cell><cell>0.059</cell><cell cols="2">0.840 0.894</cell><cell>0.711</cell><cell>0.828</cell><cell>0.100</cell><cell cols="2">0.734 0.818</cell><cell>0.541</cell><cell>0.798</cell><cell>0.144</cell><cell cols="2">0.677 0.753</cell><cell>0.454</cell></row><row><cell>NLDF+CVPR17</cell><cell>VGG-16</cell><cell>428.0</cell><cell>0.905</cell><cell>0.063</cell><cell cols="2">0.839 0.897</cell><cell>0.666</cell><cell>0.822</cell><cell>0.098</cell><cell cols="2">0.737 0.798</cell><cell>0.495</cell><cell>0.841</cell><cell>0.125</cell><cell cols="2">0.709 0.755</cell><cell>0.475</cell></row><row><cell>DSS+CVPR17</cell><cell>VGG-16</cell><cell>237.0</cell><cell>0.921</cell><cell>0.052</cell><cell cols="2">0.872 0.882</cell><cell>0.696</cell><cell>0.831</cell><cell>0.093</cell><cell cols="2">0.759 0.798</cell><cell>0.499</cell><cell>0.846</cell><cell>0.124</cell><cell cols="2">0.710 0.743</cell><cell>0.444</cell></row><row><cell>RASECCV18</cell><cell>VGG-16</cell><cell>81.0</cell><cell>0.921</cell><cell>0.056</cell><cell cols="2">0.857 0.893</cell><cell>0.741</cell><cell>0.829</cell><cell>0.101</cell><cell cols="2">0.736 0.799</cell><cell>0.560</cell><cell>0.851</cell><cell>0.124</cell><cell cols="2">0.720 0.764</cell><cell>0.544</cell></row><row><cell>PAGRNCVPR18</cell><cell>VGG-19</cell><cell>-</cell><cell>0.927</cell><cell>0.061</cell><cell cols="2">0.834 0.889</cell><cell>0.747</cell><cell>0.847</cell><cell>0.090</cell><cell cols="2">0.738 0.822</cell><cell>0.594</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BMPMCVPR18</cell><cell>VGG-16</cell><cell>-</cell><cell>0.928</cell><cell>0.045</cell><cell cols="2">0.871 0.911</cell><cell>0.770</cell><cell>0.850</cell><cell>0.074</cell><cell cols="2">0.779 0.845</cell><cell>0.617</cell><cell>0.856</cell><cell>0.108</cell><cell cols="2">0.726 0.786</cell><cell>0.562</cell></row><row><cell>PiCANetCVPR18</cell><cell>VGG-16</cell><cell>153.3</cell><cell>0.931</cell><cell>0.046</cell><cell cols="2">0.865 0.914</cell><cell>0.784</cell><cell>0.856</cell><cell>0.078</cell><cell cols="2">0.772 0.848</cell><cell>0.612</cell><cell>0.854</cell><cell>0.103</cell><cell cols="2">0.722 0.789</cell><cell>0.572</cell></row><row><cell>MLMSCVPR19</cell><cell>VGG-16</cell><cell>263.0</cell><cell>0.928</cell><cell>0.045</cell><cell cols="2">0.871 0.911</cell><cell>0.770</cell><cell>0.855</cell><cell>0.074</cell><cell cols="2">0.779 0.844</cell><cell>0.620</cell><cell>0.856</cell><cell>0.108</cell><cell cols="2">0.726 0.786</cell><cell>0.562</cell></row><row><cell>AFNetCVPR19</cell><cell>VGG-16</cell><cell>143.0</cell><cell>0.935</cell><cell>0.042</cell><cell cols="2">0.887 0.914</cell><cell>0.776</cell><cell>0.863</cell><cell>0.070</cell><cell cols="2">0.798 0.849</cell><cell>0.626</cell><cell>0.856</cell><cell>0.111</cell><cell cols="2">0.723 0.774</cell><cell>-</cell></row><row><cell>MSWSCVPR19</cell><cell>Dense-169</cell><cell>48.6</cell><cell>0.878</cell><cell>0.096</cell><cell cols="2">0.716 0.828</cell><cell>0.411</cell><cell>0.786</cell><cell>0.133</cell><cell cols="2">0.614 0.768</cell><cell>0.289</cell><cell>0.800</cell><cell>0.167</cell><cell cols="2">0.573 0.700</cell><cell>0.231</cell></row><row><cell>R 3 Net+IJCAI18</cell><cell>ResNeXt</cell><cell>215.0</cell><cell>0.934</cell><cell>0.040</cell><cell cols="2">0.902 0.910</cell><cell>0.759</cell><cell>0.834</cell><cell>0.092</cell><cell cols="2">0.761 0.807</cell><cell>0.538</cell><cell>0.850</cell><cell cols="2">0.125 0.735</cell><cell>0.759</cell><cell>0.431</cell></row><row><cell>CapSalCVPR19</cell><cell>ResNet-101</cell><cell>-</cell><cell>0.874</cell><cell>0.077</cell><cell cols="2">0.771 0.826</cell><cell>0.574</cell><cell>0.861</cell><cell>0.073</cell><cell cols="2">0.786 0.837</cell><cell>0.527</cell><cell>0.773</cell><cell>0.148</cell><cell cols="2">0.597 0.695</cell><cell>0.404</cell></row><row><cell>SRMICCV17</cell><cell>ResNet-50</cell><cell>189.0</cell><cell>0.917</cell><cell>0.054</cell><cell cols="2">0.853 0.895</cell><cell>0.672</cell><cell>0.838</cell><cell>0.084</cell><cell cols="2">0.758 0.834</cell><cell>0.509</cell><cell>0.843</cell><cell>0.128</cell><cell cols="2">0.670 0.741</cell><cell>0.392</cell></row><row><cell>DGRLCVPR18</cell><cell>ResNet-50</cell><cell>646.1</cell><cell>0.925</cell><cell>0.042</cell><cell cols="2">0.883 0.906</cell><cell>0.753</cell><cell>0.848</cell><cell>0.074</cell><cell cols="2">0.787 0.839</cell><cell>0.569</cell><cell>0.848</cell><cell>0.106</cell><cell cols="2">0.731 0.773</cell><cell>0.502</cell></row><row><cell cols="2">PiCANetRCVPR18 ResNet-50</cell><cell>197.2</cell><cell>0.935</cell><cell>0.046</cell><cell cols="2">0.867 0.917</cell><cell>0.775</cell><cell>0.857</cell><cell>0.076</cell><cell cols="2">0.777 0.854</cell><cell>0.598</cell><cell>0.856</cell><cell>0.104</cell><cell cols="2">0.724 0.790</cell><cell>0.528</cell></row><row><cell>CPDCVPR19</cell><cell>ResNet-50</cell><cell>183.0</cell><cell>0.939</cell><cell>0.037</cell><cell cols="2">0.898 0.918</cell><cell>0.811</cell><cell>0.861</cell><cell>0.071</cell><cell cols="2">0.800 0.848</cell><cell>0.639</cell><cell>0.860</cell><cell>0.112</cell><cell cols="2">0.714 0.767</cell><cell>0.556</cell></row><row><cell>PoolNetCVPR19</cell><cell>ResNet-50</cell><cell>273.3</cell><cell>0.944</cell><cell>0.039</cell><cell cols="2">0.896 0.921</cell><cell>0.813</cell><cell>0.865</cell><cell cols="2">0.075 0.798</cell><cell>0.832</cell><cell>0.644</cell><cell>0.871</cell><cell>0.102</cell><cell cols="2">0.759 0.797</cell><cell>0.606</cell></row><row><cell>BASNetCVPR19</cell><cell>ResNet-34</cell><cell>348.5</cell><cell>0.942</cell><cell>0.037</cell><cell>0.904</cell><cell>0.916</cell><cell>0.826</cell><cell>0.856</cell><cell cols="2">0.076 0.798</cell><cell>0.838</cell><cell>0.660</cell><cell>0.851</cell><cell>0.113</cell><cell cols="2">0.730 0.769</cell><cell>0.603</cell></row><row><cell>U 2 -Net (Ours)</cell><cell>RSU</cell><cell>176.3</cell><cell>0.951</cell><cell>0.033</cell><cell cols="2">0.910 0.928</cell><cell>0.836</cell><cell>0.859</cell><cell cols="2">0.074 0.797</cell><cell>0.844</cell><cell>0.657</cell><cell>0.861</cell><cell cols="2">0.108 0.748</cell><cell>0.786</cell><cell>0.613</cell></row><row><cell>U 2 -Net  ? (Ours)</cell><cell>RSU</cell><cell>4.7</cell><cell>0.943</cell><cell>0.041</cell><cell cols="2">0.885 0.918</cell><cell>0.808</cell><cell>0.849</cell><cell>0.086</cell><cell cols="2">0.768 0.831</cell><cell>0.627</cell><cell>0.841</cell><cell>0.124</cell><cell cols="2">0.697 0.759</cell><cell>0.559</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">and "O" indicate the number of input channels (C in ), middle channels and output channels (C out ) of each block. "En i" and "De j" denote the encoder and decoder stages respectively. The number "L" in "NIV-L" and "RSU-L" denotes the height of the naive U-block and our residual U-block.Architecture with different blocksStages En 1</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the Alberta Innovates Graduate Student Scholarship and Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grants Program, NO.: 2016-06365.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuli</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">R3net: Recurrent residual refinement network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Relaxed precision and recall for ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Ehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Euzenat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. K-Cap 2005 workshop on Integrating ontology</title>
		<meeting>K-Cap 2005 workshop on Integrating ontology</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
	<note>No commercial editor.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attentive feedback network for boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1623" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image analysis using mathematical morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhua</forename><surname>Sternberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="532" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5300" to="5309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrently aggregating deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6943" to="6950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual saliency detection based on multiscale deep cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5012" to="5024" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Material based salient object detection from hyperspectral images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="476" to="490" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Autodeeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for realtime salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3917" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Saliency modeling from image histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Hwee</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="321" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust and efficient saliency modeling from image co-occurrence histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheston</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Hwee</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="195" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Non-local deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshaya</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Marc</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6593" to="6601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Docunet: Document image unwarping via a stacked unet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4700" to="4709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayellet</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Design and perceptual validation of performance measures for salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vida</forename><surname>Movahedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autodiff workshop on NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Basnet: Boundaryaware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7479" to="7489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Quantized densely connected u-nets for efficient landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="339" to="354" />
		</imprint>
	</monogr>
	<note>Shaoting Zhang, and Dimitris Metaxas</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06521</idno>
		<title level="m">Yizhe Zhu, and Dimitris N Metaxas. Cu-net: coupled u-nets</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4039" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Detect globally, refine locally: A novel approach to saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3127" to="3135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A mutual learning method for salient object detection with intertwined multi-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlong</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8150" to="8159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multi-source weak supervision for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6074" to="6083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A novel graph-based optimization framework for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haikun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanjian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="39" to="50" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">You He, and Gang Wang. A bi-directional message passing model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1741" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Capsal: Leveraging captioning to boost semantics for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6024" to="6033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Salient object detection by lossless feature reflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<editor>IJ-CAI</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1149" to="1155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning uncertain convolutional features for accurate saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="212" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Salient object detection employing a local tree-structured low-rank representation and foreground consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifeng</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="119" to="134" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep embedding features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9340" to="9347" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
