<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Representation for Neural Code Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Gu</surname></persName>
							<email>gu@ifi.uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimin</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Monperrus</surname></persName>
							<email>martin.monperrus@csc.kth.se</email>
							<affiliation key="aff2">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Representation for Neural Code Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-multimodal learning</term>
					<term>program representation</term>
					<term>information completeness</term>
					<term>tree serialization</term>
					<term>code search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic code search is about finding semantically relevant code snippets for a given natural language query. In the state-of-the-art approaches, the semantic similarity between code and query is quantified as the distance of their representation in the shared vector space. In this paper, to improve the vector space, we introduce tree-serialization methods on a simplified form of AST and build the multimodal representation for the code data. We conduct extensive experiments using a single corpus that is large-scale and multi-language: CodeSearchNet. Our results show that both our tree-serialized representations and multimodal learning model improve the performance of code search. Last, we define intuitive quantification metrics oriented to the completeness of semantic and syntactic information of the code data, to help understand the experimental findings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N modern society, software systems are indispensable and have already been everywhere. It represents a wide range of applications, such as code search, which is the task to search existing code snippets. When developing or maintaining software, people tend to reuse existing scaffolds or learn from actual usage examples instead of wasting time reinventing the wheel. On one hand, code search could assist programmers in their daily work. On the other hand, it strengthens the infrastructures in open source communities, such as GitHub 1 . It is beneficial to have an efficient and effective way for code search, comparing with the general-purpose search using modern search engines <ref type="bibr" target="#b0">[1]</ref>.</p><p>The task of semantic code search is to retrieve the most semantically relevant code snippets for the given natural language queries. The input data is a query, and output data is an ordered list of code snippets. The code snippets ranked higher should be more similar to the query with regards to the text meaning. In state-of-the-art approaches, the code and query data are represented as vectors of the same length, so the computation of their semantic similarity is converted as the vector distance measurement.</p><p>We propose utilizing multimodal code representations to improve the semantic code search. Modality refers to the channels how information exists, and multimodal means the multiple modalities, namely information of multiple types. We introduce tree-serialized representations, to study whether they are more informative than the token representation and whether using them as the additional input is beneficial or not. Tree-serialized representations are generated by parsing code 1 https://github.com/ data into the tree structure and then serializing the tree into sequences. We utilize multimodal learning to reveal the power of multimodal representations and evaluate our approach by comparing with the given baselines on the CodeSearchNet corpus <ref type="bibr" target="#b1">[2]</ref>. Besides, we define two intuitive metrics, namely link coverage and node coverage, to quantify the completeness of the syntactic and semantic information conveyed by various code representations separately.</p><p>In our experiments, the SelfAtt model is the strongest model considering the context information. Among tree-serialized representations, traversal-based representations always perform better than sampling-based representations. Based on our results, tree-traversal representation improves the MRR scores by at most 16.88%. More results suggest that our multimodal approach surpasses baselines by from 5.23% to 15.70%. Compared with corresponding unimodal representations, multimodal representations bring improvements for at most 17.82%. Furthermore, we recommend adopting tree representations as well as multimodal learning for code search.</p><p>The contributions of this paper are as follows: 1) We propose the Simplified Semantic Tree, a specifically designed form of Abstract Syntax Tree for enriching the semantic representation of the program. We systemically define and assess several serialization methods based on this novel tree structure. 2) We demonstrate that multimodal learning is effective for the code search task. We compare the representation capability of both unimodal and multimodal code search using our tree-serialized representations. To the best of our knowledge, it is the first time that tree-serialized sequences are used as the modality for code search. 3) We define two simple but intuitive quantification metrics, the link coverage and the node coverage, to measure the completeness of semantic and syntactic information of various representation forms. They perform reliably in revealing the effectiveness of modalities for code search.</p><p>The rest of this paper is structured as follows. In section II, the background information is introduced. In section III, our approach on multimodal representation is proposed. In section IV, the experimental methodology is described. In section V, the experimental results are shown and explained. In subsection V-D, threats to validity are discussed. In section VI, related work and research context are reviewed. In section VII, the whole work is summarized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Code Search</head><p>The canonical task of code search is about finding the most relevant code snippet for the given natural language query. A code search engine can be made with information-retrieval techniques and neural techniques <ref type="bibr" target="#b2">[3]</ref>. In this paper, we focus on the latter.</p><p>As illustrated in Listing 1, a code-query pair is a piece of natural language query and the corresponding source code. The query could be short documentation of the target code snippet, like "send birthday messages to members". Throughout this work, we call the source code tokens "code sequence" and the query tokens "query sequence". The code and query sequences are used to train a neural code search model.</p><p>1 d e f birthday_marketing(self): </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Listing 1: A Code Snippet in Python</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Siamese Networks for Code Search</head><p>A siamese network is an artificial neural network to measure the similarity between two inputs of the same type using the same encoder <ref type="bibr" target="#b3">[4]</ref>. The pseudo-siamese network is more flexible because it is intended to measure the similarity of different data types with different encoders <ref type="bibr" target="#b4">[5]</ref>. The model architecture for code search follows the practice of utilizing the pseudo-siamese network <ref type="bibr" target="#b5">[6]</ref>, as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. In the architecture, the code and query sequences, are respectively fed into corresponding encoders to be converted into vectors. The training objective is to minimize the distance between relevant code and query vectors. The code search model uses the cosine distance between vectors to measure the similarity. Once trained, the most semantically relevant code snippets to a given query, are those whose vectors are closest to the query vector measured by cosine distance. The learning objective is to make sure that semantically similar vectors are as close as possible. The triplet loss is the objective function used in baselines and our model. It optimizes the query vector to be close to the corresponding code vector, but to be far away from other code vectors. In the training process, each code-query pair (c i , q i ) and corresponding distractor code snippets c j are fed into the code encoder E c and query encoder E q . The training objective is to minimize the loss shown below:</p><formula xml:id="formula_0">Loss = ? 1 N i log ? ? exp E c (c i ) E q (q i ) j exp E c (c j ) E q (q i ) ? ? (1)</formula><p>The triplet loss function is aimed to maximize the inner product of code c i and query q i encodings of the pair, while minimize the inner product between target code snippet c i and its distractor code snippets c j i = j <ref type="bibr" target="#b1">[2]</ref>. (c i , q i ) and (c j , q i ) respectively denotes the positive and negative samples.</p><p>There is a wide diversity of possible encoders for siamese networks for code search. Dedicated to sequential data, typical encoders include:</p><p>? NBoW: Neural Bag-of-Words (NBoW) simply computes the weighted average of all word embeddings to get the sentence embedding as the whole semantic representation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. ? 1D-CNN: Convolutional Neural Network (CNN) <ref type="bibr" target="#b8">[9]</ref> uses a convolution operation to analyze context information in receptive fields of different sizes. 1D-CNN refers to the model for 1-dimension sequential data. ? Bi-RNN: Recurrent Neural Network (RNN) <ref type="bibr" target="#b9">[10]</ref> uses hidden layers in the form of temporal sequence to capture dependencies. Bi-RNN concatenates embeddings of both forward and backward directions. ? SelfAtt: The transformer-based model uses a self-attention mechanism and the positional embedding way of BERT to learn from the context information <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multimodal Learning</head><p>Modality refers to the way how some type of information exists. For example, to identify shepherd dogs from sheep, we could fully utilize the data of various modalities, such as colors, sounds, and features of movement patterns.</p><p>Multimodal learning aims to build models that are capable to process and associate data of multiple modalities <ref type="bibr" target="#b12">[13]</ref>. It is based on the fact that data semantics can be captured in different ways. The representation resulting from data of multiple modalities is named as the multimodal representation.</p><p>Because the multimodal learning model learns features considering the information from various modalities, it usually performs better than the unimodal learning model which only studies data of unique modality <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. A NOVEL MULTIMODAL REPRESENTATION APPROACH</head><p>FOR CODE SEARCH In this section, we present a novel approach for neural code search. This approach follows the design of the pseudosiamese network for code search. Our core intuition is to build an encoder that fully utilizes information from multiple aspects of source code. In this paper, such an aspect is called a "modality", per the seminal work of multimodal learning with deep Boltzmann machines <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>Based on the raw code snippet of the particular code query, we extract code representations from multiple modalities. The workflow is as shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>The input data is the code snippet and the output data are the code sequence and tree sequence. First, we parse the code snippet into an Abstract Syntax Tree (AST). To make the tree structure semantically better for code search, we transform the AST into a novel tree structure named Simplified Semantic Tree (SST), which we newly introduce in this paper. Then, we extract a tree-serialized representation from the SST, by sampling tree-paths <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> or traversing tree-structures <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. Eventually, these tree-serialized representations separately complement the conventional token representation, namely the sequence of source code tokens, in our multimodal learning approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: Workflow for Extracting Multiple Representations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Simplified Semantic Tree</head><p>We design one novel tree structure named SST for extracting tree-serialized representations of source code. SST is an approximation to simplify the tree structure of AST and highlights the semantic information of the code snippet. Compared with AST, SST removes unnecessary tree nodes and improves the label of tree nodes. Even though AST already has the tree structure and is capable of tree serialization, SST is more semantically informative and more general to various programming languages.</p><p>For any given AST, we conduct three operations to build the corresponding SST. The first operation is to prune tree nodes that are semantically meaningless for code search, such as type declarations like "int" and "boolean", modifier keywords like "public" and "abstract", functional keywords like "async" and "await". The complete list of removed nodes varies for different languages. The second operation is to replace labels of statement nodes and expression nodes with descriptive tags, like using "loop" for for-loop and while-loop statements and "literal" for exact string variables, the goal is to help the network grasp the general concept behind syntactically different nodes. The third operation is to unify the expression of semantically similar labels from different languages, like unifying "function", "program", "define", and "module" as "module". It is expected to promote some form of transfer learning across programming languages.</p><p>To better understand SST, let us study the code snippet illustrated in Listing 1. This code iterates over all members and checks whether today is someone's birthday, if yes, it invokes the SMS function. These words in blue are identifiers. As illustrated in <ref type="figure">Figure 3</ref>, the code snippet is parsed into AST. In the tree structure, most leaf nodes in the blue blocks are meaningful, while other leaf nodes seem semantically meaningless, such as the underlined "self", which is merely meant to explicitly represent the instance of the class. In contrast to leaf nodes, most non-terminals nodes correspond to keywords, punctuations, and indents. Now turn to check the corresponding SST in <ref type="figure">Figure 4</ref>. In the tree structure, some leaf nodes are removed (grayed out in the <ref type="figure">figure)</ref>. The non-terminal nodes are replaced with unified semantic expressions, that are shared among languages. For instance, the labels containing "-Stmt" or "-Expr" postfix are replaced by the shorter, cross-language version (eg. "ForEachStmt" into "loop"). If observe the differences between AST and SST, the removed nodes are in gray and the simplified labels are in bold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Tree Serialization</head><p>Next, we serialize the SST, per the technique of sampling and traversal. The motivation is to extract a linear sequence from the tree structure. These sequences are sequential data that are more applicable to the typical encoders above mentioned. There are two options to serialize SST to a token sequence. One way is to extract tree paths from the tree structure <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, namely the connections between tree nodes, and then filter and sample over the collected tree paths. The other way is to serialize the whole structure via tree traversal <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. They are respectively called as samplingbased representation and traversal-based representation. In this paper, we investigate two sampling-based representations and two traversal representations.</p><p>RootPath <ref type="bibr" target="#b15">[16]</ref> samples the paths of non-terminal nodes starting from a single leaf node to the root node. As shown in <ref type="figure">Figure 5</ref>, it is such one rootpath connecting the leaf node whose label is "anniversary" and the root node. All nodes at the tree path are in the orange blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5: RootPath Schematic Diagram</head><p>LeafPath <ref type="bibr" target="#b16">[17]</ref> samples the paths of non-terminal nodes between two arbitrary leaf nodes. As shown in <ref type="figure">Figure 6</ref>, it is such one leafpath connecting the leaf node whose label is "anniversary" and another leaf node whose label is "SMS". All nodes at the tree path are in the orange blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6: LeafPath Schematic Diagram</head><p>Structure-Based Traversal (SBT) <ref type="bibr" target="#b17">[18]</ref> is a traversal-based representation. SBT Representation is obtained via top-down recursive tree traversal. The detailed procedure is as follows: 1) from the root node, we first use a pair of brackets to represent the tree structure and put the root node itself behind the right bracket. 2) traverse the subtrees of the root node and put all root nodes of subtrees into the brackets. 3) traverse each subtree recursively until all nodes are traversed and the final sequence is obtained.</p><p>Left-Child Right-Sibling (LCRS) <ref type="bibr" target="#b18">[19]</ref> is a traversal-based representation. The idea is to transform the general tree into a binary tree in the Left-Child Right-Sibling form, as illustrated in <ref type="figure">Figure 8</ref>. Then LCRS representation is obtained via in-order tree traversal. For each subtree, we use brackets to separate the parent node, left child tree, and right child tree. To generate these tree-serialized representations, we parse raw code snippets to build SST, then extract rootpaths starting from each leaf node whose label is an identifier to the root node. For better semantic expressions, we ignore leaf nodes whose labels are single-character identifiers, like "t" or "x", unless there are no enough rootpaths for each code snippet. Once we collect all rootpaths, we combine them into pairs randomly to generate leafpaths. Similarly, we give the priority to leafpaths whose corresponding leaf nodes are with multicharacter identifiers, like "cost" or "ratio", as labels, because they are seen as most semantically informative. In contrast, we only need to implement functions for structure-based traversal and in-order traversal as well as the tree transformation algorithm to generate the SBT and LCRS representations, without any extra processing work.</p><p>The RootPath and LeafPath representations are controlled by a number called sampled paths N . If M is the number of nodes in the tree, there are M unique rootpath sequences and M * (M ? 1)/2 unique leafpath sequences. For the LeafPath representation, we discard low-quality leafpath sequences, whose sequence length is larger than the suggested length threshold = 8 or the tree height difference of two sides is smaller than the suggested width threshold = 2 <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Meanwhile, the default parameter for the number of sampled tree paths is N = 20. A higher value promises better results but requires more computing power, so here is a trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multimodal Learning</head><p>In our approach, we extract two modalities from source code, one modality from the natural language representation, and adopt multimodal learning on top of them. The two modalities are plain and tree-serialized representations of code snippets. In the following, the modality of token representation is called the code sequence and represents tokens. The treeserialized representation of SST is called the tree sequence. The architecture of our multimodal learning model is a pseudo-siamese architecture, as shown in <ref type="figure" target="#fig_4">Figure 9</ref>. We adopt three SelfAtt models as the encoders in our model, considering that the SelfAtt model performs very well in various natural language tasks. The code and tree encoders are working in parallel for the code representation, and the query encoder is for the query representation. We feed the code encoder with code sequence and tree encoder with tree sequence, and these two encoders convert their respective sequence data to vectors, to a token vector and tree vector. The query encoder receives a query sequence as input and computes the query vector as output. More specifically, SBT or LCRS representation is used as the input data for the tree encoder, while other treeserialized representations are applicable as well.</p><p>In our multimodal learning model, all three modality vectors are of the same length. We combine the code and tree vectors by summing them. This computes a joint vector which is the multimodal representation for the source code. Then the joint vector is trained together with the query vector, to ensure that semantically similar code vectors are close to the query vector in the shared vector space. As usual, we compute the cosine distance to quantify the semantic similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL METHODOLOGY</head><p>The work investigates the following research questions:</p><p>? RQ1: What is the performance of siamese networks for code search when using different encoders? ? RQ2: What is the relative performance of the considered unimodal tree-serialized representations? ? RQ3: What is the effectiveness of multimodal representations for code search?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Corpus</head><p>In our experiment, we use the CodeSearchNet corpus as the experimental data <ref type="bibr" target="#b1">[2]</ref>. The corpus contains over two million code-query pairs in six popular programming languages, namely Go, Java, JavaScript, PHP, Python, and Ruby.</p><p>As shown in <ref type="table" target="#tab_0">Table I</ref>, the corpus is split into datasets with the 80-10-10 proportion. The first column presents datasets. The second to the seventh columns present the statistics of each programming language. The last column is for the total statistics values for all languages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Metrics</head><p>The challenge uses the mean reciprocal rank (MRR) and the normalized discounted cumulative gain (NDCG) score, as the performance metrics. The MRR score is used to measure models by competitors before their submissions, then the NDCG score is computed as the further criteria in the competition. The higher MRR or NDCG scores indicate better performance. The computation of the former is on the testing set, and the latter is on a small-sized artificial evaluation set.</p><p>The MRR score quantifies the ranking of the target code snippet to the given query, and it only cares about where the most relevant result is ranked. The most relevant code snippet should be ranked the highest, the lower its ranking position, the lower the MRR score. When computing MRR scores in the testing set, for each code-query pair, 999 code snippets from other pairs in the same batch play the role of distractors. The average value of all batches is the final MRR score.</p><p>The NDCG score quantifies the similarity between the rankings of candidate code snippets and the most ideal rankings, and it cares about the whole ranking order between all candidate results. The most ideal case is when more relevant code snippets are always ranked before those less relevant ones. The computation of NDCG scores on the extra evaluation set was specially made for the challenge. The evaluation set consists of 99 natural language queries, and 10 candidate code snippets for each query. The NDCG score is computed on the whole evaluation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Information Completeness Metrics</head><p>To intuitively measure the information completeness of code representations, we define link coverage and node coverage as the quantification metrics. The link coverage is to measure the completeness of syntactic information, and the node coverage is to measure the completeness of the semantic information. The coverage ratio is that of tree components, e.g. tree links or tree nodes, to be utilized among all components. Therefore, higher coverage indicates better informativeness or more complete information content.</p><p>The link coverage is defined as follows that how many tree links between each pair of nearest tree nodes are taken from all the tree links, so the syntactic information is expressed as to which extent the tree links between each pair of the nearest nodes are covered by tree sequences.</p><p>The node coverage is defined as follows that how many tree nodes are taken from all the tree nodes, so the semantic information is expressed as to which extent the unique labels of tree nodes are contained by tree sequences. We simply count tree nodes sharing the same label as one unique tree node.</p><p>Taking the SST diagram shown in <ref type="figure">Figure 4</ref>, the rootpath and leafpath demonstrated in <ref type="figure">Figure 5</ref> and <ref type="figure">Figure 6</ref> as an example. For the node coverage, there are 31 tree links totally because we directly count the links between the nearest tree nodes. Follow the same idea, the rootpath and leafpath separately cover 8 and 7 links, so their link coverages are 25.81% and 22.58%. For the node coverage, there are 32 tree nodes in total but sharing 16 unique labels, so the SST only has 16 unique tree nodes. Similarly, the rootpath and leafpath separately contain 8 and 6 unique nodes, so their node coverages are 50.00% and 37.50%.</p><p>The formulas for the link coverage and node coverage are defined as follows:</p><formula xml:id="formula_1">LinkCoverage C = |links C | |links T | = |? n i links Si | |links T | (2) NodeCoverage C = |nodes C | |nodes T | = |? n i nodes Si | |nodes T |<label>(3)</label></formula><p>where C represents the collection of sequences S i , and T is the tree where sequences S i are generated. Besides, links O and nodes O represent the tree links and tree nodes from a given object O, such as a sequence, a collection of sequences, or a tree, e.g. AST or SST.</p><p>According to the definitions, for a given code snippet, the link coverage and node coverage of token representation are respectively 0% and 100%. For tree-serialized representation, if sampling-based, like RootPath or LeafPath, these ratios are between 0% and 100%, if traversal-based, for SBT, both ratios are 100%, and for LCRS, the node coverage is 100%. Besides, if compute over the combination of the introduced representations. The definitions are still valid because different representations are from the same tree structure.</p><p>Even though the link coverage and node coverage could indicate how much syntactic and semantic information the input data may contain, it does not mean that more information always brings better results. Besides, considering the trade-off between the information completeness of the input data and the complexity of the feature extractor, it is not feasible to directly make a whole tree as the input data without introducing more work into model designing and training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Methodology for RQ1</head><p>What is the performance of siamese networks for code search when using different encoders?</p><p>The CodeSearchNet Challenge provides four baselines. We independently measure their performance to find the strongest baseline model.</p><p>We run experiments over the whole CodeSearchNet corpus, and we compute the MRR score over the testing set and the NDCG score over the evaluation set. For each baseline model, it is trained and tested over the data of each programming language. If one model has a high MRR score, it means it can learn well, when the samples are drawn from the same distribution. If one model has a high NDCG score, then it indicates a strong generalization, because the characteristics of the training set and the evaluation set are not alike.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Methodology for RQ2</head><p>What is the relative performance of the considered unimodal tree-serialized representations?</p><p>Our goal is to compare the performance of tree-serialized representations introduced in subsection III-C against the token representation. In all experiments, we feed the SelfAtt model with the tree sequence, not the code sequence. There are no changes to other settings compared to RQ1.</p><p>We name the experimental model of using the token representation as Uni-Code, where "Uni-" refers to using a unimodal setup. Uni-RootPath, Uni-LeafPath, Uni-SBT, or Uni-LCRS refers to the unimodal learning with RootPath, LeafPath, SBT, or LCRS representation respectively.</p><p>To make our experiments more representative, we focus on the Python and Ruby corpora, considering the contrastive dataset size and language features. The performance of models is measured by the MRR score, because it is more reliable than NDCG, considering the NDCG score is computed on a small evaluation set and its numerical values tend to be rather low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Methodology for RQ3</head><p>What is the effectiveness of multimodal representations for code search?</p><p>The motivation for introducing tree-serialized representations is to complement the token representation because we are interested in studying the effectiveness of multimodal representations, namely the combination of token representation and tree-serialized representation. In this RQ, we measure the performance of our multimodal learning approach. There are three encoders in our multimodal learning model, one query encoder is for query sequence, then the code and tree encoders are respectively for the code and tree sequences. We trained four models with different tree serialization techniques while keeping other settings the same. Otherwise stated, the corpora and metrics are the same as for RQ2.</p><p>Following the naming way of Uni-RootPath, Uni-LeafPath, Uni-SBT, or Uni-LCRS, we name the experimental models of using the multimodal representation containing RootPath, LeafPath, SBT, or LCRS representation as Multi-RootPath, Multi-LeafPath, Multi-SBT, Multi-LCRS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Implementation</head><p>Our implementations are written in Python, depending on the TensorFlow <ref type="bibr" target="#b21">[22]</ref> framework. We adopt tree-sitter 2 for parsing code. For RQ1, we use the official implementation of baselines supplied by the CodeSearchNet challenge directly. For RQ2 and RQ3, we introduce some optimization operations to make baselines stronger, such as model tuning and indexing strategy refining. In all conducted experiments, the hyperparameters remain unchanged, the batch size is 1000 and the embedding size is 128.</p><p>The experiments are conducted on the Ubuntu Linux with 56 GiB memory, powered by one piece of NVIDIA Tesla K80 GPU and six pieces of Intel Xeon E5-2690 CPU. The replication repository is available online 3 , which contains implementations, results, and the guidance for reproduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>To study the mentioned research questions, we conduct experiments and compare results. The accuracy of models is reflected by the MRR and NDCG scores. In the following comparisons, the best results are highlighted in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Results of RQ1</head><p>In this section, we compare the performance of different unimodal encoders for neural code search, per the protocols described in section IV. <ref type="table" target="#tab_0">Table II</ref> and III give the MRR score and NDCG score of the considered unimodal encoders respectively. The rows are for encoders and the columns are for the languages of the benchmark. For example, the MRR score and NDCG score of the NBow encoder on Go are respectively 0.6881 and 0.1165. Now, we focus on the MRR metric. As shown in <ref type="table" target="#tab_0">Table  II</ref>, the SelfAtt model performs the best in four languages, and NBoW has the highest MRR scores in two languages. NBoW performs slightly better than RNN even though they have very close MRR scores. CNN is the worst model when in most languages. This finding indicates that SelfAtt, NBoW, and RNN are more powerful as encoders than CNN.</p><p>Based on the comparisons of languages, the models perform better on Go, then Python, but have the worst performance on Ruby, then JavaScript. This finding is consistent with similar results from the literature <ref type="bibr" target="#b1">[2]</ref>. Let us now focus on the NDCG metric. As detailed in <ref type="table" target="#tab_0">Table  III</ref>, the NBoW model is the most powerful and it outperforms other models by a considerable margin. This finding supports that NBoW is a better encoder than the others. Meanwhile, we find that all baselines perform the best on Python and perform the worst on JavaScript, Go, and Ruby. If we relate this finding 3 https://github.com/jianguda/mrncs to that of the MRR scores, we could see it is easy for models to have decent results on Python but much harder on JavaScript and Ruby. This phenomenon is explainable as follows: based on our manual observations on the corpus, the code snippets in Python generally have stronger readability, and on the contrary, the JavaScript and Ruby snippets tend to overuse meaningless one-character names and abstractive functional programming statements, reducing the ability of the neural network to grasp the meaning of variables.</p><p>According to all these results, it is clear that the NBoW model is the strongest baseline model for code search. The most obvious difference between NBoW and other baselines is that NBoW does not use the order information of the token sequences. Considering that, SelfAtt is the most competitive model which studies the context information.</p><p>Answer to Research Question 1</p><p>What is the performance of siamese networks for code search when using different encoders?</p><p>Overall, the NBoW model is the best concerning the learning and generalization ability. Among the models which consider some context information, the SelfAtt model performs the most competitively. According to the CodeSearchNet dataset, neural code search tends to be easier on Python but harder on JavaScript and Ruby.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results of RQ2</head><p>In this section, we measure the accuracy of unimodal learning based on the different tree representations introduced in section III. We use the naming convention of Uni-Code and other short names explained in subsection IV-E.</p><p>To make the comparison between various representations more intuitive, we display the change ratio, that is, the relative increase or decrease degree of the MRR score relative to the Uni-code score. The change ratio is not a new metric.  <ref type="table" target="#tab_0">Table IV</ref> gives the MRR scores and change ratios for the considered unimodal representations. The rows are for code representations and the columns are for languages. For example, the MRR score on Python for the Uni-RootPath representation is 0.8305 and its change ratio is 10.26%. <ref type="table" target="#tab_0">Table IV</ref>, most tree-serialized representations are more effective than the token representation of Uni-Code. The best representation is Uni-SBT, for it has the best scores in both two languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As reflected in</head><p>The best representation on Python and Ruby is respectively Uni-LCRS and Uni-SBT, with an increase of 15.60% and 16.88% on MRR scores. Besides, Uni-LeafPath performs the worst. Even though not the best, Uni-RootPath still surpasses Uni-Code and brings stable improvements on both two languages. Uni-SBT has the best and most stable performance. In short, the traversal-based representations, like SBT and LCRS, perform better than the sampling-based representations, like RootPath and LeafPath. To our knowledge, this finding has never been reported in the literature. The link coverage and node coverage, defined in subsection IV-C, help explain why traversal-based representations are better. <ref type="table" target="#tab_4">Table V</ref> shows the link coverage and node coverage, and we see that Uni-SBT and Uni-LCRS representations usually preserve more semantic and syntactic information than others. It also explains why tree-serialized representation brings improvements over Uni-Code (Uni-Code contains no syntactic information at all as the link coverage is zero).</p><p>Compared with Uni-Code, the Uni-RootPath representation contains more syntactic information even though less semantic information, because of its higher link coverage but lower node coverage. This explains why Uni-RootPath has higher MRR scores on Python and Ruby. However, even though Uni-LeafPath has more syntactic information as well, its link coverage is not so high. It explains the reason why Uni-LeafPath obtains lower MRR scores.</p><p>From the perspective of link coverage and node coverage, Uni-SBT is the most competitive representation because its value of 100% is the highest. Another traversal-based representation, Uni-LCRS, obtains close but not so stable MRR scores because its link coverage is much smaller. However, if we compare with Uni-Code, then it is obvious that the syntactic information conveyed by Uni-LCRS is important. It proves that syntactic information is beneficial, meanwhile, enough syntactic information could improve the results stably.</p><p>Traversal-based representations contain more information, revealed by link coverage and node coverage, so they perform better than the sampling-based representations. For the path length specified in sampling and filtering operations, it is reasonable for Uni-RootPath to have such coverages. Besides, a leafpath is a subpath merged by a pair of rootpath, so a set of leafpath is more likely to have duplicated links and nodes. Thanks to the traversal strategy, Uni-SBT and Uni-LCRS have full node coverage. Unlike SBT, because of the transformation from binary tree to arbitrary tree, LCRS only reserves the links between the parent node and its most left child node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer to Research Question 2</head><p>What is the relative performance of the considered unimodal tree-serialized representations?</p><p>For unimodal learning, the tree-serialized representations are better than plain text representations. Overall, Uni-SBT is the best tree-serialized representation. Our results also shows that traversal-based representations tend to be better than sampling-based representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results of RQ3</head><p>In this section, we measure the accuracy of tree-serialized representations with the multimodal learning model. Recall the naming of Uni-Code introduced in subsection IV-E and other namings introduced in subsection IV-F, for the intuitive comparison with Uni-Code, we display change ratios as well.  <ref type="table" target="#tab_0">Table VI</ref> gives the MRR scores and change ratios for different multimodal representations. The rows are for representations and the columns are for languages. For example, the MRR score on Python for the Multi-RootPath representation is 0.8410 and its change ratio is 11.65%.</p><p>As reflected in <ref type="table" target="#tab_0">Table VI</ref>, Uni-Code performs always worse than multimodal representations, on both Python and Ruby. It clearly shows the advantage of our multimodal learning way, therefore, the combinations of Uni-Code and tree-serialized representations always bring improvements over Uni-Code.</p><p>Compared with others, multi-LeafPath obtains the worst performance. On Python and Ruby, multi-LCRS and multi-RootPath separately perform the best. Besides, multi-SBT has close MRR scores with theirs.</p><p>By comparing multimodal representations with their corresponding unimodal representations, we find that samplingbased representations obtain higher MRR scores. The biggest increase is on LeafPath even though both Uni-LeafPath and Multi-LeafPath are not so good as others. The change ranges from Uni-LeafPath to Multi-LeafPath on Python and Ruby are 15.69% and 17.82%. However, the MRR scores of traversalbased representations become slightly worse. Because of the combinations between the tree-serialized representations and the token representation, as shown in <ref type="table" target="#tab_0">Table  VII</ref>, all multimodal representations have full node coverages. Therefore, only the link coverage, namely the completeness of syntactic information, introduces the difference in the MRR scores. It is reasonable for Multi-LeafPtah to perform badly because its link coverage is always the lowest, and that also explains why the MRR scores increase dramatically from Uni-LeafPath to Multi-LeafPath.</p><p>Even though Multi-SBT has the highest link coverage, its scores are not always as good as Multi-RootPath or Multi-LCRS. It indicates that the combination of Uni-SBT and Uni-Code does not necessarily help reveal more information. The cause is that Uni-SBT representation has as complete semantic information as Uni-Code, and meanwhile more syntactic information. Therefore, the combination with the Uni-Code representation is not capable to complement more information, but instead, adds some noise to the syntactic information. Similarly, it to some extent explains why Uni-SBT is better than Multi-SBT. Actually, the same observation is observed in Multi-LCRS as well, on Python but not on Ruby. That it is not obvious in Multi-LCRS is because the link coverage of Uni-LCRS is a bit low, so it could be affected by the random syntactic information of Uni-Code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer to Research Question 3</head><p>What is the effectiveness of multimodal representations for code search?</p><p>The multimodal representations surpass the unimodal ones for code search. Considering conciseness and effectiveness, RootPath and SBT are the recommended sampling-based and traversal-based representations respectively. Overall, a multimodal representation can be considered as the new state-of-the-art for neural code search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Threats to Validity</head><p>The multimodal representation approach and uni-model representation approach only experiment on two languages. According to our results, the accuracy scores reveal slight differences in different languages, but we are not sure whether it is caused by the data amounts or the data characteristics. It should make our conclusions more reliable if there are adequate experiments on other languages.</p><p>The query data are not necessarily of high quality or most semantically similar to respective code snippets. The whole corpus is using documentation texts in code snippets as the query data. However, documentation texts are likely to be outdated or off-topic, sometimes these texts are even automatically generated. Besides, the code-query pairs from the corpus rely on the hypothesis that code and query data from the same pair is most semantically similar. However, this hypothesis could be affected by the quality of the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK</head><p>There have been lots of work studying code search <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. Limited to source code search, the typical application scenarios and progress of code search are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. API-based Code Search</head><p>API tokens and call sequences usually contain valuable semantic information. Some research is centered on the cluster and search problem of similar code based on the API data.</p><p>MAPO <ref type="bibr" target="#b24">[25]</ref> extracts API call sequences from code snippets and then groups them into clusters. The most frequent call sequences are recognized as usage patterns for further recommendation. Instead of the invocation sequences of API functions, eXoaDocs <ref type="bibr" target="#b25">[26]</ref> approximates the semantic feature vectors for clustering. GrouMiner <ref type="bibr" target="#b26">[27]</ref> mines API usage patterns by representing code as graphs to utilize the structural information. UPMiner <ref type="bibr" target="#b27">[28]</ref> introduces two quality metrics and designs a two-step clustering strategy to mine succinct and high-coverage API usage patterns. MUSE <ref type="bibr" target="#b28">[29]</ref> combines program slicing and text-based clone detection to rank the groups of similar examples, considering their popularity. CodeKernel <ref type="bibr" target="#b29">[30]</ref> represents code samples as object usage graphs, and then uses a graph kernel to embed them into a continuous space for clustering, and eventually rank code samples considering both centrality and specificity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Constraint-based Code Search</head><p>The input-output pairs and control flow graph of programs are capable to generate program constraints <ref type="bibr" target="#b30">[31]</ref>. They specify what behaviors the target code snippets are expected to have.</p><p>Satsy <ref type="bibr" target="#b31">[32]</ref> utilizes symbolic execution engines to support the encoding work on multi-path programs, and then ranks code candidates based on the matching degree between the query and encoded program paths. Quebio <ref type="bibr" target="#b32">[33]</ref> encodes code snippets into path constraints via symbolic analysis and returns the fulfilled programs. Compared with Satsy, Quebio supports more data types and operations, and the invocation to library APIs. When structural or semantic properties of code snippets are expressed as logic facts, ALICE <ref type="bibr" target="#b33">[34]</ref> extracts a logic query from the given code example and infers new logic queries based on feedback. Yogo <ref type="bibr" target="#b34">[35]</ref> studies the dataflow equivalences with rewrite rules, to recognize equivalent code fragments as long as they are the same higher-level concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Example-based Code Search</head><p>Given a code example, similar code snippets are retrieved at varying semantic and syntactic levels. It is common to find code snippets that are similar but subtly different as results.</p><p>SourcererCC <ref type="bibr" target="#b35">[36]</ref> computes the similarity of code snippets based on the overlap degree in token level. FaCoY <ref type="bibr" target="#b36">[37]</ref> analyzes the functionality of the code example and utilizes representative tokens from available code snippets which own that functionality, to find the most similar candidates. CodeNuance analyzes code commonalities and differences to support exploratory code search <ref type="bibr" target="#b37">[38]</ref>. Aroma <ref type="bibr" target="#b38">[39]</ref> first assembles code snippets that match the query, then computes the similarity based on their structural features, and eventually clusters and intersects relevant code candidates. Siamese <ref type="bibr" target="#b39">[40]</ref> improves code search by transforming code into a multirepresentation and reducing the long query to the query composed of only rare tokens. It computes a customized score for each code snippet to support incremental updates to the source code bases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Text-based Code Search</head><p>In text-based code search, the query intentions are expressed in natural language. It is feasible to see code data as text, so information retrieval techniques apply. Besides, deep learning methods show superiority in capturing data similarity.</p><p>Sourcerer <ref type="bibr" target="#b40">[41]</ref> simply regards the code samples as text and then computes the TF-IDF <ref type="bibr" target="#b41">[42]</ref> score to measure the relevance level with the given query. RACS <ref type="bibr" target="#b42">[43]</ref> extracts action relationship graph from the query data and represents API usage patterns from the collected code snippets as the method dependency graphs. In this way, the code search problem is reduced to the problem of finding similar method dependency graphs for a given action relationship graph. CodeExchange and CodeLikeThis are capable to leverage the results in query formulation. The former lists characteristics to search similar results for new queries and the latter directly searches results analogous to current results <ref type="bibr" target="#b43">[44]</ref>. By utilizing the API documentation, query words could be expanded <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> with the potential APIs referred to. To improve the matching degree between the query and code data, queries are studied to be well reformulated by considering the crowd knowledge and other techniques such as context awareness <ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref>.</p><p>Since the code search task is being explained as a probabilistic model <ref type="bibr" target="#b50">[51]</ref>, various deep learning solutions are proposed. CODE-NN <ref type="bibr" target="#b51">[52]</ref> leverages LSTM to build the translation model between code snippets and natural language texts. NCS <ref type="bibr" target="#b52">[53]</ref> combines word embedding, TF-IDF weighting, and a supervision layer for code search. UNIF <ref type="bibr" target="#b53">[54]</ref> extends NCS by using the bag-of-words model and turn to use the attention mechanism for embedding weights. SCS <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref> uses sequence-to-sequence networks to map code and query data into the shared vector space, to make semantically similar code-query pairs close to each other. Instead of merely regarding both code and query as text sequence, DeepCS <ref type="bibr" target="#b5">[6]</ref> extracts function names, invocation sequences, and token set from the code data, and compute the cosine similarity of code and query vectors to support the search process. MMAN <ref type="bibr" target="#b56">[57]</ref> utilizes the data of multiple modalities for better code representation. It represents tokens, abstract syntax tree, and control flow graph by using LSTM, Tree-LSTM <ref type="bibr" target="#b57">[58]</ref>, and GGNN <ref type="bibr" target="#b58">[59]</ref> respectively. Aimed to tackle certain issues in code search, CQIL <ref type="bibr" target="#b59">[60]</ref> models the semantic correlations between code and query with hybrid representations. Similarly, NJACS <ref type="bibr" target="#b60">[61]</ref>, CARLCS <ref type="bibr" target="#b61">[62]</ref>, TabCS <ref type="bibr" target="#b62">[63]</ref> and SANCS <ref type="bibr" target="#b63">[64]</ref> learn attention-based representations of code and query with the co-attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>Our work is about applying tree-serialized representations by the multimodal learning way for code search. The core idea of our multimodal representation model is to utilize both semantic and syntactic information of code snippets.</p><p>We designed a novel tree structure named Simplified Semantic Tree (SST). SST is more semantically informative than AST, then we introduced several tree-serialization methods on SST to build tree-serialized representations to complement the token representation. Moreover, we combined tree-serialized representations with the token representation as multimodal representations. Our multimodal learning model follows the pseudo-siamese network architecture and adopts the SelfAtt model as its encoders. Last, we defined two intuitive quantification metrics to quantify the completeness of the semantic information and syntactic information conveyed by token representation and tree-serialized presentations.</p><p>Based on our experiments on the large-scale multi-language corpus, the SelfAtt model is most satisfying considering the context information. Among all the introduced tree-serialized representations, traversal-based representations usually perform better than the sampling-based ones. More results suggest that tree-traversal representations bring improvements of at most 16.88% and the multimodal model raises the MRR scores by at most 17.82%. We recommend adopting tree-serialized representations as well as the multimodal learning model for code search, whenever possible.</p><p>In the future, we will explore other software engineering problems where our work might apply. Besides, we are to investigate more aspects of the side of code data processing, for example, study the design details of existing novel designs similar to SST and seek potential theoretical support.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 "</head><label>2</label><figDesc>""send birthday messages to members"""3 today = datetime.date.today() 4 f o r member i n self.members: 5 birthday = member.birthday 6 i f self.anniversary(today, birthday): 7 member.SMS()</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Pseudo-Siamese Network for Code Search<ref type="bibr" target="#b1">[2]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 : 1 Figure 4 :</head><label>314</label><figDesc>AST Diagram of Listing SST Diagram of Listing 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7</head><label>7</label><figDesc>Figure 7: SBT Representation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>Multimodal Learning Schematic Diagram</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I :</head><label>I</label><figDesc>Statistics of Code-Query Pairs in the Corpus</figDesc><table><row><cell>Dataset</cell><cell>Go</cell><cell>Java</cell><cell>JavaScript</cell><cell>PHP</cell><cell>Python</cell><cell>Ruby</cell><cell>Total</cell></row><row><cell>Train</cell><cell>317,832</cell><cell>454,451</cell><cell>123,889</cell><cell cols="2">523,712 412,178</cell><cell cols="2">48,791 1,880,853</cell></row><row><cell>Valid</cell><cell>14,242</cell><cell>15,328</cell><cell>8,253</cell><cell>26,015</cell><cell>23,107</cell><cell>2,209</cell><cell>89,154</cell></row><row><cell>Test</cell><cell>14,291</cell><cell>26,909</cell><cell>6,483</cell><cell>28,391</cell><cell>22,176</cell><cell>2,279</cell><cell>100,529</cell></row><row><cell>All</cell><cell>346,365</cell><cell>496,688</cell><cell>138,625</cell><cell cols="2">578,118 457,461</cell><cell cols="2">53,279 2,070,536</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table II :</head><label>II</label><figDesc>RQ1: MRR Scores of Baseline Models</figDesc><table><row><cell>Encoder</cell><cell>Go</cell><cell>Java</cell><cell>JavaScript</cell><cell>PHP</cell><cell>Python</cell><cell>Ruby</cell><cell>Avg.</cell></row><row><cell>NBoW</cell><cell>0.6681</cell><cell>0.5867</cell><cell>0.4268</cell><cell>0.5679</cell><cell>0.6432</cell><cell>0.3210</cell><cell>0.5356</cell></row><row><cell>1D-CNN</cell><cell>0.7044</cell><cell>0.5302</cell><cell>0.2298</cell><cell>0.5429</cell><cell>0.5383</cell><cell>0.1165</cell><cell>0.4437</cell></row><row><cell>Bi-RNN</cell><cell>0.7082</cell><cell>0.5808</cell><cell>0.3685</cell><cell>0.6012</cell><cell>0.6432</cell><cell>0.2226</cell><cell>0.5208</cell></row><row><cell>SelfAtt</cell><cell>0.7257</cell><cell>0.5510</cell><cell>0.4171</cell><cell cols="4">0.6014 0.6769 0.3506 0.5538</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table III :</head><label>III</label><figDesc>RQ1: NDCG Scores of Baseline Models</figDesc><table><row><cell>Encoder</cell><cell>Go</cell><cell>Java</cell><cell>JavaScript</cell><cell>PHP</cell><cell>Python</cell><cell>Ruby</cell><cell>Avg.</cell></row><row><cell>NBoW</cell><cell>0.1165</cell><cell>0.1989</cell><cell>0.0653</cell><cell cols="4">0.1494 0.2994 0.1294 0.1598</cell></row><row><cell>1D-CNN</cell><cell>0.0139</cell><cell>0.1163</cell><cell>0.0098</cell><cell>0.1238</cell><cell>0.2044</cell><cell>0.0395</cell><cell>0.0846</cell></row><row><cell>Bi-RNN</cell><cell>0.0311</cell><cell>0.1220</cell><cell>0.0253</cell><cell>0.0976</cell><cell>0.1845</cell><cell>0.0552</cell><cell>0.0860</cell></row><row><cell>SelfAtt</cell><cell>0.0367</cell><cell>0.0951</cell><cell>0.0261</cell><cell>0.0785</cell><cell>0.1394</cell><cell>0.1152</cell><cell>0.0818</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table IV :</head><label>IV</label><figDesc>RQ2: MRR Scores of Unimodal Representations. Uni-LCRS can be considered the best representation.</figDesc><table><row><cell>Representation</cell><cell cols="2">Python</cell><cell></cell><cell>Ruby</cell></row><row><cell></cell><cell>Score</cell><cell>Change</cell><cell>Score</cell><cell>Change</cell></row><row><cell>Uni-Code</cell><cell>0.7533</cell><cell>-</cell><cell>0.3113</cell><cell>-</cell></row><row><cell>Uni-RootPath</cell><cell>0.8305</cell><cell>10.26%</cell><cell>0.3440</cell><cell>10.50%</cell></row><row><cell>Uni-LeafPath</cell><cell>0.6744</cell><cell>-10.46%</cell><cell>0.2752</cell><cell>-11.59%</cell></row><row><cell>Uni-SBT</cell><cell>0.8662</cell><cell>14.99%</cell><cell cols="2">0.3639 16.88%</cell></row><row><cell>Uni-LCRS</cell><cell cols="2">0.8707 15.60%</cell><cell>0.3423</cell><cell>9.96%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table V :</head><label>V</label><figDesc>Coverage Ratios of Unimodal Representations</figDesc><table><row><cell>Representation</cell><cell cols="2">Python</cell><cell cols="2">Ruby</cell></row><row><cell></cell><cell>Link</cell><cell>Node</cell><cell>Link</cell><cell>Node</cell></row><row><cell>Uni-Code</cell><cell>0%</cell><cell>100%</cell><cell>0%</cell><cell>100%</cell></row><row><cell>Uni-RootPath</cell><cell>25.02%</cell><cell>67.22%</cell><cell cols="2">27.35% 67.59%</cell></row><row><cell>Uni-LeafPath</cell><cell>2.06%</cell><cell>11.48%</cell><cell>8.53%</cell><cell>30.51%</cell></row><row><cell>Uni-SBT</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell></row><row><cell>Uni-LCRS</cell><cell>10.91%</cell><cell>100%</cell><cell>9.15%</cell><cell>100%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table VI :</head><label>VI</label><figDesc>MRR Scores of Multimodal Representations</figDesc><table><row><cell>Representation</cell><cell cols="2">Python</cell><cell></cell><cell>Ruby</cell></row><row><cell></cell><cell>Score</cell><cell>Change</cell><cell>Score</cell><cell>Change</cell></row><row><cell>Uni-Code</cell><cell>0.7533</cell><cell>-</cell><cell>0.3113</cell><cell>-</cell></row><row><cell>Multi-RootPath</cell><cell>0.8410</cell><cell>11.65%</cell><cell cols="2">0.3602 15.70%</cell></row><row><cell>Multi-LeafPath</cell><cell>0.7926</cell><cell>5.23%</cell><cell>0.3307</cell><cell>6.23%</cell></row><row><cell>Multi-SBT</cell><cell>0.8536</cell><cell>13.32%</cell><cell>0.3400</cell><cell>9.22%</cell></row><row><cell>Multi-LCRS</cell><cell cols="2">0.8563 13.68%</cell><cell>0.3470</cell><cell>11.45%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table VII :</head><label>VII</label><figDesc>Coverage Ratios of Multimodal Representations</figDesc><table><row><cell>Representation</cell><cell cols="2">Python</cell><cell>Ruby</cell><cell></cell></row><row><cell></cell><cell>Link</cell><cell>Node</cell><cell>Link</cell><cell>Node</cell></row><row><cell>Uni-Code</cell><cell>0%</cell><cell>100%</cell><cell>0%</cell><cell>100%</cell></row><row><cell cols="2">Multi-RootPath 25.02%</cell><cell>100%</cell><cell cols="2">27.35% 100%</cell></row><row><cell>Multi-LeafPath</cell><cell>2.06%</cell><cell>100%</cell><cell>8.53%</cell><cell>100%</cell></row><row><cell>Multi-SBT</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell></row><row><cell>Multi-LCRS</cell><cell>10.91%</cell><cell>100%</cell><cell>9.15%</cell><cell>100%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/tree-sitter/tree-sitter</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by Knut and Alice Wallenberg Foundation, and by the Swedish Foundation for Strategic Research (SSF), with computational resources provided by the Swedish National Infrastructure for Computing.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluating How Developers Use General-Purpose Web-Search for Code Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Md Masudur Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="465" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">CodeSearchNet Challenge: Evaluating the State of Semantic Code Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamel</forename><surname>Husain</surname></persName>
		</author>
		<idno>ArXiv abs/1909.09436</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Are the Code Snippets What We Are Searching for? A Benchmark and an Empirical Study on Code Search with Natural-Language Queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhan</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER) (2020)</title>
		<imprint>
			<biblScope unit="page" from="344" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015)</title>
		<imprint>
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identifying Corresponding Patches in SAR and Optical Images With a Pseudo-Siamese CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="784" to="788" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep Code Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM 40th International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="933" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A Convolutional Neural Network for Modelling Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>ArXiv abs/1404.2188</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep Unordered Composition Rivals Syntactic Methods for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finding Structure in Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal Machine Learning: A Survey and Taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="423" to="443" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multimodal Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A general path-based representation for predicting program properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI 2018</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Code Prediction by Feeding Trees to Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seohyun</forename><surname>Kim</surname></persName>
		</author>
		<idno>ArXiv abs/2003.13848</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep Code Comment Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM 26th International Conference on Program Comprehension (ICPC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="200" to="20010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Tree-to-tree Neural Networks for Program Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><forename type="middle">Xiaodong</forename><surname>Song</surname></persName>
		</author>
		<idno>ArXiv abs/1802.03691</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">code2vec: learning distributed representations of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<idno>ArXiv abs/1803.09473</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">code2seq: Generating Sequences from Structured Representations of Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<idno>ArXiv abs/1808.01400</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>OSDI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How developers search for code: a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caitlin</forename><surname>Sadowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><forename type="middle">T</forename><surname>Stolee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering</title>
		<meeting>the 2015 10th Joint Meeting on Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Opportunities and Challenges in Code Search Tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv abs/2011.02297</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">MAPO: Mining and Recommending API Usage Patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ECOOP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Towards an Intelligent Code Search Engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhan</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph-based mining of multiple object usage patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESEC/FSE &apos;09</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mining succinct and high-coverage API usage patterns from source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th Working Conference on Mining Software Repositories (MSR) (2013)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="319" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How Can I Use This Method?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM 37th IEEE International Conference on Software Engineering</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="880" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CodeKernel: A Graph Kernel Based Approach to the Selection of API Usage Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th IEEE/ACM International Conference on Automated Software Engineering (ASE) (2019)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="590" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Solving the Search for Source Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><forename type="middle">T</forename><surname>Stolee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dobos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Software Engineering and Methodology (TOSEM)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Code search with input/output queries: Generalizing, ranking, and assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><forename type="middle">T</forename><surname>Stolee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Dwyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Syst. Softw</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="35" to="48" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Enhancing example-based code search with functional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Syst. Softw</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page">110568</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Active Inductive Logic Programming for Code Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Sivaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE) (2019)</title>
		<imprint>
			<biblScope unit="page" from="292" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic code search via equational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Varot Premtoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armando</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solar-Lezama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SourcererCC: Scaling Code Clone Detection to Big-Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitesh</forename><surname>Sajnani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM 38th International Conference on Software Engineering (ICSE) (2016)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1157" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">FaCoY -A Code-to-Code Search Engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kisub</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM 40th International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="946" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Supporting exploratory code search with differencing and visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjian</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 25th International Conference on Software Analysis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="300" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aroma: code recommendation via structural code search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Luan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PACMPL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Siamese: scalable and incremental code clone search via multiple code representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaiyong</forename><surname>Ragkhitwetsagul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krinke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Software Engineering</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sourcerer: mining and searching internet-scale software repositories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Linstead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="300" to="336" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Automatic query reformulations for text retrieval in software engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Haiduc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th International Conference on Software Engineering (ICSE) (2013)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="842" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Relationship-aware code search for JavaScript frameworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering</title>
		<meeting>the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Understanding the impact of support for iteration on code search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Martie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering</title>
		<meeting>the 2017 11th Joint Meeting on Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">CodeHow: Effective Code Search Based on API Understanding and Extended Boolean Model (E)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th IEEE/ACM International Conference on Automated Software Engineering (ASE</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Expanding Queries for Code Search Using Semantically Related API Class-names</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1070" to="1082" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Augmenting and structuring user queries to support efficient free-form code search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Sirres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Engineering</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2622" to="2654" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Effective Reformulation of Query for Code Search Using Crowdsourced Knowledge and Extra-Large Data Analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="473" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Automatic query reformulation for code search using crowdsourced knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Software Engineering</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Enhance code search via reformulating queries with evolving contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automated Software Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="705" to="732" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Bimodal Modelling of Source Code and Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Summarizing Source Code using a Neural Attention Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srini</forename><surname>Iyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Retrieval on source code: a neural code search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saksham</forename><surname>Sachdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MAPL 2018</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">When Deep Learning Met Code Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Cambronero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESEC/SIGSOFT FSE</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">How To Create Natural Language Semantic Search For Arbitrary Objects With Deep Learning. Sept</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamel</forename><surname>Husain</surname></persName>
		</author>
		<ptr target="https://towardsdatascience.com/semantic-code-search-3cd6d244a39c" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Towards Natural Language Semantic Code Search -The GitHub Blog. Sept</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>The Github</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blog</surname></persName>
		</author>
		<ptr target="https://github.blog/2018-09-18-towards-natural-language-semantic-code-search/" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-modal Attention Network Learning for Semantic Source Code Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th IEEE/ACM International Conference on Automated Software Engineering (ASE) (2019)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR abs/1511.05493</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning Code-Query Interaction for Enhancing Code Searches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Software Maintenance and Evolution (ICSME) (2020)</title>
		<imprint>
			<biblScope unit="page" from="115" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Neural joint attention code search over structure embeddings for software Q&amp;A sites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Syst. Softw</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page">110773</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Improving Code Search with Co-Attentive Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhang</forename><surname>Shuai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Program Comprehension</title>
		<meeting>the 28th International Conference on Program Comprehension</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Two-Stage Attention-Based Model for Code Search with Textual and Structural Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER) (2021)</title>
		<imprint>
			<biblScope unit="page" from="342" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Self-Attention Networks for Code Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Softw. Technol</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page">106542</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
