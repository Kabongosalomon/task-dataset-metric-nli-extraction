<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Multi-Head Convolutional Attention with Various Kernel Sizes for Medical Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Georgescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea-Iuliana</forename><surname>Miron</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Davila&quot; University of Medicine and Pharmacy</orgName>
								<address>
									<region>Carol</region>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Col?ea Hospital</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivian</forename><surname>Savencu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Davila&quot; University of Medicine and Pharmacy</orgName>
								<address>
									<region>Carol</region>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Col?ea Hospital</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolae-C?t?lin</forename><surname>Ristea</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolae</forename><surname>Verga</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Davila&quot; University of Medicine and Pharmacy</orgName>
								<address>
									<region>Carol</region>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Col?ea Hospital</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahbaz</forename><surname>Fahad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khan</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">MBZ University of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Link?ping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Multi-Head Convolutional Attention with Various Kernel Sizes for Medical Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Super-resolving medical images can help physicians in providing more accurate diagnostics. In many situations, computed tomography (CT) or magnetic resonance imaging (MRI) techniques capture several scans (modes) during a single investigation, which can jointly be used (in a multimodal fashion) to further boost the quality of superresolution results. To this end, we propose a novel multimodal multi-head convolutional attention module to superresolve CT and MRI scans. Our attention module uses the convolution operation to perform joint spatial-channel attention on multiple concatenated input tensors, where the kernel (receptive field) size controls the reduction rate of the spatial attention, and the number of convolutional filters controls the reduction rate of the channel attention, respectively. We introduce multiple attention heads, each head having a distinct receptive field size corresponding to a particular reduction rate for the spatial attention. We integrate our multimodal multi-head convolutional attention (MMHCA) into two deep neural architectures for superresolution and conduct experiments on three data sets. Our empirical results show the superiority of our attention module over the state-of-the-art attention mechanisms used in super-resolution. Moreover, we conduct an ablation study to assess the impact of the components involved in our attention module, e.g. the number of inputs or the number of heads. Our code is freely available at https: //github.com/lilygeorgescu/MHCA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Magnetic Resonance Imaging (MRI) and Computer Tomography (CT) scanners are non-invasive investigation tools that produce cross-sectional images of various organs or body parts. In common medical practice, the resulting scans are used to diagnose and treat various lesions, ranging from malignant tumors to hemorrhages. Moreover, lesion detection and segmentation from CT and MRI scans are central problems studied in medical imaging, being ad-dressed via automatic techniques <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42]</ref>. However, one voxel in typical MRI or CT scans corresponds to a cubic millimeter of tissue at best, which translates into a rather low resolution, preventing precise diagnosis and treatment. Indeed, according to Georgescu et al. <ref type="bibr" target="#b13">[14]</ref>, physicians recognize the necessity of increasing the resolution of MRI and CT scans to improve the accuracy of diagnosis and treatment. Furthermore, a recent study <ref type="bibr" target="#b33">[34]</ref> shows that superresolution can also aid deep learning models to increase segmentation performance. Due to the aforementioned benefits, we consider that medical image super-resolution (SR) is a very important task for medicine nowadays.</p><p>A common medical practice is to take multiple scans with various contrasts (modes) during a single investigation, providing richer information to physicians, who get a more clear picture of the patients. A series of previous works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> showed the benefits of using multicontrast (multimodal) scans to improve super-resolution results. While previous works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> combined a low-resolution (LR) scan with a high-resolution (HR) scan of distinct contrasts, to the best of our knowledge, we are the first to study super-resolution with multiple low resolution scans as input. Our approach is applicable to a broader set of CT/MRI scanners, as it does not require the availability of an HR input from another modality (this is rarely available in daily medical practice).</p><p>To approach multi-contrast medical image superresolution, we propose a novel multimodal multi-head convolutional attention (MMHCA) mechanism that performs joint spatial and channel attention within each head, by stacking a convolutional (conv) layer and a deconvolutional (deconv) layer, as illustrated in <ref type="figure">Figure 1</ref>. Tensors from different contrast neural branches are concatenated along the channel dimension and given as input to our attention module. The convolutional layer reduces the input tensor both spatially and channel-wise. The channel reduction rate is controlled by adjusting the number of convolutional filters, while the spatial reduction rate is controlled by adjusting the size of the kernel (receptive field). <ref type="bibr">The</ref>   <ref type="figure">Figure 1</ref>. Our multimodal multi-head convolutional attention module (MMHCA) with h heads, integrated into some neural architecture for super-resolution. Input low-resolution (LR) images of distinct contrasts are processed by independent branches and the resulting tensors are concatenated. The concatenated tensor is provided as input to every attention head. Each attention head applies conv and deconv operations, using a kernel size that is unique to the respective head. The resulting tensors are summed up and passed through a sigmoid layer. Finally, the attention tensor is multiplied (element-wise) with the concatenated tensor, and the result is further processed by the network to obtain the high-resolution (HR) image. Best viewed in color. the output of the convolutional layer back to its original size. We introduce multiple attention heads, each head having a distinct kernel size corresponding to a particular reduction rate for the spatial attention. The tensors from all attention heads are summed up and passed through a sigmoid layer, obtaining the final attention. The attention tensor is multiplied with the input tensor, enabling the neural network to focus on the most interesting regions from each input image. As other attention modules <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43]</ref>, MMHCA relies on the bottleneck principle to force the model in keeping the information that merits attention.</p><p>We integrate our multi-input multi-head convolutional attention into two deep neural architectures for superresolution <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26]</ref> and conduct experiments on three data sets: IXI, NAMIC Multimodality, and Coltea-Lung-CT-100W. These data sets contain multi-contrast investigations which allow us to evaluate our multimodal framework. Our results show that MMHCA brings significant performance gains for both neural networks on all three data sets. Moreover, our framework outperforms recently introduced attention modules <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43]</ref>, as well as state-of-the-art methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>. Aside from evaluating methods via automatic measures, i.e. the peak signal-to-noise ratio (PSNR) and the structural similarity index measure (SSIM), we conduct a subjective evaluation study, asking three physicians to compare the super-resolution results of a state-of-the-art model, before and after adding MMHCA, without disclosing the method producing each image. The least number of votes assigned by a human annotator to MMHCA is 75%, suggesting that its performance gains are indeed significant. In addition, we present ablation results indicating that each component involved in our attention module is important.</p><p>In summary, our contribution is threefold:</p><p>? We are the first to perform medical image superresolution using a multimodal low-resolution input.</p><p>? We propose a novel multimodal multi-head convolutional attention mechanism for multi-contrast medical image SR.</p><p>? We present empirical evidence showing that our attention module brings significant performance gains on three multi-contrast data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Image Super-Resolution</head><p>Most of the recent works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> addressing the superresolution task use deep learning methods in order to increase the resolution of images. One of the early studies employing deep convolution neural networks (CNNs) for super-resolution is the work of Kim et al. <ref type="bibr" target="#b23">[24]</ref>, which introduces the Very Deep Super-Resolution (VDSR) network. The VDSR model consists of 20 convolutional layers and takes as input the interpolated low-resolution (ILR) image. Different from the work of Kim et al. <ref type="bibr" target="#b23">[24]</ref> which relies on ILR images, Shi et al. <ref type="bibr" target="#b37">[38]</ref> proposed the efficient sub-pixel convolutional (ESPC) layer, which learns to upscale low-resolution feature maps into the high-resolution output. By eliminating the reliance on ILR images (which have the same height and width as HR images), ESPC is capable of decreasing the running time by a significant margin. After the introduction of the ESPC layer, many researchers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50]</ref> adopted this approach in their super-resolution (SR) models.</p><p>Super-resolution methods have also shown their benefits in medical imaging. Medical image super-resolution works can be grouped into two categories, where one category is focused on increasing the resolution of individual CT or MRI slices (2D images) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>, while the other is focused on increasing the resolution of entire 3D scans (volumes) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref>. Similar to <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>, in this work, we are focusing on increasing the resolution of CT and MRI slices. Gu et al. <ref type="bibr" target="#b15">[16]</ref> proposed the MedSRGAN model in order to upsample the resolution of 2D medical images using Generative Adversarial Networks (GANs) <ref type="bibr" target="#b14">[15]</ref>. MedSRGAN employs a residual map attention network in the generator to extract useful information from different channels. Gu et al. <ref type="bibr" target="#b15">[16]</ref> also used a multi-task loss function comprised of several losses (content loss, adversarial loss and adversarial feature loss) to train the Med-SRGAN model. Georgescu et al. <ref type="bibr" target="#b13">[14]</ref> proposed a method to increase the resolution of both 2D and 3D medical images. To super-resolve 3D images, Georgescu et al. <ref type="bibr" target="#b13">[14]</ref> used two CNNs in a sequential manner, the first CNN increasing the resolution on two axes (height and width) and the second CNN increasing the resolution on the third axis (depth). Their approach can be used to extend any method from 2D SR to 3D SR, including our own.</p><p>Most of the related works employ single-contrast superresolution (SCSR) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>, meaning that they utilize a single-contrast image as input for the upsampling network. There are also some works that approach multi-contrast super-resolution (MCSR) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> using an HR image from another modality, e.g. a T1-weighted 1 slice, to increase the resolution of the targeted LR modality, e.g. a T2-weighted slice. Zeng et al. <ref type="bibr" target="#b46">[47]</ref> proposed a model consisting of two sub-networks to simultaneously perform SCSR and MCSR. The first sub-network performs SCSR, upsampling the target modality, while the second sub-network uses the output of the first sub-network and an HR image from a differ-ent modality to further refine the target modality. Feng et al. <ref type="bibr" target="#b9">[10]</ref> proposed a multi-stage integration network (MINet) for MCSR. The MINet model uses two input images that are processed in parallel by two independent networks, and their features are fused at each layer to obtain multi-stage feature representations. Similar to Zeng et al. <ref type="bibr" target="#b46">[47]</ref>, Feng et al. <ref type="bibr" target="#b9">[10]</ref> integrated a second HR modality to increase the performance of their model. Different from previous works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>, we do not employ any HR image to guide our model. Instead, we only rely on the lowresolution images pertaining to different modalities. To the best of our knowledge, we are the first to propose an MCSR method based solely on LR medical images as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attention Mechanism</head><p>The attention mechanism is a very hot topic in the computer vision community, having broad applications ranging from mainstream computer vision tasks, such as image classification <ref type="bibr" target="#b42">[43]</ref>, to more specific tasks, such as natural image SR <ref type="bibr" target="#b28">[29]</ref> and medical image SR <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b47">48]</ref>. Attention mechanisms are integrated into neural networks to direct the attention of the model to the area with relevant information. Hu et al. <ref type="bibr" target="#b18">[19]</ref> proposed the squeeze-and-excitation (SE) block to recalibrate the channel responses, thus performing channel-wise attention. In order to further increase the power of the attention mechanism, Woo et al. <ref type="bibr" target="#b42">[43]</ref> proposed the Convolutional Block Attention Module (CBAM), which infers attention maps for two separate dimensions, spatial and channel, in a sequential manner. Niu et al. <ref type="bibr" target="#b28">[29]</ref> proposed the channel-spatial attention module (CSAM) to boost the performance of natural image SR. CSAM is composed of 3D conv layers, being able to learn the channel and spatial interdependencies of the features. In a similar fashion to Niu et al. <ref type="bibr" target="#b28">[29]</ref>, Feng et al. <ref type="bibr" target="#b9">[10]</ref> employed 3D conv layers to generate attention maps that capture both channel and spatial information. Unlike previous works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43]</ref>, our attention module is based on 2D convolution and deconvolution operations with multiple kernel sizes to perform joint multi-head spatial-channel attention.</p><p>The self-attention mechanism <ref type="bibr" target="#b39">[40]</ref> triggered the development of models solely based on attention, such as vision transformers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref>, which have been adopted at an astonishing rate by the computer vision and medical imaging communities, likely due to the impressive results across a wide range of problems, from object recognition <ref type="bibr">[6,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44]</ref> and object detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref> to medical image segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17]</ref> and medical image generation <ref type="bibr" target="#b32">[33]</ref>. Although transformers <ref type="bibr" target="#b35">[36]</ref> have been applied to several mainstream medical imaging tasks, the number of transformer-based methods applied to medical image super-resolution is relatively small <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Different from methods relying solely on attention-based architectures <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, we propose a novel and flexible at-tention module that can be integrated into various architectures. To support this claim, we integrate MMHCA into two state-of-the-art architectures <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26]</ref>, providing empirical evidence showing that our attention module can significantly boost the performance of both models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given a multi-contrast input formed of n low-resolution input images of p ? p pixels, denoted as LR 1 , LR 2 , ..., LR n , our goal is to obtain an HR image of r ? r pixels, denoted as HR k , where r &gt; p, for the target modality k ? {1, 2, ..., n}. In common practice, r is typically chosen to be equal to 2p or 4p, corresponding to super-resolution factors of 2? or 4?, respectively. In our experiments, we consider these commonly-used SR factors.</p><p>We propose a spatial-channel attention module to combine the information contained by the multi-contrast LR images. As illustrated in <ref type="figure">Figure 1</ref>, our multimodal multi-head convolutional attention (MMHCA) mechanism can be introduced at any layer of any neural architecture, thus being generic and flexible. We underline that if the baseline architecture is designed for a single-contrast input, we can easily extend the architecture to a multi-contrast input of n contrasts by replicating the neural branch that comes before our module, for a number of n times. Let f i be the neural branch that processes the input LR i . We first obtain the encoding tensor T i for each input LR i , as follows:</p><formula xml:id="formula_0">T i = f i (? fi , LR i ) ,<label>(1)</label></formula><p>where ? fi are the weights of the neural branch f i . The next step in our approach is to concatenate the encoding tensors T i of all modalities along the channel axis, obtaining the tensor T ? , as follows:</p><formula xml:id="formula_1">T ? = concat (T 1 , T 2 , ..., T n ) ,<label>(2)</label></formula><p>where concat represents the concatenation operation. If a multimodal input is not available, our multi-head convolutional attention (MHCA) module can still be applied by considering T ? = T 1 , where T 1 is the encoding tensor of the single-contrast input. We use this ablated configuration in our experiments to show the benefits of using multiple modalities as opposed to a single modality. Next, we apply the multi-head convolutional attention. Our attention module is composed of multiple heads, which are applied on the concatenation of the encoding tensors, denoted as T ? . Each head h j , j ? {1, 2, ..., h}, where h is the number of heads, is composed of a conv layer followed by a deconv layer, performing both spatial and channel attention. The conv layer jointly reduces the spatial and channel dimensions of the input, while the deconv layer is configured to revert the dimensional reduction performed by the conv layer, thus restoring the size of the output tensor to the size of the input tensor T ? .</p><p>For the j-th convolutional attention head h j , we set the conv kernel size k j to 2 ? (j ? 1) + 1. Hence, the first head is formed of kernels having a receptive field of 1 ? 1, the second head is formed of kernels having a receptive field of 3 ? 3, and so on. We note that each kernel size corresponds to a different reduction rate of the spatial attention. For each head h j , we set the number of conv filters to c r , where c is the number of input channels and r is the reduction rate of the channel attention. Then, we apply a deconv layer with c filters and the kernel size set to k j ? k j to increase the spatial size of the activation maps. We use a stride of 1 and a padding equal to 0 for both conv and deconv layers. In summary, the output H j of the attention head h j is computed as follows:</p><formula xml:id="formula_2">H j = h j ? cj , ? dj , T ? = max 0, T ? * ? cj ? dj ,<label>(3)</label></formula><p>where ? cj are the learnable weights of the conv layer comprising c r filters with a kernel size of k j ? k j , ? dj are the parameters of the deconv layer comprising c filters with a kernel size of k j ? k j , max(0, ?) is the ReLU activation function, * is the convolution operation and is the deconvolution operation. We hereby note that the tensors H 1 , H 2 , ..., H h are of the same size.</p><p>Next, we sum all the tensors H j produced by the attention heads, obtaining H + , as follows:</p><formula xml:id="formula_3">H + = h j=1 H j .<label>(4)</label></formula><p>We then pass the resulting tensor through the sigmoid function in order to obtain the final attention tensor denoted as A, as follows:</p><formula xml:id="formula_4">A = ? (H + ) .<label>(5)</label></formula><p>To apply the learned attention to the encoding tensors, the attention tensor A is multiplied with the tensor T ? , obtaining the tensor T * , as follows:</p><formula xml:id="formula_5">T * = T ? ? A,<label>(6)</label></formula><p>where ? denotes the element-wise multiplication operation. Let g denote the neural branch that takes T * as input and produces the high-resolution output. The last processing required to obtain the final output of the neural architecture is expressed as follows:</p><formula xml:id="formula_6">HR k = g (? g , T * ) ,<label>(7)</label></formula><p>where ? g are the learnable weights of g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Sets</head><p>IXI. The IXI 2 data set is the largest benchmark considered in our evaluation. We use the same version of the IXI data set as <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b49">50]</ref>. The data set contains 3D multimodal MRI scans, where each MRI has three modalities, namely T1weighted (T1w), T2-weighted (T2w) and Proton Density (PD). The data set is split into 500 multimodal MRI scans for training, 6 multimodal scans for validation and the remaining 70 multimodal scans for testing. Each MRI scan has 96 slices with the resolution of 240 ? 240 pixels. NAMIC Brain Multimodality. The National Alliance for Medical Image Computing (NAMIC) Brain Multimodality 3 data set is formed of 20 3D MRI scans. Each 3D image is formed of 176 slices with the resolution of 256 ? 256 pixels. The data set contains two modalities, namely T1w and T2w. Following <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref>, we randomly split the data set into 10 multimodal MRI scans for training and 10 multimodal MRI scans for testing. We randomly take 2 MRI scans from the training set to create a validation set for hyperparameter tuning.</p><p>Coltea-Lung-CT-100W. The Coltea-Lung-CT-100W data set was recently introduced by Ristea et al. <ref type="bibr" target="#b32">[33]</ref>. It contains 100 triphasic (multimodal) lung CT scans. Each scan has three modalities, namely native, arterial and venous. The entire data set is formed of 12,430 triphasic slices and each slice has 512 ? 512 pixels. The data set is split into 70 multimodal CT scans for training, 15 multimodal scans for validation and 15 multimodal scans for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>As evaluation metrics, we employ the peak signal-tonoise ratio (PSNR) and the structural similarity index measure (SSIM) <ref type="bibr" target="#b40">[41]</ref>. PSNR is the ratio between the maximum possible signal and the power of the noise. It only takes into account the difference between pixels, without quantifying the structural similarity. SSIM <ref type="bibr" target="#b40">[41]</ref> takes the structural similarity into account by combining the contrast, the luminance and the texture of the images. Higher values of PSNR and SSIM indicate better reconstruction. We emphasize that PSNR is represented in the log-scale. Hence, seemingly small gains in terms of PSNR can indicate significant quality improvements.</p><p>To further assess the improvement brought by our attention module, we also conduct a subjective evaluation study, asking three physicians (specialized in radiology) from the Col?ea Hospital to compare the super-resolution results of a state-of-the-art model, before and after adding MMHCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>We compare our attention module with CSAM <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29]</ref> and CBAM <ref type="bibr" target="#b42">[43]</ref>. We consider EDSR <ref type="bibr" target="#b25">[26]</ref> and CNN+ESPC <ref type="bibr" target="#b13">[14]</ref> as underlying models for the attention mechanisms. To train the EDSR 4 and CNN+ESPC 5 models, we use the official code released by the corresponding authors. For the EDSR <ref type="bibr" target="#b25">[26]</ref> network, we set B = 16 and F = 64. All the other hyperparameters are left unchanged. For CNN+ESPC <ref type="bibr" target="#b13">[14]</ref>, we use the same hyperparameters as suggested by the authors.</p><p>For a fair comparison, we integrate all the attention modules in the same manner into both networks. For the experiments with single-contrast inputs, we integrate the modules (CSAM, CBAM, MHCA) after each ResBlock. For the experiments with multi-contrast inputs, we replicate the sub-network which ends just before the upsampling layer, creating copies of the sub-network (each copy having its own learnable weights). Then, we concatenate the output of each sub-network and introduce the attention modules (MCSAM, MCBAM, MMHCA). The multi-contrast inputs are used without prior alignment.</p><p>For MHCA/MMHCA, we tune the number of heads h and the channel reduction ratio r on the validation set of each benchmark. We find that the optimal configuration is based on h = 3 heads and a channel reduction ratio of r = 0.5. This configuration uses kernel sizes of 1 ? 1 for h 1 , 3 ? 3 for h 2 , and 5 ? 5 for h 3 , respectively.</p><p>When comparing our attention modules (MHCA/MMHCA) with CSAM/MCSAM 6 <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29]</ref> and CBAM/MCBAM 7 <ref type="bibr" target="#b42">[43]</ref>, we use the official code released by the respective authors. As for MHCA/MMHCA, we tune the hyperparameters of CBAM/MCBAM on the validation sets, while CSAM/MCSAM have no hyperparameters that would require tuning. The optimal configuration for CBAM/MCBAM uses a kernel size of 7 ? 7 and a reduction ratio of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>Following <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>, we conduct experiments on the T2w target modality for the IXI and NAMIC data sets. The + sign after a method's name indicates the use of the geometric self-ensemble <ref type="bibr" target="#b25">[26]</ref>. IXI. We present the results obtained on the IXI data set for two upscaling factors, 2? and 4?, in <ref type="table">Table 1</ref>. The EDSR <ref type="bibr" target="#b25">[26]</ref> model obtains a PSNR of 39.81 and an SSIM of 0.9865 for an upscaling factor of 2?. When we add the MHCA module to the single-contrast network, the performance increases to 40.11 and 0.9871 in terms of PSNR and SSIM, respectively. When we switch to the multimodal input, we observe performance improvements, regardless of the integrated attention module (MCSAM <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29]</ref>, MCBAM <ref type="bibr" target="#b42">[43]</ref> or MMHCA). This confirms our hypothesis that the information from the multi-contrast LR images is useful. By adding MMHCA, the performance improves even further, exceeding the performance of both MCSAM and MCBAM. When EDSR is employed as underlying model, MMHCA + brings significant gains, generally outperforming the stateof-the-art methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>.  <ref type="figure">Figure 2</ref>. Examples of super-resolved MRI slices from the IXI data set, for an upscaling factor of 4?. The HR images produced by two baselines (bicubic interpolation and CNN+ESPC <ref type="bibr" target="#b13">[14]</ref>) are compared with the images given by two enhanced versions of CNN+ESPC <ref type="bibr" target="#b13">[14]</ref>, one based on our single-contrast attention module (MHCA), and another based on our multimodal attention module (MMHCA).</p><p>In <ref type="figure">Figure 2</ref>, we illustrate qualitative results obtained by two baselines (bicubic and CNN+ESPC <ref type="bibr" target="#b13">[14]</ref>) versus two enhanced versions of CNN+ESPC <ref type="bibr" target="#b13">[14]</ref>, namely CNN+ESPC <ref type="bibr" target="#b13">[14]</ref> + MHCA and CNN+ESPC <ref type="bibr" target="#b13">[14]</ref> + MMHCA, for an upscaling factor of 4?. We observe that our CNN+ESPC <ref type="bibr" target="#b13">[14]</ref> + MMHCA model obtains superior SR results compared with the baselines (bicubic, CNN+ESPC <ref type="bibr" target="#b13">[14]</ref>), being able to recover structural details that are completely lost by the baselines. NAMIC. We show the results obtained on the NAMIC data set for two upscaling factors, 2? and 4?, in <ref type="table">Table 2</ref>. The baseline CNN+ESPC <ref type="bibr" target="#b13">[14]</ref> obtains a score of 33.92 in terms of PSNR and 0.9509 in terms of SSIM, for an upscaling factor of 4?. When we add MHCA and MMHCA, the scores improve by considerable margins (in terms of PSNR, the minimum improvement is 0.69), regardless of the scale (2? or 4?) or the underlying model (EDSR <ref type="bibr" target="#b25">[26]</ref> or CNN+ESPC <ref type="bibr" target="#b13">[14]</ref>). Coltea-Lung-CT-100W. We show the results obtained on the Coltea-Lung-CT-100W for two upscaling factors, 2? and 4?, in <ref type="table">Table 3</ref>. Once again, we observe performance improvements brought by the use of multi-contrast LR inputs, the only attention that does not increase the baseline performance being MCBAM <ref type="bibr" target="#b42">[43]</ref>. Our MMHCA module exceeds the baseline and the other attention modules by significant margins (in terms of PSNR, the minimum improvement is 0.47), regardless of the scale or the network. Quality assessment by physicians. In <ref type="table">Table 4</ref>, we present the results of our subjective human evaluation study based on 100 cases, which are randomly selected from the IXI test  <ref type="table">Table 1</ref>. PSNR and SSIM scores of various state-of-the-art methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref> on the IXI data set, for the T2w target modality. For two of the existing methods (EDSR <ref type="bibr" target="#b25">[26]</ref> and CNN+ESPC <ref type="bibr" target="#b13">[14]</ref>), we evaluate enhanced versions, considering various state-of-the-art attention modules <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43]</ref>, as well as our own attention module (MMHCA). We consider both singlecontrast (CSAM, CBAM, MHCA) and multi-contrast (MCSAM, MCBAM, MMHCA, MMHCA + ) versions. The top two scores for each scaling factor (2? and 4?) are highlighted in red and blue, respectively. set, for EDSR <ref type="bibr" target="#b25">[26]</ref>, with and without MMHCA, considering an upscaling factor of 4?. The quality evaluation study was completed by three physicians with expertise in radiology. The doctors had to choose between two images (randomly positioned on the left side or right side of the ground-truth HR image), without knowing which method produced each image. The HR images obtained after adding MMHCA were chosen in a proportion of 81.3% against the images produced by the baseline EDSR <ref type="bibr" target="#b25">[26]</ref>. Upon disclosing the method producing each image, the doctors concluded that MMHCA helps to recover important details, e.g. blood vessels, which are missed by the baseline EDSR. Ablation study. In <ref type="table">Table 5</ref>, we present an ablation study on the NAMIC data set for a scaling factor of 2?. We observe that the best performance is obtained when the number of heads is equal to 3, regardless of the number of input modalities (MHCA or MMHCA). We also notice that every configuration of our MMHCA module obtains better results than the concatenation of the features without any at-  <ref type="table">Table 2</ref>. PSNR and SSIM scores of various state-of-the-art methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref> on the NAMIC data set, for the T2w target modality. For two of the existing methods (EDSR <ref type="bibr" target="#b25">[26]</ref> and CNN+ESPC <ref type="bibr" target="#b13">[14]</ref>), we evaluate enhanced versions, considering various state-of-the-art attention modules <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43]</ref>, as well as our own attention module (MMHCA). We consider both singlecontrast (CSAM, CBAM, MHCA) and multi-contrast (MCSAM, MCBAM, MMHCA, MMHCA + ) versions. The top two scores for each scaling factor (2? and 4?) are highlighted in red and blue, respectively. tention (multimodal input). To demonstrate the utility of the bottleneck principle, we test an ablated version of MHCA and MMHCA based on 3 heads, that does not reduce the input tensor through convolution, hence removing the deconv layer. This ablated version (3 heads, no deconv) attains lower performance compared to the complete version of MHCA and MMHCA based on 3 heads. This experiment supports our design based on the bottleneck principle through the conv and deconv operations. To show that the diversity of the heads is important, we compare two modules with 4 heads each and a reduction ratio of r = 2, one where all kernel sizes are 1 ? 1, and another where the kernels are of different sizes, from 1 ? 1 to 7 ? 7. The module based on diverse heads leads to significantly better results, indicating that using the same kernel size for all heads is suboptimal. In terms of the number of parameters, the module with 4 heads, r = 2 and kernels of 1 ? 1 is equivalent to the module with 1 head and r = 0.5. The module with 1 head attains superior results, showing that simply adding more heads of the same kind is not useful. In summary, the empirical results show that our method obtains superior results not because of the increased capacity of the model, but due to the diversity of the heads.</p><p>In <ref type="figure" target="#fig_1">Figure 3</ref>, we illustrate the impact of the channel re-  <ref type="table">Table 3</ref>. PSNR and SSIM scores of two state-of-the-art methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26]</ref> on the Coltea-Lung-CT-100W data set, for the native target modality. We evaluate enhanced versions of the existing methods, considering various state-of-the-art attention modules <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43]</ref>, as well as our own attention module (MMHCA). We consider both single-contrast (CSAM, CBAM, MHCA) and multi-contrast (MCSAM, MCBAM, MMHCA, MMHCA + ) versions. The top two scores for each scaling factor (2? and 4?) are highlighted in red and blue, respectively.  <ref type="table">Table 4</ref>. Subjective human evaluation results based on 100 randomly selected cases from the IXI test set for EDSR <ref type="bibr" target="#b25">[26]</ref>, with and without MMHCA, considering an upscaling factor of 4?. The reported numbers represent votes awarded by three physicians (with expertise in radiology) for each model. duction ratio on the MMHCA module based on 3 heads. We observe that the performance increases with the ratio until r = 0.5. After this point, further increasing the ratio does not improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we presented a novel multimodal multihead convolutional attention (MMHCA) module, which performs joint spatial and channel attention. We integrated our module into two neural networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26]</ref> and conducted experiments on three multimodal medical image benchmarks: IXI, NAMIC and Coltea-Lung-CT-100W. We showed that our attention module yields higher gains compared with competing attention modules <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43]</ref>, being able to bring the performance of the underlying models above the state-of-the-art results <ref type="bibr">[5, 12, 14, 21, 24, 26, 37,</ref>   <ref type="table">Table 5</ref>. Ablation results with EDSR <ref type="bibr" target="#b25">[26]</ref> on the NAMIC data set, for a scaling factor of 2? and the T2w target modality. We consider different configurations for MHCA/MMHCA, varying the number of heads, the number of inputs, and the size of kernels. In future work, we aim to integrate our module into further neural models and extend its applicability to natural images. We will also study the utility of the SR results in better solving other tasks, e.g. medical image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Attention Visualization</head><p>In <ref type="figure">Figure 5</ref>, we show various perspectives along the spatial and channel dimensions of a tensor given as input to MMHCA. Looking at the attention corresponding to individual activation maps (first six columns), we observe that our module attends to salient contours and edges, or even full organs. Analyzing the attention along the channel axis (last two columns), we observe that, in this example, our module tends to mainly focus on the first LR input, naturally because the first LR input is the modality (contrast type) that corresponds to the HR output, containing the most relevant information to super-resolve the image. In contrast, the second modality is scarcely attended by our module. Overall, we observe that MMHCA performs both spatial and channel attention, confirming that our attention module works as intended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Bicubic <ref type="bibr" target="#b25">[26]</ref> [26] + MHCA <ref type="bibr" target="#b25">[26]</ref> + MMHCA Ground-truth Native Arterial Venous Arterial Venous Native <ref type="figure">Figure 4</ref>. Examples of super-resolved CT images from the Coltea-Lung-CT-100W data set, for an upscaling factor of 4?. The HR images produced by two baselines (bicubic interpolation and EDSR <ref type="bibr" target="#b25">[26]</ref>) are compared with the images given by two enhanced versions of EDSR <ref type="bibr" target="#b25">[26]</ref>, one based on our single-contrast attention module (MHCA), and another based on our multimodal attention module (MMHCA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head><p>Attention Superimposed Spatial perspective Channel perspective <ref type="figure">Figure 5</ref>. Views (top row) of a tensor computed for an example from NAMIC, which is given as input to MMHCA, and the corresponding attention maps (middle row) along the spatial (first six columns) and channel (last two columns) dimensions, showing that MMHCA performs joint channel and spatial attention. Views with superimposed attention maps are displayed on the bottom row. Best viewed in color.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>PSNR and SSIM scores of the EDSR [26] + MMHCA method, considering channel reduction rates in the set {4, 2, 1, 0.5, 0.25}. Results are reported on the NAMIC data set for an upscaling factor of 2?.<ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Multimodal Multi-Head Convolutional Attention</head><label></label><figDesc>deconv layer brings arXiv:2204.04218v3 [eess.IV] 12 Oct 2022</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>head 1</cell><cell></cell><cell>conv</cell><cell>deconv</cell></row><row><cell></cell><cell></cell><cell>concatenate</cell><cell></cell><cell>...</cell><cell></cell><cell></cell></row><row><cell>LR input 1</cell><cell>tensor 1</cell><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNN</cell><cell></cell><cell>replicate</cell><cell>head 2</cell><cell>...</cell><cell>conv</cell><cell>deconv</cell><cell>+</cell></row><row><cell>LR input 2</cell><cell>tensor 2</cell><cell></cell><cell></cell><cell></cell><cell>. .</cell><cell></cell><cell>?</cell><cell>sigmoid</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.</cell><cell></cell></row><row><cell>CNN</cell><cell></cell><cell></cell><cell>head h</cell><cell></cell><cell>conv</cell><cell>deconv</cell></row><row><cell>. .</cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell><cell>attention</cell></row><row><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LR input n</cell><cell>tensor n</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell></row><row><cell>CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CNN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HR output</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>MMHCA + (ours) 39.76/0.9863 31.52/0.9337</figDesc><table><row><cell>Method</cell><cell>2? PSNR/SSIM PSNR/SSIM 4?</cell></row><row><cell>Bicubic</cell><cell>33.44/0.9589 27.86/0.8611</cell></row><row><cell>SRCNN [5]</cell><cell>37.32/0.9796 29.69/0.9052</cell></row><row><cell>VDSR [24]</cell><cell>38.65/0.9836 30.79/0.9240</cell></row><row><cell>IDN [21]</cell><cell>39.09/0.9846 31.37/0.9312</cell></row><row><cell>RDN [49]</cell><cell>38.75/0.9838 31.45/0.9324</cell></row><row><cell>FSCWRN [37]</cell><cell>39.44/0.9855 31.71/0.9359</cell></row><row><cell>CSN [50]</cell><cell>39.71/0.9863 32.05/0.9413</cell></row><row><cell>T 2 Net [12]</cell><cell>29.38/0.8720 28.66/0.8500</cell></row><row><cell>SERAN [48]</cell><cell>40.18/0.9872 32.40/0.9455</cell></row><row><cell>SERAN + [48]</cell><cell>40.30/0.9874 32.62/0.9472</cell></row><row><cell>EDSR [26]</cell><cell>39.81/0.9865 31.83/0.9377</cell></row><row><cell>EDSR [26] + CSAM [10, 29]</cell><cell>39.81/0.9865 31.83/0.9377</cell></row><row><cell>EDSR [26] + CBAM [43]</cell><cell>39.82/0.9865 31.81/0.9374</cell></row><row><cell>EDSR [26] + MHCA (ours)</cell><cell>40.11/0.9871 32.15/0.9418</cell></row><row><cell>EDSR [26] + MCSAM [10, 29]</cell><cell>40.12/0.9871 32.17/0.9417</cell></row><row><cell>EDSR [26] + MCBAM [43]</cell><cell>40.13/0.9871 32.18/0.9421</cell></row><row><cell>EDSR [26] + MMHCA (ours)</cell><cell>40.28/0.9874 32.51/0.9452</cell></row><row><cell>EDSR [26] + MMHCA + (ours)</cell><cell>40.43/0.9877 32.70/0.9469</cell></row><row><cell>CNN+ESPC [14]</cell><cell>38.67/0.9837 30.57/0.9210</cell></row><row><cell>CNN+ESPC [14] + CSAM [10, 29]</cell><cell>38.57/0.9835 30.58/0.9211</cell></row><row><cell>CNN+ESPC [14] + CBAM [43]</cell><cell>38.67/0.9838 30.47/0.9192</cell></row><row><cell>CNN+ESPC [14] + MHCA (ours)</cell><cell>39.04/0.9847 30.76/0.9233</cell></row><row><cell cols="2">CNN+ESPC [14] + MCSAM [10, 29] 38.98/0.9845 30.94/0.9265</cell></row><row><cell>CNN+ESPC [14] + MCBAM [43]</cell><cell>38.91/0.9844 30.79/0.9238</cell></row><row><cell cols="2">CNN+ESPC [14] + MMHCA (ours) 39.71/0.9862 31.52/0.9337</cell></row><row><cell>CNN+ESPC [14] +</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Method 2? 4? PSNR/SSIM PSNR/SSIM Bicubic 38.84/0.9477 31.47/0.8774 EDSR [26] 45.11/0.9621 39.52/0.9394 EDSR [26] + CSAM [10, 29] 45.12/0.9622 39.57/0.9395 EDSR [26] + CBAM [43] 41.14/0.9538 32.92/0.9023</figDesc><table><row><cell>EDSR [26] + MHCA (ours)</cell><cell>45.50/0.9634 40.23/0.9416</cell></row><row><cell>EDSR [26] + MCSAM [10, 29]</cell><cell>45.16/0.9623 39.62/0.9397</cell></row><row><cell>EDSR [26] + MCBAM [43]</cell><cell>45.12/0.9622 39.65/0.9397</cell></row><row><cell>EDSR [26] + MMHCA (ours)</cell><cell>45.58/0.9647 40.06/0.9404</cell></row><row><cell>EDSR [26] + MMHCA + (ours)</cell><cell>45.68/0.9649 40.22/0.9409</cell></row><row><cell>CNN+ESPC [14]</cell><cell>44.47/0.9599 38.34/0.9338</cell></row><row><cell>CNN+ESPC [14] + CSAM [10, 29]</cell><cell>44.45/0.9599 38.36/0.9337</cell></row><row><cell>CNN+ESPC [14] + CBAM [43]</cell><cell>44.44/0.9601 37.23/0.9308</cell></row><row><cell>CNN+ESPC [14] + MHCA (ours)</cell><cell>44.72/0.9608 38.62/0.9348</cell></row><row><cell cols="2">CNN+ESPC [14] + MCSAM [10, 29] 44.51/0.9599 38.38/0.9338</cell></row><row><cell>CNN+ESPC [14] + MCBAM [43]</cell><cell>44.43/0.9600 38.19/0.9318</cell></row><row><cell cols="2">CNN+ESPC [14] + MMHCA (ours) 45.05/0.9621 38.96/0.9365</cell></row><row><cell cols="2">CNN+ESPC [14] + MMHCA + (ours) 45.14/0.9623 39.10/0.9370</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://radiopaedia.org/articles/ mri-sequences-overview</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://brain-development.org/ixi-dataset/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.na-mic.org/wiki/Downloads 4 https://github.com/sanghyun-son/EDSR-PyTorch 5 https://github.com/lilygeorgescu/ 3d-super-res-cnn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/wwlCape/HAN, https://github. com/chunmeifeng/MINet 7 https://github.com/Jongchan/attention-module</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research leading to these results has received funding from the NO Grants 2014-2021, under project ELO-Hyp contract no. 24/2020.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Accurate and efficient intracranial hemorrhage detection and subtype classification in 3D CT scans with convolutional and long short-term memory neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Burduja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolae</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page">5611</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieneng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangde</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<title level="m">TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient and accurate MRI super-resolution using a generative adversarial network and 3D multi-level densely connected network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibin</forename><surname>Christodoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengwei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debiao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Superresolution reconstruction of single anisotropic 3D MR images using residual convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongshi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lulu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Gholipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingding</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">392</biblScope>
			<biblScope unit="page" from="209" to="220" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accelerated Super-resolution MR Image Reconstruction via a 3D Densely Connected Deep Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lulu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Gholipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongshi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BIBM</title>
		<meeting>BIBM</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="349" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gradient-Guided Convolutional Neural Network for MRI Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page">4874</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-Contrast MRI Super-Resolution via a Multi-Stage Integration Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Mei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Accelerated Multi-Modal MR Imaging with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Mei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14248</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Task Transformer Network for Joint MRI Reconstruction and Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Mei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="307" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="61" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks With Intermediate Loss for 3D Super-Resolution of CT and MRI Scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolae</forename><surname>Verga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="49112" to="49124" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujuan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoren</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Medsrgan</surname></persName>
		</author>
		<title level="m">Medical images super-resolution using generative adversarial networks. Multimedia Tools Application</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="21815" to="21840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">UNETR: Transformers for 3D Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daguang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning-based super-resolution applied to dental computed tomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janka</forename><surname>Hatvani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Horv?th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Michetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Basarab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Kouam?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikl?s</forename><surname>Gy?ngy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Radiation and Plasma Medical Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="120" to="128" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simultaneous Super-Resolution and Cross-Modality Synthesis of 3D Medical Images Using Weakly-Supervised Joint Convolutional Sparse Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5787" to="5796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast and accurate single image super-resolution via information distillation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiumei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="723" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep Learning for Multiple-Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Kawulok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Benecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Piechaczek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Hrynczenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kostrzewa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Nalepa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1062" to="1066" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in Vision: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ACM Computing Surveys</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Super-Resolution of Brain MRI Images Using Overcomplete Dictionaries and Nonlocal Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Guizani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="25897" to="25907" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-Contrast Super-Resolution MRI Through a Progressive Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongming</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cole</forename><forename type="middle">R</forename><surname>Steber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corbin</forename><forename type="middle">A</forename><surname>Helis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Whitlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2738" to="2749" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image super-resolution using progressive generative adversarial networks for medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dwarikanath</forename><surname>Mahapatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behzad</forename><surname>Bozorgtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahil</forename><surname>Garnavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="30" to="39" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Single Image Super-Resolution via a Holistic Attention Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangde</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="191" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-input Cardiac Image Super-Resolution Using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>De Marvao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Declan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Regan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="246" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiscale brain MRI super-resolution using deep 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Tor-D?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H?l?ne</forename><surname>Meunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Bednarek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Fablet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Passat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Rousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">101647</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea-Iuliana</forename><surname>Nicolae-Catalin Ristea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivian</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Savencu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Georgescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06400</idno>
		<title level="m">Nicolae Verga, Fahad Shahbaz Khan, and Radu Tudor Ionescu. CyTran: Cycle-Consistent Transformers for Non-Contrast to Contrast CT Translation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A new approach for brain tumor diagnosis system: single image super resolution based maximum fuzzy entropy segmentation and convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eser</forename><surname>Sert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akif</forename><surname>Fatih?zyurt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dogantekin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Hypotheses</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page">109413</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lung cancer detection from CT image using improved profuse clustering and deep learning instantaneously trained neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Mohamed</forename><surname>Shakeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Burhanuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamad</forename><forename type="middle">Ishak</forename><surname>Desa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="702" to="712" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shamshad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Haris</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09873</idno>
		<title level="m">Transformers in medical imaging: A survey</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">MR Image Super-Resolution via Wide Residual Networks With Fixed Skip Connection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihui</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaofeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingkun</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1129" to="1140" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">DeepCADx: Automated Prostate Cancer Detection and Diagnosis in mp-MRI Based on Multimodal Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACMMM</title>
		<meeting>ACMMM</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1229" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In-So Kweon. CBAM: Convolutional Block Attention Module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">CvT: Introducing Convolutions to Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">CT super-resolution GAN constrained by the identical, residual, and cycle learning ensemble (GAN-CIRCLE)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyu</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongming</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengzhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghong</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuiyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiang</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="188" to="203" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Computed tomography super-resolution using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Bramler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3944" to="3948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Simultaneous single-and multicontrast super-resolution for brain MRI images based on a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congbo</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="133" to="141" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">MR Image Super-Resolution with Squeeze and Excitation Reasoning Attention Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13420" to="13429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Channel Splitting Network for Single MR Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaole</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueming</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5649" to="5662" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-contrast brain magnetic resonance image super-resolution using the local weight similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjian</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Medical Imaging</title>
		<imprint>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multi-Contrast Brain MRI Image Super-Resolution With Gradient-Guided Edge Enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxi</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="57856" to="57867" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">End-to-end object detection with adaptive clustering transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deformable DETR: Deformable Transformers for End-to-End Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<title level="m">Figure 4, we illustrate qualitative results obtained by two baselines</title>
		<imprint/>
	</monogr>
	<note>bicubic and EDSR [26]) versus two enhanced versions of EDSR [26], namely EDSR [26] + MHCA and EDSR [26] + MMHCA, for an upscaling factor of 4?. We observe that our EDSR [26] + MMHCA model is able to create sharper reconstructions and to improve the contrast levels</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
