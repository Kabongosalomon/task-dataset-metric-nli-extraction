<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DISTRIBUTIONALLY ROBUST NEURAL NETWORKS FOR GROUP SHIFTS: ON THE IMPORTANCE OF REGULARIZATION FOR WORST-CASE GENERALIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
							<email>ssagawa@cs.stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Pang</surname></persName>
							<email>pangwei@cs.stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto Microsoft</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<email>pliang@cs.stanford.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DISTRIBUTIONALLY ROBUST NEURAL NETWORKS FOR GROUP SHIFTS: ON THE IMPORTANCE OF REGULARIZATION FOR WORST-CASE GENERALIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Overparameterized neural networks can be highly accurate on average on an i.i.d. test set yet consistently fail on atypical groups of the data (e.g., by learning spurious correlations that hold on average but not in such groups). Distributionally robust optimization (DRO) allows us to learn models that instead minimize the worst-case training loss over a set of pre-defined groups. However, we find that naively applying group DRO to overparameterized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst-case training loss. Instead, the poor worst-case performance arises from poor generalization on some groups. By coupling group DRO models with increased regularization-a stronger-than-typical 2 penalty or early stopping-we achieve substantially higher worst-group accuracies, with 10-40 percentage point improvements on a natural language inference task and two image tasks, while maintaining high average accuracies. Our results suggest that regularization is important for worst-group generalization in the overparameterized regime, even if it is not needed for average generalization. Finally, we introduce a stochastic optimization algorithm, with convergence guarantees, to efficiently train group DRO models. * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Machine learning models are typically trained to minimize the average loss on a training set, with the goal of achieving high accuracy on an independent and identically distributed (i.i.d.) test set. However, models that are highly accurate on average can still consistently fail on rare and atypical examples <ref type="bibr" target="#b29">(Hovy &amp; Sgaard, 2015;</ref><ref type="bibr" target="#b7">Blodgett et al., 2016;</ref><ref type="bibr" target="#b56">Tatman, 2017;</ref><ref type="bibr" target="#b25">Hashimoto et al., 2018;</ref><ref type="bibr" target="#b18">Duchi et al., 2019)</ref>. Such models are problematic when they violate equity considerations <ref type="bibr" target="#b32">(Jurgens et al., 2017;</ref><ref type="bibr" target="#b11">Buolamwini &amp; Gebru, 2018)</ref> or rely on spurious correlations: misleading heuristics that work for most training examples but do not always hold. For example, in natural language inference (NLI)-determining if two sentences agree or contradict-the presence of negation words like 'never' is strongly correlated with contradiction due to artifacts in crowdsourced training data <ref type="bibr" target="#b22">(Gururangan et al., 2018;</ref><ref type="bibr" target="#b38">McCoy et al., 2019)</ref>. A model that learns this spurious correlation would be accurate on average on an i.i.d. test set but suffer high error on groups of data where the correlation does not hold (e.g., the group of contradictory sentences with no negation words).</p><p>To avoid learning models that rely on spurious correlations and therefore suffer high loss on some groups of data, we instead train models to minimize the worst-case loss over groups in the training data. The choice of how to group the training data allows us to use our prior knowledge of spurious correlations, e.g., by grouping together contradictory sentences with no negation words in the NLI example above. This training procedure is an instance of distributionally robust optimization (DRO), which optimizes for the worst-case loss over potential test distributions <ref type="bibr" target="#b3">(Ben-Tal et al., 2013;</ref>. Existing work on DRO has focused on models that cannot approach zero training loss, such as generative models <ref type="bibr" target="#b45">(Oren et al., 2019)</ref> or convex predictive models with limited capacity <ref type="bibr" target="#b37">(Maurer &amp; Pontil, 2009;</ref><ref type="bibr" target="#b50">Shafieezadeh-Abadeh et al., 2015;</ref><ref type="bibr" target="#b42">Namkoong &amp; Duchi, 2017;</ref><ref type="bibr" target="#b25">Hashimoto et al., 2018)</ref>.</p><p>We study group DRO in the context of overparameterized neural networks in three applications <ref type="figure" target="#fig_0">(Figure 1)</ref>-natural language inference with the MultiNLI dataset <ref type="bibr" target="#b60">(Williams et al., 2018)</ref>, facial attribute recognition with CelebA <ref type="bibr" target="#b36">(Liu et al., 2015)</ref>, and bird photograph recognition with our modified version of the CUB dataset <ref type="bibr" target="#b58">(Wah et al., 2011)</ref>. The problem with applying DRO to overparameterized models is that if a model achieves zero training loss, then it is optimal on both the worst-case (DRO) and the average training objectives <ref type="bibr" target="#b62">(Zhang et al., 2017;</ref><ref type="bibr" target="#b59">Wen et al., 2014)</ref>. In the vanishing-trainingloss regime, we indeed find that group DRO models do no better than standard models trained to minimize average loss via empirical risk minimization (ERM): both models have high average test accuracies and worst-group training accuracies, but low worst-group test accuracies (Section 3.1). In other words, the generalization gap is small on average but large for the worst group.</p><p>In contrast, we show that strongly-regularized group DRO models that do not attain vanishing training loss can significantly outperform both regularized and unregularized ERM models. We consider 2 penalties, early stopping (Section 3.2), and group adjustments that minimize a risk measure which accounts for the differences in generalization gaps between groups (Section 3.3). Across the three applications, regularized group DRO improves worst-case test accuracies by 10-40 percentage points while maintaining high average test accuracies. These results give a new perspective on generalization in neural networks: regularization might not be important for good average performance (e.g., models can "train longer and generalize better" on average <ref type="bibr" target="#b28">(Hoffer et al., 2017)</ref>) but it appears important for good worst-case performance.</p><p>Finally, to carry out the experiments, we introduce a new stochastic optimizer for group DRO that is stable and scales to large models and datasets. We derive convergence guarantees for our algorithm in the convex case and empirically show that it behaves well in our non-convex models (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SETUP</head><p>Consider predicting labels y ? Y from input features x ? X . Given a model family ?, loss : ? ? (X ? Y) ? R + , and training data drawn from some distribution P , the standard goal is to find a model ? ? ? that minimizes the expected loss E P <ref type="bibr">[ (?; (x, y)</ref>] under the same distribution P . The standard training procedure for this goal is empirical risk minimization (ERM):</p><formula xml:id="formula_0">? ERM := arg min ??? E (x,y)?P [ (?; (x, y))],<label>(1)</label></formula><p>whereP is the empirical distribution over the training data.</p><p>In distributionally robust optimization (DRO) <ref type="bibr" target="#b3">(Ben-Tal et al., 2013;</ref>, we aim instead to minimize the worst-case expected loss over an uncertainty set of distributions Q:</p><formula xml:id="formula_1">min ??? R(?) := sup Q?Q E (x,y)?Q [ (?; (x, y))] .<label>(2)</label></formula><p>The uncertainty set Q encodes the possible test distributions that we want our model to perform well on. Choosing a general family Q, such as a divergence ball around the training distribution, confers robustness to a wide set of distributional shifts, but can also lead to overly pessimistic models which optimize for implausible worst-case distributions <ref type="bibr" target="#b18">(Duchi et al., 2019)</ref>.</p><p>To construct a realistic set of possible test distributions without being overly conservative, we leverage prior knowledge of spurious correlations to define groups over the training data and then define the uncertainty set Q in terms of these groups. Concretely, we adopt the group DRO setting <ref type="bibr" target="#b30">(Hu et al., 2018;</ref><ref type="bibr" target="#b45">Oren et al., 2019)</ref> where the training distribution P is assumed to be a mixture of m groups P g indexed by G = {1, 2, . . . , m}. 1 We define the uncertainty set Q as any mixture of these groups, i.e., Q := { m g=1 q g P g : q ? ? m }, where ? m is the (m ? 1)-dimensional probability simplex; this choice of Q allows us to learn models that are robust to group shifts. Because the optimum of a linear program is attained at a vertex, the worst-case risk (2) is equivalent to a maximum over the expected loss of each group,</p><formula xml:id="formula_2">R(?) = max g?G E (x,y)?Pg [ (?; (x, y))].</formula><p>( <ref type="formula">3)</ref> We assume that we know which group each training point comes from-i.e., the training data comprises (x, y, g) triplets-though we do not assume we observe g at test time, so the model cannot use g directly. Instead, we learn a group DRO model minimizing the empirical worst-group riskR(?):</p><formula xml:id="formula_3">? DRO := arg min ??? R (?) := max g?G E (x,y)?Pg [ (?; (x, y))] ,<label>(4)</label></formula><p>where each groupP g is an empirical distribution over all training points (x, y, g ) with g = g (or equivalently, a subset of training examples drawn from P g ). Group DRO learns models with good worst-group training loss across groups. This need not imply good worst-group test loss because of the worst-group generalization gap ? := R(?) ?R(?). We will show that for overparameterized neural networks, ? is large unless we apply sufficient regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">APPLICATIONS</head><p>In the rest of this paper, we study three applications that share a similar structure ( <ref type="figure" target="#fig_0">Figure 1</ref>): each data point (x, y) has some input attribute a(x) ? A that is spuriously correlated with the label y, and we use this prior knowledge to form m = |A| ? |Y| groups, one for each value of (a, y). We expect that models that learn the correlation between a and y in the training data would do poorly on groups for which the correlation does not hold and hence do worse on the worst-group loss R(?).</p><p>Object recognition with correlated backgrounds (Waterbirds dataset). Object recognition models can spuriously rely on the image background instead of learning to recognize the actual object <ref type="bibr" target="#b48">(Ribeiro et al., 2016)</ref>. We study this by constructing a new dataset, Waterbirds, which combines bird photographs from the Caltech-UCSD Birds-200-2011 (CUB) dataset <ref type="bibr" target="#b58">(Wah et al., 2011)</ref> with image backgrounds from the Places dataset <ref type="bibr" target="#b63">(Zhou et al., 2017)</ref>. We label each bird as one of Y = {waterbird, landbird} and place it on one of A = {water background, land background}, with waterbirds (landbirds) more frequently appearing against a water (land) background (Appendix C.1). There are n = 4795 training examples and 56 in the smallest group (waterbirds on land).</p><p>Object recognition with correlated demographics (CelebA dataset). Object recognition models (and other ML models more generally) can also learn spurious associations between the label and demographic information like gender and ethnicity <ref type="bibr" target="#b11">(Buolamwini &amp; Gebru, 2018)</ref>. We examine this on the CelebA celebrity face dataset <ref type="bibr" target="#b36">(Liu et al., 2015)</ref>, Natural language inference (MultiNLI dataset). In natural language inference, the task is to determine if a given hypothesis is entailed by, neutral with, or contradicts a given premise. Prior work has shown that crowdsourced training datasets for this task have significant annotation artifacts, such as the spurious correlation between contradictions and the presence of the negation words nobody, no, never, and nothing <ref type="bibr" target="#b22">(Gururangan et al., 2018)</ref>. We divide the MultiNLI dataset <ref type="bibr" target="#b60">(Williams et al., 2018)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COMPARISON BETWEEN GROUP DRO AND ERM</head><p>To study the behavior of group DRO vs. ERM in the overparametrized setting, we fine-tuned ResNet50 models <ref type="bibr" target="#b26">(He et al., 2016)</ref> on Waterbirds and CelebA and a BERT model <ref type="bibr" target="#b15">(Devlin et al., 2019)</ref> on MultiNLI. These are standard models for image classification and natural language inference which achieve high average test accuracies on their respective tasks.</p><p>We train the ERM (1) and group DRO (4) models using standard (minibatch) stochastic gradient descent and (minibatch) stochastic algorithm introduced in Section 5, respectively. We tune the learning rate for ERM and use the same setting for DRO (Appendix C.2). For each model, we measure its average (in-distribution) accuracy over training and test sets drawn from the same distribution, as well as its worst-group accuracy on the worst-performing group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ERM AND DRO HAVE POOR WORST-GROUP ACCURACY IN THE OVERPARAMETERIZED</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REGIME</head><p>Overparameterized neural networks can perfectly fit the training data and still generalize well on average <ref type="bibr" target="#b62">(Zhang et al., 2017)</ref>. We start by showing that these overparameterized models do not generalize well on the worst-case group when they are trained to convergence using standard regularization and hyperparameter settings <ref type="bibr" target="#b26">(He et al., 2016;</ref><ref type="bibr" target="#b15">Devlin et al., 2019)</ref>, regardless of whether they are trained with ERM or group DRO. 2 ERM. As expected, ERM models attain near-perfect worst-group training accuracies of at least 99.9% on all three datasets and also obtain high average test accuracies (97.3%, 94.8%, and 82.5% on Waterbirds, CelebA, and MultiNLI). However, they perform poorly on the worst-case group at test time with worst-group accuracies of 60.0%, 41.1%, and 65.7% respectively (Table 1, <ref type="figure">Figure 2)</ref>. Their low worst-group accuracies imply that these models are brittle under group shifts.</p><p>DRO. The ERM models trained above nearly perfectly classify every training point, and are therefore near-optimal for both the ERM (1) and DRO (4) objectives. Indeed, we find that group DRO models perform similarly to ERM models, attaining near-perfect training accuracies and high average test accuracies, but poor worst-group test accuracies (Table 1, <ref type="figure">Figure 2</ref>).</p><p>Discussion. The ERM and DRO models attain near-perfect training accuracy and vanishing training loss even in the presence of default regularization (batch normalization and standard 2 penalties for ResNet50, and dropout for BERT). However, despite generalizing well on average, they do not generalize well on the worst-case group, and consequently suffer from low worst-group accuracies. This gap between average and worst-group test accuracies arises not from poor worst-group training performance-the models are near-perfect at training time, even on the worst-case groups-but from variations in the generalization gaps across groups. Even though DRO is designed to improve worst-group performance, we find no improvements on worst-group test accuracies since the models already achieve vanishing worst-group losses on the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DRO IMPROVES WORST-GROUP ACCURACY UNDER APPROPRIATE REGULARIZATION</head><p>Classically, we can control the generalization gap with regularization techniques that constrain the model family's capacity to fit the training data. In the modern overparameterized regime, explicit regularization is not critical for average performance: models can do well on average even when all regularization is removed <ref type="bibr" target="#b62">(Zhang et al., 2017)</ref>, and default regularization settings (like in the models trained above) still allow models to perfectly fit the training data. Here, we study if increasing regularization strength-until the models no longer perfectly fit the training data-can rescue worst-case performance. We find that departing from the vanishing-training-loss regime allows DRO models to significantly outperform ERM models on worst-group test accuracy while maintaining high average accuracy. We investigate two types of regularization: 2 penalties. The default coefficient of the 2 -norm penalty ? ? 2 2 in ResNet50 is ? = 0.0001 <ref type="bibr" target="#b26">(He et al., 2016)</ref>. We find that increasing ? by several orders of magnitude-to ? = 1.0 for Waterbirds and ? = 0.1 for CelebA-does two things: 1) it prevents both ERM and DRO models from achieving perfect training accuracy, and 2) substantially reduces the generalization gap for each group.</p><p>With strong 2 penalties, both ERM and DRO models still achieve high average test accuracies. However, because no model can achieve perfect training accuracy in this regime, ERM models sacrifice worst-group training accuracy (35.7% and 40.4% on Waterbirds and CelebA; Table 1, <ref type="figure">Figure 2</ref>) and consequently obtain poor worst-group test accuracies (21.3% and 37.8%, respectively).</p><p>In contrast, DRO models attain high worst-group training accuracy (97.5% and 93.4% on Waterbirds and CelebA). The small generalization gap in the strong-2 -penalty regime means that high worstgroup training accuracy translates to high worst-group test accuracy, which improves over ERM from 21.3% to 84.6% on Waterbirds and from 37.8% to 86.7% on CelebA.</p><p>While these results show that strong 2 penalties have a striking impact on ResNet50 models for Waterbirds and CelebA, we found that increasing the 2 penalty on the BERT model for MultiNLI resulted in similar or worse robust accuracies than the default BERT model with no 2 penalty.</p><p>Early stopping. A different, implicit form of regularization is early stopping <ref type="bibr" target="#b24">(Hardt et al., 2016b)</ref>. We use the same settings in Section 3.1, but only train each model for a fixed (small) number of epochs (Section C.2). As with strong 2 penalties, curtailing training reduces the generalization gap and prevents models from fitting the data perfectly. In this setting, DRO also does substantially better than ERM on worst-group test accuracy, improving from 6.7% to 86.0% on Waterbirds, 25.0% to 88.3% on CelebA, and 66.0% to 77.7% on MultiNLI. Average test accuracies are comparably high in both ERM and DRO models, though there is a small drop of 1 ? 3% for DRO (Table 1, <ref type="figure">Figure 2</ref>).</p><p>Discussion. We conclude that regularization-preventing the model from perfectly fitting the training data-does matter for worst-group accuracy. Specifically, it controls the generalization gap for each group, even on the worst-case group. Good worst-group test accuracy then becomes a question of good worst-group training accuracy. Since no regularized model can perfectly fit the training data, ERM and DRO models make different training trade-offs: ERM models sacrifice worst-group for average training accuracy and therefore have poor worst-group test accuracies, while DRO models maintain high worst-group training accuracy and therefore do well at test time. Our findings raise questions about the nature of generalization in neural networks, which has been predominantly studied only in the context of average accuracy <ref type="bibr" target="#b62">(Zhang et al., 2017;</ref><ref type="bibr" target="#b28">Hoffer et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ACCOUNTING FOR GENERALIZATION THROUGH GROUP ADJUSTMENTS IMPROVES DRO</head><p>In the previous section, we optimized for the worst-group training loss via DRO (4), relying on regularization to control the worst-group generalization gap and translate good worst-group training loss to good worst-group test loss. However, even with regularization, the generalization gap can vary significantly across groups: in the Waterbirds DRO model with a strong 2 penalty, the smallest group has a train-test accuracy gap of 15.4% compared to just 1.0% for the largest group. This suggests that we can obtain better worst-group test loss if at training time, we prioritize obtaining lower training loss on the groups that we expect to have a larger generalization gap.</p><p>We make this approach concrete by directly minimizing an estimated upper bound on the worstgroup test loss, inspired by ideas from structural risk minimization <ref type="bibr" target="#b57">(Vapnik, 1992)</ref>. The key consideration is that each group g has its own generalization gap ? g = E (x,y)?Pg [ (?; (x, y))] ? E (x,y)?Pg [ (?; (x, y))]. To approximate optimizing for the worst-group test loss R(?) = max g?G E (x,y)?Pg [ (?; (x, y))] + ? g , we propose using the simple, parameter-independent heuristi? ? g = C/ ? n g , where n g is the group size for g and C is a model capacity constant which we treat as a hyperparameter. This gives the group-adjusted DRO estimator</p><formula xml:id="formula_4">? adj := arg min ??? max g?G E (x,y)?Pg [ (?; (x, y))] + C ? n g .<label>(5)</label></formula><p>The scaling with 1/ ? n g reflects how smaller groups are more prone to overfitting than larger groups, and is inspired by the general size dependence of model-complexity-based generalization bounds (see, e.g., <ref type="bibr" target="#b13">Cao et al. (2019)</ref>).</p><p>By incorporating group adjustments in <ref type="formula" target="#formula_4">(5)</ref>, we encourage the model to focus more on fitting the smaller groups. We note that this method of using a 1/ ? n surrogate for the generalization gap only works in the group DRO setting, where we consider the worst-group loss over groups of different sizes. It does not apply in the ERM setting; if we were minimizing average training loss, the 1/ ? n term would simply be a constant and not affect the optimization.</p><p>Results. We evaluate group adjustments using group DRO models with strong 2 penalties (as in Section 3.2). In Waterbirds (? = 1.0), worst-group test accuracy improves by 5.9%, cutting the   When C = 0, the generalization gap for waterbirds on land (green line) is large, dragging down worst-group accuracy. At C = 2, which has the best worst-group validation accuracy, the accuracies are balanced. At C = 4, we overcompensate for group sizes, so smaller groups (e.g., waterbirds on land) do better at the expense of larger groups (e.g., landbirds on land). error rate by more than a third (Table 2 and <ref type="figure" target="#fig_2">Figure 3</ref>). The improvements in CelebA (? = 0.1) are more modest, with worst-group accuracy increasing by 1.1%; 2 penalties are more effective in CelebA and there is not as much variation in the generalization gaps by group at ? = 0.1. We did not evaluate group adjustments on MultiNLI as it did not benefit from stronger 2 penalties.</p><p>Empirically, group adjustments also help in the early stopping setting of Section 3.2 (in the next section, we evaluate models with group adjustments and early stopping across a grid of 2 penalty strengths). However, it is difficult to rigorously study the effects of early stopping (e.g., because the group losses have not converged to a stable value), so we leave a more thorough investigation of the interaction between early stopping and group adjustments to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">COMPARISON BETWEEN DRO AND IMPORTANCE WEIGHTING</head><p>Our results above show that strongly-regularized DRO models can be significantly more robust than ERM models. Here, we show theoretically and empirically that DRO also outperforms a strong importance weighting baseline that is commonly used in machine learning tasks where the train and test distributions differ <ref type="bibr" target="#b52">(Shimodaira, 2000;</ref><ref type="bibr" target="#b12">Byrd &amp; Lipton, 2019)</ref>. Recall that in our setting, the test distribution can be any mixture of the group distributions. For some assignment of weights w ? ? m to groups, an importance-weighted estimator would learn</p><formula xml:id="formula_5">? w := arg min ??? E (x,y,g)?P [w g (?; (x, y))].<label>(6)</label></formula><p>Empirical comparison. We consider an importance-weighted baseline with weights set to the inverse training frequency of each group, w g = 1/E g ?P [I(g = g)]. This optimizes for a test distribution with uniform group frequencies and is analogous to the common upweighting technique for label shifts <ref type="bibr" target="#b14">(Cui et al., 2019;</ref><ref type="bibr" target="#b13">Cao et al., 2019)</ref>; intuitively, this attempts to equalize average and worst-group error by upweighting the minority groups. Concretely, we train our weighted model by sampling from each group with equal probability <ref type="bibr" target="#b51">(Shen et al., 2016)</ref>, since a recent study found this to be more effective than similar reweighting/resampling methods <ref type="bibr" target="#b9">(Buda et al., 2018)</ref>.</p><p>Unlike group DRO, upweighting the minority groups does not necessarily yield uniformly low training losses across groups in practice, as some groups might be easier to fit than others. To compare upweighting (UW) with ERM and DRO, we train models across the same grid of 2 penalty strengths and early stopping at the epoch with best worst-group validation accuracy <ref type="table">(Table 3)</ref>. <ref type="bibr">3</ref> In CelebA and Waterbirds, upweighting performs much better than ERM but is slightly outperformed by DRO. However, upweighting fails on MultiNLI, achieving lower average and worst-group accuracies than even ERM. With upweighting, it appears that the rare group is overemphasized and extremely low training accuracy is achieved for that group at the cost of others.  <ref type="table">Table 3</ref>: Comparison of ERM, upweighting (UW), and group DRO models, with binomial standard deviation in parenthesis. For each objective, we grid search over 2 penalty strength, number of epochs, and group adjustments and report on the model with highest validation accuracy. These numbers differ from the previous tables because of the larger grid search. Theoretical comparison. Should we expect importance weighting to learn models with good worstcase loss? We show that importance weighting and DRO can learn equivalent models in the convex setting under some importance weights, but not necessarily when the models are non-convex.</p><p>We analyze the general framework of having weights w(z) for each data point z, which is more powerful than the specific choice above of assigning weights by groups. By minimizing the weighted loss E z?P [w(z) (?; z)] over some source distribution P , we can equivalently minimize the expected loss E z?Q [ (?; z)] over a target distribution Q where Q(z) ? w(z)P (z). However, we want good worst-case performance over a family of Q ? Q, instead of a single Q. Are there weights w such that the resulting model? w achieves optimal worst-group risk? In the convex regime, standard duality arguments show that this is the case (see Appendix A.1 for the proof): Proposition 1. Suppose that the loss (?; z) is continuous and convex for all z in Z, and let the uncertainty set Q be a set of distributions supported on Z. Assume that Q and the model family ? ? R d are convex and compact, and let ? * ? ? be a minimizer of the worst-group objective R(?). Then there exists a distribution Q * ? Q such that ? * ? arg min ? E z?Q * [ (?; z)].</p><p>However, this equivalence breaks down when the loss is non-convex: Counterexample 1. Consider a uniform data distribution P supported on two points Z = {z 1 , z 2 }, and let (?; z) be as in <ref type="figure" target="#fig_3">Figure 4</ref>, with ? = [0, 1]. The DRO solution ? * achieves a worst-case loss of R(? * ) = 0.6. Now consider any weights (w 1 , w 2 ) ? ? 2 and w.l.o.g. let w 1 ? w 2 . The minimizer of the weighted loss w 1 (?; z 1 )+w 2 (?; z 2 ) is ? 1 , which only attains a worst-case loss of R(? * ) = 1.0.</p><p>Remark. Under regularity conditions, there exists a distribution Q such that ? * is a first-order stationary point of E z?Q [ (?; z)] (see e.g., <ref type="bibr" target="#b0">Arjovsky et al. (2019)</ref>). However, as the counterexample demonstrates, in the non-convex setting this does not imply that ? * actually minimizes E z?Q <ref type="bibr">[ (?; z)</ref>].</p><p>This negative result implies that in the non-convex setting, there may not be any choice of weights w such that the resulting minimizer? w is robust. Even if such weights did exist, they depend on ? * and obtaining these weights requires that we solve a dual DRO problem, making reweighting no easier to implement than DRO. Common choices of weights, such as inverse group size, are heuristics that may not yield robust solutions (as observed for MultiNLI in <ref type="table">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ALGORITHM</head><p>To train group DRO models efficiently, we introduce an online optimization algorithm with convergence guarantees. Prior work on group DRO has either used batch optimization algorithms, which do not scale to large datasets, or stochastic optimization algorithms without convergence guarantees.</p><p>In the convex and batch case, there is a rich literature on distributionally robust optimization which treats the problem as a standard convex conic program <ref type="bibr" target="#b3">(Ben-Tal et al., 2013;</ref><ref type="bibr" target="#b5">Bertsimas et al., 2018;</ref><ref type="bibr" target="#b34">Lam &amp; Zhou, 2015)</ref>. For general non-convex DRO problems, two types of stochastic optimization methods have been proposed: (i) stochastic gradient descent (SGD) on the Lagrangian dual of the objective <ref type="bibr" target="#b25">Hashimoto et al., 2018)</ref>, and (ii) direct minimax optimization . The first approach fails for group DRO because the gradient of the dual objective is difficult to estimate in a stochastic and unbiased manner. 4 An algorithm of the second type has been proposed for group DRO <ref type="bibr" target="#b45">(Oren et al., 2019)</ref>, but this work does not provide convergence guarantees, and we observed instability in practice under some settings.</p><p>Recall that we aim to solve the optimization problem <ref type="formula" target="#formula_3">(4)</ref>, which can be rewritten as</p><formula xml:id="formula_6">min ??? sup q??m m g=1 q g E (x,y)?Pg [ (?; (x, y))].<label>(7)</label></formula><p>Extending existing minimax algorithms for DRO <ref type="bibr" target="#b45">Oren et al., 2019)</ref>, we interleave gradient-based updates on ? and q. Intuitively, we maintain a distribution q over groups, with high masses on high-loss groups, and update on each example proportionally to the mass on its group. Concretely, we interleave SGD on ? and exponentiated gradient ascent on q (Algorithm 1).</p><p>(In practice, we use minibatches and a momentum term for ?; see Appendix C.2 for details.) The key improvement from the existing group DRO algorithm <ref type="bibr" target="#b45">(Oren et al., 2019)</ref> is that q is updated using gradients instead of picking the group with worst average loss at each iteration, which is important for stability and obtaining convergence guarantees. The run time of the algorithm is similar to that of SGD for a given number of epochs (less than a 5% difference), as run time is dominated by the computation of the loss and its gradient.</p><p>Algorithm 1: Online optimization algorithm for group DRO Input:</p><p>Step sizes ? q , ? ? ; P g for each g ? G Initialize ? (0) and q (0) for t = 1, . . . , T do g ? Uniform(1, . . . , m) // Choose a group g at random x, y ? P g // Sample x, y from group g q ? q (t?1) ; q g ? q g exp(? q (? (t?1) ; (x, y))) // Update weights for group g</p><formula xml:id="formula_7">q (t) ? q / g q g // Renormalize q ? (t) ? ? (t?1) ? ? ? q (t) g ? (? (t?1) ; (x, y)) // Use q to update ? end</formula><p>We analyze the convergence rate by studying the error ? T of the average iterate? (1:T ) :</p><formula xml:id="formula_8">? T = max q??m L ? (1:T ) , q ? min ??? max q??m L ?, q ,<label>(8)</label></formula><p>where L(?, q) := m g=1 q g E (x,y)?Pg <ref type="bibr">[ (?; (x, y)</ref>)] is the expected worst-case loss. Applying results from <ref type="bibr" target="#b43">Nemirovski et al. (2009)</ref>, we can show that Algorithm 1 has a standard convergence rate of O 1/ ? T in the convex setting (proof in Section A.2): Proposition 2. Suppose that the loss <ref type="figure">(?; (x, y)</ref>) is non-negative, convex, B ? -Lipschitz continuous, and bounded by B for all (x, y) in X ? Y, and ? 2 ? B ? for all ? ? ? with convex ? ? R d . Then, the average iterate of Algorithm 1 achieves an expected error at the rate</p><formula xml:id="formula_9">E[? T ] ? 2m 10(B 2 ? B 2 ? + B 2 log m) T .<label>(9)</label></formula><p>where the expectation is taken over the randomness of the algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>The problem of non-uniform accuracy. Existing approaches to addressing non-uniform accuracy over the data distribution include domain adaptation techniques for known target distributions <ref type="bibr" target="#b2">(Ben-David et al., 2006;</ref><ref type="bibr" target="#b21">Ganin &amp; Lempitsky, 2015)</ref> and work in ML fairness <ref type="bibr" target="#b19">(Dwork et al., 2012;</ref><ref type="bibr" target="#b23">Hardt et al., 2016a;</ref><ref type="bibr" target="#b33">Kleinberg et al., 2017)</ref>. As we discuss in Section 4, importance weighting is a classic example of the former <ref type="bibr" target="#b52">(Shimodaira, 2000)</ref>. <ref type="bibr" target="#b12">Byrd &amp; Lipton (2019)</ref> empirically study importance weighting in neural networks and demonstrate that it has little effect unless regularization is applied. This is consistent with the theoretical analysis in <ref type="bibr" target="#b59">Wen et al. (2014)</ref>, which points out that weighting has little impact in the zero-loss regime, and with our own observations in the context of DRO.</p><p>Distributionally robust optimization. Prior work in DRO typically defines the uncertainty set Q as a divergence ball around the training distribution over (x, y) <ref type="bibr" target="#b3">(Ben-Tal et al., 2013;</ref><ref type="bibr" target="#b34">Lam &amp; Zhou, 2015;</ref><ref type="bibr" target="#b40">Miyato et al., 2018;</ref><ref type="bibr" target="#b20">Esfahani &amp; Kuhn, 2018;</ref><ref type="bibr" target="#b5">Bertsimas et al., 2018;</ref><ref type="bibr" target="#b6">Blanchet &amp; Murthy, 2019)</ref>. With small divergence balls of radii O(1/n), DRO acts as a regularizer <ref type="bibr" target="#b50">(Shafieezadeh-Abadeh et al., 2015;</ref><ref type="bibr" target="#b42">Namkoong &amp; Duchi, 2017)</ref>. However, when the radius is larger, the resulting Q can be too pessimistic. In contrast, group DRO considers Q that is of wider radius but with fewer degrees of freedom (shifts over groups instead of over (x, y)). Prior work proposed group DRO in the context of label shifts <ref type="bibr" target="#b30">(Hu et al., 2018)</ref> and shifts in data sources <ref type="bibr" target="#b45">(Oren et al., 2019)</ref>. Our work studies group DRO in the overparameterized regime with vanishing training loss and poor worst-case generalization. In contrast, most DRO work has focused on the classic (underparameterized) model setting <ref type="bibr" target="#b42">(Namkoong &amp; Duchi, 2017;</ref><ref type="bibr" target="#b30">Hu et al., 2018;</ref><ref type="bibr" target="#b18">Duchi et al., 2019)</ref>. <ref type="bibr" target="#b53">Sinha et al. (2018)</ref> study neural networks but with a more conservative Wasserstein uncertainty set that leads to non-vanishing training loss; and <ref type="bibr" target="#b45">Oren et al. (2019)</ref> study neural networks but for generative modeling where loss tradeoffs arise naturally.</p><p>Generalization of robust models. There is extensive work investigating generalization of neural networks in terms of average loss, theoretically and empirically <ref type="bibr" target="#b24">(Hardt et al., 2016b;</ref><ref type="bibr" target="#b55">Szegedy et al., 2016;</ref><ref type="bibr" target="#b28">Hoffer et al., 2017)</ref>. However, analysis on robust losses is limited. For label shifts, prior work has observed overfitting on rare labels and proposed algorithms to mitigate it <ref type="bibr" target="#b9">(Buda et al., 2018;</ref><ref type="bibr" target="#b14">Cui et al., 2019;</ref><ref type="bibr" target="#b13">Cao et al., 2019)</ref>. In the DRO literature, generalization bounds on the DRO objective exist for particular uncertainty sets (e.g., ), but those works do not study overparameterized models. Invariant prediction models, mostly from the causal inference literature, similarly aim to achieve high performance on a range of test distributions <ref type="bibr" target="#b46">(Peters et al., 2016;</ref><ref type="bibr" target="#b27">Heinze-Deml &amp; Meinshausen, 2017;</ref><ref type="bibr" target="#b49">Rothenh?usler et al., 2018;</ref><ref type="bibr" target="#b61">Yang et al., 2019;</ref><ref type="bibr" target="#b0">Arjovsky et al., 2019)</ref>. For example, the maximin regression framework <ref type="bibr" target="#b39">(Meinshausen &amp; B?hlmann, 2015)</ref> also assumes group-based shifts, but focuses on settings without the generalization problems identified in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>In this paper, we analyzed group DRO in overparameterized neural networks and highlighted the importance of regularization for worst-case group generalization. When strongly regularized, group DRO significantly improves worst-group accuracy at a small cost in average accuracy.</p><p>As an application, we showed that group DRO can prevent models from learning pre-specified spurious correlations. Our supplemental experiments also suggest that group DRO models can maintain high worst-group accuracy even when groups are imperfectly specified (Appendix B). While handling shifts beyond pre-specified group shifts is important future work, existing work has identified many distributional shifts that can be expressed with pre-specified groups, e.g., batch effects in biology <ref type="bibr" target="#b35">(Leek et al., 2010)</ref>, or image artifacts  and patient demographics <ref type="bibr" target="#b1">(Badgeley et al., 2019)</ref> in medicine.</p><p>More generally, our observations call for a deeper analysis of average vs. worst-case generalization in the overparameterized regime. Such analysis may shed light on the failure modes of deep neural networks as well as provide additional tools (beyond strong 2 penalties or early stopping) to counter poor worst-case generalization while maintaining high average accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We are grateful to Shyamal Buch, Yair Carmon, Zhenghao Chen, John Duchi, Jean Feng, Christina Heinze-Deml, Robin Jia, Daphne Koller, Ananya Kumar, Tengyu Ma, Jesse Mu, Hongseok Namkoong, Emma Pierson, and Fanny Yang for helpful discussions and suggestions. This work was funded by an Open Philanthropy Project Award. Toyota Research Institute ("TRI") also provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity. SS was supported by a Stanford Graduate Fellowship and PWK was supported by the Facebook Fellowship Program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRODUCIBILITY</head><p>Code for training group DRO models is available at https://github.com/kohpangwei/ group_DRO. The datasets used in this paper are also available at that link, as well as scripts to modify dataset generation (e.g., to choose different spurious attributes for CelebA and MultiNLI, or different object backgrounds or relative group sizes for Waterbirds).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOFS A.1 EQUIVALENCE OF DRO AND IMPORTANCE WEIGHTING IN THE CONVEX SETTING</head><p>Proposition 1. Suppose that the loss (?; z) is continuous and convex for all z in Z, and let the uncertainty set Q be a set of distributions supported on Z. Assume that Q and the model family ? ? R d are convex and compact, and let ? * ? ? be a minimizer of the worst-group objective R(?). Then there exists a distribution Q * ? Q such that ? * ? arg min ? E z?Q * [ (?; z)].</p><p>Proof. Let h(?, Q) := E z?Q <ref type="bibr">[ (?; z)</ref>]. Since the loss (?; z) is continuous and convex in ? for all z in Z, we have that h(?, Q) is continuous, convex in ?, and concave (linear) in Q. Moreover, since convexity and lower semi-continuity are preserved under arbitrary pointwise suprema, sup Q?Q h(?, Q) is also convex and lower semi-continuous (therefore proper).</p><p>Together with the compactness of ? and Q, the above conditions imply (by Weierstrass' theorem, proposition 3.2.1, <ref type="bibr" target="#b4">Bertsekas (2009)</ref>), that the optimal value of the DRO objective </p><p>is attained at some ? * ? ?.</p><p>A similar argument implies that the sup-inf objective</p><formula xml:id="formula_11">sup Q?Q inf ??? h(?, Q)<label>(11)</label></formula><p>attains its optimum at some Q * ? Q.</p><p>Moreover, because ? and Q are compact and h is continuous, we have the max-min equality (see, e.g., Ex 5.25 in <ref type="bibr" target="#b8">Boyd &amp; Vandenberghe (2004)</ref>)</p><formula xml:id="formula_12">sup Q?Q inf ??? h(?, Q) = inf ??? sup Q?Q h(?, Q).<label>(12)</label></formula><p>Together, the above results imply that (? * , Q * ) form a saddle point <ref type="bibr">(proposition 3.4.1, Bertsekas (2009)</ref>), that is,</p><formula xml:id="formula_13">sup Q?Q h(? * , Q) = h(? * , Q * ) = inf ??? h(?, Q * ).<label>(13)</label></formula><p>In particular, the second equality indicates that the optimal DRO model ? * also minimizes the weighted risk h(?, Q * ) = E Z?Q * [ (?; Z)], as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 CONVERGENCE RATE OF ALGORITHM 1</head><p>Proposition 2. Suppose that the loss (?; (x, y)) is non-negative, convex, B ? -Lipschitz continuous, and bounded by B for all (x, y) in X ? Y, and ? 2 ? B ? for all ? ? ? with convex ? ? R d . Then, the average iterate of Algorithm 1 achieves an expected error at the rate</p><formula xml:id="formula_14">E[? T ] ? 2m 10[B 2 ? B 2 ? + B 2 log m] T .<label>(14)</label></formula><p>where the expectation is taken over the randomness of the algorithm.</p><p>Proof. Our proof is an application of the regret bound for online mirror descent on saddle point optimization from <ref type="bibr" target="#b43">Nemirovski et al. (2009)</ref>.</p><p>We first introduce the existing theorem. Consider the saddle-point optimization problem</p><formula xml:id="formula_15">min ??? max q??m m g=1 q g f g (?)<label>(15)</label></formula><p>under the following assumptions:</p><p>Assumption 1. f g is convex on ?.</p><p>Assumption 2. f g (?) = E ??q [F g (?; ?)] for some function F g .</p><p>Assumption 3. We generate i.i.d. examples ? ? q. For a given ? ? ? and ? ? ?, we can compute F g (?, ?) and unbiased stochastic subgradient ?F g (?; ?), that is, E ??q [?F g (?; ?)] = ?f g (?).</p><p>Online mirror descent with some c-strongly convex norm ? ? , yielding iterates ? (1) , . . . , ? (T ) and q (1) , . . . , q (T ) , has the following guarantee. Waterbirds. The CUB dataset <ref type="bibr" target="#b58">(Wah et al., 2011)</ref> contains photographs of birds annotated by species as well as and pixel-level segmentation masks of each bird. To construct the Waterbirds dataset, we label each bird as a waterbird if it is a seabird (albatross, auklet, cormorant, frigatebird, fulmar, gull, jaeger, kittiwake, pelican, puffin, or tern) or waterfowl (gadwall, grebe, mallard, merganser, guillemot, or Pacific loon). Otherwise, we label it as a landbird.</p><p>To control the image background, we use the provided pixel-level segmentation masks to crop each bird out from its original background and onto a water background (categories: ocean or natural lake) or land background (categories: bamboo forest or broadleaf forest) obtained from the Places dataset <ref type="bibr" target="#b63">(Zhou et al., 2017)</ref>. In the training set, we place 95% of all waterbirds against a water background and the remaining 5% against a land background. Similarly, 95% of all landbirds are placed against a land background with the remaining 5% against water.</p><p>We refer to this combined CUB-Places dataset as the Waterbirds dataset to avoid confusion with the original fine-grained species classification task in the CUB dataset.</p><p>We use the official train-test split of the CUB dataset, randomly choosing 20% of the training data to serve as a validation set. For the validation and test sets, we allocate distribute landbirds and waterbirds equally to land and water backgrounds (i.e., there are the same number of landbirds on land vs. water backgrounds, and separately, the same number of waterbirds on land vs. water backgrounds). This allows us to more accurately measure the performance of the rare groups, and it is particularly important for the Waterbirds dataset because of its relatively small size; otherwise, the smaller groups (waterbirds on land and landbirds on water) would have too few samples to accurately estimate performance on. We note that we can only do this for the Waterbirds dataset because we control the generation process; for the other datasets, we cannot generate more samples from the rare groups.</p><p>In a typical application, the validation set might be constructed by randomly dividing up the available training data. We emphasize that this is not the case here: the training set is skewed, whereas the validation set is more balanced. We followed this construction so that we could better compare ERM vs. reweighting vs. group DRO techniques using a stable set of hyperparameters. In practice, if the validation set were also skewed, we might expect hyperparameter tuning based on worst-group accuracy to be more challenging and noisy.</p><p>Due to the above procedure, when reporting average test accuracy in our experiments, we calculate the average test accuracy over each group and then report a weighted average, with weights corresponding to the relative proportion of each group in the (skewed) training dataset.</p><p>CelebA. We use the official train-val-test split that accompanies the CelebA celebrity face dataset <ref type="bibr" target="#b36">(Liu et al., 2015)</ref>. We use the Blond Hair attribute as the target label and the Male attribute as the spuriously-associated variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 MODELS</head><p>ResNet50. We use the Pytorch torchvision implementation of the ResNet50 model, starting from pretrained weights.</p><p>We train the ResNet50 models using stochastic gradient descent with a momentum term of 0.9 and a batch size of 128; the original paper used batch sizes of 128 or 256 depending on the dataset <ref type="bibr" target="#b26">(He et al., 2016)</ref>. As in the original paper, we used batch normalization <ref type="bibr" target="#b31">(Ioffe &amp; Szegedy, 2015)</ref> and no dropout <ref type="bibr" target="#b54">(Srivastava et al., 2014)</ref>. For simplicity, we train all models without data augmentation.</p><p>We use a fixed learning rate instead of the standard adaptive learning rate schedule to make our different model types easier to directly compare, since we expected the scheduler to interact differently with different model types (e.g., due to the different definition of loss). The interaction between batch norm and 2 penalties means that we had to adjust learning rates for each different 2 penalty strength (and each dataset). The learning rates below were chosen to be the highest learning rates that still resulted in stable optimization.</p><p>For the standard training experiments in Section 3.1, we use a 2 penalty of ? = 0.0001 (as in <ref type="bibr" target="#b26">He et al. (2016)</ref>) for both Waterbirds and CelebA, with a learning rate of 0.001 for Waterbirds and 0.0001 for CelebA. We train the CelebA models for 50 epochs and the Waterbirds models for 300 epochs.</p><p>For the early stopping experiments in Section 3.2, we train each ResNet50 model for 1 epoch. For the strong 2 penalty experiments in that section, we use ? = 1.0 for Waterbirds and ? = 0.1 for CelebA, with both datasets using a learning rate of 0.00001. These settings of ? differ because we found that the lower value was sufficient for controlling overfitting on CelebA but not on Waterbirds.</p><p>For the group adjustment experiments in Section 3.3, we use the same settings of ? = 1.0 for Waterbirds and ? = 0.1 for CelebA, with both datasets using a learning rate of 0.00001. For both datasets, we use the value of C ? {0, 1, 2, 3, 4, 5} found in the benchmark grid search described below.</p><p>For the benchmark in Section 4 ( <ref type="table">Table 3</ref>), we grid search over 2 penalties of ? ? {0.0001, 0.1, 1.0} for Waterbirds and ? ? {0.0001, 0.01, 0.1} for CelebA, using the corresponding learning rates for each ? and dataset listed above. (Waterbirds and CelebA at ? = 0.1, which is not listed above, both use a learning rate of 0.0001.) To avoid advantaging DRO by allowing it to try many more hyperparameters, we only test group adjustments (searching over C ? {0, 1, 2, 3, 4, 5}) on the 2 penalties used in Section 3.3, i.e., ? = 1.0 for Waterbirds and ? = 0.1 for CelebA. All benchmark models were evaluated at the best early stopping epoch (as measured by robust validation accuracy).</p><p>BERT. We use the Hugging Face pytorch-transformers implementation of the BERT bert-base-uncased model, starting from pretrained weights <ref type="bibr" target="#b15">(Devlin et al., 2019)</ref>. <ref type="bibr">5</ref> We use the default tokenizer and model settings from that implementation, including a fixed linearly-decaying learning rate starting at 0.00002, AdamW optimizer, dropout, and no 2 penalty (? = 0), except that we use a batch size of 32 (as in <ref type="bibr" target="#b15">Devlin et al. (2019)</ref>) instead of 8. We found that this slightly improved robust accuracy across all models and made the optimization less noisy, especially on the ERM model.</p><p>For the standard training experiments in Section 3.1, we train for 20 epochs.</p><p>For the 2 penalty experiments in Section 3.2, we tried penalties of ? ? {0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10.0}. However, these models had similar or worse robust accuracies compared to the default BERT model with no 2 penalty.</p><p>For the early stopping experiments in Section 3.2, we train for 3 epochs, which is the suggested early-stopping time in <ref type="bibr" target="#b15">Devlin et al. (2019)</ref>.</p><p>For the benchmark in Section 4 <ref type="table">(Table 3)</ref>, we similarly trained for 3 epochs. All benchmark models were evaluated at the best early stopping epoch (as measured by robust validation accuracy).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Representative training and test examples for the datasets we consider. The correlation between the label y and the spurious attribute a at training time does not hold at test time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>using hair color (Y = {blond, dark}) as the target and gender (A = {male, female}) as the spurious attribute. There are n = 162770 training examples in the CelebA dataset, with 1387 in the smallest group (blond-haired males).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Training (light)  and validation (dark) accuracies for each group over time, for different adjustments C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Toy example illustrating that DRO and importance weighting are not equivalent. The DRO solution is ? * , while any importance weighting would result in solutions at ? 1 or ? 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4?</head><label></label><figDesc>The dual optimization problem for group DRO is min ?,? 1 Eg[max(0, E x,y?Pg [ (?; (x, y)) | g]??)]+? for constant ?. The max over expected loss makes it difficult to obtain an unbiased, stochastic gradient estimate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>inf ??? R(?) = inf ??? sup Q?Q h(?, Q).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Theorem 1 (</head><label>1</label><figDesc><ref type="bibr" target="#b43">Nemirovski et al. (2009)</ref>, Eq 3.23). Suppose that Assumptions 1-3 hold. Then the pseudo-regret of the average iteratesq g(1:T ) andq g (1:T ) can be bounded asE max q??m m g=1 q g f g (? (1:T ) ) ? min ??? m g=1q g (1:T ) f g (?) ? 2 10[R 2 ? M 2 * ,? + M 2 * ,q log m] T ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>into m = 6 groups, one for each pair of labels Y = {entailed, neutral, contradictory} and spurious attributes A = {no negation, negation}. There are n = 206175 examples in our training set, with 1521 examples in the smallest group (entailment with negations); see Appendix C.1 for more details on dataset construction and the training/test split.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Average Accuracy</cell><cell>Worst-Group Accuracy</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ERM</cell><cell>DRO</cell><cell>ERM</cell><cell>DRO</cell></row><row><cell></cell><cell>Standard</cell><cell>Regularization</cell><cell cols="2">CelebA Waterbirds MultiNLI</cell><cell>Train Test Train Test Train Test</cell><cell>100.0 97.3 100.0 94.8 99.9 82.5</cell><cell>100.0 97.4 100.0 94.7 99.3 82.0</cell><cell>100.0 60.0 99.9 41.1 99.9 65.7</cell><cell>100.0 76.9 100.0 41.1 99.0 66.4</cell></row><row><cell></cell><cell></cell><cell>Strong ? 2 Penalty</cell><cell cols="2">CelebA Waterbirds</cell><cell>Train Test Train Test</cell><cell>97.6 95.7 95.7 95.8</cell><cell>99.1 96.6 95.0 93.5</cell><cell>35.7 21.3 40.4 37.8</cell><cell>97.5 84.6 86.7 93.4</cell></row><row><cell></cell><cell></cell><cell>Early Stopping</cell><cell cols="2">CelebA Waterbirds MultiNLI</cell><cell>Train Test Train Test Train Test</cell><cell>86.2 93.8 91.3 94.6 91.5 82.8</cell><cell>80.1 93.2 87.5 91.8 86.1 81.4</cell><cell>7.1 6.7 14.2 25.0 78.6 66.0</cell><cell>74.2 86.0 85.1 88.3 83.3 77.7</cell></row><row><cell>1</cell><cell cols="3">ERM Standard Regularization</cell><cell cols="3">DRO Standard Regularization</cell><cell cols="2">ERM Strong 2 Penalty</cell><cell>DRO Strong 2 Penalty</cell></row><row><cell>Accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell cols="2">Training Time</cell><cell cols="4">Training Time Dark hair, female Dark hair, male</cell><cell cols="2">Training Time Blond, female</cell><cell>Blond, male</cell><cell>Training Time</cell></row></table><note>: Average and worst-group accuracies for each training method. Both ERM and DRO models perform poorly on the worst-case group in the absence of regularization (top). With strong regu- larization (middle, bottom), DRO achieves high worst-group performance, significantly improving from ERM. Cells are colored by accuracy, from low (red) to medium (white) to high (blue) accuracy.Figure 2: Training (light) and validation (dark) accuracy for CelebA throughout training. With de- fault hyperparameters and training to convergence, ERM and DRO models achieve perfect training accuracy across groups, but generalize badly on the worst-case group (red line in the left panels). With strong 2 penalties, ERM models get high average train and test accuracies at the cost of the rare group (panel 3). DRO models achieve high train and test accuracies across groups (panel 4).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Average and worst-group test accuracies with and without group adjustments. Group adjustments improve worst-group accuracy, though average accuracy drops for Waterbirds.</figDesc><table><row><cell>1.0</cell><cell>C = 0</cell><cell>C = 1</cell><cell>C = 2</cell><cell>C = 4</cell></row><row><cell>Accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.7</cell><cell>Training Time Landbird, land</cell><cell>Landbird, water Training Time</cell><cell>Waterbird, land Training Time</cell><cell>Waterbird, water Training Time</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Published as a conference paper at ICLR 2020DISTRIBUTIONALLY ROBUST NEURAL NETWORKS FOR GROUP SHIFTS: ON THE IMPORTANCE OF REGULARIZATION FOR WORST-CASE GENERALIZATIONShiori Sagawa *</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In our main experiments, m = 4 or 6; we also use m = 64 in our supplemental experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Training to convergence is a widespread practice for image models<ref type="bibr" target="#b62">(Zhang et al., 2017;</ref><ref type="bibr" target="#b28">Hoffer et al., 2017)</ref>. Pre-trained language models are typically pretrained until convergence<ref type="bibr" target="#b15">(Devlin et al., 2019;</ref><ref type="bibr" target="#b47">Radford et al., 2019)</ref> but fine-tuned for a fixed small number of epochs because average test accuracy levels off quickly; we verified that training to convergence gave equally high average test accuracy.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">To avoid advantaging the DRO models by allowing them to tune additional hyperparameters, we restrict our search for group adjustments to the one 2 penalty strength used in Section 3.3. See Appendix C.2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/huggingface/pytorch-transformers</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<title level="m">Invariant risk minimization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning predicts hip fracture using confounding patient and healthcare variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Badgeley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Zech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oakden-Rayner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Glicksberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Mcconnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Percha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Dudley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust solutions of optimization problems affected by uncertain probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hertog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Waegenaere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Melenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rennen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="341" to="357" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Convex Optimization Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Athena Scientific Belmont</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Data-driven robust optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kallus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming Series A</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quantifying distributional model risk via optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blanchet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="565" to="600" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Demographic dialectal variation in social media: A case study of African-American English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1119" to="1130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convex Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Magging: maximin aggregation for inhomogeneous large-scale data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>B?hlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gender shades: Intersectional accuracy disparities in commercial gender classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Fairness, Accountability and Transparency</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="77" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What is the effect of importance weighting in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="872" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning models with uniform performance via distributionally robust optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08750</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Glynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<title level="m">Statistics of robust optimization: A generalized empirical likelihood approach. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distributionally robust losses against mixture covariate shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<ptr target="https://cs.stanford.edu/?thashim/assets/publications/condrisk.pdf" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fairness through awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Theoretical Computer Science (ITCS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="214" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data-driven distributionally robust optimization using the wasserstein metric: Performance guarantees and tractable reformulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Esfahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="166" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Annotation artifacts in natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="107" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3315" to="3323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Train faster, generalize better: Stability of stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fairness without demographics in repeated loss minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heinze-Deml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11469</idno>
		<title level="m">Conditional variance penalties and domain shift robustness</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1731" to="1741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tagging performance correlates with age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="483" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Does distributionally robust supervised learning give robust classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Incorporating dialectal variability for socially equitable language identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="51" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inherent trade-offs in the fair determination of risk scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Theoretical Computer Science (ITCS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Quantifying input uncertainty in stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 Winter Simulation Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tackling the widespread and critical impact of batch effects in high-throughput data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Scharpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Bravo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simcha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Langmead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Baggerly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Irizarry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Empirical bernstein bounds and sample variance penalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory (COLT</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Maximin effects in inhomogeneous large-scale data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>B?hlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stochastic gradient methods for distributionally robust optimization with f-divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Variance regularization with convex objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust stochastic approximation approach to stochastic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Juditsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on optimization</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1574" to="1609" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Hidden stratification causes clinically meaningful failures in machine learning for medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oakden-Rayner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12475</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Distributionally robust language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Causal inference by using invariant prediction: identification and confidence intervals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>B?hlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological</title>
		<imprint>
			<biblScope unit="page">78</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">why should I trust you?&quot;: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Anchor regression: heterogeneous data meets causality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rothenh?usler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>B?hlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06229</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Distributionally robust logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shafieezadeh-Abadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Esfahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="467" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Certifiable distributional robustness with principled adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rethinking the Inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Gender and dialect bias in youtubes automatic captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tatman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Ethics in Natural Langauge Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="53" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Principles of risk minimization for learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="831" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Robust learning under uncertain test distributions: Relating covariate shift to model misspecification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="631" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Invariance-inducing regularization using worst-case transformations suffices to boost accuracy and spatial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heinze-Deml</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
