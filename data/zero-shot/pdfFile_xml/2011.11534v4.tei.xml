<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Accurate 3D Hand Pose Estimation for Whole-Body 3D Human Mesh Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of ECE &amp; ASRI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of ECE &amp; ASRI</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
							<email>kyoungmu@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of ECE &amp; ASRI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">IPAI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Accurate 3D Hand Pose Estimation for Whole-Body 3D Human Mesh Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Whole-body 3D human mesh estimation aims to reconstruct the 3D human body, hands, and face simultaneously. Although several methods have been proposed, accurate prediction of 3D hands, which consist of 3D wrist and fingers, still remains challenging due to two reasons. First, the human kinematic chain has not been carefully considered when predicting the 3D wrists. Second, previous works utilize body features for the 3D fingers, where the body feature barely contains finger information. To resolve the limitations, we present Hand4Whole, which has two strong points over previous works. First, we design Pose2Pose, a module that utilizes joint features for 3D joint rotations. Using Pose2Pose, Hand4Whole utilizes hand MCP joint features to predict 3D wrists as MCP joints largely contribute to 3D wrist rotations in the human kinematic chain. Second, Hand4Whole discards the body feature when predicting 3D finger rotations. Our Hand4Whole is trained in an end-toend manner and produces much better 3D hand results than previous whole-body 3D human mesh estimation methods. The codes are available here 1,2 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Whole-body 3D human mesh estimation aims to localize mesh vertices of all human parts, including body, hands, and face, simultaneously in the 3D space. By combining 3D mesh of all human parts, we can understand not only human body pose and shape but also human intention and feeling through hand poses and facial expressions. The challenges of this problem are small image sizes and complicated articulations of hands, and smoothly connecting estimated 3D body and hands meshes.</p><p>Existing whole-body 3D human mesh estimation systems <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b37">36]</ref> consist of body, hands, and face networks. As hands and face take small areas in the human image, the 1 https://github.com/mks0601/Hand4Whole RELEASE 2 https://github.com/mks0601/Pose2Pose RELEASE  <ref type="figure">Figure 1</ref>. (a) Four hand MCP joints (red circles) provide essential 3D wrist rotation information as they are child nodes of a wrist in hand kinematic chain. (b) The average of width and height (pixel) of hand boxes in MSCOCO <ref type="bibr" target="#b12">[12]</ref>. The detailed finger information is almost missing at the output of the standard backbone, ResNet (less than 1 pixel). previous works crop and resize the hands and face images to higher resolutions. Then, they process the human, hands, and face images using body, hand, and face networks, respectively. Their body, hand, and face networks perform global average pooling (GAP) to the extracted image features and predict 3D body, hands, and face, respectively, which consist of 3D joint rotations and other parameters (e.g., body shape and facial expressions). The 3D body, hands, and face are passed to SMPL-X layer <ref type="bibr" target="#b26">[26]</ref> for the final whole-body 3D mesh.  <ref type="figure">Figure 2</ref>. Qualitative results comparison between the proposed Hand4Whole, ExPose <ref type="bibr" target="#b4">[4]</ref>, and FrankMocap <ref type="bibr" target="#b28">[27]</ref> when hands are invisible. Taking a body feature is necessary for plausible 3D wrist rotations. rotation predictions. Accurate 3D hands recovery is a key for the whole-body 3D human mesh estimation; however, previous works <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b37">36]</ref> produce inaccurate 3D hands due to two reasons. First, for the 3D wrist rotation, they have not carefully considered the human kinematic chain.</p><p>In the human kinematic chain, the wrist connects the human body and fingers' root joints (i.e., hand MCP joints); therefore, utilizing human body and MCP joint information is necessary for accurate 3D wrist rotation. In particular, the MCP joint information provides the necessary cue to determine 3D wrist rotations, as shown in <ref type="figure">Figure 1</ref> (a). Furthermore, the body information provides overall body posture, which can make the predicted 3D wrist rotation be anatomically plausible and smoothly connected with the body, even when hands are occluded or truncated, as shown in <ref type="figure">Figure 2</ref>. However, none of the previous works used a combination of the body feature and MCP joint features for the 3D wrist rotations. All of them used one of the body feature <ref type="bibr" target="#b37">[36]</ref>, hand feature <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b28">27]</ref>, and their combination <ref type="bibr" target="#b5">[5]</ref>, where the hand feature contains lots of unnecessary information, such as finger information, while the MCP joint feature contains essential 3D wrist rotation information. In particular, the finger information can hurt the 3D wrist rotation accuracy as the fingers often move independently to the wrists and have highly complicated articulations. Second, for the 3D finger rotations, previous works <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b37">36]</ref> used body features in addition to hand features. The body feature mainly contains much unnecessary information, such as body and backgrounds, while having very coarse hand information due to the small sizes of hands, as shown in <ref type="figure">Figure 1 (b)</ref>. Therefore, such unnecessary information can corrupt the 3D finger rotation accuracy.</p><p>To resolve the above limitations, we present Hand4Whole, a whole-body 3D human mesh estimation system that produces much better 3D hands in the whole-body 3D mesh. Hand4Whole has two stronger points than previous works. First, we design Pose2Pose, a 3D positional pose-guided 3D rotational pose prediction framework. Here, the 3D positional and rotational pose represent 3D joint positions and rotations, respectively. In contrast to previous works <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b28">27]</ref> that vectorize the image feature by performing GAP, ours extract joint features by our positional pose-guided pooling (PPP). The joint features enable Pose2Pose to understand different semantic information of each joint, while the previously used vectorized image feature only provides only an instance-level understanding. Using Pose2Pose, Hand4Whole utilizes a combination of the body feature and eight MCP joint features of two hands for the 3D wrist rotations. The body feature makes the 3D wrist rotations anatomically plausible when hands are occluded or truncated, as shown in <ref type="figure">Figure 2</ref>. Also, from an anatomical point of view, the eight hand MCP joint features provide essential 3D wrist rotations among thirty finger joints of two hands, as shown in <ref type="figure">Figure 1</ref> (a). By combining both features, Hand4Whole produces much more accurate 3D wrist rotations.</p><p>Second, we discard body features when predicting 3D finger rotations. As a result, we can prevent the coarse hand information from corrupting the 3D finger rotation accuracy. Although we do not use the body feature for 3D finger rotation, the 3D hands can be anatomically plausible and smoothly connected with body joints as our 3D wrist rotations already consider the body network's feature. In particular, the wrists are root nodes of a human hand kinematic chain, which determine global 3D rotations of fingers. Our Hand4Whole is trained in an end-to-end manner and significantly outperforms previous whole-body 3D human mesh estimation methods.</p><p>Our contributions can be summarized as follows.</p><p>? Our Hand4Whole produces much more accurate 3D hands, which consists of the 3D wrist and finger rotations. Using our Pose2Pose, Hand4Whole utilizes both body and hand MCP joint features for accurate 3D wrist rotation and smooth connection between 3D body and hands.</p><p>? In addition, we discard body features when predicting 3D finger rotations.</p><p>? Hand4Whole is trained in an end-to-end manner and largely outperforms previous whole-body 3D human mesh estimation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Although there have been remarkable progress in 3D body-only <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b31">30]</ref>, hand-only <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b38">37]</ref>, and face-only <ref type="bibr" target="#b29">[28]</ref> mesh estimation, there have been few attempts to simultaneously recover the 3D body, hands, and face. Several early works are based on an optimizationbased approach, which fits a 3D human model to the 2D/3D evidence. Joo et al. <ref type="bibr" target="#b13">[13]</ref> fits their human models (i.e., Frank and Adam) to 3D human joints coordinates and point clouds in a multi-view studio environment. Xiang et al. <ref type="bibr" target="#b33">[32]</ref> extended Joo et al. <ref type="bibr" target="#b13">[13]</ref> to the single RGB case. SMPLif-X <ref type="bibr" target="#b26">[26]</ref> and Xu et al. <ref type="bibr" target="#b35">[34]</ref> fit their human models, SMPL-X and GHUM, respectively, to 2D human joint coordinates. As the above optimization-based methods can be slow and prone to noisy evidence, a regression-based approach is presented recently. Recently, several neural network-based methods have been proposed. All of them consist of body, hand, and face networks, where each takes a human image, hand-cropped image, and face-cropped image, respectively. The three networks predict SMPL-X parameters for a final whole-body 3D human mesh. ExPose <ref type="bibr" target="#b4">[4]</ref> is the earliest regression-based approach, which utilizes the three separated networks. FrankMocap <ref type="bibr" target="#b28">[27]</ref> consists of body and hand networks, and their hand prediction is attached to 3D body results. Zhou et al. <ref type="bibr" target="#b37">[36]</ref> utilize 3D joint coordinates for the 3D joint rotation prediction. PIXIE <ref type="bibr" target="#b5">[5]</ref> introduced a moderator to predict 3D joint rotations.</p><p>Compared to the recent whole-body 3D human mesh estimation methods, our Hand4Whole has three clear novel contributions. First, we design Pose2Pose as the main module of our Hand4Whole. Pose2Pose enables Hand4Whole to understand different semantic information of each joint by introducing joint features, while previous works <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b28">27]</ref> provides only an instance-level feature to their system by performing the GAP. Second, Hand4Whole uses both body and hand MCP joint features for the 3D wrist rotations. On the other hand, ExPose <ref type="bibr" target="#b4">[4]</ref> and FrankMocap <ref type="bibr" target="#b28">[27]</ref> predict 3D wrist rotation only from a hand feature without a body feature. The absence of the body feature when predicting the 3D wrist rotations results in implausible 3D wrist rotations when hands are occluded or truncated, as shown in <ref type="figure">Figure 2</ref>. Zhou et al. <ref type="bibr" target="#b37">[36]</ref> predict 3D wrist rotations only from a body feature without a hand feature. They suffer from inaccurate 3D wrist rotations as they do not utilize MCP joint features. PIXIE <ref type="bibr" target="#b5">[5]</ref> combines body and hand features by a moderator to predict 3D wrist rotations. However, their hand feature contains lots of unnecessary information, such as finger information, which can hurt the 3D wrist rotation accuracy as fingers often move independently to the wrists. Third, Hand4Whole discards the body feature when predicting 3D finger rotations. In contrast, Zhou et al. <ref type="bibr" target="#b37">[36]</ref> and PIXIE <ref type="bibr" target="#b5">[5]</ref> uses the body feature, which contains very coarse hand information due to the small pixel size of the hands. <ref type="figure" target="#fig_2">Figure 3</ref> shows the overall pipeline of the proposed Hand4Whole for whole-body 3D human mesh estimation. It consists of BodyNet, HandNet, and FaceNet, which take cropped and resized human, hands, and face images, respectively. The outputs of each network are fed to SMPL-X <ref type="bibr" target="#b26">[26]</ref> layer to obtain the final whole-body 3D human mesh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Hand4Whole</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pose2Pose</head><p>To start, we describe Pose2Pose, a 3D positional pose (i.e., 3D joint positions)-guided 3D rotational pose (i.e., 3D joint rotations) prediction framework. <ref type="figure" target="#fig_3">Figure 4</ref> shows the overall pipeline of Pose2Pose when it is used for the 3D body. Pose2Pose enables the proposed Hand4Whole to understand different semantic information of each joint. We use Pose2Pose as the main module of our BodyNet and HandNet. Pose2Pose consists of two stages, described in the following. 3D joint coordinates estimation. Pose2Pose firsts predicts the 3D joint coordinates P ? R J?3 from a human image. J denotes the number of joints. xand y-axis of P are in pixel space, and z-axis of it is in root joint (i.e., pelvis for the body and wrist for the hand)-relative depth space. To this end, ResNet-50 <ref type="bibr" target="#b10">[10]</ref> extracts 2048-dimensional image feature map F from the input image. We use ResNet after removing the GAP and fully-connected layer of the last part of the original ResNet. Then, a 1-by-1 convolutional layer predicts 3D heatmaps of human joints H from F, where H has the same height and width as F. To make 3D heatmaps from the 2D feature map, the 1-by-1 convolutional layer changes the channel dimension from 2048 to 8J. Then, we reshape the 8J-dimensional feature map to J dimensional 3D heatmap whose depth size is 8. The 3D joint coordinates P is obtained from H by the soft-argmax operation <ref type="bibr" target="#b30">[29]</ref> in a differentiable way. Positional pose-guided pooling (PPP). Positional poseguided pooling extracts joint features from the 2D feature map F. To this end, we apply a 1-by-1 convolution to F to change its channel dimension from 2048 to 512. Then, we perform a bilinear interpolation to the output of the convolution at (x, y) position of 3D joint coordinates P, which produce joint features F P ? R J?512 . Please note that as (x, y) coordinates of P are in pixel space, we use them directly for the interpolation without a 3D-to-2D projection. The joint features provide different contextual information of each joint, useful for the 3D joint rotation <ref type="bibr" target="#b8">[8]</ref>. 3D joint rotations estimation. For the 3D joint rotation ?, we concatenate the joint features F P and 3D joint coordinates P and flatten it, denoted by v ? R 512J+3J . Then, v  is passed to a fully-connected layer, which produces the 3D joint rotations ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">BodyNet</head><p>The BodyNet outputs 3D body joint rotations, SMPL-X shape parameters, camera parameters, and boxes of hands and face. Pose2Pose plays a central role in the prediction of the 3D body joint rotation. 3D body joint rotations. Using the Pose2Pose, BodyNet predicts 3D body joint rotation ? b ? R 22?3 from a human image I b ? R 3?256?192 , downsampled from a highresolution human image I ? R 3?512?384 . The downsampling is necessary to save the computational cost. One modification we made to the Pose2Pose when predicting the 3D joint rotation is that instead of only using v b , a concatena-tion of 3D body joint coordinates and body joint features, BodyNet additionally uses v m , a concatenation of 3D coordinates and features of MCP joints. v m is obtained from the HandNet, and Section 3.3 describes how we obtain it in detail. The additional usage of MCP joint information enables BodyNet to produce more accurate 3D wrist rotations. Please note that the predicted 3D body joint rotations ? b include 3D wrist rotations. The hand MCP joint features are not only beneficial for 3D wrist rotations, but also for 3D elbow rotations, as 3D elbow rotations in the roll-axis are highly related to the hand MCP joints. This is the reason why the hand MCP joint features are used for 3D rotations of all body joints, not restricted to 3D wrist rotations.</p><p>SMPL-X shape/camera parameters. The shape parameter ? b ? R 10 and 3D global translation vector t b ? R 3 are predicted from the image feature F b using the GAP and a single fully-connected layer.</p><p>Hands and face boxes. The BodyNet predicts hand and face bounding boxes by concatenating the image feature F b and 3D heatmap H b and passing it to two convolutional layers. The 3D heatmap H b is predicted for the 3D positional pose of the body joints. The box centers are obtained by applying the soft-argmax <ref type="bibr" target="#b30">[29]</ref> to the output of the two convolutional layers. The widths and heights of the boxes are computed by performing bilinear interpolation to F b at the box centers and passing the features of each box center to two fully-connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">HandNet</head><p>Like the BodyNet, the HandNet outputs 3D finger rotations ? h ? R 15?3 using Pose2Pose. HandNet takes a hand image I h , cropped and resized from the high-resolution human image I by applying RoIAlign <ref type="bibr" target="#b9">[9]</ref> at the predicted hand bounding box area. Taking the hand images from the highresolution image I instead of the downsampled human image I b allows HandNet to utilize detailed finger information. The hand-cropped images of the left hands are flipped to the right hands before being fed to the HandNet. After predicting the 3D finger rotations, we flip back the outputs of the flipped left hands. We denote the 3D finger rotations of the left and right hands as ? rh and ? lh , respectively. MCP joints features to the BodyNet. As described in Section 3.2, we pass v m ? R 3 * 8+512 * 8 , a concatenation of the 3D coordinates and features of eight MCP joints (i.e., four from the right hand and four from the left hand) to the BodyNet for the accurate 3D wrist rotation prediction. To this end, we take the joint features and 3D coordinates of MCP joints of the left and right hands from the outputs of Pose2Pose. The taken 3D coordinates and joint features are concatenated and flattened to a vector v m ? R 3 * 8+512 * 8 , which is passed to the BodyNet. Among hand joints, we choose the MCP joints as they show low positional errors than other hand joints while providing essential 3D wrist rotation information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">FaceNet</head><p>We design FaceNet as a simple GAP-based regressor instead of Pose2Pose architecture as deformations of human faces (e.g., facial expressions) are not fully modeled by the 3D joint rotations. The FaceNet predicts 3D jaw rotation ? f ? R 3 and facial expression code ? ? R 10 from a facecropped image I f ? R 3?192?192 . The face-cropped image is cropped and resized from the high-resolution human image I by applying RoIAlign <ref type="bibr" target="#b9">[9]</ref> at the predicted face bounding box area to utilize detailed face information. We use ResNet after removing the GAP and fully-connected layer of the last part of the original ResNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss functions</head><p>Our framework is trained in an end-to-end manner by minimizing the loss function L, defined as follows.</p><formula xml:id="formula_0">L = L param + L coord + L box ,<label>(1)</label></formula><p>where L param is a L1 distance between predicted and GT SMPL-X parameters. L coord is a L1 distance between predicted and GT joint coordinates, and three types of joint coordinates are used to calculate the loss function: 1) 3D joint coordinates from the Pose2Pose of body and hands, 2) 3D joint coordinates, obtained by multiplying a joint regression matrix of SMPL-X to the 3D mesh, and 3) 2D joint coordinates, obtained by projecting the 3D coordinates from the 3D mesh, to the 2D space using the perspective projection. For the projection, the predicted 3D global translation vector t b , fixed focal length (5000,5000), and fixed principal points (i.e., a center point of I b ) are used, following <ref type="bibr" target="#b17">[17]</ref>. Finally, L box is a L1 distance between the predicted and GT center and scale of the hands' and face's boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation details</head><p>PyTorch <ref type="bibr" target="#b24">[24]</ref> is used for implementation. The ResNet of the body branch is initialized with that of Xiao et al. <ref type="bibr" target="#b34">[33]</ref>, pre-trained on MSCOCO 2D human body pose dataset. For the training, we use Adam optimizer <ref type="bibr" target="#b16">[16]</ref> with a mini-batch size of 96. Data augmentations, including scaling, rotation, random horizontal flip, and color jittering, are performed during the training. All the 3D rotations are initially predicted in the 6D rotational representation of <ref type="bibr" target="#b36">[35]</ref> and converted to the 3D axis-angle rotations. The initial learning rate is set to 10 ?4 and reduced by a factor of 10 at the 10 th epoch. A neutral gender SMPL-X model is used for the training and testing. All other details will be available in our codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and evaluation metrics</head><p>Datasets. For the training, Human3.6M <ref type="bibr" target="#b11">[11]</ref>, the wholebody version of MSCOCO <ref type="bibr" target="#b12">[12]</ref>, MPII <ref type="bibr" target="#b1">[1]</ref>, and Frei-HAND <ref type="bibr" target="#b39">[38]</ref> are used. The 3D pseudo-GTs for the training are obtained by NeuralAnnot <ref type="bibr" target="#b21">[21]</ref>. For the 3D bodyonly, 3D hand-only, and 3D face-only evaluations, we use 3DPW <ref type="bibr" target="#b32">[31]</ref>, FreiHAND <ref type="bibr" target="#b39">[38]</ref>, and Stirling <ref type="bibr" target="#b6">[6]</ref>, respectively. For the 3D whole-body evaluation, we use EHF <ref type="bibr" target="#b26">[26]</ref> and AGORA <ref type="bibr" target="#b25">[25]</ref>. We provide qualitative results on the MSCOCO validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metrics.</head><p>Mean per joint position error (MPJPE) and mean per-vertex position error (MPVPE) are used to evaluate 3D joint and mesh vertices positions, respectively. Each calculates the average 3D joint distance (mm) and 3D mesh vertex distance (mm) between the predicted and GT, after aligning a root joint translation. The pelvis is used as the root joint when calculating the 3D errors of the whole body and body. On the other hand, the wrists and neck are used as the root joints when calculating the 3D errors of the hands and face. PA MPJPE and PA MPVPE further align a rotation and scale. The 3D joint coordinates for the MPJPE and PA MPJPE are obtained by multiplying a joint regression matrix, defined in SMPL-X, to the mesh, following previous works <ref type="bibr" target="#b4">[4]</ref>. We report average 3D errors of the left and right hands for the 3D hands' error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>With MCP features (Ours) Without MCP features Input image</head><p>With MCP features (Ours) Without MCP features Input image <ref type="figure">Figure 5</ref>. Qualitative comparison of models that doese not take and take hand MCP joint features for the 3D wrist rotation. Utilizing MCP features improves the 3D wrist rotation significantly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation study</head><p>MCP joint features for 3D wrist rotations. <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure">Figure 5</ref> show that Hand4Whole produces accurate 3D wrist rotations by taking a combination of body and hand MCP joint features. The hand MPVPE is measured by separating 3D hand meshes from a whole-body mesh without rotation alignment; therefore, wrong 3D wrist rotation significantly increases the MPVPE of hands. To analyze the effectiveness of the hand MCP joint features, we design three variants. They have exactly the same network architectures as ours except the BodyNet that predicts 3D body joint rotations including 3D wrist rotations. In their BodyNet, they take the same body feature (i.e., a combination of 3D joint coordinates and joint features of the body) and different hand features. Please note that the body feature is essential for plausible 3D wrist rotations when hands are occluded, as shown in <ref type="figure">Figure 2</ref>. As they have the same HandNet architecture, all of them have similar 3D finger rotation predictions. Therefore, most of the hand errors are from wrist rotations, not from finger rotations. The first variant uses only the body features for the 3D wrist rotation like Zhou et al. <ref type="bibr" target="#b37">[36]</ref>. It suffers from inaccurate 3D wrist rotation, which indicates additional hand feature is necessary to determine 3D wrist rotation accurately. The second variant uses a combination of the body feature and hand global average pooled (GAPed) feature  <ref type="table">Table 3</ref>. Comparison of whole-body PA MPVPEs, obtained by models that take various input combinations for the 3D joint rotation prediction, on EHF.</p><p>for 3D wrist rotation prediction like PIXIE <ref type="bibr" target="#b5">[5]</ref>. The hand GAP feature is obtained by performing GAP at the output of HandNet's ResNet. It still produces less accurate 3D wrist rotation than ours, which indicates that the hand GAP feature fails to capture essential information of the 3D wrist rotations. The third variant uses a combination of the body features and 3D joint coordinates and joint features of all two hand joints for 3D wrist rotation prediction. It still achieves worse results than ours because the joint features of all two hand joints (i.e., 40 finger joints) contain too much unnecessary information considering that the body consists of 25 joints. From an anatomical point of view, eight MCP joints of two hands mainly contribute to the 3D wrist rotation, as shown in <ref type="figure">Figure 1</ref>, while other finger joints often move independently with the 3D body joint rotation. Our Hand4Whole achieves the lowest 3D errors and accurate 3D wrist rotation by taking body and MCP joint features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PA MPVPE MPVPE All Hands Face All Hands Face</head><p>ExPose <ref type="bibr" target="#b4">[4]</ref> 54. <ref type="bibr" target="#b5">5</ref>  Removing body features for 3D finger rotations. <ref type="table">Table 2</ref> shows that removing the body feature produces better 3D finger rotations. The setting that uses the body feature is similar to previous works <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b37">36]</ref>. As PA MPVPE is calculated after the rotation alignment, aligned outputs have almost correct 3D wrist rotations. Therefore, most of the 3D errors come from 3D fingers. To analyze how the body feature affects the 3D finger rotation, we design a variant whose HandNet takes additional coarse hand information, obtainable from hand areas at the body feature. To extract the coarse hand information, we perform RoIAlign <ref type="bibr" target="#b9">[9]</ref> to the output of the first residual block in the BodyNet's ResNet using the predicted hand bounding box. Then, the output of RoIAlign is added in an element-wise manner to the first residual block in the HandNet's ResNet. The reason why only removing the body feature produces better 3D finger rotation is that the coarse hand information in the body feature contains various unnecessary information, such as body and backgrounds, while barely having finger information due to the small resolution of hands. As such unnecessary information corrupts the hand feature of the HandNet, using only the hand feature from the HandNet like 3D handonly reconstruction methods <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b38">37]</ref>, produces better 3D finger rotations. Effectiveness of joint features. <ref type="table">Table 3</ref> shows that the joint feature of our Pose2Pose is much more beneficial than previous GAPed feature <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b28">27]</ref>. In addition, combining the joint feature with 3D joint coordinates like ours achieves the best results. For the demonstration, we designed four variants that take different combinations of features for the 3D body and hand joint rotation prediction. The first and second variants take a GAP feature vector and joint feature, respectively. The GAP feature is obtained by performing GAP to the output of ResNet. As GAP marginalizes the spatial domain, the feature vector losses detailed human articulation information. On the other hand, our joint feature preserves such detailed articulation information by interpolating human joint positions, beneficial for accurate 3D joint rotation prediction. The third and fourth variants take 2D and 3D joint coordinates, respectively. Compared to the third variant, the fourth variant achieves lower PA MPVPE by utilizing additional depth information. Finally, we combine the 3D joint coordinates and joint features for the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with state-of-the-art methods</head><p>EHF (Whole-body evaluation benchmark). <ref type="table">Table 4</ref> shows that our Hand4Whole largely outperforms all previous whole-body methods on EHF. As existing works mainly have reported PA metrics, we use their released codes and pre-trained weights to report MPVPEs. AGORA (Whole-body evaluation benchmark). <ref type="table">Table 5</ref> shows that our Hand4Whole largely outperforms all previous whole-body methods on AGORA. We obtained all numbers using their released codes with pre-trained weights and an official evaluation tool <ref type="bibr" target="#b3">3</ref> . We use the same human bounding boxes for all methods. 3DPW (Body-only evaluation benchmark). <ref type="table">Table 6</ref> shows that our Hand4Whole largely outperforms all previ-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hand4Whole (Ours)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ExPose</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input image</head><p>FrankMocap PIXIE <ref type="figure">Figure 6</ref>. Qualitative comparison of the proposed Hand4Whole, ExPose <ref type="bibr" target="#b4">[4]</ref>, FrankMocap <ref type="bibr" target="#b28">[27]</ref>, and PIXIE <ref type="bibr" target="#b5">[5]</ref>. Implausible 3D wrist rotations are highlighted.</p><p>ous whole-body methods on 3DPW, although it is slightly beaten by recent body-only methods. The stronger performance on the 3D body benchmark than previous wholebody methods results from a combination of 3D joint coordinates and joint features for the 3D body joint rotation prediction. On the other hand, all previous whole-body methods use GAP features for the 3D body joint rotation prediction.</p><p>FreiHAND (Hand-only evaluation benchmark). <ref type="table">Table 7</ref> shows that our Hand4Whole largely outperforms all previous whole-body methods on FreiHAND, although it is slightly beaten by recent hand-only methods. We obtained the results of FrankMocap using their released codes and pre-trained weights as they did not report their results on FreiHAND. In particular, Hand4Whole achieves the best results even after changing our hand branch ResNet-50 to ResNet-18 following ExPose <ref type="bibr" target="#b4">[4]</ref>.</p><p>MSCOCO (Qualitative comparison of the whole-body). <ref type="figure">Figure 6</ref> shows that our Hand4Whole produces much more accurate 3D wrists and finger rotations than previous wholebody methods.</p><p>Overall, Hand4Whole largely outperforms all previous whole-body methods. In particular, it produces much better 3D hand results. The comparisons are consistent with the results of the ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We present Hand4Whole, a whole-body 3D human mesh estimation system that produces much better 3D hands in the whole-body 3D mesh. Hand4Whole utilizes hand MCP joint features for the 3D wrist rotation by introducing Pose2Pose. In addition, it discards the body feature when predicting 3D finger rotations. Hand4Whole largely outperforms previous 3D whole-body human mesh estimation methods on all benchmarks. Acknowledgements. This work was supported in part by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for "Accurate 3D Hand Pose Estimation for Whole-Body 3D Human Mesh Estimation"</head><p>In this supplementary material, we present more experimental results that could not be included in the main manuscript due to the lack of space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Qualitative comparisons</head><p>A.1. MSCOCO <ref type="figure">Figure A</ref> and B show that our Hand4Whole produces more accurate results on in-the-wild images of MSCOCO. In particular, ours produce much better 3D hands results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. 3DPW</head><p>The video in this link <ref type="bibr" target="#b4">4</ref> shows that our Hand4Whole produces more accurate and plausible expressive 3D human mesh than ExPose <ref type="bibr" target="#b4">[4]</ref> and FrankMocap <ref type="bibr" target="#b28">[27]</ref> on videos of 3DPW <ref type="bibr" target="#b32">[31]</ref>. In particular, ours achieves much better and stable hands results when hands are invisible by using body and hand MCP joint features. On the other hand, Ex-Pose and FrankMocap do not use body features, which results in implausible 3D hands. Hand4Whole, ExPose, and FrankMocap are run on every single frame without leveraging temporal information. We did not apply any postprocessing, such as average filtering, on the outputs. The results of ExPose and FrankMocap are obtained by their officially released codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>License of the Used Assets</head><p>? MSCOCO dataset <ref type="bibr" target="#b20">[20]</ref> belongs to the COCO Consortium and are licensed under a Creative Commons Attribution 4.0 License. ? Human3.6M dataset <ref type="bibr" target="#b11">[11]</ref>'s licenses are limited to academic use only. ? MPII dataset <ref type="bibr" target="#b1">[1]</ref> is released for academic research only and it is free to researchers from educational or research institutes for non-commercial purposes. ? 3DPW dataset <ref type="bibr" target="#b32">[31]</ref> is released for academic research only and it is free to researchers from educational or research institutes for non-commercial purposes. ? FreiHAND dataset <ref type="bibr" target="#b39">[38]</ref> is released for academic research only and it is free to researchers from educational or research institutes for non-commercial purposes.  <ref type="bibr" target="#b6">[6]</ref> is released for academic research only and it is free to researchers from educational or research institutes for non-commercial purposes. ? EHF dataset <ref type="bibr" target="#b26">[26]</ref> is released for academic research only and it is free to researchers from educational or research institutes for non-commercial purposes. ? AGORA dataset <ref type="bibr" target="#b25">[25]</ref> is released for academic research only and it is free to researchers from educational or research institutes for non-commercial purposes. ? ExPose <ref type="bibr" target="#b4">[4]</ref> codes are released for academic research only and it is free to researchers from educational or research institutes for non-commercial purposes. ? FrankMocap <ref type="bibr" target="#b28">[27]</ref> codes are CC-BY-NC 4.0 licensed.</p><p>? PIXIE <ref type="bibr" target="#b5">[5]</ref> codes are released for academic research only and it is free to researchers from educational or research institutes for non-commercial purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hand4Whole (Ours)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ExPose</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input image</head><p>FrankMocap PIXIE <ref type="figure">Figure A</ref>. Qualitative comparison of the proposed Hand4Whole, ExPose <ref type="bibr" target="#b4">[4]</ref>, FrankMocap <ref type="bibr" target="#b28">[27]</ref>, and PIXIE [5] on MSCOCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hand4Whole (Ours) ExPose Input image</head><p>FrankMocap PIXIE <ref type="figure">Figure B</ref>. Qualitative comparison of the proposed Hand4Whole, ExPose <ref type="bibr" target="#b4">[4]</ref>, FrankMocap <ref type="bibr" target="#b28">[27]</ref>, and PIXIE <ref type="bibr" target="#b5">[5]</ref> on MSCOCO.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Hand4Whole</head><label></label><figDesc>3D hands recovery consists of the 3D wrist and finger arXiv:2011.11534v4 [cs.CV] 19 Apr 2022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The overall pipeline of Hand4Whole. The BodyNet predicts 3D body parameters, which include 3D wrist rotations, from a combination of body and hand MCP joint information. The HandNet predicts 3D finger rotation only from fine hand information without combining coarse hand information, extracted from the BodyNet. The weights in HandNet for the right and flipped left hand images are shared. The final whole-body 3D human mesh is obtained by forwarding the outputs of BodyNet, HandNet, and FaceNet to the SMPL-X<ref type="bibr" target="#b26">[26]</ref> layer. The hand and face boxes are predicted from BodyNet, not shown in the figure. Our Hand4Whole is trained in an end-to-end manner. y: pixel, z: root-relative depth)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of the Pose2Pose in our BodyNet. It extracts the joint features by interpolating (x, y) pixel position of the 3D joint coordinates P b on the output of 1-by-1 convolution. For the simplicity, we describe only right elbow and ankle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>?</head><label></label><figDesc>FFHQ dataset [15]'s individual images were published in Flickr by their respective authors under either Creative Commons BY 2.0, Creative Commons BY-NC 2.0, Public Domain Mark 1.0, Public Domain CC0 1.0, or U.S. Government Works license. The dataset itself (including JSON metadata, download script, and documentation) is 4 https://www.youtube.com/watch?v=Ym CH8yxBso made available under Creative Commons BY-NC-SA 4.0 license by NVIDIA Corporation. ? Stirling dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>Inputs for 3D wrist prediction</cell><cell>MPVPE (Hands)</cell></row><row><cell>Body</cell><cell>50.4</cell></row><row><cell>Body + Hand GAP</cell><cell>43.1</cell></row><row><cell>Body + All hand joints</cell><cell>43.4</cell></row><row><cell>Body + MCP joints (Ours)</cell><cell>39.8</cell></row></table><note>Comparison of hands MPVPEs, obtained by models that predict 3D wrist rotation from various features, on EHF.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>3D errors comparison on EHF. 3D errors comparison on AGORA.</figDesc><table><row><cell></cell><cell cols="3">12.8 5.8 77.1 51.6 35.0</cell></row><row><cell>FrankMocap [27]</cell><cell>57.5 12.6</cell><cell>-107.6 42.8</cell><cell>-</cell></row><row><cell>PIXIE [5]</cell><cell cols="3">55.0 11.1 4.6 89.2 42.8 32.7</cell></row><row><cell cols="4">Hand4Whole (Ours) 50.3 10.8 5.8 76.8 39.8 26.1</cell></row><row><cell>Methods</cell><cell cols="3">PA MPVPE All Hands Face All Hands Face MPVPE</cell></row><row><cell>ExPose [4]</cell><cell cols="3">88.0 12.1 4.8 219.8 115.4 103.5</cell></row><row><cell>FrankMocap [27]</cell><cell cols="3">90.6 11.2 4.9 218.0 95.2 105.4</cell></row><row><cell>PIXIE [5]</cell><cell cols="3">82.7 12.8 5.4 203.0 89.9 95.4</cell></row><row><cell cols="4">Hand4Whole (Ours) 73.2 9.7 4.7 183.9 72.8 81.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/pixelite1201/agora evaluation</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
				<idno>No. 2021-0-01343</idno>
	</analytic>
	<monogr>
		<title level="m">IITP grant funded by the Korea government (MSIT)</title>
		<imprint/>
		<respStmt>
			<orgName>Seoul National University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D hand shape and pose from images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adnane</forename><surname>Boukhayma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>De Bem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pose2Mesh: Graph convolutional network for 3D human pose and mesh recovery from a 2D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Monocular expressive body regression through body-driven attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collaborative regression of expressive bodies using moderation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluation of dense 3D reconstruction from 2D face images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Koppen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R?tsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FG</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D hand shape and pose estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">HoloPose: Holistic 3D human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Whole-body human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lumin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Total capture: A 3D deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weaklysupervised mesh-convolutional hand reconstruction in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Kulon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">NeuralAnnot: Neural annotator for 3D human mesh training sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">I2L-MeshNet: Image-to-Lixel prediction network for accurate 3D human pose and mesh estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On self-contact and human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Hao P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">AGORA: Avatars in geography optimized for regression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyanka</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chun-Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">T</forename><surname>Tesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<imprint>
			<pubPlace>Ahmed AA Osman, Dimitrios Tzionas, and</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3D hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FrankMocap: A monocular 3d whole-body pose estimation system via regression and integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Shiratori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to regress 3D face shape and expression from an image without 3D supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soubhik</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Monocular, one-stage, regression of multiple 3d people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Black</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using IMUs and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Monocular Total Capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">GHUM &amp; GHUML: Generative 3D human shape and articulated pose models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Monocular realtime full body capture with inter-part correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Monocular realtime hand shape and motion capture using multi-modal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">FreiHAND: A dataset for markerless capture of hand pose and shape from single RGB images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Argus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
