<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Efficient and Scale-Robust Ultra-High-Definition Image Demoir?ing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Ma</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">TCL AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">TCL AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Efficient and Scale-Robust Ultra-High-Definition Image Demoir?ing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image demoir?ing</term>
					<term>Image restoration</term>
					<term>Ultra-high-definition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the rapid development of mobile devices, modern widelyused mobile phones typically allow users to capture 4K resolution (i.e., ultra-high-definition) images. However, for image demoir?ing, a challenging task in low-level vision, existing works are generally carried out on low-resolution or synthetic images. Hence, the effectiveness of these methods on 4K resolution images is still unknown. In this paper, we explore moir? pattern removal for ultra-high-definition images. To this end, we propose the first ultra-high-definition demoir?ing dataset (UHDM), which contains 5,000 real-world 4K resolution image pairs, and conduct a benchmark study on current state-of-the-art methods. Further, we present an efficient baseline model ESDNet for tackling 4K moir? images, wherein we build a semantic-aligned scale-aware module to address the scale variation of moir? patterns. Extensive experiments manifest the effectiveness of our approach, which outperforms state-of-the-art methods by a large margin while being much more lightweight. Code and dataset are available at https://xinyu-andy.github.io/uhdm-page.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When photographing the contents displayed on the digital screen, an inevitable frequency aliasing between the camera's color filter array (CFA) and the screen's LCD subpixel widely exists. The captured images are thus mixed with colorful stripes, named moir? patterns, which severely degrade the perceptual quality of images. Currently, efficiently removing moir? patterns from a single moir? image is still challenging and receives growing attention from the research community.</p><p>Recently, several image demoir?ing methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">47,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr">29,</ref><ref type="bibr">22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr">20,</ref><ref type="bibr">40]</ref> have been proposed, yielding a plethora of dedicated designs such as moir? pattern classification <ref type="bibr" target="#b11">[12]</ref>, frequency domain modeling <ref type="bibr">[22,</ref><ref type="bibr">47]</ref>, and multi-stage framework <ref type="bibr" target="#b12">[13]</ref>. Apart from FHDe 2 Net <ref type="bibr" target="#b12">[13]</ref> which is specially designed for high-definition images, most of the research efforts have been devoted to studying low-resolution images indicates the corresponding author. arXiv:2207.09935v1 [cs.CV] 20 Jul 2022 <ref type="bibr">[29]</ref> or synthetic images <ref type="bibr">[40]</ref>. However, the fast development of mobile devices enables modern mobile phones to capture ultra-high-definition images, so it is more practical to conduct research on 4K image demoir?ing for real applications. Unfortunately, the highest resolution in current public demoir?ing datasets (see <ref type="table" target="#tab_0">Table 1</ref>) is 1080p <ref type="bibr" target="#b12">[13]</ref> (1920 ? 1080). Whether methods investigated on such datasets can be trivially transferred into the 4K scenario is still unknown due to the data distribution change and dramatically increased computational cost.</p><p>Under this circumstance, we explore the more practical yet more challenging demoir?ing scenario, i.e., ultra-high-definition image demoir?ing. To evaluate the demoir?ing methods in this scenario, we build the first large-scale real-world ultra-high-definition demoir?ing dataset (UHDM), which consists of 4, 500 training image pairs and 500 testing image pairs with diverse scenes (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Benchmark study and limitation analysis: Based upon our dataset, we conduct a benchmark study on state-of-the-art methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">47,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr">29,</ref><ref type="bibr">22,</ref><ref type="bibr" target="#b7">8]</ref>. Our empirical study reveals that most methods <ref type="bibr">[29,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr">47]</ref> struggle to remove moir? patterns with a much wider range of scales in 4K images while simultaneously tolerating the growing demands for computational cost (see <ref type="figure" target="#fig_2">Fig. 3</ref>) or fine image detail <ref type="bibr" target="#b12">[13]</ref> (see <ref type="figure" target="#fig_1">Fig. 2</ref>). We attribute their deficiencies to the lack of an effective multi-scale feature extraction strategy. Concretely, existing methods attempting to address the scale challenge can be coarsely categorized into two lines of research. One line of research develops multi-stage models, such as FHDe 2 Net <ref type="bibr" target="#b12">[13]</ref>, to process large moir? patterns at a low-resolution stage and then refines the textures at a high-resolution stage, which however incurs huge computational cost when applied to 4K images (see <ref type="figure" target="#fig_2">Fig. 3</ref>: FHDe 2 Net). Another line of research utilizes features from different depths of a network to build multi-scale representations, in which the most representative work [47] achieves a better trade-off between accuracy and efficiency (see <ref type="figure" target="#fig_2">Fig. 3: MBCNN</ref>), yet still cannot be generally scale-robust (see <ref type="figure" target="#fig_1">Fig. 2</ref> and <ref type="figure" target="#fig_4">Fig. 5</ref>). We note that the extracted multi-scale features are from different semantic levels which may result in misaligned features when fused together, potentially limiting its capabilities. Detailed study and analysis are unfolded in Section 3.2.</p><p>To this end, inspired by HRNet [33], we propose a plug-and-play semanticaligned scale-aware module (SAM) to boost the network's capability in handling moir? patterns with diverse scales without incurring too much computational cost, serving as a supplement to existing methods. Specifically, SAM incorporates a pyramid context extraction module to effectively and efficiently extract multiscale features aligned at the same semantic level. Further, a cross-scale dynamic fusion module is developed to selectively fuse multi-scale features where the fusion weights are learned and dynamically adapted to individual images.</p><p>Equipped with SAM, we develop an efficient and scale-robust network for 4K image demoir?ing, named ESDNet. ESDNet adopts a simple encoder-decoder network with skip-connections as its backbone and stacks SAM at different semantic levels to boost the model's capability in addressing scale variations of 4K moir? images. ESDNet is easy to implement while achieving state-of-the-art performance (see <ref type="figure" target="#fig_4">Fig. 5</ref> and <ref type="table" target="#tab_3">Table 2</ref>) on the challenging ultra-high-definition image demoir?ing dataset and three other public demoir?ing datasets <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">40,</ref><ref type="bibr">29]</ref>. In particular, ESDNet exceeds multi-stage high-resolution method FHDe 2 Net, 1.8dB in terms of PSNR while being 300? faster (5.620s vs 0.017s) in the UHDM dataset. Our major contributions are summarized as follows:</p><p>-We are the first to explore the ultra-high-definition image demoir?ing problem, which is more practical yet more challenging. To this end, we build a large-scale real-world 4K resolution demoir?ing dataset UHDM. -We conduct a benchmark study for the existing state-of-the-art methods on this dataset, summarizing several challenges and analyses. Motivated by these analyses, we propose an efficient baseline model ESDNet for ultra-highdefinition image demoir?ing. -Our ESDNet achieves state-of-the-art results on the UHDM dataset and three other public demoir?ing datasets, in terms of quantitative evaluation and qualitative comparisons. Moreover, ESDNet is lightweight and can process standard 4K (3840 ? 2160) resolution images at 60 fps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image demoir?ing: To remove moir? patterns caused by the frequency aliasing, Liu et al. <ref type="bibr">[20]</ref> propose a synthetic dataset by simulating the camera imaging process and develop a GAN-based <ref type="bibr" target="#b9">[10]</ref> framework. Further, a large-scale synthetic dataset <ref type="bibr">[40]</ref> is proposed and promotes many follow-up works <ref type="bibr">[47,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr">40</ref>]. However, it is difficult for models trained on synthetic data to handle real-world scenarios due to the sim-to-real gap. For real-world image demoir?ing, Sun et al. <ref type="bibr">[29]</ref> propose the first real-world moir? image dataset (i.e., TIP2018) and develop a multi-scale network (DMCNN). To distinguish different types of moir? patterns, He et al. <ref type="bibr" target="#b11">[12]</ref> manually annotate moir? images with category labels to train a moir? pattern classification model. Frequency domain methods <ref type="bibr">[22,</ref><ref type="bibr">47]</ref> are also studied for moir? removal. To deal with high-resolution images, He et al. <ref type="bibr" target="#b12">[13]</ref> construct a high-definition dataset FHDMi and develop the multi-stage framework FHDe 2 Net. Although significant progress has been achieved, the above methods either cannot achieve satisfactory results <ref type="bibr">[47,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr">29,</ref><ref type="bibr" target="#b7">8]</ref> or suffer from heavy computational cost [47, <ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8]</ref>. More importantly, the highest resolution of existing image demoir?ing datasets is FHDMi <ref type="bibr" target="#b12">[13]</ref> with 1080p resolution, which is not suitable for practical use considering the ultra-high-definition (4K) images captured by current mobile cameras. We focus on developing a lightweight model that can process ultra-high-definition images.</p><p>Image restoration: To this point, plenty of learning-based image restoration models have been proposed. For instance, residual learning <ref type="bibr" target="#b13">[14]</ref> and dense connection <ref type="bibr" target="#b14">[15]</ref> are widely used to develop very deep neural networks for different low-level vision tasks [43, <ref type="bibr" target="#b0">1,</ref><ref type="bibr">19,</ref><ref type="bibr">17,</ref><ref type="bibr">46]</ref>. In order to capture multi-scale information, encoder-decoder [25] structures or hierarchical architectures are frequently exploited in image restoration tasks <ref type="bibr">[42,</ref><ref type="bibr">41,</ref><ref type="bibr" target="#b8">9]</ref>. Inspired by iterative solvers, some methods utilize recurrent structures <ref type="bibr" target="#b8">[9,</ref><ref type="bibr">31]</ref> to gradually recover images while reducing the number of parameters. To preserve structural and semantic information, many works [36,21,28,37,30,34] adopt the perceptual loss <ref type="bibr" target="#b15">[16]</ref> or generative loss <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2]</ref> to guide the training procedure. In our work, we also take advantage of the well-designed dense blocks for efficient feature reuse and the perceptual loss for semantically guided optimization.</p><p>Multi-scale network: The multi-scale network has been widely adopted in various tasks <ref type="bibr">[33,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr">48,</ref><ref type="bibr">38,</ref><ref type="bibr" target="#b5">6]</ref> due to its ability to leverage features with different receptive fields. U-Net [25], as one representative multi-scale network, extracts multi-scale information using an encoder-decoder structure, and enhances features in decoder with skip-connections. To preserve the high-resolution representation, the full resolution residual network [24] extends the U-Net by introducing an extra stream containing information of the full resolution, and similar operations can be found in the HRNet <ref type="bibr">[33]</ref>. Considering that the extracted multi-scale features have different semantic meanings, the question of how to fuse features with different meanings is also important and has been widely studied in many works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>. In this work, we design a semantic-aligned scale-aware module to handle moir? patterns with diverse scales without incurring too great a computational cost, which renders our method highly practical for 4K images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">UHDM Dataset</head><p>We study ultra-high-definition image demoir?ing, which has more practical applications. For the training of 4K demoir?ing models and the evaluation of existing methods, we collect a large-scale ultra-high-definition demoir?ing dataset (UHDM). Dataset collection and benchmark study are elaborated upon below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection and Selection</head><p>To obtain the real-world 4K image pairs, we first collect high-quality images with resolutions ranging from 4K to 8K from the Internet. We note that Internet resources lack document scenes, which also constitute a vital application scenario (e.g., slides, papers), so we manually generate high-quality text images and make sure they maintain 3000 dpi (Dots Per Inch). Finally, the collected moir?-free images cover a wide range of scenes (see <ref type="figure" target="#fig_0">Fig. 1</ref>), such as landscapes, sports, video clips, and documents. Given these high-quality images, we generate diverse realworld moir? patterns elaborated upon below. First, to produce realistic moir? images and ease the difficulties of calibrations, we shoot the clean pictures displayed on the screen with a camera phone fixed on a DJI OM 5 smartphone gimbal, which allows us to conveniently and flexibly adjust the camera view through its control button, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Second, we note that the characteristics of moir? patterns highly are highly dependent upon the geometric relationship between the screen and the camera (see supplement for more details). Therefore, during the capturing process, we continuously adjust the viewpoint every ten shots to produce diverse moir? patterns. Third, we adopt multiple &lt; mobile phone, screen &gt; (i.e., three mobile phones and three digital screens, see supplement for more details) combinations to cover various device pairs, since they will also have an impact on the styles of moir? patterns. Finally, to obtain aligned pairs, we utilize RANSAC algorithm [32] to estimate the homography matrix between the original high-quality image and the captured moir? screen image. Since it is difficult to ensure accurate pixel-wise calibration due to the camera's internal nonlinear distortions and perturbations of moir? artifacts, manual selection is performed to rule out severely misaligned image pairs, thereby ensuring quality.</p><p>Our dataset contains 5, 000 image pairs in total. We randomly split them into 4, 500 for training and 500 for validation. As we collect moir? images using various mobile phones, the resolution can either be 4032 ? 3024 or 4624 ? 3472. Comparisons with other existing datasets are shown in <ref type="table" target="#tab_0">Table 1</ref>, and the characteristics of our dataset are summarized as below.</p><p>-Ultra-high resolution UHDM is the first 4K resolution demoir?ing dataset, consisting of 5,000 image pairs in total. -Diverse image scenes The dataset includes diverse scenes, such as landscapes, sports, video clips, and documents. -Real-world capture settings The moir? images are generated following practical routines, with different device combinations and viewpoints to produce diverse moir? patterns.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Benchmark Study on 4K Demoir?ing</head><p>As the image resolution is increased to the 4K resolution, the scale of moir? patterns has a very wide range, from very large moir? patterns to small ones (see <ref type="figure" target="#fig_0">Fig. 1</ref>). This poses a major challenge to demoir?ing methods as they are required to be scale-robust. Furthermore, increased image resolution also leads to dramatically increased computational cost and high requirements of detail restoration/preservation. Here, we carry out a benchmark study on the existing state-of-the-art methods [47,29,12,13,22,8] on our 4K demoir?ing dataset to evaluate their effectiveness. Main results are summarized in <ref type="figure" target="#fig_1">Fig. 2</ref> and <ref type="figure" target="#fig_2">Fig. 3</ref>: existing methods are mostly not capable of achieving a good balance of accuracy and computational efficiency. More detailed results are shown in Section 5. Analysis and discussions: Although existing methods also attempt to address the scale challenge by developing multi-scale strategies, they still have several deficiencies regarding computational efficiency and restoration quality when applied to 4K high-resolution images (see <ref type="figure" target="#fig_1">Fig. 2</ref>  It suffers, however, from heavy computational cost when applied to 4K images (see <ref type="figure" target="#fig_2">Fig. 3</ref>) yet is still not sufficient to remove moir? patterns (see <ref type="figure" target="#fig_4">Fig. 5</ref>) or recover fine image detail (see <ref type="figure" target="#fig_1">Fig. 2</ref> and <ref type="figure" target="#fig_4">Fig. 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Method</head><p>Motivated by observations in Section 3.2, we introduce a baseline approach to advance 4K resolution image demoir?ing, aimed towards a more scale-robust and efficient model. In the following, we first present an overview of our pipeline and then elaborate on our core semantic-aligned scale-aware module (SAM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pipeline</head><p>The overall architecture is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, where a pre-processing head is utilized to enlarge the receptive field, followed by an encoder-decoder architecture Encoder Decoder  for image demoir?ing. The pre-processing head adopts pixel shuffle [26] to downsample the image by two times and a 5 ? 5 convolution layer to further extract low-level features. Then, the extracted low-level features are fed into an encoder-decoder backbone architecture that consists of three downsampling and upsampling levels. Note that the encoder and decoder are connected via skipconnections to allow features containing high-resolution information to facilitate the restoration of corresponding moir?-free images. At each decoder level, the network would produce intermediate results through a convolution layer and a pixelshuffle upsampling operation (see the upper part of <ref type="figure" target="#fig_3">Fig. 4</ref>), which are also supervised by the ground-truth, serving the purpose of deep supervision to facilitate training. Specifically, each encoder or decoder level (see <ref type="figure" target="#fig_3">Fig. 4</ref>) contains a dilated residual dense block <ref type="bibr">[46,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">39]</ref> for refining the input features (as detailed below) and a proposed semantic-aligned multi-scale module (SAM) for extracting and dynamically fusing multi-scale features at the same semantic level (as elaborated in Section 4.2). Dilated residual dense block: For each level i ? {1, 2, 3, 4, 5, 6} (i.e., three encoder levels and three decoder levels), the input feature F i first goes through a convolutional block, i.e., dilated residual dense block, for refining input features. It incorporates the residual dense block (RDB) [46, <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref> and dilated convolution layers [39] to process the input features and output refined ones. Specifically, given an input feature F 0 i to the i-th level encoder or decoder, the cascaded local features from each layer inside the block can be formulated as Eq. (1):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corrupted Screen Image Restored Hierarchical Images Skip Connection</head><formula xml:id="formula_0">!"#$&amp; " ' " ( " ) F $ -th</formula><formula xml:id="formula_1">c S . SAM Overall F $ F "+, F "+, F $ F $ ?? $ ? $ F F F "+,</formula><formula xml:id="formula_2">F l i = C l ([F 0 i , F 1 i , ..., F l?1 i ]), (l = 1, 2, ..., L)<label>(1)</label></formula><p>where</p><formula xml:id="formula_3">[F 0 i , F 1 i , ..., F l?1 i</formula><p>] denotes the concatenation of all intermediate features inside the block before layer l, and C l is the operator to process the concatenated features, consisting of a 3 ? 3 Conv with dilated rate d l and a rectified linear unit (ReLU). After that, we apply a 1 ? 1 convolution to keep the output channel number the same as that of F 0 i . Finally, we exploit the residual connection to produce the refined feature representation F r i , formulated as Eq. <ref type="formula" target="#formula_4">(2)</ref>:</p><formula xml:id="formula_4">F r i = F 0 i + Conv 1?1 (F L i ).<label>(2)</label></formula><p>The refined feature representation F r i is then fed to our proposed SAM for semantic-aligned multi-scale feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic-Aligned Scale-Aware Module</head><p>Given the input feature F r i , the SAM is intended to extract multi-scale features within the same semantic level i and allow them to interact and be dynamically fused, significantly improving the model's ability to handle moir? patterns with a wide range of scales. As demonstrated in <ref type="table" target="#tab_12">Table 3</ref>, SAM enables us to develop a lightweight network while still being more effective in comparison with existing methods. In the following, we detail the design of SAM which encompasses two major modules: pyramid feature extraction and cross-scale dynamic fusion.</p><p>Pyramid context extraction: Given an input feature map F r ? R H?W ?C (we simplify F r i by F r in the following discussion), we first produce pyramid</p><formula xml:id="formula_5">input features F r ? R H?W ?C , F r ? ? R H 2 ? W 2 ?C and F r ?? ? R H 4 ? W 4</formula><p>?C through bilinear interpolation, then feed them into a corresponding convolutional branch with five convolution layers to yield pyramid outputs Y 0 , Y 1 , Y 2 (see the lower part of <ref type="figure" target="#fig_3">Fig. 4</ref>):</p><formula xml:id="formula_6">Y 0 = E 0 (F r ), Y 1 = E 1 F r ? , Y 2 = E 2 F r ?? ,<label>(3)</label></formula><p>where we build E 0 , E 1 , and E 2 via the dilated dense block, followed by a 1 ? 1 convolution layer. In addition, the up-sampling operations will be performed in E 1 , E 2 to align the size of three outputs, i.e., Y i ? R H?W ?C , (i = 0, 1, 2). Note that, as the internal architectures of E 0 , E 1 , and E 2 are identical, their corresponding learnable parameters can be shared to lower the cost of parameter number. In fact, as proven in Section 5, the improvement primarily comes from the pyramid architecture instead of additional parameters.</p><p>Cross-scale dynamic fusion: Given the pyramid features Y 0 , Y 1 , Y 2 , the crossscale dynamic fusion module fuses them together to produce fused multi-scale features for the next level to process. The insight for this module is that scale of moir? patterns vary from image to image and thus the importance of different scale features would also vary across images. Therefore, we develop the following cross-scale dynamic fusion module to make the fusion process dynamically adjusted and adapted to each image. Specifically, we learn dynamic weights to</p><formula xml:id="formula_7">fuse Y 1 , Y 2 , Y 3 .</formula><p>Given Y i ? R H?W ?C (i = 0, 1, 2), we first apply global average pooling in the spatial dimension of each feature map to obtain the 1D global feature v i ? R C for each scale i following Eq. <ref type="bibr" target="#b3">(4)</ref>.</p><formula xml:id="formula_8">v i = 1 H ? W H s=1 W t=1 Y i (s, t)<label>(4)</label></formula><p>Then, we concatenate them along the channel dimension and learn the dynamic weights through an MLP module as:</p><formula xml:id="formula_9">[w 0 , w 1 , w 2 ] = MLP([v 0 , v 1 , v 2 ])<label>(5)</label></formula><p>where "MLP" consists of three fully connected layers and outputs w 0 , w 1 ,</p><formula xml:id="formula_10">w 2 ? R C to fuse Y 1 , Y 2 , Y 3 dynamically.</formula><p>Finally, with fusion weights, we channel-wisely fuse the pyramid features with the input-adaptive weights, and then add the input feature F r to get the final output of SAM:</p><formula xml:id="formula_11">F out = F r + w 0 ? Y 0 + w 1 ? Y 1 + w 2 ? Y 2<label>(6)</label></formula><p>where ? denotes the channel-wise multiplication, and the output F out will go through the next level (i ? i + 1) for further feature extraction and image reconstruction.</p><p>Comparisons and analysis: Existing methods [47,22] utilize features from different depths to obtain multi-scale representations. However, features at different depths have different levels of semantic information. Thus, they are incapable of representing multi-scale information at the same semantic level, which might provide important cues for boosting the model's multi-scale modeling capabilities, as indicated in <ref type="bibr">[33]</ref>. We offer SAM as a supplement to existing methods as Y 0 , Y 1 , Y 2 include semantic-aligned information with different local receptive fields. The dynamic fusion methods further make the module adaptive to different images and boost its abilities. This strategy can also be treated as an implicit classifier compared with the explicit one in MopNet <ref type="bibr" target="#b11">[12]</ref>, which is more efficient and avoids the ambiguous hand-craft attribute definition. We include more detailed analysis in our supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Loss Function</head><p>To boost optimization, we adopt the deep supervision strategy, which has been proven useful in <ref type="bibr">[47]</ref>. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, in each decoder level, the network will produce hierarchical predictions? 1 ,? 2 ,? 3 , which are also supervised by ground-truth images. We note that moir? patterns disrupt image structures since they generate new strip-shaped structures. Therefore, we adopt the perceptual loss <ref type="bibr" target="#b15">[16]</ref> for feature-based supervision. At each level, we build our loss function by combining the pixel-wise L 1 loss and the feature-based perceptual loss L p . Hence, the final loss function is formulated as:</p><formula xml:id="formula_12">L total = 3 i=1 L 1 (I i ,? i ) + ? ? L p (I i ,? i )<label>(7)</label></formula><p>For the perceptual loss, we extract features from conv3 3 (after ReLU) using a pre-trained VGG16 [27] network and compute the L 1 distance in the feature space; we simply set ? = 1 during training. We find that this perceptual loss is effective in removing moir? patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Datasets  Quantitative comparison:  <ref type="table" target="#tab_12">Table 3</ref>: Ablation study of the proposed SAM. "A" represents the baseline model. "A + " denotes a stronger baseline which is of similar model capacity compared to our full model "E". "B" adds the pyramid context extraction with shared weights across all branches to "A" while "D" adopts adaptive weights. "C" and "E" add the cross-scale dynamic fusion based on "B" and "D", respectively </p><formula xml:id="formula_13">(a) Input (b) DMCNN (c) MDDM (d) WDNet (e) MopNet (f) MBCNN (g) FHDe 2 Net (h) ESDNet (Ours) (i) ESDNet-L (Ours) (j) GT (a) Input (b) DMCNN (c) MDDM (d) WDNet (e) MopNet (f) MBCNN (g) FHDe 2 Net (h) ESDNet (Ours) (i) ESDNet-L (Ours) (j) GT (a) Input (b) DMCNN (c) MDDM (d) WDNet (e) MopNet (f) MBCNN (g) FHDe 2 Net (h) ESDNet (Ours) (i) ESDNet-L (Ours) (j) GT</formula><formula xml:id="formula_14">Dataset Metrics A A + B C D</formula><formula xml:id="formula_15">(a) Input (b) Model A (c) Model D (d) Model E (e) GT</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 6: Qualitative effects of different components in SAM</head><p>Computational cost: As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, our method strikes a sweet point of balancing the parameter number, computation cost (MACs), and demoir?ing performance. Also, we test the inference speed of our method on an NVIDIA RTX 3090 GPU. Surprisingly, our ESDNet only needs 17ms (i.e., 60fps) to process a standard 4K resolution image, almost 300? faster than FHDe 2 Net. The competitive performance and low computational cost render our method highly practical in the 4K scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>In this section, we tease apart which components of our network contribute most to the final performance on the UHDM dataset. As shown in <ref type="table" target="#tab_12">Table 3</ref>, we start from the baseline model (model "A"), which ablates the pyramid context extraction and the cross-scale dynamic fusion strategies. To make a fair comparison, we further build a stronger baseline model (model "A + ") that is comparable to our full model (model "E") in terms of the model capacity. Pyramid context extraction: We construct two variants (model "B" and model "D") for exploring the effectiveness of this design. Compared with the baseline (model "A"), we observe that the proposed pyramid context extraction can significantly boost the model performance. To validate whether the improvement comes from more parameters in the additional two sub-branches, we exploit a weight-sharing strategy across all branches (model "B"). The observations in <ref type="table" target="#tab_12">Table 3</ref> demonstrate that the performance gain mainly stems from the pyramid design rather than the increase of parameters. Further, as shown in <ref type="figure" target="#fig_9">Fig. 6</ref>, we find our pyramid design can successfully remove the moir? patterns that are not well addressed in the baseline model. Cross-scale dynamic fusion: To verify the importance of the proposed dynamic fusion scheme, we increasingly add this design to model "B" and model "D", resulting in model "C" and model "E". We observe consistent improvements for both models, especially on PSNR. Also, <ref type="figure" target="#fig_9">Fig. 6</ref> shows that the artifacts retained in model "D" are totally removed in the result of model "E", achieving a more harmonious color style.</p><p>Loss function: Through our experiments, we find the perceptual loss plays an essential role in image demoir?ing. As shown in <ref type="table" target="#tab_7">Table 4</ref>, when replacing our loss function with a single L 1 loss, we notice obvious performance drops in our method, especially on LPIPS. Also, we make further exploration by applying our loss function to other state-of-the-art methods <ref type="bibr">[29,</ref><ref type="bibr" target="#b7">8]</ref>. The significant improvements on LPIPS illustrate the importance of the loss design in yielding a higher perceptual quality of recovered images. We suggest our loss is more robust to address the large-scale moir? patterns and the misaligned issue in the real-world datasets <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">29]</ref>. More discussions are included in the supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, to explore the more practical yet challenging 4K image demoir?ing scenario, we propose the first real-world ultra-high-definition demoir?ing dataset (UHDM). Based upon this dataset, we conduct a benchmark study and limitation analysis of current methods, which motivates us to build a lightweight semantic-aligned scale-aware module (SAM) to strengthen the model's multiscale ability without incurring much computational cost. By leveraging SAM in different depths of a simple encoder-decoder backbone network, we develop ESDNet to handle 4K high-resolution image demoir?ing effectively. Our method is computationally efficient and easy to implement, achieving state-of-the-art results on four benchmark demoir?ing datasets (including our UHDM). We hope our investigation could inspire future research in this more practical setting. Supplementary Material for "Towards Efficient and Scale-Robust Ultra-High-Definition Image Demoir?ing" Outline We supplement the main body of our paper with additional details, discussions, and results in this document. In Section A, we present more details about our dataset capture, which includes a brief analysis of the formation of degraded screen images. In Section B, we provide more implementation details of our network architecture as well as a simple empirical study of loss functions to assist us in selecting a suitable training objective for moir? removal. In Section C, we provide more implementation details of experiments and show more qualitative results and comparisons with other state-of-the-art methods. Furthermore, as shown in Section C.2, we investigate why FHDe 2 Net fails on this more challenging 4K dataset. We conduct a more detailed discussion of current methods' strategies for handling scale-variation of moir? patterns in Section D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Capture and Analysis</head><p>In this section, we first present a brief introduction of the formation of the moir? pattern, and then we provide more details about our capture settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Image Degradation Analysis</head><p>The formation of degraded screen images taken with mobile devices can be divided into two processes: the generation of moir? patterns caused by frequency aliasing; and the global color degradation of the image, caused by a series of ISP operations (e.g., auto exposure control, white balance correction, gamma correction, and global tone mapping).</p><p>We can model the formation of moir? patterns as a local color unbalanced scaling in the camera's color filter array (CFA). Without loss of generality, consider how one of the green channels in the RGBG raw pattern is collected. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, due to a slight misalignment between sensor pixels and LED screen pixels, the energy may shift from one pixel to its neighbors. This flow eventually aligns again after passing a few pixels. Hence, the value of each pixel in this period could be modeled as being multiplied by different scaling factors:</p><formula xml:id="formula_16">R(i, j) = R(i, j) * S(i, j),<label>(1)</label></formula><p>whereR(i, j) = r ij ,? </p><formula xml:id="formula_17">s r ij , s g1 ij , s b ij , s g2 ij</formula><p>is the scaling factor for four channels (RGBG) caused by frequency aliasing; * denotes the point-wise multiplication. However, since the LED display and camera Bayer array both emit or receive each channel information in an alternate form, the scaling rules for different channels are not consistent within a cycle. Hence, the camera stores a wrong color distribution, causing the moir? pattern we see.</p><p>Furthermore, there is an unavoidable gap when we re-capture an image on the screen. For instance, ambient light can lead to incorrect exposure control, wrong auto white balance, and unnatural tone mapping. Also, corrupted raw data can affect the process of raw image demosaicing. All of these factors contribute to the overall degradation of the color, which can be formulated as:</p><formula xml:id="formula_18">M = F (R * S) ,<label>(2)</label></formula><p>where M is the final degraded screen image and F is a nonlinear function that globally affects the image quality. Given the above analysis, we could explain the following characteristics (see <ref type="figure" target="#fig_0">Fig. 1</ref>) of moir? patterns: Structural distortions: Since the RGB color distributions change in an alternate form, the local illuminance contrasts among the three channels are not consistent. Thus, new structures are created and mixed with original contents. Diverse degraded forms: In <ref type="figure" target="#fig_0">Fig. 1</ref>, we show the simplest case of misalignment between two patterns, in which the camera plane and the screen plane are parallel to each other. Obviously, the scaling rule would be quite different if the angle and distance between these two planes were to change, resulting in moir? patterns in different shapes and scales. This explains why the moir? pattern characteristics highly depend on the geometric relationship between the screen and the camera. Large-scale patterns in low-frequency regions: Unlike the natural image captured from real scenes, we capture discrete signals emitted from the LED screen and store them in new discrete forms. Thus, the low-frequency image areas actually become signals with the highest frequency and are more likely to continuously alias with the camera sensor over a long period, resulting in larger moir? patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 More Details about Capture Settings</head><p>Based on the above analysis, we thus shoot the screen images via different camera views to produce different patterns and combine multiple devices to produce diverse degradation styles (including pattern appearance and global color style). Specifically, we apply three mobile phones and three digital screens, as shown in <ref type="table" target="#tab_0">Table 1</ref> (3 ? 3 = 9 combinations here totally). Notably, the "4K" challenge means the obtained moir? image is at a resolution of ultra-high-definition (i.e., the shooting resolution is 4K). We also compare our dataset with other datasets visually. As seen in <ref type="figure" target="#fig_1">Fig. 2</ref>, we crop patches from these four datasets at the same resolution 256?256 (the image in TIP2018 dataset <ref type="bibr">[29]</ref> is already at a resolution of 256?256). Obviously, compared with other datasets, the image UHDM suffers from more severe moir? artifacts and has less clean image content to harvest in a local window. As a result, it is more challenging for the network to identify the moir? pattern or fill clean content into the degraded region, which has also been demonstrated in <ref type="bibr" target="#b12">[13]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Method</head><p>In this section, we give details of our network architecture. The overview of our network is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. We use skip-connections to connect each level of the encoder and decoder, wherein the features are concatenated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Semantic-Aligned Scale-Aware Module (SAM)</head><p>As seen in <ref type="figure" target="#fig_2">Fig. 3</ref>, there are three branches in the pyramid context extraction module wherein the dilated dense block (L = 5) is utilized as the backbone block to extract the context information. Two bilinear upsampling layers with upsampling ratios 2 and 4 are applied to the second and third branches to align the spatial resolution of the first branch. There are three fully connected layers for the MLP in the cross-scale dynamic fusion module to learn the adaptive weights. We adopt ReLU for the first two layers and Sigmoid for the last layer as our nonlinear activation functions. Specifically, for an input tensor v ? R 1?1?3C , the channel number is squeezed by a dividing factor 4 in the first layer and then expanded to the original number in the last layer.</p><p>Weight-sharing SAM: We apply a weight-sharing strategy for one of our models, denoted as WS-ESDNet, which shares the learnable parameters among   the three branches. The WS-ESDNet has fewer parameters while keeping comparable quantitative and qualitative results compared to our standard model ESDNet. The quantitative results have already been shown in Section.5.2 in the main body of our paper, and qualitative results are illustrated in Section C. This demonstrates that the performance gain primarily benefits from our architecture design rather than increased model parameters.  </p><formula xml:id="formula_19">F $ F "+, F "+, F $ F $ ?? $ ? $ F F F "+,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Empirical Study of Loss Functions</head><p>The loss function plays an essential role in guiding model updates and encouraging the model to learn natural patterns from data. To this end, we carry out an empirical study to investigate the impacts of different loss functions on image demoir?ing.</p><p>We evaluate traditional L 1 loss and its combination with perceptual losses <ref type="bibr" target="#b15">[16]</ref> where the features are respectively from the end of block 1, block 2, block 3, block 4 and block 5 of a pre-trained VGG-16 network <ref type="bibr">[27]</ref>. We develop a simple task to study the effectiveness of these loss functions on removing undesirable moir? patterns. Specifically, we choose a degraded screen image M with severe structural distortions and its corresponding clean ground-truth I; our aim is to restore M by optimizing ? ? = arg min ? D(I, f ? (M )) through our designed network f ? , where D denotes the loss function, and? = f ? ? (M ) is the recovered image. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the single L 1 loss or its combination with the shallow block 1 perceptual loss cannot guide the network to remove unnecessary structures; they are effective in restoring the pixel-level color due to their low-level nature. Meanwhile, the loss functions derived from block 4 and block 5 features, containing too deep semantic-level information, will lead the predicted image to lose its textures. In contrast, perceptual loss with features from block 2 and block 3 can encourage the network to remove undesirable structures while preserving the original texture, a good signal for image demoir?ing. In particular, the model trained with block 3 recovers more details with satisfying local contrasts. Hence, the block 3 might be the most suitable layer to construct the training objective.  <ref type="figure" target="#fig_3">Fig. 4</ref>: The optimal results by fitting different loss functions for a single moir? image Although many previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr">22]</ref> have already adopted the perceptual loss as a regularization term, they often overlook the importance of precisely choosing a suitable layer for this specific task, which is crucial, as different features will encourage the network to optimize the network in different directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Implementation Details</head><p>We implement all the experiments using PyTorch on an NVIDIA RTX 3090 GPU card. The learning rate is initially set to 0.0002 and scheduled by cyclic cosine annealing [23], and models are optimized by Adam [18] with ? 1 = 0.9 and ? 2 = 0.999. For UHDM dataset, we set the batch size as 2. Notably, we conduct benchmark implementations of other methods <ref type="bibr">[29,</ref><ref type="bibr">47,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr">22</ref>] on our dataset sufficiently. <ref type="figure" target="#fig_1">For DMCNN [29]</ref>, MDDM <ref type="bibr" target="#b7">[8]</ref>, WDNet [22] and MBCNN [47], we randomly crop a 768 ? 768 patch from the ultra-high-definition images, and train the model for 150 epochs, i.e., the totally same setting with ours. For FHDe 2 Net <ref type="bibr" target="#b12">[13]</ref>, due to its different multi-stage nature and high computational cost, we can only follow its default setting in the official released code for training (i.e., down-sampled-resolution 384 ? 384 for training its global stage and cropped 384 ? 384 region for training the following three cascaded networks). For MopNet <ref type="bibr" target="#b11">[12]</ref>, we freeze its pre-trained classification sub-network and train its edge-prediction sub-network and demoir?ing sub-network for 150 epochs, wherein we also crop a 384 ? 384 region for training. During inference, since MopNet cannot directly process the 4K image due to its heavy memory cost, we downsample the input image into 1080p (the highest resolution it can process on a single GPU) resolution and then upsample the result back to 4K resolution.</p><p>Other datasets: For FHDMi <ref type="bibr" target="#b12">[13]</ref> and LCDmoir? [40] dataset, we randomly crop a 512 ? 512 patch from the high-definition images, and train the model for 150 epochs with the batch size as 2. For TIP2018 dataset [29], we follow the benchmark setting, i.e., we first resize the image into a 286 ? 286 resolution and then do center crop to produce a 256 ? 256 resolution image for both training and testing. We train our models for 70 epochs and set batch size to 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Discussion about FHDe 2 Net</head><p>We find that in the new dataset UHDM, FHDe 2 Net suffers from a more significant performance drop than other methods. To this end, we conduct a parameters searching and analysis. Specifically, since we find the key challenge is to fuse the high-frequency detail, we mainly analyze the training of the last stage, i.e., the FDN and FRN (please refer to <ref type="bibr" target="#b12">[13]</ref> for more details). Since the learning rate is scheduled by cyclic cosine annealing, which warms up every 50 epochs, we evaluate the performances after the FDN and FRN (the last stage of FHDe 2 Net) have been trained for 50, 100, and 150 epochs, respectively. As shown in <ref type="table" target="#tab_7">Table 4</ref>, with the increase of training time, SSIM improves significantly, but LPIPS degrades simultaneously. For this phenomenon, we attribute the reasons to two aspects, as elaborated upon below.</p><p>On the one hand, current low-level metrics have several limitations and cannot fully measure the demoir?ing performance (see <ref type="figure" target="#fig_4">Fig. 5</ref>). For example, PSNR is a pixel-wise metric sensitive to pixel misalignment and slight color shift, which has limited effect in measuring the structural distortion caused by the moir? pattern. SSIM is more robust to evaluate structural distortion yet still sensitive to the unstructured distortion (e.g., pixel shift, rotation.), which is unavoidable in real-world data pairs. LPIPS has been proven to be more consistent with human perception; however, it is sensitive to blur as demonstrated in <ref type="bibr">[44]</ref>.  On the other hand, this indicates FHDe 2 Net has reached its limit in making the trade-off between large-scale moir? removal and high-frequency details preservation. To explore whether this stage plays a role in high-frequency detail recovery, we compare it with the initial low-resolution result produced by the global demoir?ing stage. As shown in <ref type="table" target="#tab_7">Table 4</ref>, compared with the initial result (i.e., "Low-resolution"), the fine-tuned model (i.e., "150 epoch") achieves a significant improvement in LPIPS which indicates the detail has been recovered to some degree (but not been fully recovered, see <ref type="figure" target="#fig_9">Fig. 6</ref>). However, the PSNR is almost unchanged, indicating that this stage may not work well for color recovery. One possible reason is that the fusion stage only utilizes the Y-channel's information of the original high-resolution image but lacks UV-channels' high-resolution information. Besides, to avoid the effect of pixel misalignment, FHDe 2 Net does not adopt pixel-wise loss terms (e.g., L 1 , L 2 ), which may prevent it from recovering the global color style. Under this circumstance, the accurate color information loses significantly, negatively affecting all three metrics, especially for the PSNR. In fact, we have conducted several parameters searching for the last stage's training (consists of two sub-networks FDN and FRN), trying to improve the performance of FHDe 2 Net. To be precise, we adjust the loss weights to guide the networks' optimization. As illustrated in Eq.(3), the overall loss function of the last stage consists of two parts: L FDN and L FRN , where L FDN aims to reconstruct the high-resolution gray-scale image (i.e., the Y-channel of YUV color space) and L FRN aims to further fuse the color information (more details can be referred to <ref type="bibr" target="#b12">[13]</ref>):</p><formula xml:id="formula_20">L last (I,?) = L FDN (I Y ,? Y ) + ? ? L FRN (I,?)<label>(3)</label></formula><p>where I is the ground-truth and? is the network's output, I Y and? Y denote their Y-channel components, respectively. Moreover, for L FRN , it is essentially a CoBi [45] loss, which aims to measure the similarity between unaligned image pairs, consisting of a term D to measure feature similarity and a term D ? to compute the spatial distance between these two pixels (with a weight w s ), i.e.,:</p><formula xml:id="formula_21">L FRN (?, I) = 1 N N i min j=1,...,M ((1 ? w s )D (p i , q j ) + w s D ? (p i , q j ))<label>(4)</label></formula><p>where p i , q j stand for the feature vectors from the output image? and clean image I at the spatial position indexed by i and j, respectively. N , M denote the amounts of features (i.e., the amounts in searching space). We try several (?, w s ) combinations to train the model. For fast exploration, we train every model for 50 epochs and compare their results, as shown in <ref type="table" target="#tab_15">Table 5</ref>. However, since the metrics' changes of each model are not significant, we use the default parameter settings to report the results in our main paper.</p><p>In summary, although FHDe 2 Net achieves the best (except for ours) result on the FHDMi dataset <ref type="bibr" target="#b12">[13]</ref>, this framework is not robust under the higher resolution setting. Moreover, its complex module designs further render it hard to be applied to the 4K scenario due to unacceptable increased computational costs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 SAM for Other Methods</head><p>We demonstrate that equipping with the proposed SAM can also help other methods to achieve performance gain. Here we conduct experiments on MDDM <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr">DMCNN [29]</ref> and MBCNN [47], where we stack SAM in these networks. As shown in <ref type="table" target="#tab_16">Table 6</ref>, all metrics have improvements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 More Qualitative Comparisons</head><p>As seen in <ref type="figure" target="#fig_0">Fig. 8-15</ref>, we provide more visual results and comparisons with current state-of-the-art methods on three real-world demoir?ing datasets: UHDM (resolution: 3840?2160), FHDMi <ref type="bibr" target="#b12">[13]</ref> (resolution: 1920?1080) and TIP2018 [29] (resolution: 256 ? 256). Apparently, our model can remove moir? patterns more cleanly and preserve high-frequency details better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Revisit Current Multi-Scale Schemes in Image Demoir?ing</head><p>We have discussed in our main paper that a key challenge in image demoir?ing is the scale variation of the moir? pattern. In this section, we conduct a more detailed analysis of multi-scale schemes in current demoir?ing works. As shown in <ref type="figure">Fig. 7</ref>, we summarize these schemes into two parts: single-stage training and multi-stage training. We figure out their inefficiency and insufficiency, which limit their performance when processing ultra-high-definition images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Single-Stage Training</head><p>Most of the demoir?ing works adopt a single-stage framework, i.e., given a moir? image I moir? ? R h?w?3 , an end-to-end network F is trained to produce the final demoir?d image I demoir? :</p><formula xml:id="formula_22">I demoir? = F(I moir? )<label>(5)</label></formula><p>Specifically, they embed different multi-scale schemes into their networks, which can be simplified and summarized into two topological architectures: parallel multi-scale and cascaded multi-scale. Cascaded multi-scale: Adopted by MopNet <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr">MBCNN[47]</ref> and WDNet <ref type="bibr">[22]</ref> (Note that although MopNet is a multi-stage framework, it harvests multi-scale information in one sub-network), the insight in cascaded multi-scale strategy is utilizing features from different-depth layers to get multi-scale representations. As shown in the right upper part of <ref type="figure">Fig. 7</ref>, the moir? image first goes through an encoder that contains three levels to extract features. Then the intermediate results in each level are fused together and fed to the decoder for reconstruction. Since features are produced in different-depth layers, their receptive fields are different (the receptive field is larger for a deeper feature). However, another fact is ignored: features at different depths have different semantic meanings. For example, features extracted in the early layer usually contain low-level information such as edge, while features in the deeper layers contain more abstract attributes learned by the network. Recall that the scale-variation challenge means that the observed object remains the same for all attributes (e.g., color, shape) except for the scale that appeared in an image (i.e., pixels it counts). Thus, a more reasonable design is the network can extract multi-scale information at the same semantic level (i.e., depth level). Further, a robust network should harvest multiscale information at each semantic level to handle different attributes. Based on this analysis, we find that this cascaded strategy lacks multi-scale ability at a specific semantic level, limiting its scale-robust ability.</p><p>Parallel multi-scale: The parallel multi-scale indicates construction of parallel high-resolution to low-resolution branches to process different-scale features, as adopted in DMCNN[29] and MDDM <ref type="bibr" target="#b7">[8]</ref>. At each scale, several convolutional blocks are stacked to extract features and finally produce a three-channel output. Without loss of generality, we suppose there are three scales and three convolutional blocks in each scale to illustrate and analyze this strategy.</p><p>As shown in the left upper part of <ref type="figure">Fig. 7</ref>, the moir? image first goes through several downsampling convolutional heads with different strides to obtain shallow representations with different resolutions:</p><formula xml:id="formula_23">J i = Conv i (I moir? ), i = 1, 2, 3<label>(6)</label></formula><p>where Conv i denotes convolutional block with stride s = 2 i?1 , J i ? R h 2 i?1 ? w 2 i?1 ?c . After that, each J i is fed to several convolutional blocks in parallel:</p><formula xml:id="formula_24">X i = F 3 i (F 2 i (F 1 i (J i ))), i = 1, 2, 3<label>(7)</label></formula><p>where F j i denotes the j-th blocks in i-th scale (branch),</p><formula xml:id="formula_25">X i ? R h 2 i?1 ? w 2 i?1 ?3</formula><p>. Then an upsampling layer would be utilized to align the spatial size of eachscale outputs, followed by a summation operation to get the final prediction I demoir? :</p><formula xml:id="formula_26">I demoir? = X 1 + X 2? + X 3??<label>(8)</label></formula><p>Unlike the cascaded multi-scale scheme, the insight here is to reduce the resolution at the input stage, so different branches have different receptive fields. However, the problem is, this framework only fuses the results at the end of each branch, ignoring the interaction of the intermediate features. As a result, each extracted feature is only determined by its current branch (scale), dramatically limiting the network's representation ability. For example, to produce the feature F 2 2 , the network only utilizes the information from F 1 2 . However, a more representative feature needs to harvest multi-scale information from last semantic level. Only fusing information in the last layer results in coarse moir? pattern removal, as shown in <ref type="figure" target="#fig_0">Fig. 8-15</ref>. is the only current work which proposes to tackle real-world high-definition moir? images. Due to the increased resolution, the scale of the moir? pattern would expand extremely larger, which has been the main challenge in the high-definition demoir?ing. The central insight in this work is adopting a multi-stage framework to handle this problem, the networks of which are trained step by step. As shown in the lower part of <ref type="figure">Fig. 7</ref>, the overall framework can be divided into two stages: the global stage and the local refine stage (In fact, it consists of four sub-networks, but we summarize it into two stages here for analysis). The input of the global stage is a downsampled low-resolution (384 ? 384) moir? image, so the network in this stage can obtain a full-image-size receptive field. Although the large-scale moir? pattern can be removed, the images' highfrequency details are severely lost due to the downsampling operation. Hence, in the local refinement stage, the original high-resolution image would be utilized to guide the low-resolution demoir?d image to recover the details. However, our experiments find it hard for the network to differentiate the moir? pattern from the image textures, leading to the reintroduction of the moir? pattern and unsatisfactory texture recovery. Furthermore, its internal complex module design shows a heavy computational burden, which is unacceptable for ultra-high-definition image demoir?ing.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Upper left: Our dataset contains diversified scenarios. Upper right: we capture the moir? image with a DJI OM 5 smartphone gimbal. Lower: moir? images in our dataset show a wide range of scale variations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Limitations of current methods: they are often unable to remove the moir? pattern with a wider scale range or lose high-frequency details</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Comparisons of computational cost of different methods: The x-axis and the y-axis denote the MultiAdds (T) and PSNR (dB). The number of parameters is expressed by the area of the circle the intermediate features from interacting with and refining each other, leading to sub-optimal results, i.e., significantly sacrificing accuracy on 4K image demoir?ing despite being lightweight (seeFig. 3andFig. 2). Another line of methods, such as MBCNN [47], exploits multi-scale features at different network depths following a U-Net-like architecture. Compared with other existing methods, although it achieves the best trade-off between accuracy and efficiency, it still suffers from moir? patterns with a wide-scale range (the second row ofFig. 2 and Fig. 5). One possible issue is that the combined multi-scale features come from different semantic levels [33], prohibiting a specific feature level to harvest multi-resolution representations[33], which could also be an important cue for image demoir?ing. On the other hand, FHDe 2 Net [13] designs a coarseto-fine two-stage model to simultaneously address the scale and detail challenge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>The pipeline of our ESDNet and the proposed semantic-aligned scaleaware module (SAM)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative comparisons with state-of-the-art methods on the UHDM dataset. Please zoom in for a better view. More results are given in the supplementary file5.1 Comparisons with State-of-the-Art MethodsWe provide two versions of our model: ESDNet and ESDNet-L. ESDNet is the default lightweight model and ESDNet-L is a larger model, stacking one more SAM in each network level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 ij ,b ij ,? 2 ijFig. 1 :</head><label>21</label><figDesc>represents the degraded pixel at the location (i, j) in the Bayer pattern and R(i, j) denotes the clean pixel. S(i, j) Left: the formation of the moir? pattern. Notice that there are small gaps between the light-emitting diodes. Right: the characteristics of moir? patterns</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 :</head><label>3</label><figDesc>The pipeline of our ESDNet and the proposed semantic-aligned scaleaware module (SAM)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 :</head><label>5</label><figDesc>(a) Input (PSNR/SSIM/LPIPS) (b) MBCNN (23.423/0.7643/0.3199) (c) FHDe 2 Net (22.347/0.7122/0.3206) (d) GT (?/?/0) Current metrics have some limitations. In this case, FHDe 2 Net removes the moir? pattern more cleanly yet is still behind the MBCNN if evaluated by the three metrics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 :</head><label>6</label><figDesc>(a) Input (PSNR/SSIM/LPIPS) (b) Global Demoir ? e (18.448/0.7391/0.4650) (c) Detail Refine (18.265/0.7737/0.2716) (d) GT (?/?/0) Comparisons between the result produced by global demoir?ing stage and the final result (i.e., "Detail Refine"), in which the PSNR is almost unchanged while LPIPS achieves significant improvement</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>"A" denotes the default model where (?, w s ) = (1, 0.5); "B" denotes (?, w s ) = (0.5, 0.5); "C" denotes (?, w s ) = (2, 0.5); "D" denotes (?, w s ) = (1, 0.7); "E" denotes (?, w s ) = (1, 0.2) Metrics Input Pre-train Model A Model B Model C Model D Model E PSNR? 17.117 18.052 20.312 20.282 20.174 20.251 19.050 SSIM? 0.5089 0.5986 0.7290 0.7392 0.7350 0.7435 0.7240 LPIPS? 0.5314 0.4929 0.3397 0.3409 0.3359 0.3497 0.3566</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 7 :FHDe 2</head><label>72</label><figDesc>A summary of current works for solving the multi-scale challenge in image demoir?ing D.2 Multi-Stage Training Net<ref type="bibr" target="#b12">[13]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 8 :Fig. 9 :Fig. 10 :Fig. 11 :Fig. 12 :Fig. 13 :Fig. 14 :</head><label>891011121314</label><figDesc>Qualitative comparisons of our models with other state-of-the-art methods on the UHDM dataset, ESDNet is our standard model and ESDNet-L is our larger model Qualitative comparisons of our models with other state-of-the-art methods on the UHDM dataset, ESDNet is our standard model and ESDNet-L is our larger model Qualitative comparisons of our models with other state-of-the-art methods on the UHDM dataset, ESDNet is our standard model and ESDNet-L is our larger model Qualitative comparisons of our models with other state-of-the-art methods on the UHDM dataset, ESDNet is our standard model and ESDNet-L is our larger model Qualitative comparisons of our models with three representative stateof-the-art methods on the FHDMi dataset [13], including DMCNN [29], MBCNN [47] and FHDe 2 Net [13]. ESDNet is our standard model and ESDNet-L is our larger model. WS-ESDNet is our more lightweight model, the parameters of which are shared in three branches of pyramid context extraction module Qualitative comparisons of our models with three representative stateof-the-art methods on the FHDMi dataset [13], including DMCNN [13], MBCNN [47] and FHDe 2 Net [13]. ESDNet is our standard model and ESDNet-L is our larger model. WS-ESDNet is our more lightweight model, the parameters of which are shared in three branches of pyramid context extraction module Qualitative comparisons of our models with three representative stateof-the-art methods on the FHDMi dataset [13], including DMCNN [29], MBCNN [47] and FHDe 2 Net [13]. ESDNet is our standard model and ESDNet-L is our larger model. WS-ESDNet is our more lightweight model, the parameters of which are shared in three branches of pyramid context extraction module</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 :</head><label>15</label><figDesc>Qualitative comparisons of our models with two representative stateof-the-art methods on the TIP2018 dataset [29], including MopNet [12] and MBCNN [47]. ESDNet is our standard model and ESDNet-L is our larger model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of different demoir?ing datasets; our dataset is the first ultra-high-definition dataset ("London's Buildings" is not available currently)</figDesc><table><row><cell>Dataset</cell><cell>Avg. Resolution</cell><cell>Size</cell><cell>Diversity</cell><cell>Real-world</cell></row><row><cell>TIP18 [29]</cell><cell>256 ? 256</cell><cell>135,000</cell><cell>No text scenes</cell><cell>?</cell></row><row><cell>LCDMoir? [40]</cell><cell>1024 ? 1024</cell><cell>10,200</cell><cell>Only text scenes</cell><cell>?</cell></row><row><cell>FHDMi [13]</cell><cell>1920 ? 1080</cell><cell>12,000</cell><cell>Diverse scenes</cell><cell>?</cell></row><row><cell>London's Buildings [22]</cell><cell>2100 ? 1700</cell><cell>460</cell><cell>Only urban scenes</cell><cell>?</cell></row><row><cell>UHDM</cell><cell>4328 ? 3248</cell><cell>5,000</cell><cell>Diverse scenes</cell><cell>?</cell></row><row><cell>Input moir ? e image</cell><cell>Zoom-in region</cell><cell>DMCNN</cell><cell>Ours</cell><cell>Ground-truth</cell></row><row><cell>Input moir ? e image</cell><cell>Zoom-in region</cell><cell>MBCNN</cell><cell>Ours</cell><cell>Ground-truth</cell></row><row><cell>Input moir ? e image</cell><cell>Zoom-in region</cell><cell>FHDe 2 Net</cell><cell>Ours</cell><cell>Ground-truth</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparisons between our model and state-of-the-art methods on four datasets. (?) denotes the larger the better, and (?) denotes the smaller the better. Red: best and Blue: second-best</figDesc><table><row><cell>Dataset</cell><cell>Metrics</cell><cell cols="9">Input DMCNN[29] MDDM[8] WDNet[22] MopNet[12] MBCNN[47] FHDe 2 Net[13] ESDNet ESDNet-L</cell></row><row><cell></cell><cell cols="2">PSNR? 17.117</cell><cell>19.914</cell><cell>20.088</cell><cell>20.364</cell><cell>19.489</cell><cell>21.414</cell><cell>20.338</cell><cell>22.119</cell><cell>22.422</cell></row><row><cell>UHDM</cell><cell>SSIM?</cell><cell>0.5089</cell><cell>0.7575</cell><cell>0.7441</cell><cell>0.6497</cell><cell>0.7572</cell><cell>0.7932</cell><cell>0.7496</cell><cell>0.7956</cell><cell>0.7985</cell></row><row><cell></cell><cell cols="2">LPIPS? 0.5314</cell><cell>0.3764</cell><cell>0.3409</cell><cell>0.4882</cell><cell>0.3857</cell><cell>0.3318</cell><cell>0.3519</cell><cell>0.2551</cell><cell>0.2454</cell></row><row><cell></cell><cell cols="2">PSNR? 17.974</cell><cell>21.538</cell><cell>20.831</cell><cell>-</cell><cell>22.756</cell><cell>22.309</cell><cell>22.930</cell><cell>24.500</cell><cell>24.882</cell></row><row><cell>FHDMi</cell><cell>SSIM?</cell><cell>0.7033</cell><cell>0.7727</cell><cell>0.7343</cell><cell>-</cell><cell>0.7958</cell><cell>0.8095</cell><cell>0.7885</cell><cell>0.8351</cell><cell>0.8440</cell></row><row><cell></cell><cell cols="2">LPIPS? 0.2837</cell><cell>0.2477</cell><cell>0.2515</cell><cell>-</cell><cell>0.1794</cell><cell>0.1980</cell><cell>0.1688</cell><cell>0.1354</cell><cell>0.1301</cell></row><row><cell>TIP2018</cell><cell>PSNR? SSIM?</cell><cell>20.30 0.738</cell><cell>26.77 0.871</cell><cell>--</cell><cell>28.08 0.904</cell><cell>27.75 0.895</cell><cell>30.03 0.893</cell><cell>27.78 0.896</cell><cell>29.81 0.916</cell><cell>30.11 0.920</cell></row><row><cell>LCDMoir?</cell><cell>PSNR? SSIM?</cell><cell>10.44 0.5717</cell><cell>35.48 0.9785</cell><cell>42.49 0.9940</cell><cell>29.66 0.9670</cell><cell>--</cell><cell>44.04 0.9948</cell><cell>41.40 -</cell><cell>44.83 0.9963</cell><cell>45.34 0.9966</cell></row><row><cell>-</cell><cell>Params (M)</cell><cell>-</cell><cell>1.426</cell><cell>7.637</cell><cell>3.360</cell><cell>58.565</cell><cell>14.192</cell><cell>13.571</cell><cell>5.934</cell><cell>10.623</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>and metrics: We conduct experiments on the proposed UHDM dataset and three other public datasets: FHDMi<ref type="bibr" target="#b12">[13]</ref>, TIP2018 [29] and LCD-Moir?[40]. In our UHDM dataset, we keep the original two resolutions (see Section 3) and models are trained with cropped patches. During the evaluation phase, we do center crop from the original images to obtain test pairs with a resolution of 3840 ? 2160 (standard 4K size). We adopt the widely used PSNR, SSIM [35] and LPIPS [44] metrics for quantitative evaluation. It has been proven that LPIPS is more consistent with human perception and suitable for measuring demoir?ing quality<ref type="bibr" target="#b12">[13]</ref>. Note that existing methods only report PSNR and SSIM on the TIP2018 and LCDMoir?, so we follow this setup for comparisons. Implementation details: We implement our algorithm using PyTorch on an NVIDIA RTX 3090 GPU card. During training, we randomly crop a 768 ? 768 patch from the ultra-high-definition images, and set the batch size to 2. The model is trained for 150 epochs and optimized by Adam [18] with ? 1 = 0.9 and ? 2 = 0.999. The learning rate is initially set to 0.0002 and scheduled by cyclic cosine annealing[23]. Details for implementations on other benchmarks are unfolded in the supplementary file. We also train other methods on our dataset faithfully and sufficiently and unfold details in the supplementary file.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc>shows quantitative performance of existing approaches. The proposed method achieves state-of-the-art results on all four datasets. Specifically, both of our two models outperform other methods by a large margin in the ultra-high-definition UHDM dataset and high-definition FHDMi dataset, demonstrating the effectiveness of our method in high-resolution scenarios. It is worthwhile to note that our ESDNet, though possessing far fewer parameters, already shows competitive performance. Qualitative comparison: We present visual comparisons between our algorithm and existing methods inFig. 5. Apparently, our method obtains more perceptually satisfactory results. In comparison, MDDM<ref type="bibr" target="#b7">[8]</ref>, DMCNN [29] and WDNet [22] often fail to remove moir? patterns, while MBCNN [47] and Mop-Net [12] cannot handle large-scale patterns well. Though performing better than other methods (except for ours), FHDe 2 Net<ref type="bibr" target="#b12">[13]</ref> usually suffers from severe loss of details. All these facts manifest the superiority of our method.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of the loss function. The left and the right of "/" denote results trained by the pixel-wise L 1 loss and trained by our loss, respectively</figDesc><table><row><cell cols="2">Dataset Metrics</cell><cell>DMCNN</cell><cell>MDDM</cell><cell>Ours</cell></row><row><cell></cell><cell cols="4">PSNR? 19.914/19.911 20.088/20.333 21.489/22.119</cell></row><row><cell>UHDM</cell><cell cols="4">SSIM? 0.7575/0.7212 0.7441/0.7412 0.7893/0.7956</cell></row><row><cell></cell><cell cols="4">LPIPS? 0.3764/0.3089 0.3409/0.2986 0.3330/0.2551</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>17. Kim, J., Lee, J.K., Lee, K.M.: Accurate image super-resolution using very deep convolutional networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1646-1654 (2016) 3 18. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014) 11, 24 19. Lim, B., Son, S., Kim, H., Nah, S., Mu Lee, K.: Enhanced deep residual networks for single image super-resolution. In: Proceedings of the IEEE conference on computer vision and pattern recognition workshops. pp. 136-144 (2017) 3 20. Liu, B., Shu, X., Wu, X.: Demoir\'eing of camera-captured screen images using deep convolutional neural network. arXiv preprint arXiv:1804.03809 (2018) 1, 3 21. Liu, G., Reda, F.A., Shih, K.J., Wang, T.C., Tao, A., Catanzaro, B.: Image inpainting for irregular holes using partial convolutions. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 85-100 (2018) 4 22. Liu, L., Liu, J., Yuan, S., Slabaugh, G., Leonardis, A., Zhou, W., Tian, Q.Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014) 11, 23 28. Song, Y., Yang, C., Lin, Z., Liu, X., Huang, Q., Li, H., Kuo, C.C.J.</figDesc><table><row><cell>34. Wang, Y., Chen, Y.C., Tao, X., Jia, J.: Vcnet: A robust approach to blind image</cell></row><row><cell>inpainting. In: Computer Vision-ECCV 2020: 16th European Conference, Glasgow,</cell></row><row><cell>UK, August 23-28, 2020, Proceedings, Part XXV 16. pp. 752-768. Springer (2020)</cell></row><row><cell>4</cell></row><row><cell>35. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:</cell></row><row><cell>from error visibility to structural similarity. IEEE transactions on image processing</cell></row><row><cell>13(4), 600-612 (2004) 11</cell></row><row><cell>36. Xie, C., Liu, S., Li, C., Cheng, M.M., Zuo, W., Liu, X., Wen, S., Ding, E.: Im-</cell></row><row><cell>age inpainting with learnable bidirectional attention maps. In: Proceedings of the</cell></row><row><cell>IEEE/CVF International Conference on Computer Vision. pp. 8858-8867 (2019)</cell></row><row><cell>4</cell></row><row><cell>37. Yang, C., Lu, X., Lin, Z., Shechtman, E., Wang, O., Li, H.: High-resolution image</cell></row><row><cell>inpainting using multi-scale neural patch synthesis. In: Proceedings of the IEEE</cell></row><row><cell>conference on computer vision and pattern recognition. pp. 6721-6729 (2017) 4</cell></row><row><cell>38. Yeh, R., Chen, C., Lim, T.Y., Hasegawa-Johnson, M., Do, M.N.: Semantic image</cell></row><row><cell>inpainting with perceptual and contextual losses. arXiv preprint arXiv:1607.07539</cell></row><row><cell>2(3) (2016) 4</cell></row><row><cell>39. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. arXiv</cell></row><row><cell>preprint arXiv:1511.07122 (2015) 8</cell></row><row><cell>40. Yuan, S., Timofte, R., Slabaugh, G., Leonardis, A., Zheng, B., Ye, X., Tian, X.2021) 3</cell></row><row><cell>42. Zhang, H., Dai, Y., Li, H., Koniusz, P.: Deep stacked hierarchical multi-patch</cell></row><row><cell>network for image deblurring. In: Proceedings of the IEEE/CVF Conference on</cell></row><row><cell>Computer Vision and Pattern Recognition. pp. 5978-5986 (2019) 3</cell></row><row><cell>43. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser:</cell></row><row><cell>Residual learning of deep cnn for image denoising. IEEE transactions on image</cell></row><row><cell>processing 26(7), 3142-3155 (2017) 3</cell></row><row><cell>44. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable</cell></row><row><cell>effectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE</cell></row><row><cell>conference on computer vision and pattern recognition. pp. 586-595 (2018) 11, 25</cell></row><row><cell>-4172 45. Zhang, X., Chen, Q., Ng, R., Koltun, V.: Zoom to learn, learn to zoom. In: Proceed-</cell></row><row><cell>(2018) 1, 2, 3, 6, 11, 12, 14, 20, 21, 24, 27, 28, 35, 37, 38 ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.</cell></row><row><cell>30. Suvorov, R., Logacheva, E., Mashikhin, A., Remizova, A., Ashukha, A., Silvestrov, pp. 3762-3770 (2019) 26</cell></row><row><cell>A., Kong, N., Goka, H., Park, K., Lempitsky, V.: Resolution-robust large mask 46. Zhang, Y., Tian, Y., Kong, Y., Zhong, B., Fu, Y.: Residual dense network for image</cell></row><row><cell>inpainting with fourier convolutions. arXiv preprint arXiv:2109.07161 (2021) 4 super-resolution. In: Proceedings of the IEEE conference on computer vision and</cell></row><row><cell>31. Tao, X., Gao, H., Shen, X., Wang, J., Jia, J.: Scale-recurrent network for deep pattern recognition. pp. 2472-2481 (2018) 3, 8</cell></row><row><cell>image deblurring. In: Proceedings of the IEEE Conference on Computer Vision 47. Zheng, B., Yuan, S., Slabaugh, G., Leonardis, A.: Image demoireing with learnable</cell></row><row><cell>and Pattern Recognition. pp. 8174-8182 (2018) 4 bandpass filters. In: Proceedings of the IEEE/CVF Conference on Computer Vision</cell></row><row><cell>32. Vedaldi, A., Fulkerson, B.: Vlfeat: An open and portable library of computer vision and Pattern Recognition. pp. 3636-3645 (2020) 1, 2, 3, 6, 7, 10, 11, 12, 24, 27, 28,</cell></row><row><cell>algorithms. In: Proceedings of the 18th ACM international conference on Multi-35, 36, 37, 38</cell></row><row><cell>media. pp. 1469-1472 (2010) 5 48. Zhou, T., Tucker, R., Flynn, J., Fyffe, G., Snavely, N.: Stereo magnification: Learn-</cell></row><row><cell>33. Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., Liu, D., Mu, Y., Tan, ing view synthesis using multiplane images. arXiv preprint arXiv:1805.09817 (2018)</cell></row><row><cell>M., Wang, X., et al.: Deep high-resolution representation learning for visual recog-4</cell></row><row><cell>nition. IEEE transactions on pattern analysis and machine intelligence 43(10),</cell></row><row><cell>3349-3364 (2020) 2, 4, 7, 10</cell></row></table><note>: Wavelet- based dual-branch network for image demoir?ing. In: Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XIII 16. pp. 86-102. Springer (2020) 1, 2, 3, 6, 10, 11, 12, 23, 24, 28 23. Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983 (2016) 11, 24 24. Pohlen, T., Hermans, A., Mathias, M., Leibe, B.: Full-resolution residual networks for semantic segmentation in street scenes. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4151-4160 (2017) 4 25. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi- cal image segmentation. In: International Conference on Medical image computing and computer-assisted intervention. pp. 234-241. Springer (2015) 3, 4 26. Shi, W., Caballero, J., Husz?r, F., Totz, J., Aitken, A.P., Bishop, R., Rueckert, D., Wang, Z.: Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1874-1883 (2016) 8 27.: Contextual- based image inpainting: Infer, match, and translate. In: Proceedings of the Euro- pean Conference on Computer Vision (ECCV). pp. 3-19 (2018) 4 29. Sun, Y., Yu, Y., Wang, W.: Moir? photo restoration using multiresolution convolu- tional neural networks. IEEE Transactions on Image Processing 27(8), 4160, Chen, Y., Cheng, X., Fu, Z., et al.: Aim 2019 challenge on image demoireing: Methods and results. In: 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW). pp. 3534-3545. IEEE (2019) 1, 2, 3, 6, 11, 24 41. Zamir, S.W., Arora, A., Khan, S., Hayat, M., Khan, F.S., Yang, M.H., Shao, L.: Multi-stage progressive image restoration. In: Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition. pp. 14821-14831 (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 1 :</head><label>1</label><figDesc>The capture devices we apply to get the moir? image</figDesc><table><row><cell cols="2">Mobile Phone Shooting Resolution</cell><cell>Digital Screen</cell><cell>Display Resolution</cell></row><row><cell>iPhone XR</cell><cell>4032 ? 3024</cell><cell>LG 27UL650-W</cell><cell>3840 ? 2160</cell></row><row><cell>iPhone 13</cell><cell>4032 ? 3024</cell><cell>AOC U2790PQU</cell><cell>3840 ? 2160</cell></row><row><cell>Redmi K30 Pro</cell><cell>4624 ? 3472</cell><cell>Philips 243S7EHMB</cell><cell>1920 ? 1080</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 2 :</head><label>2</label><figDesc>Detail of the encoder; DRDB denotes the dilated residual dense block consisting of three convolution layers</figDesc><table><row><cell>Level</cell><cell>Block Type</cell><cell cols="4">Input Channels Output Channels Inter Channels Dilation Rates</cell></row><row><cell>1</cell><cell>Pixelshuffle downsampling</cell><cell>3</cell><cell>12</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>5 ? 5 Conv + ReLU</cell><cell>12</cell><cell>48</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DRDB</cell><cell>48</cell><cell>48</cell><cell>32</cell><cell>(1, 2, 1)</cell></row><row><cell></cell><cell>SAM</cell><cell>48</cell><cell>48</cell><cell>32</cell><cell>(1, 2, 3, 2, 1)</cell></row><row><cell>2</cell><cell>Stride=2, 3 ? 3 Conv</cell><cell>48</cell><cell>96</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DRDB</cell><cell>96</cell><cell>96</cell><cell>32</cell><cell>(1, 2, 1)</cell></row><row><cell></cell><cell>SAM</cell><cell>96</cell><cell>96</cell><cell>32</cell><cell>(1, 2, 3, 2, 1)</cell></row><row><cell>3</cell><cell>Stride=2, 3 ? 3 Conv</cell><cell>96</cell><cell>192</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DRDB</cell><cell>192</cell><cell>192</cell><cell>32</cell><cell>(1, 2, 1)</cell></row><row><cell></cell><cell>SAM</cell><cell>192</cell><cell>192</cell><cell>32</cell><cell>(1, 2, 3, 2, 1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 3 :</head><label>3</label><figDesc>Detail of the decoder; DRDB denotes the dilated residual dense block consisting of three convolution layers</figDesc><table><row><cell>Level</cell><cell>Block Type</cell><cell cols="4">Input Channels Output Channels Inter Channels Dilation Rates</cell></row><row><cell>3</cell><cell>3 ? 3 Conv + ReLU</cell><cell>192</cell><cell>64</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DRDB</cell><cell>64</cell><cell>64</cell><cell>32</cell><cell>(1, 2, 1)</cell></row><row><cell></cell><cell>SAM</cell><cell>64</cell><cell>64</cell><cell>32</cell><cell>(1, 2, 3, 2, 1)</cell></row><row><cell>Output Layer</cell><cell>3 ? 3 Conv</cell><cell>64</cell><cell>12</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Pixelshuffle upsampling</cell><cell>12</cell><cell>3</cell><cell>-</cell><cell>-</cell></row><row><cell>Transition Layer</cell><cell>Bilinear-Up Layer</cell><cell>64</cell><cell>64</cell><cell>-</cell><cell>-</cell></row><row><cell>2</cell><cell>3 ? 3 Conv + ReLU</cell><cell>160</cell><cell>64</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DRDB</cell><cell>64</cell><cell>64</cell><cell>32</cell><cell>(1, 2, 1)</cell></row><row><cell></cell><cell>SAM</cell><cell>64</cell><cell>64</cell><cell>32</cell><cell>(1, 2, 3, 2, 1)</cell></row><row><cell>Output Layer</cell><cell>3 ? 3 Conv</cell><cell>64</cell><cell>12</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Pixelshuffle upsampling</cell><cell>12</cell><cell>3</cell><cell>-</cell><cell>-</cell></row><row><cell>Transition Layer</cell><cell>Bilinear-Up Layer</cell><cell>64</cell><cell>64</cell><cell>-</cell><cell>-</cell></row><row><cell>1</cell><cell>3 ? 3 Conv + ReLU</cell><cell>112</cell><cell>64</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DRDB</cell><cell>64</cell><cell>64</cell><cell>32</cell><cell>(1, 2, 1)</cell></row><row><cell></cell><cell>SAM</cell><cell>64</cell><cell>64</cell><cell>32</cell><cell>(1, 2, 3, 2, 1)</cell></row><row><cell>Output Layer</cell><cell>3 ? 3 Conv</cell><cell>64</cell><cell>12</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Pixelshuffle upsampling</cell><cell>12</cell><cell>3</cell><cell>-</cell><cell>-</cell></row><row><cell>Transition Layer</cell><cell>Bilinear-Up Layer</cell><cell>64</cell><cell>64</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 4 :</head><label>4</label><figDesc>Quantitative results of different implementations of FHDe 2 Net on UHDM dataset. "Pre-train" denotes the inference result by directly applying the official released pre-train model on FHDMi dataset<ref type="bibr" target="#b12">[13]</ref>, "Low-resolution" denotes the intermediate result produced by the first global demoir?ing stage in FHDe 2 Net</figDesc><table><row><cell cols="5">Metrics Input Pre-train Low-resolution 50 epoch 100 epoch 150 epoch</cell></row><row><cell>PSNR? 17.117 18.052</cell><cell>20.333</cell><cell>20.312</cell><cell>20.313</cell><cell>20.338</cell></row><row><cell>SSIM? 0.5089 0.5986</cell><cell>0.7408</cell><cell>0.7290</cell><cell>0.7365</cell><cell>0.7496</cell></row><row><cell>LPIPS? 0.5314 0.4929</cell><cell>0.4669</cell><cell>0.3397</cell><cell>0.3429</cell><cell>0.3519</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 5 :</head><label>5</label><figDesc>Quantitative comparisons of different weights for training FHDe 2 Net.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 6 :</head><label>6</label><figDesc>Effects of the proposed SAM. We add our SAM to current methods DMCNN [29], MDDM<ref type="bibr" target="#b7">[8]</ref> and MBCNN [47] to improve their performances</figDesc><table><row><cell cols="3">Metrics Input DMCNN/(+SAM) MDDM/(+SAM) MBCNN/(+SAM)</cell></row><row><cell>PSNR? 17.117 19.914/20.769</cell><cell>20.088/20.883</cell><cell>21.414/21.532</cell></row><row><cell>SSIM? 0.5089 0.7575/0.7699</cell><cell>0.7441/0.7640</cell><cell>0.7932/0.7940</cell></row><row><cell>LPIPS? 0.5314 0.3764/0.3630</cell><cell>0.3409/0.3299</cell><cell>0.3318/0.3302</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Densely residual laplacian super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to see in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-scale dynamic feature encoding network for image demoir?ing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring with parameter selective sharing and nested skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<title level="m">Improved training of wasserstein gans</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mop moire patterns using mopnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fhde 2 net: Full high definition demoireing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">37</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part XXII 16</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
