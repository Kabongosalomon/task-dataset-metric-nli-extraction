<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-modal Learning for Image-Guided Point Cloud Shape Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Aiellop</surname></persName>
							<email>emanuele.aiello@polito.it</email>
							<affiliation key="aff0">
								<orgName type="institution">olitecnico di Torino</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Valsesia</surname></persName>
							<email>diego.valsesia@polito.it</email>
							<affiliation key="aff1">
								<orgName type="institution">Politecnico di Torino</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Magli</surname></persName>
							<email>enrico.magli@polito.it</email>
							<affiliation key="aff2">
								<orgName type="institution">Politecnico di Torino</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-modal Learning for Image-Guided Point Cloud Shape Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we explore the recent topic of point cloud completion, guided by an auxiliary image. We show how it is possible to effectively combine the information from the two modalities in a localized latent space, thus avoiding the need for complex point cloud reconstruction methods from single views used by the state-of-the-art. We also investigate a novel weakly-supervised setting where the auxiliary image provides a supervisory signal to the training process by using a differentiable renderer on the completed point cloud to measure fidelity in the image space. Experiments show significant improvements over state-of-the-art supervised methods for both unimodal and multimodal completion. We also show the effectiveness of the weakly-supervised approach which outperforms a number of supervised methods and is competitive with the latest supervised models only exploiting point cloud information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The rise in popularity of 3D sensing technologies such as depth cameras, laser scanners, LiDARs, etc. is making the processing of point cloud data ever more important. The acquisitions produced by those instruments are often incomplete due to occlusions by objects in the environment, reflections, and viewing angles. This limits the exploitability of those data in tasks like scene understanding [1], robotic vision [2], autonomous driving [3] and many more. Completing a point cloud from partial observations is a challenging ill-posed inverse problem that requires strong prior knowledge about shapes to be effectively regularized.</p><p>At the same time, we know that humans are very proficient at mapping the visual concepts learnt from 2D images to understand the 3D world, and are able to successfully infer the shape of partial 3D objects from their 2D experiences. It is thus sensible to expect that point cloud completion techniques can benefit from 2D images to better characterize the 3D shape to be completed. Indeed, several applications of interest in robotic vision can take advantage of multimodal data where the 3D acquisitions of a depth-sensing instrument are paired with images from an RGB camera. It is also worth noting that the two modalities may be acquired from different vantage points, either thanks to disparities in the acquisition geometry or because the vantage point has changed with the passing of time. This makes it clear that the two modalities may carry complementary information and effectively fusing it is key to unlock better completion performance. Nevertheless, the literature on the topic of point cloud completion <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> has largely focused on the single-modalit?</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>problem, where only priors about 3D shapes are exploited. Only recently, image-guided completion has started to receive attention <ref type="bibr" target="#b11">[12]</ref>.</p><p>In this paper, we study how the side information offered by a single image can be used in addition to shape priors to complete a partial point cloud. While following the setting of ViPC <ref type="bibr" target="#b11">[12]</ref>, we extend the multimodal completion methodology in several different ways. First, ViPC <ref type="bibr" target="#b11">[12]</ref> is bottlenecked by the need to estimate a coarse point cloud from the image via single-view reconstruction techniques to fuse the information. We avoid this task by proposing a novel architecture that performs fusion in a latent domain via cross-attention operations on fine-grained, localized representations of the two modalities, coupled with a flexible decoder that allows to complete areas of varied size. Moreover, the multimodal setting is uniquely poised for weakly-supervised learning. In fact, the input image, especially when captured from a different vantage point, may offer a supervisory signal to guide the completion of those areas occluded in the partial point cloud but visible in the image. This is especially interesting for practical applications where it could be difficult to have access to complete shapes, but significantly easier to have images from a different viewing angle. Therefore, we propose to augment the known 3D self-reconstruction losses with the exploitation of a differentiable renderer to measure the fidelity of the completed point cloud in the image space.</p><p>Our experiments show that the proposed model significantly outperforms the state-of-the-art on both the supervised and weakly-supervised settings. In particular, the addition of the rendering loss allows the weakly-supervised image-guided model to outperform several supervised baselines and to be competitive with the latest supervised models only exploiting point cloud information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Point cloud completion 3D shape completion is a long-standing problem in computer vision. Early works devised explicit geometric descriptors or relied on shape retrieval from large datasets <ref type="bibr" target="#b12">[13]</ref>  <ref type="bibr" target="#b13">[14]</ref> [15] <ref type="bibr" target="#b15">[16]</ref>. Since the advent of neural networks operating on raw point cloud data, several models for the completion problem have been studied <ref type="bibr" target="#b16">[17]</ref>. They are mostly based on the encoder-decoder architecture, pioneered by PCN <ref type="bibr" target="#b3">[4]</ref>, which was the first model that did not require any assumption of structure or annotation information about the underlying shape. TopNet <ref type="bibr" target="#b4">[5]</ref> presents a hierarchical rooted tree structure that generates structured point clouds as a collection of its subsets. AtlasNet <ref type="bibr" target="#b5">[6]</ref> and MSN <ref type="bibr" target="#b17">[18]</ref>, on the other hand, recreate the point cloud by assessing a set of parametric surface elements. Convolutional-based approaches ( <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>) use a voxelixed representation of shapes as input to 3D CNNs; nevertheless, this representation introduces undesirable approximations in the shape due to coordinate quantization effects. GRNet <ref type="bibr" target="#b20">[21]</ref> uses techniques to represent point cloud onto a 3D grid, so that CNNs can be exploited, without losing structural information. Recently, VRCNet <ref type="bibr" target="#b7">[8]</ref> has proposed a dual path architecture and a VAE-based relation enhancement module for probabilistic modeling. Architectures based on transformers have also been proposed. PointTr <ref type="bibr" target="#b8">[9]</ref> changes the transformer block to take advantage of the inductive bias of 3D geometries, creating a geometry-aware block that models local geometry relations. Moreover, SnowflakeNet <ref type="bibr" target="#b21">[22]</ref> generates child points by gradually splitting parent points by means of a Skip-Transformer that learns the appropriate splitting modes for particular regions. As a result, the network is able to predict highly detailed shape geometries. Finally, it is worth mentioning that the point cloud completion literature is split between two settings: one, as in the aforementioned works, where the partial input has the same number of points as the completed point cloud and another where the completed point cloud has more points than the input such as in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. In this paper, we will consider the former setting.</p><p>View-guided completion Recently, the usage of auxiliary data to complement point cloud completion has been introduced by ViPC <ref type="bibr" target="#b11">[12]</ref>. The idea is to help the reconstruction objective using side information available as a different imaging modality. In particular, ViPC assumes that an image corresponding to a view of the same object is also available for the completion task, and it exploits the image to retrieve the global shape information that is lacking in the incomplete point cloud. The image is processed by a pre-trained single-view reconstruction model, which estimates a coarse point cloud from the image, representing the entire shape. The key challenge in this setting is how to effectively combine features extracted from the two modalities. Unlike ViPC, our approach leverages direct fusion at a feature level, avoiding the need to explicitly reconstruct a coarse point cloud from a single image, a generally hard inverse problem in itself and full of pitfalls. Self-supervised strategies for completion All the previously mentioned approaches rely on complete ground truth as a supervisory training signal. This may be difficult to obtain in real-word scenarios. Self-supervised training strategies avoid the need for retrieving such expensive ground truths. However, the amount of work on this topic is rather limited. Wang et al. <ref type="bibr" target="#b9">[10]</ref> use resampling that removes further points from an already partial point cloud and mixup among shapes to train their completion network in a self-supervised manner. Similarly, Mittal et al. <ref type="bibr" target="#b10">[11]</ref> also propose an inpainting procedure that leverages further partializations of partial point clouds. To the best of our knowledge, there is no work exploring the availability of a different modality, namely an image to provide a weak supervisory signal to the point cloud completion task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Methods</head><p>We address the setting in which a partial point cloud needs to be completed with the assistance of an image of the object taken from a certain viewpoint. Our goal is to study how to leverage this side information in the most effective manner. To this purpose we study i) a supervised learning setting, for which we show that the image features can be effectively fused with those of the partial point cloud in a latent space; ii) a weakly-supervised setting based on the idea that the image may contain clues about the missing part and can thus serve as a supervisory signal. <ref type="figure" target="#fig_0">Fig. 1</ref> shows an overview of the proposed method, named XMFnet (Cross-Modal Fusion network) which will be detailed in the next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture and Supervised Setting</head><p>At a high level, the architecture of XMFnet is composed by two modality-specific feature extractors that capture localized features of the input point cloud and image, summarized at a small number of points/pixels, followed by a sequence of cross-attention and self-attention operations that progressively merge the two feature spaces. Finally, a decoder upsamples this localized information to estimate a predefined number of points of the missing component. More formally, we denote the partial point cloud as X P R N?3 , the input view as an image I P R Px?Py?3 and the complete point cloud as Y P R N?3 . The task of our model is to predict a complete shape? P R N?3 given X and I as inputs.</p><p>Notice we follow the conventional setting where the partial and complete point clouds have the same number of points, meaning that there is a resampling of the known part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Point Cloud and Image Encoder</head><p>The point cloud encoder should extract localized features from the partial shape X. It is important to keep a degree of locality, i.e., associating features to a small number of points N X ? N rather than a single global embedding because the information about the missing part that needs to be estimated by the entire model is also mostly localized. However, it is also important to have a sufficiently large receptive field to infer some global information about the object. For this reason, we adopt a graph-convolutional architecture with graph pooling operations. The architecture is a sequence of graph-convolutional layers (EdgeConv <ref type="bibr" target="#b24">[25]</ref>) interleaved by pooling operations (Self-Attention Graph Pooling <ref type="bibr" target="#b25">[26]</ref>) to reduce the cardinality of the point cloud. Pooling has the double purpose of expanding the receptive field to also include more global information and reduce the complexity of the subsequent cross-attention operations fusing the two modalities.</p><p>Any network that extracts features from an image can be utilized as encoder for view I. The design principles follow those of the point-cloud encoder, i.e., features localized at a subset N I ? P x P y of the image pixels obtained from a sufficiently large receptive field are produced as output.</p><p>We will refer to the features produced by the point cloud encoder as H X P R N X?FX and by the image encoder as H I P R N I?FI .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Modality Fusion</head><p>Once we have collected localized information from the two modalities, we need to combine them effectively to capture their complementary information, despite the obvious domain gap. The attention mechanism is particularly suited to find correspondences between the features of a region of the point cloud and a region of the image. The cross-attention layer in our architecture uses the Transformer's multihead attention mechanism <ref type="bibr" target="#b26">[27]</ref>. The point cloud features are projected to form the query tensor, while the image features are projected to form the key and value tensors, and then attention mechanism aggregates the features from different image regions according to the weights determined by the cross-correlation between the two modalities. More formally:</p><formula xml:id="formula_0">Q X " H X W Q , K I " H I W K , V I " H I W V (1) H fused " softmax?Q K T ? F?V (2) being W Q P R F X?F , W K , W V P R F I?F the projection weights.</formula><p>The fused features H fused P R N X?F produced by the cross-attention mechanism can be regarded as the original point cloud features enriched by the image features.</p><p>The XMFnet architecture depicted in <ref type="figure" target="#fig_0">Fig. 1</ref> shows a self-attention layer after the cross-attention fusion. The goal of this operation is to have a permutation-invariant transformation of the features with a global receptive field so that any information from the image not properly integrated can be rectified. Self-attention works exactly like Eq. (2) except for the fact that Q, K, V are all different projections of the same features. Furthermore, a sequence of multiple cross-and self-attention layers can be used to more effectively integrate the information from the two modalities via a "slow" fusion. We remark that at the end of this sequence we use a special cross-attention layer that merges information from the end and the beginning of the sequence allowing better flexibility in the decision of the desired abstraction level (higher-level features cross-attend lower-level features).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Decoder and Supervised Loss</head><p>The decoder is a crucial component of our architecture as it should take the joint feature embedding and learn to reconstruct a complete point cloud preserving both global and local structure. To be precise, the decoder seeks to estimate the positions of a number of points that upper bounds the size of the missing part, so that they can be concatenated to a version of the input partial point cloud subsampled by means of farthest point sampling (FPS). This mechanism is reminiscent of what is done in the setting with a variable number of points where only the missing part is estimated <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>For example, in our experiments, we upper bounded the size of the missing part to 50% of the total number of points, thus having N 1 points estimated by the decoder concatenated to N 1 points from the subsampled input. However, our method allows to be flexible and handle more incomplete inputs by simply tuning the desired ratio of points to be estimated and points provided from the partial input. Typically, the latent space where we perform feature fusion is much more localized to constrain complexity and allow higher-level features so that N X ! N 1 , thus requiring the decoder to upsample the feature field. We perform this operation by using a number of attention-based operations, inspired by the work in <ref type="bibr" target="#b27">[28]</ref>, that convert features to points in parallel with the idea that each branch specializes on the reconstruction of a sub-region of the missing part. The structure is depicted in <ref type="figure" target="#fig_0">Fig.1</ref>. More formally, calling H P R N X?F the features provided to the decoder, the output? i P R N 1 {K?3 of each of the K branches is computed as:</p><formula xml:id="formula_1">Z i " MLP proj i pHq i " 1, . . . , K<label>(3)</label></formula><formula xml:id="formula_2">Y i "`softmaxpMLP dec i pZ i qq T Z i?Wout,i i " 1, . . . , K<label>(4)</label></formula><p>where MLP proj</p><formula xml:id="formula_3">i : R F ? R F 1 , MLP dec i : R F 1 ? R N 1 {K</formula><p>are multilayer perceptrons with different weights for each branch, projecting features to K subspaces and generating attention weights for the resampling process, respectively. W out,i P R F 1?3 is projection matrix to 3D space. Finally, the completed point cloud is generated by concatenation of the outputs of all decoder branches and the partial input subsampled by FPS as:</p><formula xml:id="formula_4">Y " "? 1 ,? 2 , . . . ,? K , FPSpXq ? .<label>(5)</label></formula><p>Supervised training is performed using the L1 Chamfer Distance (CD) between the generated shapes and the ground truth shapes, defined as follows:</p><formula xml:id="formula_5">L CD pY,?q " 1 2N ? yPY min yP? }y??}`1 2N ? yP? min yPY }??y}.<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Weakly-supervised Setting</head><p>The multimodal completion problem addressed in this paper is uniquely poised for weaklysupervised learning. In fact, the image available as input may contain complementary information with respect to the point cloud and, crucially, cues about the missing part. This is especially true if the image is collected from a different viewpoint or at a different time with respect to the point cloud, resulting in different kinds of occlusions. Existing architectures for self-supervised completion <ref type="bibr" target="#b9">[10]</ref> [11] rely solely on point cloud supervision, due to their unimodal nature. The key insight of the proposed method <ref type="figure" target="#fig_1">(Fig. 2)</ref> is to supplement completion losses on points with a loss measuring a reprojection error in the image space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DIFF. RENDER</head><p>In particular, we measure whether the reconstructed point cloud produced by the architecture described in Sec. 3.1 leads to an image similar to the input one, when captured from the correct viewpoint.</p><p>In order to measure this information and use it in the training process, we include a differentiable rendering module based on alpha compositing <ref type="bibr" target="#b28">[29]</ref> which generates a rasterized version of the object, employing provided camera parameters. In order to ensure consistency with the input image, intrinsic and extrinsic camera parameters may be estimated with a number of well-known methods <ref type="bibr" target="#b29">[30]</ref> [31] <ref type="bibr" target="#b31">[32]</ref>. In order to minimize the domain shift between the input image and the result of the rendering process, we work on silhouettes, i.e., binary masks of objects. The differentiable renderer produces a soft silhouette with continuous values, while the input image is directly binarized. Inevitable inaccuracies in the camera parameters and rendering process will typically yield unreliable borders of the silhouette. Therefore, we also compute a border mask with a simple edge detector (Laplacian of Gaussian) and discount the loss function by a factor ? ? 1 for the pixels in the mask. In summary, our rendering loss is defined as:</p><formula xml:id="formula_6">L render " ? ? ?M d " Rp?q?SpIq ?? ? ? 1 , M i,j "</formula><p>" ? if pi, jq P edge 1 otherwise <ref type="formula">(7)</ref> being R the differentiable silhouette renderer and S the silhouette binarizer. We remark that <ref type="bibr" target="#b32">[33]</ref> proposes to use rendering to improve performance in point cloud completion, aiding the learning process through an image domain supervision. However, their approach is supervised and is based on rendering the ground truth and generated point cloud in depth-maps with different view-points.</p><p>In addition to the rendering loss, our weakly-supervised framework also adopts self-supervision in the point cloud domain. In particular, we use a combination of the resampling and mixup approaches proposed in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. Resampling consists in removing random portions of the original partial input point cloud, yielding even more partial shapes. As a result, the original partial input is employed as a pseudo-ground truth. Mixup combines a pair of partial shapes weighed according to Beta distribution in an attempt at increasing the complexity of the shapes processed by the network. Differently from <ref type="bibr" target="#b9">[10]</ref>, we also have images associated with a partial shape in our setting. Hence, we also mix the images in such a way that the mixup technique is carried out symmetrically for the two modalities.</p><p>We remark that the rendering loss is comparatively weaker than the point cloud loss and, by itself, has a number of ambiguities due to the lack of depth information and the use of silhouettes. For this reason, it is important to combine it with the point cloud loss. We found that the density-aware Chamfer Distance (DCD) [34], a version of CD that is more sensitive to non-uniform point distributions is superior in this weakly-supervised scenario to regularize the ambiguities of the rendering loss. Our overall weakly-supervised training procedure is therefore as follows. We alternate between a step that optimizes the point cloud loss consisting in a weighted CD:</p><formula xml:id="formula_7">L PC " 1?? 2N ? yPY min yP? }y??}`? 2N ? yP? min yPY }??y},<label>(8)</label></formula><p>and a step optimizing a combination of DCD and rendering loss:</p><formula xml:id="formula_8">L I " ? -1 2N ? yPY?1?1 N e?? }y?w}2?`1 2N ? yP??1?1 N e?? }??z}2?fi fl`? L render<label>(9)</label></formula><p>where w " arg min? P? }y??} 2 and z " arg min yPY }y??} 2 . Notice that L PC and the DCD part of L I use resampling and mixup for point clouds, while L render does not (L I is computed with full minibatch for the point-cloud part and half minibatch with the original partials for rendering).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings and Implementation Details</head><p>All the experiments are conducted on the ShapeNet-ViPC <ref type="bibr" target="#b11">[12]</ref> dataset. The dataset contains 38,328 objects from 13 categories; for each object it comprises 24 partial point clouds with occlusions generated under 24 viewpoints, using the same settings as ShapeNetRendering <ref type="bibr" target="#b33">[35]</ref>. The input and ground truth point clouds contains N " 2048 points each. Each 3D shape is rotated to the pose corresponding to a certain view point after being normalized within the bounding sphere with radius of 1. Images are generated from the 24 view points of ShapeNetRendering and have a resolution of 224?224 pixels. For all the experiments in this paper, we employ the same selection used in <ref type="bibr" target="#b11">[12]</ref>: we used 31,650 objects from eight categories, with 80% of them for training and 20% for testing.</p><p>The partial point cloud is downsampled by farthest point sampling to N 1 " 1024 points and concatenated to the output of the decoder that produces N 1 " 1024 points leading to a completed point cloud with 2048 points. The decoder has K " 8 branches, each of them producing M " 128 points. The point cloud encoder employs EdgeConv and SAGPooling layers; the EdgeConv layers selects k " 20 nearest neighbors, while the two pooling layers use k " 16 and k " 6 nearest neighbors, respectively. The original point cloud is overall downsampled by a factor of 16, resulting in N X " 128 points with F X " 256 features. The image encoder is built with a ResNet18 <ref type="bibr" target="#b34">[36]</ref> as  backbone, it extracts N I " 14?14 " 196 pixels with F I " 256 features. The multihead attention has 4 attention heads, with embedding size F " 256. In the L I loss we use ? " 0.15. The mask factor for the edge detector has been set to ? " 0.4.</p><p>The differentiable renderer has been implemented with PyTorch3D <ref type="bibr" target="#b35">[37]</ref>. The rendered silhouettes H?W has size 224?224 that is the same size of input views in our experiments. We adopt radius ? " 0.025 in point rasterization. The proposed framework is implemented in PyTorch and trained on an Nvidia V100 GPU. Class-specific training is performed for all models, using the Adam optimizer <ref type="bibr" target="#b36">[38]</ref> for roughly 200 epochs with a batch size of 128. The learning rate is initialized to 0.001 and reduced by a factor of 10 at epoch 25 and 125.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Supervised Learning</head><p>We first compare XMFnet against several baselines for the point cloud completion under supervised learning. Since the new multimodal setting with an auxiliary image has been introduced only recently, ViPC <ref type="bibr" target="#b11">[12]</ref> represents the only method fully comparable to ours. However, we also report the results of a number of state-of-the-art architectures for completion with only point cloud input, when retrained on the ViPC dataset. AtlasNet <ref type="bibr" target="#b5">[6]</ref> reconstructs a point cloud by estimating parametric surface elements. FoldingNet <ref type="bibr" target="#b37">[39]</ref> is a 2-D grid based auto-encoder. PCN <ref type="bibr" target="#b3">[4]</ref> is an encoder-decoder framework that reconstructs the point cloud in a coarse-to-fine manner. TopNet <ref type="bibr" target="#b4">[5]</ref> has a rooted tree structure in the decoder. ECG <ref type="bibr" target="#b6">[7]</ref> is an edge-aware completion method based on Graph Convolutions. VRC-Net <ref type="bibr" target="#b7">[8]</ref> is the most recent method adopting a VAE-based model with a dual path architecture and probabilistic modeling. In line with the previous evaluation protocols, we use CD and F-score <ref type="bibr" target="#b38">[40]</ref> as metrics for the reconstruction quality. Before evaluating the CD, we normalize the output of the models to fit into the unit sphere. <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref> report the experimental results and show XMFnet outperforming the other techniques by a significant margin. While part of this gain with respect to state-of-the-art models for point cloud completion can be attributed to the use of the input image, it is worth noting that we also report significant improvements over the multimodal ViPC. This highlights the sub-optimality of performing modality fusion by resorting to estimating a coarse point cloud from the single input image, as in ViPC, rather than working in a latent feature space. Qualitative comparisons 2 are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Our method is capable of producing cleaner completions than the other baselines, with fewer outliers and a more uniform point distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Weakly-supervised</head><p>We remark that we are the first to propose a weakly-supervised training strategy for multimodal completion, so the setting in which supervisory information can be gathered from an input image is unexplored. For this reason, we compare to a number of unimodal and multimodal supervised methods as well as a self-supervised version using resampling and mixup of a unimodal state-of-theart model <ref type="bibr" target="#b2">3</ref> . The results are reported in <ref type="table" target="#tab_2">Table 3</ref> for the CD and <ref type="table" target="#tab_3">Table 4</ref> for the F-Score. We notice that our weakly-supervised method outperforms a number of supervised baselines and it is close to the performance of the most recent unimodal supervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>In order to verify the effectiveness of the proposed design, we study the impact of the auxiliary image input on the completion performance. To ensure a comparison as fair as possible, the version of XMFnet that uses only point cloud information has the image encoder removed and the cross-attention blocks replaced with self-attention ones.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Airplane Lamp Watercraft</head><p>AtlasNet <ref type="bibr" target="#b5">[6]</ref> 0.509 0.426 0.551 FoldingNet <ref type="bibr" target="#b37">[39]</ref> 0.432 0.360 0.518 PCN <ref type="bibr" target="#b3">[4]</ref> 0.578 0.456 0.577 TopNet <ref type="bibr" target="#b4">[5]</ref> 0.593 0.491 0.615 ECG <ref type="bibr" target="#b6">[7]</ref> 0.880 0.689 0.810 VRC-Net <ref type="bibr" target="#b7">[8]</ref> 0.902 0.823 0.832 VRC-Net(self-sup.) 0.689 0.710 0.673 ViPC <ref type="bibr" target="#b11">[12]</ref> 0.803 0.706 0.730 XMFnet (sup.) 0.961 0.792 0.901</p><p>XMFnet (weakly-sup.) 0.742 0.542 0.704  The unimodal architecture is then trained with the same settings as the multimodal one, and the results are reported in <ref type="table" target="#tab_4">Table 5</ref>. The results show that the addition of the image input provides a significant improvement in performance. Notice that besides the result averaged over all the possible 24 views, we also report the performance with the worst view and the best view. Indeed, we are interested in investigating how the viewpoint of the image affects completion.  <ref type="figure">Figure 4</ref>: Impact of image contribution as function of point of view, sorted by reconstruction CD (from worst to best) averaged over cabinet category, supervised setting. <ref type="figure">Fig. 4</ref> reports the average CD for different views, ordered from worst to best, for the cabinet category. It is clear that some views provide complementary information due to their different vantage point and allow to substantially improve over the average result. A small number of "bad" views leads to results comparable to the unimodal case.</p><p>Furthermore, it is interesting to study the impact of our novel rendering loss in the weakly-supervised setting. We noticed that it allows the training process to have a faster and smoother convergence and that the overall completion performance is increased from both a qualitative and quantitative point of view. A qualitative comparison between the weakly-supervised strategy with and without the rendering module can be visualized in <ref type="figure">Fig. 5</ref> and quantitative Partial Without Rendering With Rendering GT <ref type="figure">Figure 5</ref>: Qualitative visualization of the effect of the proposed weakly-supervised rendering loss. The sample without rendering has CD " 7.812, the one with rendering has CD" 3.743.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Partial</head><p>Normal CD DCD GT <ref type="figure">Figure 6</ref>: Qualitative visualization of the effect of the DCD for the weakly-supervised setting.</p><p>results are reported in <ref type="table">Table 6</ref>. The mixup loss provides only a small improvement from the perspective of the CD. However, it substantially improves the completed shape from a qualitative point of view, helping the network generate more complete shapes. Moreover, the computational overhead due to creating the mixed input shape is very small, so we decided to keep the method as it offered a very favorable cost-performance trade-off. We also include the ablation for the DCD component of the weakly-supervised loss, we found it helpful from both a qualitative and quantitative point of view. <ref type="table" target="#tab_5">Table 7</ref> reports the impact of the DCD on the cabinet category, while <ref type="figure">Fig. 6</ref> provides a qualitative example, where the shape completed with the DCD presents a more uniform distribution of points and a better overall quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we explored the topic of point cloud completion guided by an auxiliary image, discovering that effective fusion can be achieved in a latent space via cross-attention. Our method achieves state-of-the-art results on the ShapeNetViPC-Dataset. Moreover, we showed how this setting lends itself to weakly-supervised learning where the image can be used for supervision via a differentiable rendering approach. The major limitation of our work is the lack of study of a real-world scenario for the proposed framework. In future work, we will focus on extending the work to real acquisitions, thus dealing with complex effects like acquisition noise, background or additional occlusions in the auxiliary images, and many more. This paper has provided a proof of concept that effective multimodal completion is possible but a more in-depth study of such issues on real scenes is needed, along with suitable improvements to our design towards increased robustness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture overview. Localized features from the partial point cloud and the input image are jointly processed via cross-and self-attentions. A decoder reconstructs the target number of points from the feature space with attention-based upsampling. The input partial point cloud is downsampled with farthest point sampling (FPS) and concatenated. Supervised training only uses the point cloud reconstruction loss with respect to the complete point cloud. Weakly-supervised training has a point cloud reconstruction loss with respect to a less partial point cloud and a rendering loss. N is the number of points of the input point cloud; M is the number of points generated by each branch of the decoder, while F represents the feature dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Weakly-supervised training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative comparison of completed point clouds for different classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Mean Chamfer Distance per point (?10?3). ShapeNet-ViPC dataset, supervised.</figDesc><table><row><cell>Methods</cell><cell cols="4">Avg Airplane Cabinet Car Chair Lamp Sofa Table Watercraft</cell></row><row><cell>AtlasNet [6]</cell><cell>6.062</cell><cell>5.032</cell><cell>6.414 4.868 8.161 7.182 6.023 6.561</cell><cell>4.261</cell></row><row><cell cols="2">FoldingNet [39] 6.271</cell><cell>5.242</cell><cell>6.958 5.307 8.823 6.504 6.368 7.080</cell><cell>3.882</cell></row><row><cell>PCN [4]</cell><cell>5.619</cell><cell>4.246</cell><cell>6.409 4.840 7.441 6.331 5.668 6.508</cell><cell>3.510</cell></row><row><cell>TopNet [5]</cell><cell>4.976</cell><cell>3.710</cell><cell>5.629 4.530 6.391 5.547 5.281 5.381</cell><cell>3.350</cell></row><row><cell>ECG [7]</cell><cell>4.957</cell><cell>2.952</cell><cell>6.721 5.243 5.867 4.602 6.813 4.332</cell><cell>3.127</cell></row><row><cell>VRC-Net [8]</cell><cell>4.598</cell><cell>2.813</cell><cell>6.108 4.932 5.342 4.103 6.614 3.953</cell><cell>2.925</cell></row><row><cell>ViPC [12]</cell><cell>3.308</cell><cell>1.760</cell><cell>4.558 3.183 2.476 2.867 4.481 4.990</cell><cell>2.197</cell></row><row><cell>XMFnet</cell><cell>1.443</cell><cell>0.572</cell><cell>1.980 1.754 1.403 1.810 1.702 1.386</cell><cell>0.945</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Mean F-Score @ 0.001. ShapeNet-ViPC dataset, supervised</cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">Avg Airplane Cabinet Car Chair Lamp Sofa Table Watercraft</cell></row><row><cell>AtlasNet [6]</cell><cell>0.410</cell><cell>0.509</cell><cell>0.304 0.379 0.326 0.426 0.318 0469</cell><cell>0.551</cell></row><row><cell cols="2">FoldingNet [39] 0.331</cell><cell>0.432</cell><cell>0.237 0.300 0.204 0.360 0.249 0.351</cell><cell>0.518</cell></row><row><cell>PCN [4]</cell><cell>0.407</cell><cell>0.578</cell><cell>0.270 0.331 0.323 0.456 0.293 0.431</cell><cell>0.577</cell></row><row><cell>TopNet [5]</cell><cell>0.467</cell><cell>0.593</cell><cell>0.358 0.405 0.388 0.491 0.361 0.528</cell><cell>0.615</cell></row><row><cell>ECG [7]</cell><cell>0.704</cell><cell>0.880</cell><cell>0.542 0.713 0.671 0.689 0.534 0.792</cell><cell>0.810</cell></row><row><cell>VRC-Net [8]</cell><cell>0.764</cell><cell>0.902</cell><cell>0.621 0.753 0.722 0.823 0.654 0.810</cell><cell>0.832</cell></row><row><cell>ViPC [12]</cell><cell>0.591</cell><cell>0.803</cell><cell>0.451 0.512 0.529 0.706 0.434 0.594</cell><cell>0.730</cell></row><row><cell>XMFnet</cell><cell>0.796</cell><cell>0.961</cell><cell>0.662 0.691 0.809 0.792 0.723 0.830</cell><cell>0.901</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Mean Chamfer Distance per point (?10?3). ShapeNet-ViPC dataset.</figDesc><table><row><cell>Methods</cell><cell>Airplane Lamp Watercraft</cell></row><row><cell>AtlasNet [6]</cell><cell>5.032 7.182 4.261</cell></row><row><cell>FoldingNet [39]</cell><cell>6.504 6.368 3.882</cell></row><row><cell>PCN [4]</cell><cell>4.246 6.331 3.510</cell></row><row><cell>TopNet [5]</cell><cell>3.710 5.547 3.350</cell></row><row><cell>ECG [7]</cell><cell>2.952 4.602 3.127</cell></row><row><cell>VRC-Net [8]</cell><cell>2.813 4.103 2.925</cell></row><row><cell>VRC-Net(self-sup.)</cell><cell>4.315 8.023 7.259</cell></row><row><cell>ViPC [12]</cell><cell>1.760 2.867 2.197</cell></row><row><cell>XMFnet (sup.)</cell><cell>0.572 1.810 0.945</cell></row><row><cell cols="2">XMFnet (weakly-sup.) 2.426 6.269 3.423</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Mean F-Score @ 0.001. ShapeNet-ViPC</cell></row><row><cell>dataset.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Unimodal vs.</figDesc><table><row><cell></cell><cell>Multimodal comple-</cell></row><row><cell cols="2">tion (supervised)</cell></row><row><cell>Method</cell><cell>Avg Airplane Cabinet Lamp</cell></row><row><cell>Unimodal</cell><cell>1.570 0.626 2.114 1.980</cell></row><row><cell cols="2">Multimodal 1.470 0.572 1.973 1.810</cell></row><row><cell cols="2">? best view 1.223 0.545 1.426 1.697</cell></row><row><cell cols="2">? worst view 1.819 0.722 2.621 2.115</cell></row><row><cell cols="2">Table 6: Ablation study for the Weakly-</cell></row><row><cell cols="2">Supervised method (airplane)</cell></row><row><cell cols="2">Resampling Mixup Rendering CD(10?3)</cell></row><row><cell></cell><cell>4.568</cell></row><row><cell></cell><cell>4.239</cell></row><row><cell></cell><cell>2.426</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table><row><cell>Ablation study for the Weakly-</cell></row><row><cell>Supervised method -DCD (cabinet)</cell></row><row><cell>DCD CD(10?3)</cell></row><row><cell>3.012</cell></row><row><cell>2.426</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Visualizations for ViPC<ref type="bibr" target="#b11">[12]</ref> have been kindly provided by the original authors.<ref type="bibr" target="#b2">3</ref> We remark the difficulty in reproducing several published methods in the self-supervised setting. The code for<ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> was not available. The code to reproduce ViPC<ref type="bibr" target="#b11">[12]</ref> is also incomplete so we cannot retrain the most sensible self-supervised baseline of ViPC + Resampling + Mixup.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">[34] T. Wu, L. Pan, J. Zhang, T. Wang, Z. Liu, and D. Lin, "Balanced Chamfer Distance as a Comprehensive Metric for Point Cloud Completion," Advances in Neural Information Processing Systems, vol. 34, 2021.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3D-SIS: 3D semantic instance segmentation of RGB-D scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4421" to="4430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape completion enabled robotic grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dechant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2442" to="2447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning for lidar point clouds in autonomous driving: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3412" to="3432" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PCN: Point completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="728" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Topnet: Structural point cloud decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="383" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A papier-m?ch? approach to learning 3d surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ECG: Edge-aware Point Cloud Completion with Graph Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4392" to="4398" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variational relational point completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8524" to="8533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PoinTr: Diverse point cloud completion with geometry-aware transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cascaded refinement network for point cloud completion with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-Supervised Point Cloud Completion via Inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Okorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jangid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">View-guided point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="890" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data-driven structural priors for shape completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Angst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Partial and approximate symmetry detection for 3d geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="560" to="568" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discovering structural regularity in 3d geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wallner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pottmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Completion and reconstruction with primitive shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Degener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="503" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Comprehensive review of deep learning-based 3d point clouds completion processing and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.03311</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Morphing and sampling network for dense point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Shape completion using 3D-encoder-predictor CNNs and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Ruizhongtai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5868" to="5877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High-resolution shape completion using deep neural networks for global structure and local geometry inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="85" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">GRNet: Gridding residual network for dense point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="365" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Snowflakenet: Point cloud completion by snowflake point deconvolution with skip-transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5499" to="5509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pf-net: Point fractal network for 3d point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7662" to="7670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Denoise and contrast for category agnostic shape completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alliegro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Valsesia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fracastoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Magli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4629" to="4638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Attention-based transformation from latent features to point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05324</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Synsin: End-to-end view synthesis from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7467" to="7477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised learning of shape and pose with differentiable point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Geometric correspondence fields: Learned differentiable rendering for 3d pose refinement in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="102" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end 6-dof object pose estimation through differentiable rasterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Palazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergamini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Style-based point generator with adversarial rendering for point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4619" to="4628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">ShapeNet: An Information-Rich 3D Model Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Stanford University -Princeton University -Toyota Technological Institute at Chicago</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Accelerating 3d deep learning with pytorch3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Novotn?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Asia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internation Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">FoldingNet: Point Cloud Auto-Encoder via Deep Grid Deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="206" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tanks and temples: Benchmarking large-scale scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knapitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
