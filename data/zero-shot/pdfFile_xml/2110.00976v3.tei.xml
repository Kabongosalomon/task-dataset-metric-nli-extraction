<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LexGLUE: A Benchmark Dataset for Legal Language Understanding in English</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilias</forename><surname>Chalkidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhik</forename><surname>Jana</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universit?t Hamburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hartung</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Bucerius Law School</orgName>
								<address>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">CodeX</orgName>
								<orgName type="department" key="dep2">Stanford Law School</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country>United States, Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bommarito ? ? Ion</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Bucerius Law School</orgName>
								<address>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">CodeX</orgName>
								<orgName type="department" key="dep2">Stanford Law School</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country>United States, Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Androutsopoulos</forename><surname>Daniel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Katz</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Bucerius Law School</orgName>
								<address>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">CodeX</orgName>
								<orgName type="department" key="dep2">Stanford Law School</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country>United States, Greece</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Illinois Tech -Chicago Kent College of Law</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LexGLUE: A Benchmark Dataset for Legal Language Understanding in English</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Laws and their interpretations, legal arguments and agreements are typically expressed in writing, leading to the production of vast corpora of legal text. Their analysis, which is at the center of legal practice, becomes increasingly elaborate as these collections grow in size. Natural language understanding (NLU) technologies can be a valuable tool to support legal practitioners in these endeavors. Their usefulness, however, largely depends on whether current state-of-the-art models can generalize across various tasks in the legal domain. To answer this currently open question, we introduce the Legal General Language Understanding Evaluation (LexGLUE) benchmark, a collection of datasets for evaluating model performance across a diverse set of legal NLU tasks in a standardized way. We also provide an evaluation and analysis of several generic and legal-oriented models demonstrating that the latter consistently offer performance improvements across multiple tasks. . 2020a. Iteratively questioning and answering for interpretable legal judgment prediction. In</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Law is a field of human endeavor dominated by the use of language. As part of their professional training, law students consume large bodies of text as they seek to tune their understanding of the law and its application to help manage human behavior. Virtually every modern legal system produces massive volumes of textual data <ref type="bibr">(Katz et al., 2020)</ref>. Lawyers, judges, and regulators continuously author legal documents such as briefs, memos, statutes, regulations, contracts, patents and judicial decisions <ref type="bibr">(Coupette et al., 2021)</ref>. Beyond the consumption and production of language, law and the art of lawyering is also an exercise centered around the analysis and interpretation of text.</p><p>Natural language understanding (NLU) technologies can assist legal practitioners in a variety of legal tasks (Chalkidis and Kampas, 2018; Aletras  <ref type="bibr">Zhong et al., 2020b;</ref><ref type="bibr" target="#b11">Bommarito et al., 2021)</ref>, from judgment prediction <ref type="bibr" target="#b2">(Aletras et al., 2016;</ref><ref type="bibr" target="#b37">Sim et al., 2016;</ref><ref type="bibr">Katz et al., 2017;</ref><ref type="bibr" target="#b60">Zhong et al., 2018;</ref><ref type="bibr">Chalkidis et al., 2019a;</ref><ref type="bibr">Malik et al., 2021)</ref>, information extraction from legal documents <ref type="bibr" target="#b3">(Chalkidis et al., 2018</ref><ref type="bibr">(Chalkidis et al., , 2019c</ref><ref type="bibr">Chen et al., 2020;</ref><ref type="bibr">Hendrycks et al., 2021)</ref> and case summarization <ref type="bibr" target="#b9">(Bhattacharya et al., 2019)</ref> to legal question answering <ref type="bibr">(Ravichander et al., 2019;</ref><ref type="bibr">Kien et al., 2020;</ref><ref type="bibr">Zhong et al., 2020a,c)</ref> and text classification <ref type="bibr" target="#b31">(Nallapati and Manning, 2008;</ref><ref type="bibr">Chalkidis et al., 2019b</ref><ref type="bibr">Chalkidis et al., , 2020a</ref>. Transformer models <ref type="bibr" target="#b45">(Vaswani et al., 2017)</ref> pre-trained on legal, rather than generic, corpora have also been studied <ref type="bibr">(Chalkidis et al., 2020b;</ref><ref type="bibr" target="#b59">Zheng et al., 2021;</ref>.</p><p>Pre-trained Transformers, including BERT <ref type="bibr">(Devlin et al., 2019)</ref>, <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>, T5 <ref type="bibr" target="#b36">(Raffel et al., 2020)</ref>, BART <ref type="bibr" target="#b15">(Lewis et al., 2020)</ref>, <ref type="bibr">DeBERTa (He et al., 2021)</ref> and numerous variants, are currently the state of the art in most natural language processing (NLP) tasks. Rapid performance improvements have been witnessed, to the extent that ambitious multi-task benchmarks <ref type="bibr" target="#b48">(Wang et al., 2018</ref><ref type="bibr" target="#b49">(Wang et al., , 2019b</ref> are considered almost 'solved' a few years after their release and need to be made more challenging <ref type="bibr" target="#b47">(Wang et al., 2019a)</ref>. <ref type="bibr">Recently, Bommasani et al. (2021)</ref> named these pre-trained models (e.g., BERT, DALL-E, GPT-3) foundation models. The term may be controversial, but it emphasizes the paradigm shift these models have caused and their interdisciplinary potential. Studying the latter includes the question of how to adapt these models to legal text <ref type="bibr" target="#b11">(Bommarito et al., 2021)</ref>. As discussed by <ref type="bibr">Zhong et al. (2020b)</ref> and <ref type="bibr">Chalkidis et al. (2020b)</ref>, legal text has distinct characteristics, such as terms that are uncommon in generic corpora (e.g., 'restrictive covenant', 'promissory estoppel', 'tort', 'novation'), terms that have different meanings than in everyday language (e.g., an 'executed' contract is signed and effective, a 'party' is a legal entity), older expressions (e.g., pronominal adverbs like 'herein', 'hereto', 'wherefore'), uncommon expressions from other languages (e.g., 'laches', 'voir dire', 'certiorari', 'sub judice'), and long sentences with unusual word order (e.g., "the provisions for termination hereinafter appearing or will at the cost of the borrower forthwith comply with the same") to the extent that legal language is often classified as a 'sublanguage' <ref type="bibr" target="#b40">(Tiersma, 1999;</ref><ref type="bibr" target="#b51">Williams, 2007;</ref><ref type="bibr">Haigh, 2018)</ref>. Furthermore, legal documents are often much longer than the maximum length state-ofthe-art deep learning models can handle, including those designed to handle long text <ref type="bibr" target="#b7">(Beltagy et al., 2020;</ref><ref type="bibr" target="#b58">Zaheer et al., 2020;</ref>.</p><p>Inspired by the recent widespread use of the GLUE multi-task benchmark NLP dataset <ref type="bibr" target="#b48">(Wang et al., 2018</ref><ref type="bibr" target="#b49">(Wang et al., , 2019b</ref>, the subsequent more difficult SuperGLUE <ref type="bibr" target="#b47">(Wang et al., 2019a)</ref>, other previous multi-task NLP benchmarks <ref type="bibr">(Conneau and Kiela, 2018;</ref><ref type="bibr" target="#b25">McCann et al., 2018)</ref>, and similar initiatives in other domains <ref type="bibr" target="#b35">(Peng et al., 2019)</ref>, we introduce LexGLUE, a benchmark dataset to evaluate the performance of NLP methods in legal tasks. LexGLUE is based on seven English existing legal NLP datasets, selected using criteria largely from SuperGLUE (discussed in Section 3.1).</p><p>We anticipate that more datasets, tasks, and languages will be added in later versions of LexGLUE. <ref type="bibr" target="#b64">1</ref> As more legal NLP datasets become available, we also plan to favor datasets checked thoroughly for validity (scores reflecting real-life performance), annotation quality, statistical power, and social bias <ref type="bibr">(Bowman and Dahl, 2021)</ref>.</p><p>As in GLUE and SuperGLUE <ref type="bibr">(Wang et al., 2019b,a)</ref>, one of our goals is to push towards generic (or 'foundation') models that can cope with multiple NLP tasks, in our case legal NLP tasks, possibly with limited task-specific fine-tuning. Another goal is to provide a convenient and informative entry point for NLP researchers and practitioners wishing to explore or develop methods for legal NLP. Having these goals in mind, the datasets we include in LexGLUE and the tasks they address have been simplified in several ways, discussed below, to make it easier for newcomers and generic models to address all tasks. We provide Python APIs integrated with Hugging Face <ref type="bibr" target="#b52">(Wolf et al., 2020;</ref><ref type="bibr">Lhoest et al., 2021)</ref> to easily import all the datasets we experiment with and evaluate the performance of different models (Section 4.4). By unifying and facilitating the access to a set of law-related datasets and tasks, we hope to attract not only more NLP experts, but also more interdisciplinary researchers (e.g., law doctoral students willing to take NLP courses). More broadly, we hope LexGLUE will speed up the adoption and transparent evaluation of new legal NLP methods and approaches in the commercial sector, too. Indeed, there have been many commercial press releases in the legal tech industry on high-performing systems, but almost no independent evaluation of the performance of machine learning and NLPbased tools. A standard publicly available benchmark would also allay concerns of undue influence in predictive models, including the use of metadata which the relevant law expressly disregards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The rapid growth of the legal text processing field is demonstrated by numerous papers presented in top-tier conferences in NLP and artificial intelligence <ref type="bibr" target="#b22">(Luo et al., 2017;</ref><ref type="bibr" target="#b60">Zhong et al., 2018;</ref><ref type="bibr">Chalkidis et al., 2019a;</ref><ref type="bibr" target="#b44">Valvoda et al., 2021)</ref> as well as surveys <ref type="bibr">(Chalkidis and Kampas, 2018;</ref><ref type="bibr">Zhong et al., 2020b;</ref><ref type="bibr" target="#b11">Bommarito et al., 2021)</ref>. Moreover, specialized workshops on NLP for legal text <ref type="bibr" target="#b1">(Aletras et al., 2019;</ref><ref type="bibr">Di Fatta et al., 2020;</ref><ref type="bibr">Aletras et al., 2020)</ref> are regularly organized.</p><p>A core task in this area has been legal judgment prediction (forecasting), where the goal is to predict the outcome (verdict) of a court case. In this direction, there have been at least three lines of work. The first one <ref type="bibr" target="#b2">(Aletras et al., 2016;</ref><ref type="bibr">Chalkidis et al., 2019a;</ref><ref type="bibr" target="#b28">Medvedeva et al., 2020</ref><ref type="bibr" target="#b27">Medvedeva et al., , 2021</ref> predicts violations of human rights in cases of the European Court of Human Rights (ECtHR). The second line of work <ref type="bibr" target="#b22">(Luo et al., 2017;</ref><ref type="bibr" target="#b60">Zhong et al., 2018;</ref><ref type="bibr" target="#b56">Yang et al., 2019)</ref> considers Chinese criminal cases where the goal is to predict relevant law articles, criminal charges, and the term of the penalty. The third line of work <ref type="bibr">(Ruger et al., 2004;</ref><ref type="bibr">Katz et al., 2017;</ref><ref type="bibr">Kaufman et al., 2019)</ref> includes methods for predicting the outcomes of cases of the Supreme Court of the United States (SCOTUS).</p><p>The same or similar tasks have also been studied with court cases in many other jurisdictions including France <ref type="bibr">(? ulea et al., 2017)</ref>, Philippines <ref type="bibr" target="#b46">(Virtucio et al., 2018)</ref>, Turkey <ref type="bibr" target="#b30">(Mumcuoglu et al., 2021)</ref>, Thailand <ref type="bibr" target="#b13">(Kowsrihawat et al., 2018)</ref>, United Kingdom <ref type="bibr" target="#b39">(Strickson and De La Iglesia, 2020)</ref>, Germany <ref type="bibr" target="#b43">(Urchs et al., 2021)</ref>, and Switzerland <ref type="bibr" target="#b32">(Niklaus et al., 2021)</ref>. Apart from predicting court decisions, there is also work aiming to interpret (explain) the decisions of particular courts <ref type="bibr" target="#b57">(Ye et al., 2018;</ref><ref type="bibr">Chalkidis et al., 2021c;</ref><ref type="bibr">Branting et al., 2021)</ref>.</p><p>Another popular task is legal topic classification. <ref type="bibr" target="#b31">Nallapati and Manning (2008)</ref> highlighted the challenges of legal document classification compared to more generic text classification by using a dataset including docket entries of US court cases. <ref type="bibr">Chalkidis et al. (2020a)</ref> classify EU laws into Eu-roVoc concepts, a task earlier introduced by Mencia and F?rnkranzand <ref type="formula">(2007)</ref>, with a special interest in few-and zero-shot learning. Luz de <ref type="bibr" target="#b23">Araujo et al. (2020)</ref> also studied topic classification using a dataset of Brazilian Supreme Court cases. There are similar interesting applications in contract law <ref type="bibr" target="#b17">(Lippi et al., 2019;</ref><ref type="bibr" target="#b42">Tuggener et al., 2020)</ref>.</p><p>Several studies <ref type="bibr" target="#b3">(Chalkidis et al., 2018</ref><ref type="bibr">(Chalkidis et al., , 2019c</ref><ref type="bibr">Hendrycks et al., 2021)</ref> explored information extraction from contracts, to extract important information such as the contracting parties, agreed payment amount, start and end dates, applicable law, etc. Other studies focus on extracting information from legislation <ref type="bibr">(Cardellino et al., 2017;</ref><ref type="bibr" target="#b3">Angelidis et al., 2018)</ref> or court cases <ref type="bibr" target="#b14">(Leitner et al., 2019)</ref>.</p><p>Legal Question Answering (QA) is another task of interest in legal NLP, where the goal is to train models for answering legal questions <ref type="bibr">(Kim et al., 2015;</ref><ref type="bibr">Ravichander et al., 2019;</ref><ref type="bibr">Kien et al., 2020;</ref><ref type="bibr">Zhong et al., 2020a,c;</ref><ref type="bibr" target="#b21">Louis and Spanakis, 2022)</ref>. Not only is this task interesting for researchers but it could support efforts to help laypeople better understand their legal rights. In the general task setting, this requires identifying relevant legislation, case law, or other legal documents, and extracting elements of those documents that answer a particular question. A notable venue for legal QA has been the Competition on Legal Information Extraction and Entailment (COLIEE) <ref type="bibr">(Kim et al., 2016;</ref><ref type="bibr">Kano et al., 2017</ref><ref type="bibr">Kano et al., , 2018</ref>.</p><p>More recently, there have also been efforts to pre-train Transformer-based language models on legal corpora <ref type="bibr">(Chalkidis et al., 2020b;</ref><ref type="bibr" target="#b59">Zheng et al., 2021;</ref>, leading to state-of-theart results in several legal NLP tasks, compared to models pre-trained on generic corpora.</p><p>Overall, the legal NLP literature is overwhelming, and the resources are scattered. Documentation is often not available, and evaluation measures vary across articles studying the same task. Our goal is to create the first unified benchmark to access the performance of NLP models on legal NLU. As a first step, we selected a representative group of tasks, using datasets in English that are also publicly available, adequately documented and have an appropriate size for developing modern NLP methods. We also introduce several simplifications to make the new benchmark more standardized and easily accessible, as already noted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LexGLUE Tasks and Datasets</head><p>We present the Legal General Language Understanding 2 Evaluation (LexGLUE) benchmark, a collection of datasets for evaluating model performance across a diverse set of legal NLU tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Desiderata</head><p>The datasets of LexGLUE were selected to satisfy the following desiderata:</p><p>? Language: In this first version of LexGLUE, we only consider English datasets, which also makes experimentation easier for researchers across the globe. We hope to include other languages in future versions of LexGLUE.</p><p>? Substance: 3 The datasets should check the ability of systems to understand and reason about legal text to a certain extent in order to perform tasks that are meaningful for legal practitioners.  where top-ranked models now achieve average scores higher than 90%). Unlike SuperGLUE <ref type="bibr" target="#b47">(Wang et al., 2019a)</ref>, we did not rule out, but rather favored, datasets requiring domain (in our case legal) expertise.</p><p>? Availability &amp; Size: We consider only publicly available datasets, documented by published articles, avoiding proprietary, untested, poorly documented datasets. We also excluded very small datasets, e.g., with fewer than 5K documents. Although large pre-trained models often perform well with relatively few task-specific training instances, newcomers may wish to experiment with simpler models that may perform disappointingly with small training sets. Small test sets may also lead to unstable and unreliable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tasks and Datasets</head><p>LexGLUE comprises seven datasets.  <ref type="bibr">1k, 2017-2019)</ref>. For each case, the dataset provides a list of factual paragraphs (facts) from the case description. Each case is mapped to articles of the ECHR that were violated (if any). In Task A, the input to a model is the list of facts of a case, and the output is the set of violated articles. In the most recent version of the dataset (Chalkidis et al., 2021c), each case is also mapped to articles of ECHR that were allegedly violated (considered by the court). In Task B, the input is again the list of facts of a case, but the output is the set of allegedly violated articles.</p><p>The total number of ECHR articles is currently 66. Several articles, however, cannot be violated, are rarely (or never) discussed in practice, or do not depend on the facts of a case and concern procedural technicalities. Thus, we use a simplified version of the label set (ECHR articles) in both Task A and B, including only 10 ECHR articles that can be violated and depend on the case's facts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCOTUS</head><p>The US Supreme Court (SCOTUS) 5 is the highest federal court in the United States of America and generally hears only the most controversial or otherwise complex cases which have not been sufficiently well solved by lower courts. We release a new dataset combining information from SCOTUS opinions 6 with the Supreme Court DataBase (SCDB) 7 <ref type="bibr" target="#b38">(Spaeth et al., 2020)</ref>. SCDB provides metadata (e.g., decisions, issues, decision directions) for all cases (from 1946 up to 2020). We opted to use SCDB to classify the court opinions in the available 14 issue areas (e.g., Criminal Procedure, Civil Rights, Economic Activity, etc.). This is a single-label multi-class classification task (Table 1). The 14 issue areas cluster 278 issues whose focus is on the subject matter of the controversy (dispute). The SCOTUS cases are chronologically split into training <ref type="bibr">(5k, 1946-1982)</ref>, development <ref type="bibr">(1.4k, 1982-1991)</ref>, test <ref type="bibr">(1.4k, 1991-2016)</ref>   <ref type="table">Table 2</ref>: Key specifications of the examined models. We report the number of parameters, the size of vocabulary, the maximum sequence length, the core pre-training specifications (training steps and batch size), and the training corpora (OWT = OpenWebText, BC = BookCorpus). Starred models have been warm-started from RoBERTa. document, the task is to predict its EuroVoc labels (concepts). The dataset is chronologically split in training <ref type="bibr">(55k, 1958-2010)</ref> CaseHOLD The CaseHOLD (Case Holdings on Legal Decisions) dataset <ref type="bibr" target="#b59">(Zheng et al., 2021)</ref> contains approx. 53k multiple choice questions about holdings of US court cases from the Harvard Law Library case law corpus. Holdings are short summaries of legal rulings that accompany referenced decisions relevant for the present case, e.g.:</p><p>". . . to act pursuant to City policy, re d 503, 506-07 (3d Cir.l985)(holding that for purposes of a class certification motion the court must accept as true all factual allegations in the complaint and may draw reasonable inferences therefrom)."</p><p>The input consists of an excerpt (or prompt) from a court decision, containing a reference to a particular case, where the holding statement (in boldface) is masked out. The model must identify the correct (masked) holding statement from a selection of five choices. We split the dataset in training (45k), development (3.9k), test (3.9k) sets, excluding samples that are shorter than 256 tokens. Chronological information is missing from CaseHOLD, thus we cannot perform a chronological re-split. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-trained Transformer Models</head><p>We experiment with Transformer-based <ref type="bibr" target="#b45">(Vaswani et al., 2017)</ref> pre-trained language models, which achieve state of the art performance in most NLP tasks <ref type="bibr">(Bommasani et al., 2021)</ref> and NLU benchmarks <ref type="bibr" target="#b47">(Wang et al., 2019a)</ref>. These models are pretrained on very large unlabeled corpora to predict masked tokens (masked language modeling) and typically also to perform other pre-training tasks that still do not require any manual annotation (e.g., predicting if two sentences were adjacent in the corpus or not, dubbed next sentence prediction). The pre-trained models are then fine-tuned (further trained) on task-specific (typically much smaller) annotated datasets, after adding task-specific layers. We fine-tune and evaluate the performance of the following publicly available models ( <ref type="table">Table 2)</ref>.</p><p>BERT (Devlin et al., 2019) is the best-known pretrained Transformer-based language model. It is pre-trained to perform masked language modeling and next sentence prediction.</p><p>RoBERTa <ref type="bibr" target="#b19">(Liu et al., 2019)</ref> is also a pre-trained Transformer-based language model. Unlike BERT, RoBERTa uses dynamic masking, it eliminates the next sentence prediction pre-training task, uses a larger vocabulary, and has been pre-trained on much larger corpora. <ref type="bibr" target="#b19">Liu et al. (2019)</ref> reported improved results on NLU benchmarks using RoBERTa, compared to BERT.</p><p>DeBERTa <ref type="bibr">(He et al., 2021)</ref> is another improved BERT model that uses disentangled attention, i.e., four separate attention mechanisms considering the content and the relative position of each token, and an enhanced mask decoder, which explicitly considers the absolute position of the tokens. De-BERTa has been reported to outperform BERT and RoBERTa in several NLP tasks <ref type="bibr">(He et al., 2021)</ref>.</p><p>Longformer <ref type="bibr" target="#b7">(Beltagy et al., 2020)</ref> extends Transformer-based models to support longer sequences, using sparse-attention. The latter is a combination of local (window-based) attention and global (dilated) attention that reduces the computational complexity of the model and thus can be deployed in longer documents (up to 4096 tokens). Longformer outperforms RoBERTa on long document tasks and QA benchmarks.</p><p>BigBird <ref type="bibr" target="#b58">(Zaheer et al., 2020)</ref> is another sparseattention based transformer that uses a combination of a local (window-based) attention, global (dilated), and random attention, i.e., all tokens also attend a number of random tokens on top of those in the same neighborhood (window) and the global ones. BigBird has been reported to outperform Longformer on QA and summarization tasks.</p><p>Legal-BERT (Chalkidis et al., 2020b) is a BERT model pre-trained on English legal corpora, consisting of legislation, contracts, and court cases. It uses the original pre-training BERT configuration. The sub-word vocabulary of Legal-BERT is built from scratch, to better support legal terminology.</p><p>CaseLaw-BERT <ref type="bibr" target="#b59">(Zheng et al., 2021)</ref> is another law-specific BERT model. It also uses the original pre-training BERT configuration and has been pre-trained from scratch on the Harvard Law case corpus, 12 which comprises 3.4M legal decisions from US federal and state courts. This model is called Custom Legal-BERT by <ref type="bibr" target="#b59">Zheng et al. (2021)</ref>. We call it CaseLaw-BERT to distinguish it from the previously published Legal-BERT of <ref type="bibr">Chalkidis et al. (2020b)</ref> and to better signal that it is trained exclusively on case law (court opinions).</p><p>Hierarchical Variants Legal documents are usually much longer (i.e., consisting of thousands of words) than other text types (e.g., tweets, customer reviews, news articles) often considered in various NLP tasks. Thus, standard Transformer-based models that can typically process up to 512 subword units cannot be directly applied across all LexGLUE datasets, unless documents are severely truncated to the model's limit. <ref type="figure" target="#fig_2">Figure 2</ref> shows the distribution of text input length across all LexGLUE datasets. Even for Transformer-based models specifically designed to handle long text (e.g., Longformer, BigBird), handling longer legal documents remains a challenge.</p><p>Given the length of the text input in three of the seven LexGLUE tasks, i.e., ECtHR (A and B) and SCOTUS, we employ a hierarchical variant of each pre-trained Transformer-based model that has not been designed for longer text (BERT, RoBERTa, DeBERTa, Legal-BERT, CaseLaw-BERT) during fine-tuning and inference. The hierarchical variants are similar to those of <ref type="bibr">Chalkidis et al. (2021c)</ref>. They use the corresponding pre-trained Transformer-based model to encode each paragraph of the input text independently and obtain the top-level representation h <ref type="bibr">[cls]</ref> of each paragraph. A second-level shallow <ref type="bibr">(2-layered)</ref> Transformer encoder with always the same (across BERT, RoBERTa, DeBERTa etc.) specifications (e.g., hidden units, number of attention heads) is fed with the paragraph representations to make them context-aware (aware of the surrounding paragraphs). We then max-pool over the context-aware paragraph representations to obtain a document representation, which is fed to a classification layer. 13</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Task-Specific Fine-Tuning</head><p>Text Classification Tasks For EUR-LEX, LEDGAR and UNFAIR-ToS tasks, we feed each document to the pre-trained model (e.g., BERT) and obtain the top-level representation h <ref type="bibr">[cls]</ref> of the special <ref type="bibr">[cls]</ref> token as the document representation, following Devlin et al. <ref type="formula">(2019)</ref>. The latter goes through a dense layer of L output units, one per label, followed by a sigmoid (in EUR-LEX, UNFAIR-ToS) or softmax (in LEDGAR) activation, respectively. For the two ECtHR tasks (A and B) and SCOTUS, where the hierarchical variants are employed, we feed the max-pooled (over paragraphs) document representation to a classification linear layer. The linear layer is again followed by a sigmoid (EctHR) or softmax (SCOTUS) activation.</p><p>Multiple-Choice QA Task For CaseHOLD, we convert each training (or test) instance (the prompt and the five candidate answers) into five input pairs following <ref type="bibr" target="#b59">Zheng et al. (2021)</ref>. Each pair consists of the prompt and one of the five candidate answers, separated by the special delimiter token <ref type="bibr">[sep]</ref>. The top-level representation h <ref type="bibr">[cls]</ref> of each pair is fed to a linear layer to obtain a logit, and the five logits are then passed through a softmax yielding a probability distribution over the five candidate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Data Repository and Code</head><p>For reproducibility purposes and to facilitate future experimentation with other models, we pre-process <ref type="bibr">13</ref> In Appendix D, we present results from preliminary experiments using the standard version of BERT for ECtHR Task A (-12.2%), Task B(-10.6%), and SCOTUS (-3.5%). and release all datasets on Hugging Face Datasets <ref type="bibr">(Lhoest et al., 2021)</ref>. 14 We also release the code 15 of our experiments, which relies on the Hugging Face Transformers <ref type="bibr" target="#b52">(Wolf et al., 2020)</ref> library. <ref type="bibr">16</ref> Appendix A explains how to load the datasets and run experiments with our code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Set Up</head><p>For TFIDF-based linear SVM models, we use the implementation of Scikit-learn <ref type="bibr" target="#b34">(Pedregosa et al., 2011)</ref> and grid-search for hyper parameters (number of features, C, and loss function). For all the pre-trained models, we use publicly available Hugging Face checkpoints. <ref type="bibr">17</ref> We use the *-base configuration of each pre-trained model, i.e., 12 Transformer blocks, 768 hidden units, and 12 attention heads. We train models with the Adam optimizer <ref type="bibr" target="#b12">(Kingma and Ba, 2015)</ref> and an initial learning rate of 3e-5 up to 20 epochs using early stopping on development data. We use mixed precision (fp16) to decrease the memory footprint in training and gradient accumulation for all hierarchical models. The hierarchical models can read up to 64 paragraphs of 128 tokens each. We use Longformer and BigBird in default settings, i.e., Longformer uses windows of 512 tokens and a single global token ([cls]), while BigBird uses blocks of 64 tokens (windows: 3? block, random: 3? block, global: 2? initial block; each token attends 512 tokens in total). The batch size is 8 in all experiments. We run five repetitions with different random seeds and report the test scores based on the seed with the best scores on development data. We evaluate performance using micro-F1 (?-F 1 ) and macro-F1 (m-F 1 ) across all datasets to take into account class imbalance. For completeness, we also report the arithmetic, harmonic, and geometric mean across tasks following Shavrina and Malykh (2021). 18 <ref type="table" target="#tab_6">Table 3 presents the test results for  all models across all LexGLUE tasks, while Table 4</ref> Method ECtHR (A)* ECtHR (B)* SCOTUS* EUR-LEX LEDGAR UNFAIR-ToS CaseHOLD    presents the aggregated (averaged) results. We observe that the two legal-oriented pre-trained models (Legal-BERT, CaseLaw-BERT) perform overall better, especially considering m-F 1 that accounts for class imbalance (considers all classes equally important). Their in-domain (legal) knowledge seems to be more critical in the two datasets relying on US case law data (SCOTUS, CaseHOLD) with an improvement of approx. +2-4% p.p. (m-F 1 ) over equally sized Transformer-based models, which are pre-trained on generic corpora. These results are explained by the fact that these tasks are more domain-specific in terms of language, compared to the rest. No single model performs best in all tasks, and the results of <ref type="table" target="#tab_6">Table 3</ref> show that there is still large scope for improvement (Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><formula xml:id="formula_0">?-F 1 m-F 1 ?-F 1 m-F 1 ?-F 1 m-F 1 ?-F 1 m-F 1 ?-F 1 m-F 1 ?-F 1 m-F 1 ?-F 1 / m-F 1<label>TFIDF+SVM</label></formula><formula xml:id="formula_1">Method A-Mean H-Mean G-Mean ?-F 1 m-F 1 ?-F 1 m-F 1 ?-F 1 m-F 1<label>BERT</label></formula><p>An exceptional case of the dominance of the pretrained Transformer models is the SCOTUS dataset, where the TFIDF-based linear SVM performs best. We suspect the large size of the SCOTUS opinions ( <ref type="figure" target="#fig_2">Figure 2</ref>) to be the main reason, i.e., in many cases full paragraphs or parts of them are not considered by the hierarchical models (limited to 64 paragraphs of 128 tokens each).</p><p>Legal-oriented Models Interestingly, the performance of Legal-BERT and CaseLaw-BERT, the two legal-oriented pre-trained models, is almost identical on CaseHOLD, despite the fact that CaseLaw-BERT is solely trained on US case law. On the other hand, Legal-BERT has been exposed to a wider variety of legal corpora, including EU and UK legislation, ECtHR, ECJ and US court cases, and US contracts. Legal-BERT performs as well as or better than CaseLaw-BERT on all datasets. These results suggest that domain-specific pre-training (and learning a domain-specific subword vocabulary) is beneficial, but over-fitting a specific (niche) sub-domain (e.g., US case law), similarly to <ref type="bibr" target="#b59">Zheng et al. (2021)</ref>, has no benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Vision -Future Considerations</head><p>Beyond the scope of this work and the examined baseline models, we identify four major factors that could potentially advance the state of the art in LexGLUE and legal NLP more generally:</p><p>Long Documents: Several Transformer-based models <ref type="bibr" target="#b7">(Beltagy et al., 2020;</ref><ref type="bibr" target="#b58">Zaheer et al., 2020;</ref><ref type="bibr" target="#b18">Liu et al., 2022)</ref> have been proposed to handle long documents by exploring sparse attention mechanisms. These models can handle sequences up to 4096 sub-words, which is largely exceeded in three out of seven LexGLUE tasks <ref type="figure" target="#fig_2">(Figure 2)</ref>. Contrary, the hierarchical model of Section 4.2 can handle sequences up to 8192 sub-words in our experiments, but a part of the model (the additional Transformer blocks that make the paragraph embeddings aware of the other paragraphs) is not pre-trained, which possibly negatively affects performance.</p><p>Structured Text: Current models for long documents, like Longformer and BigBird, do not consider the document structure (e.g., sentences, paragraphs, sections). For example, window-based attention may consider a sequence of sentences across paragraph boundaries or even consider truncated sentences. To exploit the document structure,  proposed SMITH, a hierarchi-cal Transformer model that hierarchically encodes increasingly larger blocks (e.g., words, sentences, documents). SMITH is very similar to the hierarchical model of Section 4.2, but it is pre-trained endto-end with two objectives: token-level masked and sentence block language modeling.</p><p>Large-scale Legal Pre-training: Recent studies <ref type="bibr">(Chalkidis et al., 2020b;</ref><ref type="bibr" target="#b59">Zheng et al., 2021;</ref><ref type="bibr" target="#b6">Bambroo and Awasthi, 2021;</ref> introduced language models pre-trained on legal corpora, but of relatively small sizes, i.e., 12-36 GB. In the work of <ref type="bibr" target="#b59">Zheng et al. (2021)</ref>, the pre-training corpus covered only a narrowly defined area of legal documents, US court opinions. The same applies to Lawformer , which was pre-trained on Chinese court opinions. Future work could curate and release a legal version of the C4 corpus <ref type="bibr" target="#b36">(Raffel et al., 2020)</ref>, containing multijurisdictional legislation, court decisions, contracts and legal literature at a size of hundreds of GBs. Given such a corpus, a large language model capable of processing long structured text could be pre-trained and it might excel in LexGLUE.</p><p>Even Larger Language Models: Scaling up the capacity of pre-trained models has led to increasingly better results in general NLU benchmarks (Kaplan et al., 2020), and models have been scaled up to billions of parameters <ref type="bibr">(Brown et al., 2020;</ref><ref type="bibr" target="#b36">Raffel et al., 2020;</ref><ref type="bibr">He et al., 2021)</ref>. In Appendix E, we observe that using the large version of RoBERTa leads to substantial performance improvements compared to the base version. The results are comparable or better -in some casescompared to the legal-oriented language models (Legal-BERT, CaseLaw-BERT). Considering that the two legal-oriented models are much smaller and have been pre-trained with (5?10?) less data (Section 2), we have a strong indication for performance gains by pre-training larger legal-oriented models using larger legal corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations and Future Work</head><p>Although, our benchmark inevitably cannot cover "everything in the whole wide (legal) world" (Raji et al., 2021), we include a representative collection of English datasets that also ground to a certain degree in practically interesting applications.</p><p>In its current version, LexGLUE can only be used to evaluate English models. As legal documents are typically written in the official language of the particular country of origin, there is an increasing need for developing models for other languages. The current scarcity of datasets in other languages (with the exception of Chinese) makes a multilingual extension of LexGLUE challenging, but an interesting avenue for future research.</p><p>Beyond language barriers, legal restrictions currently inhibit the creation of more datasets. Important document types, such as contracts and scholarly publications are protected by copyright or considered trade secrets. As a result, their owners are concerned with data-leakage when they are used for model training and evaluation. Providing both legal and technical solutions, e.g., using privacy-aware infrastructure and models <ref type="bibr">(Downie, 2004;</ref><ref type="bibr">Feyisetan et al., 2020)</ref> is a challenge to be addressed.</p><p>Access to court decisions can also be hindered by bureaucratic inertia, outdated technology and data protection concerns, which collectively result in these otherwise public decisions not being publicly available <ref type="bibr" target="#b33">(Pah et al., 2020)</ref>. While the anonymization of personal data provides a solution to this problem, it is itself an open challenge for legal NLP (Jana and Biemann, 2021). In lack of suitable datasets and benchmarks, we have refrained from including anonymization in this version of LexGLUE, but plan to do so at a later stage.</p><p>Another limitation of the current version of LexGLUE is that human evaluation is missing. All datasets rely on ground truth labels automatically extracted from data (e.g., court decisions) produced as part of official judicial or archival procedures. These resources should be highly reliable (valid), but we cannot statistically assess their quality. In the future, re-annotating part of the datasets with multiple legal experts would provide an estimation of human level performance and inter-annotator agreement, though the cost would be high, because of the required legal expertise.</p><p>While LexGLUE offers a much needed unified testbed for legal NLU, there are several other critical aspects that need to be studied carefully. These include multi-disciplinary research to better understand the limitations and challenges of applying NLP to law <ref type="bibr" target="#b10">(Binns, 2020)</ref>, while also considering fairness and robustness <ref type="bibr" target="#b4">(Angwin et al., 2016;</ref><ref type="bibr">Dressel and Farid, 2018;</ref><ref type="bibr" target="#b5">Baker Gillis, 2021;</ref><ref type="bibr" target="#b50">Wang et al., 2021;</ref><ref type="bibr">Chalkidis et al., 2022)</ref>, and broader legal considerations of AI technologies in general <ref type="bibr">(Schwemer et al., 2021;</ref><ref type="bibr" target="#b41">Tsarapatsanis and Aletras, 2021;</ref><ref type="bibr">Delacroix, 2022)</ref>. Education and Research (BMBF) kmu-innovativ program under funding code 01IS18085. We would like to thank Desmond Elliott for providing valuable feedback (baselines for truncated documents presented in Appendix D), Xiang Dai and Joel Niklaus for reviewing and pointing out issues in the new resources (code, datasets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement Original Work Attribution</head><p>All datasets included in LexGLUE, except SCO-TUS, are publicly available and have been previously published. If datasets or the papers that introduced them were not compiled or written by ourselves, we referenced the original work and encourage LexGLUE users to do so as well. In fact, we believe this work should only be referenced, in addition to citing the original work, when experimenting with multiple LexGLUE datasets and using the LexGLUE evaluation infrastructure. Otherwise only the original work should be cited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Social Impact</head><p>We believe that this work does not contain any grounds for ethical concerns. A transparent and rigorous benchmark for NLP in the legal domain might serve as an orientation for scholars and industry researchers. As a result, the capabilities of tools that are trained using natural language data from the legal domain will become clearer, thereby helping their users to better understand them. This increased certainty would also raise the awareness within research and industry communities to potential risks associated with the use of these tools. We regard this contribution to a more realistic, more informed discussion as an important use case of the work presented. Ideally, it could help both beginners and seasoned professionals to understand the limitations of using NLP tools in the legal domain and thereby prevent exaggerated expectations and potential applications that might risk endangering fundamental rights or the rule of law. We currently cannot imagine use cases of this particular work that would lead to ethical concerns or potential harm <ref type="bibr" target="#b41">(Tsarapatsanis and Aletras, 2021)</ref>. 19 https://innovationsfonden.dk/en</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Licensing &amp; Personal Information</head><p>LexGLUE comprises seven datasets: ECtHR Task A and B, SCOTUS, EUR-LEX, LEDGAR, UNFAIR-ToS, and CaseHOLD that are available for re-use and re-share with appropriate attribution. The data is in general partially anonymized in accordance with the applicable national law. The data is considered to be in the public sphere from a privacy perspective. This is a very sensitive matter, as the courts try to keep a balance between transparency (the public's right to know) and privacy (respect for private and family life).</p><p>ECtHR contains personal data of the parties and other people involved in the legal proceedings. Its data is processed and made public in accordance with the European data protection laws. This includes either implied consent or legitimate interest to process the data for research purposes. As a result, their processing by us or other future users of the benchmark is not likely to raise ethical concerns.</p><p>SCOTUS contains personal data of a similar nature. Again, the data is processed and made available by the US Supreme Court, whose proceedings are public. While this ensures compliance with US law, it is very likely that similarly to the ECtHR any processing could be justified by either implied consent or legitimate interest under European law.</p><p>EUR-LEX by contrast is merely a collection of legislation material and therefore not likely to contain personal data, except signatory information (e.g., president of EC). It is openly published by the European Union and processed by the EU's Publication Office. In addition, since our work qualifies as research, it is privileged pursuant to Art. 6 (1) (f) GDPR.</p><p>LEDGAR contains publicly available contract provisions published in the EDGAR database of the US Securities and Exchange Commission (SEC). As far as personal information might be contained, it should equally fall into the public sphere and be covered by research privilege. Our processing does not focus on personal information at all, rather attributing content labels to provisions.</p><p>UNFAIR-ToS contains Terms of Services from business entities such as YouTube, Ebay, Facebook, etc., which makes it unlikely for the data to include personal information. These companies keep user data separate from contractual provisions, so to the best of our knowledge not contained in this dataset.</p><p>CaseHOLD contains parts of legal decisions from US Court decisions, obtained from the Harvard library case law corpus. All of the decisions were previously published in compliance with US law. In addition, most instances (case snippets) are too short to contain identifiable information. Should such data be contained, their processing would equally be covered either by implicit consent or a public interest exception. We use all datasets in accordance with copyright terms and under the licenses set forth by their creators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations &amp; Potential Harms</head><p>We have not employed any crowd-workers or annotators for this work. The paper outlines the main limitations with regard to speaker population (English) and generalizability in a dedicated section (Section 7). As a benchmark paper, our claims naturally match the results of the experiments, which -given the current detail of instructions -should be easily reproduced. We provide several ways of accessing the datasets and running the experiments both with and without Hugging Face infrastructure. We do not currently foresee any potential harms for vulnerable or marginalized populations and we do not use, to the best of our knowledge, any identifying characteristics for populations of these kinds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets, Code, and Participation</head><p>Where are the datasets? We provide access to LexGLUE on Hugging Face Datasets <ref type="bibr">(Lhoest et al., 2021)</ref> at https://huggingface.co/datasets/ lex_glue. For example, to load the SCOTUS dataset, you first simply install the datasets Python library and then make the following call:</p><p>___________________________________________________ from datasets import load_dataset dataset = load_dataset('lex_glue', task='scotus')</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>___________________________________________________</head><p>How do I run experiments? To make reproducing the results of the already examined models or future models even easier, we release our code on GitHub (https://github.com/coastalcph/ lex-glue). In that repository (in the folder /experiments), there are Python scripts, relying on the Hugging Face Transformers library <ref type="bibr" target="#b52">(Wolf et al., 2020)</ref>, to run and evaluate any Transformer-based model (e.g., BERT, RoBERTa, LegalBERT, and their hierarchical variants, as well as, Longformer, and BigBird). We also provide bash scripts to replicate the experiments for each dataset with 5 random seeds, as we did for the reported results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B No labeling as an additional class</head><p>In ECtHR Tasks A &amp; B and UNFAIR-ToS, there are unlabeled samples. Concretely, in ECtHR Task A, a possible event is no violation, i.e., the court ruled that the defendant did not violate any ECHR article. Contrary, no violation is not a possible event in the original ECtHR Task B dataset, i.e., at least a single ECHR article is allegedly violated (considered by the court) in every case; however, there is such a rare scenario after the simplifications we introduced, i.e., some cases were originally labeled only with rare labels that were excluded from our benchmark (Section 3.2). In UNFAIR-ToS, the vast majority of sentences are not labeled with any type of unfairness (unfair term against users), i.e., most sentences do not raise any questions of possible violations of the European consumer law.</p><p>In multi-label classification, the set of labels per instance is represented as a one-hot vector Y = [y 1 , y 2 , . . . , y L ], where y i = 1 if the instance is labeled with the i-th class, and y i = 0 otherwise. If an instance is not labeled with any class, its Y includes only zeros. During training, binary cross-entropy correctly penalizes such instances, if the predictions (? = [? 1 ,? 2 . . . ,? L ]) diverge from zeros. During evaluation, however, the F1-score (F1 = TP TP+ 1 2 (FP+FN) ) ignores instances with Y =? = [0, 0, . . . , 0], because it considers only the true positives (TP), false positives (FP), and false negatives (FN), and instances where Y =? = [0, 0, . . . , 0] contribute no TPs, FPs, FNs. In order to make F1 sensitive to the correct labeling of such examples, during evaluation (not training) we include an additional label (y 0 or? 0 ) in both targets (Y) and predictions (?), whose value is 1 (positive) if the original (without y 0 ,? 0 ) Y and? are Y = [0, 0, . . . , 0] or? = [0, 0, . . . , 0], respectively, and 0 (negative) otherwise. This is particularly important for proper evaluation, as across three datasets a considerable portion of the examples are unlabeled (11.5% in ECtHR Task A, 1.6% in ECtHR Task B, and 95.5% in UNFAIR-ToS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Results</head><p>Tables 5 and 6 show development results for all examined models across datasets. We report the mean and standard deviations (?) for the three seeds (among the five used) with the best development scores per model to exclude catastrophic failures, i.e., runs with severely low performance. The standard deviations are relatively low across models and datasets (up to 0.5% for ?-F 1 and up to 1% for m-F 1 ). The development results are generally higher compared to the test ones (cf. <ref type="table" target="#tab_6">Table 3</ref>) in many cases, as one would expect.     <ref type="figure">A, B)</ref> and SCOTUS, i.e., the datasets with long documents. Light blue denotes the average score across 5 runs for the hierarchical variant (used in <ref type="table" target="#tab_6">Table 3</ref> for these datasets), while dark blue corresponds to standard BERT (not used in in <ref type="table" target="#tab_6">Table 3</ref> for these datasets). The error bars show the standard error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Use of 512-token BERT models</head><p>In <ref type="figure" target="#fig_3">Figure 3</ref>, we show results for the standard BERT model of <ref type="bibr">Devlin et al. (2019)</ref>, which can process up to 512 tokens, compared to its hierarchical variant (Section 4.2), which can process up to 64?128 tokens. We observe that across all datasets that contain long documents (ECtHR A &amp; B, SCOTUS, cf. <ref type="figure" target="#fig_2">Fig. 2(a)</ref>), the hierarchical variant clearly outperforms the standard model fed with truncated documents (ECtHR A: +10.2% p.p., ECtHR B: 7.5% p.p., SCOTUS: 4.9% p.p.). Compared to the ECtHR tasks, the gains are lower in SCOTUS, a topic classification task where long-range reasoning is not needed; by contrast, for ECtHR multiple distant facts need to be combined. Based on these results, we conclude that using severely truncated documents is not a plausible option for LexGLUE, and other directions for processing long documents should be considered in the future, ideally fully pre-trained hierarchical models, contrary to our semi-pre-trained hierarchical models (Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Use of Roberta Large</head><p>We additionally evaluate RoBERTa-large, i.e., 24 Transformer blocks, 1024 hidden units, and 18 attention heads, to better understand the dynamics between domain specificity and model size. In this  <ref type="table" target="#tab_1">T  T/e  T  T/e  T  T/e  T  T/e  T  T/e  T  T/e   BERT  3h 42m 28m  3h 9m  28m 1h 24m 11m 3h 36m 19m  6h 9m  21m 4h 24m 24m  RoBERTa  4h 11m 27m 3h 43m 27m 2h 46m 17m 3h 36m 19m 6h 22m 21m 4h 21m 24m  DeBERTa  7h 43m 46m 6h 48m 46m 3h 42m 29m 5h 34m 36m 9h 29m 40m 6h 42m 45m   Longformer  6h 47m 56m 7h 31m 56m 6h 27m 34m 11h 10m 45m 15h 47m 50m 4h 45m 30m  BigBird  8h 41m 1h 2m 8h 17m 1h 2m 5h 51m 37m 3h 57m 24m 8h 13m 27m 6h 4m 49m</ref> Legal-BERT 3h 52m 28m 3h 2m 28m 2h 2m 17m 3h 22m 19m 5h 23m 21m 4h 13m 23m CaseLaw-BERT 3h 2m 28m 2h 57m 28m 2h 34m 34m 3h 40m 19m 6h 8m 21m 4h 21m 24m   case, we use the AdamW optimizer with a 1e-5 maximum learning rate, warm-up ratio of 0.1, and a weight decay rate of 0.06, and we use a similar mini-batch size of 8 examples. 20 <ref type="table" target="#tab_14">Table 8</ref> reports the development and test results using the seed (run) with the best development scores. We observe that using the large version of RoBERTa, dubbed RoBERTa (L), with more than 2? parameters (355M), leads to substantial performance improvements compared to the base version of RoBERTa, dubbed RoBERTa (B), in many tasks. The results are comparable, or better in some cases, compared to the legal-oriented language models (Legal-BERT, CaseLaw-BERT). Considering that the two legal-oriented models are much smaller and have been pre-trained with (5?10?) less data (Section 2), we have a strong indication for performance gains by pre-training larger legal-oriented models using larger legal corpora (Section 6). <ref type="bibr">20</ref> Large models tend to be very sensitive to parameter updates, especially in the initial training steps; hence a smaller learning rate and warm up steps are very crucial. instance, we consider pairs that consist of a question and a paragraph, separated by the special delimiter token <ref type="bibr">[sep]</ref>. The top-level representations [h 1 , . . . , h N ] of the tokens of the paragraph are fed into a linear layer to obtain two logits per token (for the token being the start or end of the answer span), which are then passed through a softmax activation (separately for start and end) to obtain probability distributions. The tokens with the highest start and end probabilities are selected as boundaries of the answer span. We evaluated performance with token-level F1 score, similarly to SQUAD.</p><p>We trained all the models of <ref type="table">Table 2</ref>, which scored approx. 10-20% in token-level F1, with Legal-BERT performing slightly better than the rest (+5% F1). <ref type="bibr">22</ref> In the paper that introduced CUAD (Hendrycks et al., 2021), several other measures (Precision@ N% Recall, AUPR, Jaccard similarity) are used to more leniently estimate a model's ability to approximately locate answers in context paragraphs. Through careful manual inspection of the dataset, we noticed the following points that seem to require more careful consideration.</p><p>? Contractual insights (categories, shown in italics below) include both entity-level (short) answers (e.g., "SERVICE AGREEMENT" for Document Name, and "Imprimis Pharmaceuticals, Inc." for Parties) and paragraph-level (long) answers (e.g., "If any of the conditions specified in Section 8 shall not have been fulfilled when and as required by this Agreement, or by the Closing Date, or waived in writing by Capital Resources, this Agreement and all of Capital Resources obligations hereunder may be canceled [...] except as otherwise provided in Sections 2, 7, 9 and 10 hereof." for Termination for Convenience).</p><p>These two different types of answers (short and paragraph-long) seem to require different models and different evaluation measures, unlike how they are treated in the original CUAD paper.</p><p>? Some contractual insights (categories), e.g., Parties, have been annotated with both short (e.g., "Imprimis Pharmaceuticals, Inc.") and long (e.g., "together, Blackwell and Munksgaard shall be referred to as 'the Publishers'.") answers. Annotations of this kind introduce noise during both training and evaluation. For example, it becomes unclear when a short (finer/strict) or a long (loose) annotation should be taken to be the correct one.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>LexGLUE: A new benchmark dataset to evaluate the capabilities of NLU models on legal text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Our first baseline model is a linear Support Vector Machine (SVM) (Cortes and Vapnik, 1995) with TF-IDF features for the top-K frequent n-grams of the training set, where n ?<ref type="bibr" target="#b64">[1,</ref><ref type="bibr" target="#b65">2,</ref> 3].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Distribution of text input length, measured in BERT sub-word units, across LexGLUE datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Development m-F 1 scores of standard BERT (up to 512 tokens) and its hierarchical variant (Section 4.2, 64?128 tokens) in ECtHR (Task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>1 m-F 1 ?-F 1 m-F 1 ?-F 1 m-F 1 ?-F 1 m-F 1 ?-F 1 m-F 1 ?-F 1 m-F 1 ?-F 1 / m-F 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the LexGLUE datasets, including simplifications made.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>shows</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>64.5 51.7 74.6 65.1 78.2 69.5 71.3 51.4 87.2 82.4 95.4 78.8 n/a BERT 71.2 63.6 79.7 73.4 68.3 58.3 71.4 57.2 87.6 81.8 95.6 81.3 70.8 RoBERTa 69.2 59.0 77.3 68.9 71.6 62.0 71.9 57.9 87.9 82.3 95.2 79.2 71.4 DeBERTa 70.0 60.8 78.8 71.0 71.1 62.7 72.1 57.4 88.2 83.1 95.5</figDesc><table><row><cell></cell><cell></cell><cell>80.3</cell><cell>72.6</cell></row><row><cell>Longformer</cell><cell>69.9 64.7 79.4 71.7 72.9 64.0 71.6 57.7 88.2 83.0 95.5</cell><cell>80.9</cell><cell>71.9</cell></row><row><cell>BigBird</cell><cell>70.0 62.9 78.8 70.9 72.8 62.0 71.5 56.8 87.8 82.6 95.7</cell><cell>81.3</cell><cell>70.8</cell></row><row><cell>Legal-BERT</cell><cell>70.0 64.0 80.4 74.7 76.4 66.5 72.1 57.4 88.2 83.0 96.0</cell><cell>83.0</cell><cell>75.3</cell></row><row><cell cols="2">CaseLaw-BERT 69.8 62.9 78.8 70.3 76.6 65.9 70.7 56.6 88.3 83.0 96.0</cell><cell>82.3</cell><cell>75.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Test results for all examined models across LexGLUE tasks. In starred datasets, we use the hierarchical variant of each model, except for Longformer and BigBird, discussed in Section 4.2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Test scores aggregated over tasks: arithmetic (A), harmonic (H), and geometric (G) mean.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Emmanouil Fergadiotis, Prodromos Malakasiotis, and Ion Androutsopoulos. 2019b. Large-scale multi-label text classification on EU legislation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6314-6322, Florence, Italy. Legal English. Routledge. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: Decoding-enhanced bert with disentangled attention. In International Conference on Learning Representations.</figDesc><table><row><cell>Cristian Cardellino, Milagro Teruel, Laura Alonso Ale-many, and Serena Villata. 2017. Legal NERC with ontologies, Wikipedia and curriculum learning. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Lin-guistics: Volume 2, Short Papers, pages 254-259, Valencia, Spain. Association for Computational Lin-guistics. Ilias Chalkidis and Ion Androutsopoulos. 2017. A deep learning approach to contract element extraction. In Proceedings of the 30th International Conference on Legal Knowledge and Information Systems (JURIX 2017), Luxembourg City, Luxembourg. Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. 2019a. Neural legal judgment prediction in English. In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguis-tics, pages 4317-4323, Florence, Italy. Association for Computational Linguistics. Ilias Chalkidis, Ion Androutsopoulos, and Achilleas Michos. 2018. Obligation and prohibition extrac-tion using hierarchical RNNs. In Proceedings of the 56th Annual Meeting of the Association for Com-putational Linguistics (Volume 2: Short Papers), pages 254-259, Melbourne, Australia. Association for Computational Linguistics. Association for Computational Linguistics, Dublin, Ireland. Yanguang Chen, Yuanyuan Sun, Zhihao Yang, and Hongfei Lin. 2020. Joint entity and relation extrac-tion for legal documents with legal feature enhance-ment. In Proceedings of the 28th International Con-ference on Computational Linguistics, pages 1561-1571, online. Alexis Conneau and Douwe Kiela. 2018. SentEval: An evaluation toolkit for universal sentence repre-sentations. In Proceedings of the Eleventh Interna-tional Conference on Language Resources and Eval-uation (LREC-2018), Miyazaki, Japan. European Languages Resources Association (ELRA). Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine Learning, pages 273-297. Corinna Coupette, Janis Beckedorf, Dirk Hartung, Michael Bommarito, and Daniel Martin Katz. 2021. Measuring law over time: A network analytical framework with an application to statutes and regu-lations in the United States and Germany. Frontiers in Physics, 9:269. Sylvie Delacroix. 2022. Diachronic interpretability and machine learning systems. Journal of Cross-disciplinary Research in Computational Law, 1(1). Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ-ation for Computational Linguistics. Giuseppe Di Fatta, Victor Sheng, and Alfredo Cuz-zocrea. 2020. The IEEE ICDM 2020 workshops. In 2020 International Conference on Data Mining Workshops (ICDMW), pages 26-29. IEEE. John S. Downie. 2004. IMIRSEL: a secure music re-trieval testing environment. In Internet Multimedia Management Systems V, volume 5601, pages 91 -99. International Society for Optics and Photonics, SPIE. Julia Dressel and Hany Farid. 2018. The accuracy, fair-ness, and limits of predicting recidivism. Science Advances, 4(10). Oluwaseyi Feyisetan, Borja Balle, Thomas Drake, and Tom Diethe. 2020. Privacy-and utility-preserving textual analysis via calibrated multivariate perturba-tions. In Proceedings of the 13th International Con-ference on Web Search and Data Mining, pages 178-186. Inioluwa Deborah Raji, Emily Denton, Emily M. Ben-der, Alex Hanna, and Amandalynne Paullada. 2021. AI and the everything in the whole wide world benchmark. In Thirty-fifth Conference on Neural In-formation Processing Systems Datasets and Bench-marks Track (Round 2). Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natu-ral Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics. Abhilasha Ravichander, Alan W Black, Shomir Wilson, Thomas Norton, and Norman Sadeh. 2019. Ques-tion answering for privacy policies: Combining com-putational and legal perspectives. In Proceedings of the 2019 Conference on Empirical Methods in Nat-ural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4947-4958, Hong Kong, China. Rupert Haigh. 2018. Dan Hendrycks, Collin Burns, Anya Chen, and Theodore W Ruger, Pauline T Kim, Andrew D Mar-tin, and Kevin M Quinn. 2004. The supreme court forecasting project: Legal and political science ap-proaches to predicting supreme court decisionmak-Ilias Chalkidis, Ilias Chalkidis, Manos Fergadiotis, and Ion Androut-sopoulos. 2021a. MultiEURLEX -a multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Ilias Chalkidis, Manos Fergadiotis, Sotiris Kotitsas, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. 2020a. An empirical study on large-scale multi-label text classification including few and zero-shot labels. In Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 7503-7515, On-line. Ilias Chalkidis, Manos Fergadiotis, Prodromos Malaka-siotis, Nikolaos Aletras, and Ion Androutsopoulos. 2020b. LEGAL-BERT: The muppets straight out of law school. In Findings of the Association for Com-putational Linguistics: EMNLP 2020, pages 2898-2904, Online. Ilias Chalkidis, Manos Fergadiotis, Prodromos Malaka-siotis, and Ion Androutsopoulos. 2019c. Neural con-tract element extraction revisited. In Proceedings of the Document Intelligence Workshop at NeurIPS 2019, Vancouver, Canada. Ilias Chalkidis, Manos Fergadiotis, Nikolaos Manginas, Eva Katakalou, and Prodromos Malakasiotis. 2021b. Regulatory compliance through Doc2Doc informa-tion retrieval: A case study in EU/UK legislation where text similarity has limitations. In Proceed-ings of the 16th Conference of the European Chap-ter of the Association for Computational Linguistics: Main Volume, pages 3498-3511, Online. Associa-tion for Computational Linguistics. Ilias Chalkidis, Manos Fergadiotis, Dimitrios Tsarapat-sanis, Nikolaos Aletras, Ion Androutsopoulos, and Prodromos Malakasiotis. 2021c. Paragraph-level rationale extraction through regularization: A case study on european court of human rights cases. In Proceedings of the Annual Conference of the North American Chapter of the Association for Computa-tional Linguistics, online. Ilias Chalkidis and Dimitrios Kampas. 2018. Deep learning in law: early adaptation and legal word em-beddings trained on large corpora. Artificial Intelli-gence and Law, 27(2):171-198. Ilias Chalkidis, Tommaso Passini, Sheng Zhang, Letizia Tomada, Sebastian Felix Schwemer, and An-ders S?gaard. 2022. Fairlex: A multilingual bench-mark for evaluating fairness in legal text processing. In Proceedings of the 60th Annual Meeting of the Spencer Ball. 2021. CUAD: An expert-annotated NLP dataset for legal contract review. In Thirty-fifth Conference on Neural Information Processing Sys-tems Datasets and Benchmarks Track (Round 1). Abhik Jana and Chris Biemann. 2021. An investigation towards differentially private sequence tagging in a federated framework. In Proceedings of the Third Workshop on Privacy in Natural Language Process-ing, pages 30-35. Ken Satoh. 2017. Overview of coliee 2017. In COL-IEE@ ICAIL, pages 1-8. Yoshinobu Kano, Mi-Young Kim, Masaharu Yosh-ioka, Yao Lu, Juliano Rabelo, Naoki Kiyota, Randy Goebel, and Ken Satoh. 2018. Coliee-2018: Eval-uation of the competition on legal information ex-traction and entailment. In JSAI International Sym-posium on Artificial Intelligence, pages 177-192. Springer. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. CoRR, abs/2001.08361. Daniel Martin Katz, Michael J Bommarito, and Josh Blackman. 2017. A general approach for predicting the behavior of the supreme court of the united states. PloS one, 12(4):e0174698. Daniel Martin Katz, Corinna Coupette, Janis Becke-dorf, and Dirk Hartung. 2020. Complex societies and the growth of the law. Scientific Reports, 10:18737. Aaron Russell Kaufman, Peter Kraft, and Maya Sen. 2019. Improving supreme court forecasting us-ing boosted decision trees. Political Analysis, 27(3):381-387. Phi Manh Kien, Ha-Thanh Nguyen, Ngo Xuan Bach, Vu Tran, Minh Le Nguyen, and Tu Minh Phuong. 2020. Answering legal questions by learning neu-ral attentive text representation. In Proceedings of the 28th International Conference on Computational Linguistics, pages 988-998, online. Mi-Young Kim, Randy Goebel, Yoshinobu Kano, and Ken Satoh. 2016. Coliee-2016: evaluation of the competition on legal information extraction and entailment. In International Workshop on Juris-informatics (JURISIN 2016). Mi-young Kim, Ying Xu, and Randy Goebel. 2015. A Convolutional Neural Network in Legal Question Answering. Ninth International Workshop on Juris-informatics (JURISIN). ing. Columbia Law Review, pages 1150-1210. Sebastian Felix Schwemer, Letizia Tomada, and Tom-maso Pasini. 2021. Legal ai systems in the eu's pro-posed artificial intelligence act. In In Joint Proceed-ings of the Workshops on Automated Semantic Anal-ysis of Information in Legal Text (ASAIL 2021) and AI and Intelligent Assistance for Legal Professionals in the Digital, Workplace (LegalAIIA 2021). Tatiana Shavrina and Valentin Malykh. 2021. How not to lie with a benchmark: Rearranging NLP learder-Yoshinobu Kano, Mi-Young Kim, Randy Goebel, and boards.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc>reports training times per dataset and model; both the time per epoch (T/e), and the total training time (T ) across all epochs. All fullattention BERT models, except Longformer and</figDesc><table><row><cell>Method</cell><cell cols="5">ECtHR (A)* ECtHR (B)* SCOTUS* EUR-LEX LEDGAR UNFAIR-ToS CaseHOLD</cell></row><row><cell>BERT</cell><cell>71.0 ? 0.7</cell><cell>79.6 ? 0.5</cell><cell>72.7 ? 0.2 77.3 ? 0.2 87.9 ? 0.1</cell><cell>95.5 ? 0.0</cell><cell>72.8 ? 0.1</cell></row><row><cell>RoBERTa</cell><cell>70.4 ? 0.5</cell><cell>78.4 ? 0.7</cell><cell>76.9 ? 0.6 77.6 ? 0.0 88.1 ? 0.1</cell><cell>94.8 ? 0.2</cell><cell>74.1 ? 0.2</cell></row><row><cell>DeBERTa</cell><cell>69.3 ? 0.7</cell><cell>79.0 ? 0.3</cell><cell>76.1 ? 0.5 77.8 ? 0.1 88.3 ? 0.2</cell><cell>95.5 ? 0.1</cell><cell>73.8 ? 0.1</cell></row><row><cell>Longformer</cell><cell>71.0 ? 0.3</cell><cell>80.4 ? 0.9</cell><cell>76.9 ? 0.0 77.5 ? 0.0 88.1 ? 0.2</cell><cell>95.1 ? 0.2</cell><cell>73.9 ? 0.2</cell></row><row><cell>BigBird</cell><cell>71.0 ? 0.2</cell><cell>80.1 ? 0.5</cell><cell>75.9 ? 0.2 77.3 ? 0.1 88.0 ? 0.1</cell><cell>95.2 ? 0.4</cell><cell>73.7 ? 0.2</cell></row><row><cell>Legal-BERT</cell><cell>71.9 ? 0.4</cell><cell>79.8 ? 0.2</cell><cell>80.4 ? 0.3 77.6 ? 0.1 88.5 ? 0.0</cell><cell>95.1 ? 0.2</cell><cell>76.4 ? 0.3</cell></row><row><cell>CaseLaw-BERT</cell><cell>72.1 ? 0.3</cell><cell>79.6 ? 0.0</cell><cell>81.3 ? 0.6 77.2 ? 0.1 88.4 ? 0.2</cell><cell>95.3 ? 0.4</cell><cell>77.4 ? 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Development ?-F 1 results for all examined models across all LexGLUE tasks. We report the mean and standard deviations (?) for the three seeds with the best development scores per model. In starred datasets, we use the hierarchical variant of each model, except for Longformer and BigBird, as discussed in Section 4.2.</figDesc><table><row><cell>Method</cell><cell cols="5">ECtHR (A)* ECtHR (B)* SCOTUS* EUR-LEX LEDGAR UNFAIR-ToS CaseHOLD</cell></row><row><cell>BERT</cell><cell>65.4 ? 1.2</cell><cell>74.8 ? 0.6</cell><cell>65.9 ? 0.8 62.6 ? 0.8 81.8 ? 0.1</cell><cell>75.8 ? 1.3</cell><cell>72.8 ? 0.1</cell></row><row><cell>RoBERTa</cell><cell>65.4 ? 0.2</cell><cell>74.2 ? 1.1</cell><cell>69.5 ? 0.8 63.5 ? 0.4 81.9 ? 0.2</cell><cell>74.4 ? 0.7</cell><cell>74.1 ? 0.2</cell></row><row><cell>DeBERTa</cell><cell>63.5 ? 0.9</cell><cell>74.0 ? 0.4</cell><cell>68.4 ? 0.8 63.6 ? 0.3 82.0 ? 0.5</cell><cell>77.1 ? 1.2</cell><cell>73.8 ? 0.1</cell></row><row><cell>Longformer</cell><cell>65.5 ? 1.6</cell><cell>77.7 ? 1.0</cell><cell>70.4 ? 0.5 63.8 ? 0.5 82.0 ? 0.3</cell><cell>75.2 ? 1.2</cell><cell>73.9 ? 0.2</cell></row><row><cell>BigBird</cell><cell>65.8 ? 1.1</cell><cell>74.1 ? 0.5</cell><cell>69.1 ? 0.2 63.0 ? 0.3 81.7 ? 0.2</cell><cell>76.5 ? 1.8</cell><cell>73.7 ? 0.2</cell></row><row><cell>Legal-BERT</cell><cell>68.0 ? 0.2</cell><cell>76.1 ? 0.5</cell><cell>72.7 ? 0.2 62.0 ? 0.9 82.2 ? 0.3</cell><cell>76.9 ? 1.3</cell><cell>76.4 ? 0.3</cell></row><row><cell>CaseLaw-BERT</cell><cell>67.1 ? 0.7</cell><cell>74.6 ? 0.5</cell><cell>74.0 ? 1.2 62.9 ? 0.3 82.3 ? 0.3</cell><cell>76.5 ? 0.3</cell><cell>77.4 ? 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Development m-F 1 results for all examined models across all LexGLUE tasks. We report the mean and standard deviation (?) for the three seeds with the best development scores per model. In starred datasets, we use the hierarchical variant of each model, except for Longformer and BigBird, as discussed in Section 4.2.Big-Bird, have comparable times with the exception of DeBERTa that has four separate attention mechanisms. We observe that when the hierarchical variant of these models is deployed, i.e., in EC-tHR tasks and SCOTUS, it is approximately twice (2?) as fast compared to Longformer and BigBird.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Training time in total (T ) and per epoch (T/e) across LexGLUE tasks. In starred datasets, we use the hierarchical variant of each model, except for Longformer and BigBird, as described in Section 4.2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Development and test results across LexGLUE tasks. In starred datasets, we use the hierarchical variant of each model, discussed in Section 4.2. (B) and (L) denote the base and large version of RoBERTa, respectively.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See https://nllpw.org/resources/ and https:// github.com/thunlp/LegalPapers for lists of papers, datasets, and other resources related to NLP for legal text.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The term 'understanding' is, of course, as debatable as in NLU and GLUE, but is commonly used in NLP to refer to systems that analyze, rather than generate text.3  We reuse this term from the work of<ref type="bibr" target="#b47">Wang et al. (2019a)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In Appendix G, we provide examples, i.e., pairs of (inputs, outputs), for all datasets and tasks.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://www.supremecourt.gov 6 https://www.courtlistener.com 7 http://scdb.wustl.edu 8 http://eur-lex.europa.eu/ 9 http://eurovoc.europa.eu/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">https://case.law/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">https://huggingface.co/datasets/lex_glue 15 https://github.com/coastalcph/lex-glue 16 https://huggingface.co/transformers 17 http://huggingface.co/models 18 We acknowledge that the use of scores aggregated over tasks has been criticized in general NLU benchmarks (e.g., GLUE), as models are trained with different numbers of samples, task complexity, and evaluation metrics per task. We believe that the use of a standard common metric (F1) across tasks and averaging with harmonic mean alleviate this issue.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21">The question mostly resembles a prompt, rather than a natural question, as there is a closed set of 41 alternatives.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22">F1 is one of the two official SQUAD measures. In the second one, Exact Answer Accuracy, all models scored 0%.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23">https://huggingface.co/datasets/lex_glue</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="0">(Choice A)Table 9: Training examples (pairs of inputs, outputs) for LeXGLUE datasets and tasks.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partly funded by the Innovation Fund Denmark (IFD) 19  under File No. 0175-00011A and by the German Federal Ministry of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Other Tasks and Datasets Considered</head><p>We considered including the Contract Understanding Atticus Dataset (CUAD) (Hendrycks et al.,  2021), an expertly curated dataset that comprises 510 contracts annotated with 41 valuable contractual insights (e.g., agreement date, parties, governing law). The task is formulated as a SQUAD-like question answering task, where given a question (the name of an insight) and a paragraph from the contract, the model has to identify the answer span in the paragraph. 21  The original dataset follows the SQUAD v2.0 setting, including unanswerable questions. Following SQUAD v1. 1 (Rajpurkar et al.,  2016), we simplified the task by removing all unanswerable pairs (question, paragraph), which are the majority in the original dataset. We also excluded pairs whose answers exceeded 128 full words to alleviate the imbalance between short and long answers. We then re-split the dataset chronologically into training (5.2k, 1994-2019), development (572,  2019-2020), and test (604, 2020) sets.</p><p>Following Devlin et al. (2019), and similarly to Hendrycks et al. (2021), for each training (or test)</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>? Annotations may include indirect mentions, e.g., 'Franchisee', 'Service Provider' for Parties, instead of the actual entities (the company name).</p><p>? Annotations may include semi-redacted text (e.g., "</p><p>, 1996" for Agreement Date), or even fully redacted text (e.g., "</p><p>" for Parties). This practice may be necessary to hide sensitive information, but for the purposes of a benchmark dataset such cases could have been excluded.</p><p>The points above, which seem to require revisiting the annotations of CUAD, and the very low F1 scores of all models led us to exclude CUAD from LexGLUE. We also note that there is related work covering similar topics, such as Contract Element Extraction (Chalkidis and Androutsopoulos, 2017), Contractual Obligation Extraction <ref type="bibr" target="#b3">(Chalkidis et al., 2018)</ref>, and Contractual Provision Classification <ref type="bibr" target="#b42">(Tuggener et al., 2020)</ref>, where models perform much better (in terms of accuracy), relying on simpler (separate) more carefully designed tasks and much bigger datasets. Thus we believe that the points mentioned above, which blur the task definition of CUAD and introduce noise, and the limited (compared to larger datasets) number of annotations strongly affect the performance of the models on CUAD, underestimating their true potential.</p><p>We also initially considered some very interesting legal Information Retrieval (IR) datasets <ref type="bibr" target="#b20">(Locke and Zuccon, 2018;</ref><ref type="bibr">Chalkidis et al., 2021b</ref>) that aim to examine crucial real-life tasks (relevant case law retrieval, regulatory compliance). However, we decided to exclude them from the first version of LexGLUE, because they rely on processing multiple long documents and require more task-specific neural network architectures (e.g., siamese networks), and different evaluation measures. Hence, they would make LexGLUE more complex and a less attractive entry point for newcomers to legal NLP. We plan, however, to include more demanding tasks in future LexGLUE versions, as the legal NLP community will be growing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Dataset Examples</head><p>In <ref type="table">Table 9</ref>, we present training examples, i.e., pairs of input(s), output(s), for LeXGLUE datasets and tasks. More examples can be inspected using the dataset preview functionality provided in the online dataset card of Hugging Face. <ref type="bibr">23</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">2020. Proceedings of the 2nd Natural Legal Language Processing Workshop at KDD 2020</title>
		<editor>Nikolaos Aletras, Ion Androutsopoulos, Leslie Barrett, Adam Meyers, and Daniel Preotiuc-Pietro</editor>
		<imprint>
			<publisher>Online</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliott</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Preotiuc-Pietro</surname></persName>
		</author>
		<title level="m">Proceedings of the 1st Natural Legal Language Processing Workshop at NAACL 2019. Minneapolis, Minnesota</title>
		<meeting>the 1st Natural Legal Language Processing Workshop at NAACL 2019. Minneapolis, Minnesota<address><addrLine>David Rosenberg, and Amanda Stent</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting judicial decisions of the european court of human rights: A natural language processing perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tsarapatsanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Preo?iuc-Pietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Lampos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Named entity recognition, linking and generation for greek legislation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Angelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilias</forename><surname>Chalkidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koubarakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JURIX</title>
		<meeting><address><addrLine>Groningen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>The Netherlands</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Machine bias: There&apos;s software used across the country to predict future criminals. and it&apos;s biased against blacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Angwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Mattu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauren</forename><surname>Kirchner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ProPublica</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sexism in the judiciary: The importance of bias definition in NLP and in our courts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Baker Gillis</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.gebnlp-1.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing</title>
		<meeting>the 3rd Workshop on Gender Bias in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Legaldb: Long distilbert for legal document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purbid</forename><surname>Bambroo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Awasthi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICAECT49130.2021.9392558</idno>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/2004.05150</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A comparative study of summarization algorithms applied to legal case judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paheli</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustubh</forename><surname>Hiware</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subham</forename><surname>Rajgaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilay</forename><surname>Pochhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kripabandhu</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saptarshi</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="413" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analogies and disanalogies between machine-driven and human-driven legal judgement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuben</forename><surname>Binns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cross-disciplinary Research in Computational Law</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Lexnlp: Natural language processing and information extraction for legal and regulatory texts. Research Handbook on Big Data Law</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Bommarito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">Martin</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Detterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="216" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 5th International Conference on Learning Representations (ICLR)<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predicting judicial decisions of criminal cases from thai supreme court using bi-directional gru with attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kankawin</forename><surname>Kowsrihawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peerapon</forename><surname>Vateekul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prachya</forename><surname>Boonkwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 5th Asian Conference on Defense Technology (ACDT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="50" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fine-grained named entity recognition in legal documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Leitner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Rehm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Moreno-Schneider</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-030-33220-4_20</idno>
	</analytic>
	<monogr>
		<title level="m">Semantic Systems. The Power of AI and Knowledge Graphs</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="272" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Villanova Del Moral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunjan</forename><surname>Mario?a?ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavitvya</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Brandeis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Patry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Delangue</surname></persName>
		</author>
		<imprint>
			<pubPlace>Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Fran?ois Lagunas, Alexander M. Rush</pubPlace>
		</imprint>
	</monogr>
	<note>Th?o Matussi?re, Lysandre Debut. and Thomas Wolf. 2021. Datasets: A community library for natural language processing</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CLAUDETTE: an automated detector of potentially unfair clauses in online terms of service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Przemys?aw</forename><surname>Pa?ka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Contissa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Lagioia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Wolfgang</forename><surname>Micklitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Sartor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Torroni</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10506-019-09243-2</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Law</title>
		<imprint>
			<biblScope unit="page" from="117" to="139" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">ERNIE-SPARSE: Robust efficient transformer through hierarchically unifying isolated information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhida</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A test collection for evaluating legal case law search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Locke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Zuccon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209978.3210161</idno>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;18</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1261" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A statutory article retrieval dataset in french</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerasimos</forename><surname>Spanakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>page To appear. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning with noise: Enhance distantly supervised relation extraction with dynamic transition matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1040</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="430" to="439" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Te?filo Em?dio de Campos, Fabricio Ataides Braz, and Nilton Correia da Silva</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro Henrique Luz De</forename><surname>Araujo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1449" to="1458" />
		</imprint>
	</monogr>
	<note>VICTOR: a dataset for Brazilian legal documents classification. European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shouvik Kumar Guha, Arnab Bhattacharya, and Ashutosh Modi. 2021. ILDC for CJPE: indian legal documents corpus for court judgmentprediction and explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijit</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kripa</forename><surname>Shubham Kumar Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021)</title>
		<imprint/>
	</monogr>
	<note>online</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The natural language decathlon: Multitask learning as question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1806.08730</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic judgement forecasting for pending applications of the European Court of Human Rights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masha</forename><surname>Medvedeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ahmet?stun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martijn</forename><surname>Vols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wieling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Workshop on Automated Semantic Analysis of Information in Legal Text</title>
		<meeting>the Fifth Workshop on Automated Semantic Analysis of Information in Legal Text</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>ASAIL 2021</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using machine learning to predict decisions of the European Court of Human Rights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masha</forename><surname>Medvedeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Vols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martijn</forename><surname>Wieling</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/article/10.1007/s10506-019-09255-y</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Law</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="266" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An Evaluation of Efficient Multilabel Classification Algorithms for Large-Scale Problems in the Legal Domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneldo</forename><surname>Loza Mencia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>F?rnkranzand</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-642-12837-0_11</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Linguistic Annotation Workshop</title>
		<meeting>the 1st Linguistic Annotation Workshop<address><addrLine>Halle, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="126" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Natural language processing in law: Prediction of outcomes in the higher courts of turkey. Information Processing &amp; Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Mumcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyhun</forename><surname>E?zt?rk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haldun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aykut</forename><surname>Ozaktas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ko?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">102684</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Legal docket classification: Where machine learning stumbles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="438" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Swiss-Court-Predict: A Multilingual Legal Judgment Prediction Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilias</forename><surname>Chalkidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>St?rmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Natural Legal Language Processing Workshop Workshop</title>
		<meeting>the 3rd Natural Legal Language Processing Workshop Workshop<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How to build a more open justice system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam R Pah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">D</forename><surname>Sanga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clopton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><forename type="middle">Davis</forename><surname>Dicola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlotte</forename><forename type="middle">S</forename><surname>Mersey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><forename type="middle">J</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu?s A Nunes</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amaral</surname></persName>
		</author>
		<idno type="DOI">https:/www.science.org/doi/10.1126/science.aba6914</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="issue">6500</biblScope>
			<biblScope unit="page" from="134" to="136" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Workshop on Biomedical Natural Language Processing</title>
		<meeting>the 2019 Workshop on Biomedical Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Friends with motives: Using text to infer influence on SCOTUS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchuan</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Routledge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1178</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1724" to="1733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Supreme Court Database, Version 2020 Release 01</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Spaeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">A</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Segal Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><forename type="middle">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><forename type="middle">C</forename><surname>Ruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
		<respStmt>
			<orgName>Washington University Law</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Legal judgement prediction for uk courts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Strickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz De La</forename><surname>Iglesia</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.1145/3388176.3388183</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 The 3rd International Conference on Information Science and System</title>
		<meeting>the 2020 The 3rd International Conference on Information Science and System</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="204" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Legal language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tiersma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the ethical limits of natural language processing on legal text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tsarapatsanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.314</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3590" to="3599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">LEDGAR: A large-scale multi-label corpus for text classification of legal provisions in contracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Tuggener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pius Von D?niken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cieliebak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1235" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Design and Implementation of German Legal Decision Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Urchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelena</forename><surname>Mitrovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Granitzer</surname></persName>
		</author>
		<idno type="DOI">10.5220/0010187305150521</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Agents and Artificial Intelligence</title>
		<meeting>the 13th International Conference on Agents and Artificial Intelligence</meeting>
		<imprint>
			<publisher>Online. SCITEPRESS -Science and Technology Publications</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="515" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">What about the precedent: An information-theoretic analysis of common law</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Valvoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Stoehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.181</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2275" to="2288" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Predicting decisions of the philippine supreme court using natural language processing and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Benedict</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Virtucio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">A</forename><surname>Aborot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Abonita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxanne</forename><forename type="middle">S</forename><surname>Avinante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rother</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Copino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><forename type="middle">P</forename><surname>Neverida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vanesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elmer</forename><forename type="middle">C</forename><surname>Osiana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><forename type="middle">G</forename><surname>Peramo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Syjuco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="130" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Equality before the law: Legal judgment consistency analysis for fairness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China -Information Sciences</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Tradition and change in legal English: Verbal constructions in prescriptive texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Peter Lang</publisher>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Lawformer: A pre-trained language model for chinese legal long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/2105.03887</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Beyond 512 Tokens: Siamese Multi-Depth Transformer-Based Hierarchical Encoder for Long-Form Document Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3411908</idno>
	</analytic>
	<monogr>
		<title level="m">Association for Computing Machinery</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1725" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Legal judgment prediction via multiperspective bi-feedback network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/567</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4085" to="4091" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Interpretable Charge Predictions for Criminal Cases: Learning to Generate Court Views from Fact Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhunchen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Chao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1168</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1854" to="1864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Big Bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kumar Avinava Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="17283" to="17297" />
		</imprint>
	</monogr>
	<note>online</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">When does pretraining help? assessing self-supervised learning for law and the casehold dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Artificial Intelligence and Law. Association for Computing Machinery</title>
		<meeting>the 18th International Conference on Artificial Intelligence and Law. Association for Computing Machinery</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Legal judgment prediction via topological learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3540" to="3549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Dataset Input(s) Output(s) / Label(s)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">On 29 April 1988 a ministerial order was issued by the French Ministry of the Interior under section 14 of the Law of 29 July 1881, as amended by the decree of 6 May 1939, banning the circulation, distribution and sale of the book in France in any of its four versions on the ground that &quot;the circulation in France of this book, which promotes separatism and vindicates recourse to violence, is likely to constitute a threat to public order</title>
		<idno>ECtHR Text: 12</idno>
	</analytic>
	<monogr>
		<title level="m">There were four versions -Basque, English, Spanish and French -and the book was distributed in numerous countries, including France and Spain</title>
		<imprint>
			<date type="published" when="1987-05-06" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note>According to the applicant association, this was a collective work containing contributions from a number of academics with specialist knowledge of the Basque Country and giving an account of the historical, cultural, linguistic and socio-political aspects of the Basque cause. pursuant to the aforementioned order, the d?partement director of the airport and border police refused to allow over two thousand copies of the book to be brought into France. [...] 3 (Right to a fair trial) 6 (Freedom of expression) SCOTUS Text: 329 U.S. 29 67 S.Ct. 1 91 L</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Appeal from the District Court of the United States for the Western District of Oklahoma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Champlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Co V. United</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>States</surname></persName>
		</author>
		<editor>Messrs. Dan Moody, of Austin, Tex., and Harry O. Glasser, of Enid, Okla., for appellant. Mr. Edward Dumbauld</editor>
		<imprint>
			<date type="published" when="1946" />
			<pubPlace>Washington, D.C.</pubPlace>
		</imprint>
	</monogr>
	<note>for appellees. Mr. Justice JACKSON delivered the opinion of the Court</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">The Interstate Commerce Commission, acting under ? 19a of the Interstate Commerce Act,1 ordered the appellant to furnish certain inventories, schedules, maps and charts of its pipe line property</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Champlin&apos;s objections that the Act does not authorize the order, or if it be construed to do so is unconstitutional, were overruled by the Commission and again by the District Court which dismissed the company&apos;s suit for an injunction.3 These questions of law are brought here by appeal</title>
		<imprint/>
	</monogr>
	<note>Economic Activity</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">on the common organisation of the markets in the sugar sector(1), as amended by Commission Regulation (EC) No 1527/2000(2), and in particular point (a) of the second subparagraph of Article 18(5) thereof, Whereas: (1) Article 18 of Regulation (EC) No 2038/1999 provides that the difference between quotations or prices on the world market for the products listed in Article 1(1)(a) of that Regulation and prices for those products within the Community may be covered by an export refund</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eur-Lex</forename><surname>Text</surname></persName>
		</author>
		<idno>1156/2001 of 13</idno>
	</analytic>
	<monogr>
		<title level="m">fixing the export refunds on white sugar and raw sugar exported in its unaltered state THE COMMISSION OF THE EUROPEAN COMMUNITIES Having regard to the Treaty establishing the European Community, Having regard to Council Regulation</title>
		<imprint>
			<date type="published" when="1999-09-13" />
			<biblScope unit="page">93</biblScope>
		</imprint>
	</monogr>
	<note>Beverages and Sugar), 94 (Foodstuff ) LEDGAR Text: The validity or unenforceability of any provision or provisions of this Agreement shall not affect the validity or enforceability of any other provision hereof, which will remain in full force and effect. Should a court or other body of competent jurisdiction determine that any provision of this Agreement is excessive in scope or otherwise illegal, invalid, void or unenforceable. such provision shall be adjusted rather than voided, if possible, so that it is enforceable to the maximum extent possible. 79 (Severability</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">tinder stacks&quot;), whether through a mobile device , mobile application or computer (collectively, the &quot;service&quot;) you agree to be bound by (i) these terms of use, (ii) our privacy policy and safety tips, each of which is incorporated by reference into this agreement, and (ii ) any terms disclosed and agreed to by you if you purchase additional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Unfair-Tos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Text</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">By creating a tinder account or by using the tinder imessage app</title>
		<imprint/>
	</monogr>
	<note>products or services we offer on the service (collectively, this &quot;agreement&quot;)</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">)(B)(ii) (defining a &quot;violent felony&quot; as: &quot;any crime punishable by imprisonment for a term exceeding one year ... that ... involves use of explosives&quot;). Courts have found possession of a&apos;bomb to be a crime of violence based on the lack of a nonviolent purpose for a bomb and the fact that, by its very nature, there is a substantial risk that the bomb would be used against the person or property of another</title>
	</analytic>
	<monogr>
		<title level="m">CaseHOLD Context: Drapeau&apos;s cohorts, the cohort would be a &quot;victim&quot; of making the bomb. Further, firebombs are inherently dangerous</title>
		<editor>See 18 U.S.C. ? 924(e</editor>
		<imprint/>
	</monogr>
	<note>Felony offenses that involve explosives qualify as &quot;violent crimes&quot; for purposes of enhancing the sentences of career offenders. See United States v. Newman, 125 F.3d 863 (10th Cir.1997) (unpublished) ([HOLDING])</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">United States v. Dodge, 846 F</title>
		<imprint>
			<biblScope unit="page">181</biblScope>
		</imprint>
	</monogr>
	<note>Supp</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">holding for the purposes of 18 usc 924e that being a felon in possession of a firearm is not a violent felony as defined in 18 usc 924e2b&quot;, (E) &quot;holding that a court must only look to the statutory definition not the underlying circumstances of the crime to determine whether a given offense is by</title>
	</analytic>
	<monogr>
		<title level="m">Choices (Holdings): (A) &quot;holding that possession of a pipe bomb is a crime of violence for purposes of 18 usc 3142f1</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>holding that bank robbery by force and violence or intimidation under 18 usc 2113a is a crime of violence</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
