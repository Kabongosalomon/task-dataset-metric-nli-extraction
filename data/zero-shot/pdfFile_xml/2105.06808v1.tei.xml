<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WASTE DETECTION IN POMERANIA: NON-PROFIT PROJECT FOR DETECTING WASTE IN ENVIRONMENT A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-17">May 17, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylwia</forename><surname>Majchrowska</surname></persName>
							<email>sylwia.majchrowska@pwr.edu.pl</email>
							<affiliation key="aff0">
								<orgName type="institution">Wroclaw University of Science and Technology Wroc?aw</orgName>
								<address>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Miko?ajczyk</surname></persName>
							<email>agnieszka.mikolajczyk@pg.edu.pl</email>
							<affiliation key="aff1">
								<orgName type="institution">Gda?sk University of Technology</orgName>
								<address>
									<settlement>Gda?sk</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Ferlin</surname></persName>
							<email>maria.ferlin@pg.edu.pl</email>
							<affiliation key="aff2">
								<orgName type="institution">Gda?sk University of Technology</orgName>
								<address>
									<settlement>Gda?sk</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuzanna</forename><surname>Klawikowska</surname></persName>
							<email>zuwikowska@gmail.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Gda?sk University of Technology</orgName>
								<address>
									<settlement>Gda?sk</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">A</forename><surname>Plantykow</surname></persName>
							<email>m.plantykow@gmail.com</email>
							<affiliation key="aff4">
								<orgName type="institution">Intel Corporation Gda?sk</orgName>
								<address>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadiusz</forename><surname>Kwasigroch</surname></persName>
							<email>arkadiusz.kwasigroch@pg.edu.pl</email>
							<affiliation key="aff5">
								<orgName type="institution">Gda?sk University of Technology</orgName>
								<address>
									<settlement>Gda?sk</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Majek</surname></persName>
							<email>karolmajek@cufix.pl</email>
							<affiliation key="aff6">
								<orgName type="department">Cufix Warsaw</orgName>
								<address>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WASTE DETECTION IN POMERANIA: NON-PROFIT PROJECT FOR DETECTING WASTE IN ENVIRONMENT A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-17">May 17, 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Litter detection ? Waste detection ? object detection</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Waste pollution is one of the most significant environmental issues in the modern world. The importance of recycling is well known, either for economic or ecological reasons, and the industry demands high efficiency. Our team conducted comprehensive research on Artificial Intelligence usage in waste detection and classification to fight the world's waste pollution problem. As a result an open-source framework that enables the detection and classification of litter was developed. The final pipeline consists of two neural networks: one that detects litter and a second responsible for litter classification. Waste is classified into seven categories: bio, glass, metal and plastic, nonrecyclable, other, paper and unknown. Our approach achieves up to 70% of average precision in waste detection and around 75% of classification accuracy on the test dataset. The code used in the studies is publicly available online 1 .</p><p>A PREPRINT -MAY 17, 2021    In recent years machine learning (ML) based systems that can support or fully cover sorting processes were implemented, accelerating this procedure as a result. The most commonly used solutions include self-sorting smart bins, which are capable of classifying one object located on a clear background at a time <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. When a single compartment is used, the camera is usually located at the top of the upper container.The deep learning (DL) model assigns proper class based on the photo and the garbage is moved to the appropriate bottom container <ref type="bibr" target="#b4">[5]</ref>. Another way is to a mount camera or a sensor above few separate bins, and direct the consumer handling waste to the correct one <ref type="bibr" target="#b6">[7]</ref>. In this approach, the rubbish must be well exposed to the imaging device. Here it comes to the fact that the problem of identifying garbage in the wild environment is ambiguous. An image could present different types of trash, which additionally can be deformed, and/or recorded in a uncontrolled various natural scenarios. On the other hand, as the same object may be a junk or not depending on the context, it must be defined at what point localized object should be treated as a litter.</p><p>This paper addresses some of the identified limitations. At first, to provide more efficient recycling, we mixed publicity available datasets of waste observed in different environments, and proposed seven well-defined categories for sorting litter: bio, glass, metal and plastic, non-recyclable, paper, other and unknown. Their selection was inspired by rules in Gdansk (Poland). Secondly, we implemented a two-stage DL-based framework for waste detection that consisted of two separate neural networks: detector and classifier. The proposed framework is freely available and can be used for different purposes, such as studying the common types of waste observed in nature. To our knowledge, we presented the first experiments that allow for such universal litter detection and classification. Additionally we provided the first comprehensive review of existing waste datasets.</p><p>An overview of the actual work for deep learning classification and detection, as well as existing public datasets of waste are described in Section 2. Section 3 illustrates our framework to detect and classify waste from images, and statistics of examined data. More specifically, we described used data in details, provide the training details of chosen neural networks, and reported the obtained results. Finally, conclusions are drawn and future work is outlined in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>ML and DL techniques empower many aspects of modern society, like recommendation systems, text-to-speech devices, or even objects identification in images <ref type="bibr" target="#b7">[8]</ref>. Also the waste management problem has attracted a lot of interest <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref>, where the main goal is to create an ML-based image recognition system to sort litter. The majority of the proposed approaches are based on the deep learning algorithms utilized in the computer vision field. This section describes chosen techniques used in classification and object detection challenges and delivered a comprehensive review of existing public waste datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Classification</head><p>Convolutional neural networks (CNNs) have had a massive impact on large-scale image classification tasks and made it possible to achieve significantly higher accuracy than solutions based on classical image processing. Multiple convolutional layers stacked together can automatically learn representations from the data reducing the need for manual feature extraction engineering <ref type="bibr" target="#b7">[8]</ref>. The design of neural network structure impacts the performance, latency, and computational requirements. Thus efforts have shifted from feature extraction design to architecture design. Architectural enhancements of deep neural networks improve the performance of classification, segmentation and detection tasks.</p><p>A breakthrough in CNN performance came with AlexNet [10], a neural network that won the ImageNet Large Scale Visual Recognition 2012 challenge. Since then, the quality of image recognition structures has advanced rapidly. AlexNet consists of a stack of five convolutional layers of different kernel sizes. The structure incorporates nonsaturating ReLU activation functions, overlapping pooling, and normalization layers. Dropout layers were used to reduce overfitting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Litter is scattered in a wide variety of environments. The massive production of disposable goods in the last years resulted in an exponential increase in produced garbage, reaching about 2 billion tons per year <ref type="bibr" target="#b0">[1]</ref>. It was estimated that humans eat up to 250 grams of micro plastics each year <ref type="bibr" target="#b1">[2]</ref>. Almost 90 percent of this plastic comes from bottled and tap water. Today, more than 300 million tons of plastic is produced annually and only 30 years are left for the amount of garbage in the ocean to exceed the number of sea creatures <ref type="bibr" target="#b2">[3]</ref>.</p><p>The most difficult challenge in the waste management is complex, and not unified guidelines regarding segregation rules. Due to the large energy requirements and related costs plastic waste separation on many litter sorting lines is done manually. However, new possibilities for the automatic selection of these materials and their reuse are constantly being tested <ref type="bibr" target="#b3">[4]</ref>.</p><p>Inception <ref type="bibr" target="#b12">[13]</ref> network stacks modules called Inception instead of single layers. Inception blocks incorporate multiscale convolutional transformations utilizing a split-transform-merge strategy. Moreover, it allows for increasing the number of units at each stage without an uncontrolled blow-up in computational complexity. The Inception models have evolved by utilizing batch normalization and factorization of convolutional layers <ref type="bibr" target="#b13">[14]</ref>, simplification, and the use of asymmetric filters <ref type="bibr" target="#b14">[15]</ref>. The proposed Inception-Resnet structure <ref type="bibr" target="#b14">[15]</ref> combines both the power of inception modules and residual connections.</p><p>The improvement of connectivity pattern in the DenseNet structure <ref type="bibr" target="#b15">[16]</ref> facilitated the training and accuracy. For each layer, the feature maps of all preceding layers are used as inputs, and its feature maps are used as inputs into all subsequent layers. DenseNets have several significant advantages: they minimize the vanishing gradient problem, improve feature propagation, encourage feature reuse, and reduce the number of parameters.</p><p>ResNeXt <ref type="bibr" target="#b16">[17]</ref> structure similarly to Inception utilizes a split-transform-merge strategy. However, unlike all Inception or Inception-ResNet modules, the same topology among the multiple paths is shared and paths are aggregated by summation. The authors introduced a conception of cardinality -the number of paths in one module. It was observed that an increase in cardinality is more effective than going deeper or wider when increasing the capacity.</p><p>EfficientNet <ref type="bibr" target="#b17">[18]</ref> architecture consists of modules constructed by the neural architecture search process that optimizes both for accuracy and FLOPS. The mobile-size baseline model called EfficientNet-B0 was built stacking those modules. Scaling strategies were used to produce more complex and accurate models EfficientNet-B1-7. Scaling of a convolutional neural network most often is performed in one of the following dimensions width, depth, or resolution. Whereas, EfficientNets scale three dimensions jointly, significantly improving efficiency and accuracy EfficientNetv2 <ref type="bibr" target="#b18">[19]</ref> improves EfficientNet by providing faster training and better parameter efficiency by eliminating EfficientNet bottlenecks. Neural architecture search was used to optimize accuracy, training speed, and parameter size. Unlike, standard EfficientNet, EfficientNetv2 uses non-uniform scaling of depth, resolution, and width. Moreover, to limit computational cost an increase in resolution was limited.</p><p>Inspired by the successes in Natural Language Processing, recent computer vision architecture advances rely on the Transformer module. The authors of <ref type="bibr" target="#b19">[20]</ref> trained Visual Transformer (Vit) -a pure transformer architecture applied directly to sequences of image patches that perform very well on image classification tasks. The authors stated that training transformers for vision tasks requires large amounts of training samples and extensive computing resources. This particular trait was addressed in Data-efficient image Transformers (DeiT) <ref type="bibr" target="#b20">[21]</ref> in which a new model distillation procedure is shown. In <ref type="bibr" target="#b21">[22]</ref> authors present Pyramid Vision Transformer (PVT) which is designed for dense prediction tasks such as object detection. PVT introduces to transformers the pyramid structure known from CNNs. Therefore it is more suitable for object detection tasks than Vit, which was designed for image classification. The application of transformers in vision is further investigated in <ref type="bibr" target="#b22">[23]</ref>. Authors introduce convolutions to ViT architecture to introduce shift, scale, and distortion invariance. Proposed Convolutional vision Transformer (CvT) architecture accomplishes this goal by two main modifications: introduction of a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. The application of transformers in vision tasks is not yet a well-investigated topic, and we observe many teams that are currently working on improving it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Object Detection</head><p>Object detection is a well-studied task in computer vision <ref type="bibr" target="#b23">[24]</ref>. It is defined as a localization of Axis-Aligned Bounding Box (AABB) and classification -assignment of a single or multi-label. In many previous works object detection was approached using two types of techniques namely one-stage and two-stage detection. One-stage architectures provide both locations and classes for each object in a single step, while two-stage detectors find class-agnostic object proposals first, classifies them into the class-specific detections in the second stage.</p><p>Two-stage detectors were the first object detection methods. They used the sliding window approach in the image pyramid to generate object proposals in multiple scales. Then in a second stage a classifier such as a cascade classifier <ref type="bibr" target="#b24">[25]</ref> was used. Such a system achieved 15 Hz for the face detection system which is a single class object detection problem. A significant improvement was presented in <ref type="bibr" target="#b25">[26]</ref> where authors introduced a novel descriptor namely Histograms of Oriented Gradient (HOG) and showed a method of human detection using Support Vector Machine (SVM) to classify objects. The authors used a detection window to scan across the image at all positions and scales. Classification is invoked multiple times in similar regions and using multiple image pyramid levels.</p><p>Using convolutional neural networks for object detection was a challenging task since CNNs' inability to localize features. In most approaches, this problem was solved by using the recognition using regions paradigm <ref type="bibr" target="#b26">[27]</ref>. Identified regions can yield richer information than single pixels or pixels with a fixed local neighbourhood. Regions are used for object detection successfully in the Selective Search algorithm <ref type="bibr" target="#b27">[28]</ref>. Authors generate initial regions which are grouped by similarity and merged in the iterative process. Object hypotheses from regions were used to extract features using key point feature descriptors and classify those proposals using SVM. Selective Search was also used in R-CNN <ref type="bibr" target="#b28">[29]</ref> to extract region proposals, on which CNN features where computed to classify regions using per class SVMs finally. Fast R-CNN <ref type="bibr" target="#b29">[30]</ref> solved R-CNN's main problems: multiple training sessions required since SVMs are involved, inefficient processing of regions through the CNN. Fast R-CNN computes convolutional features for the whole image in the first step to reduce computations for overlapping regions. The other improvement is integrating classification into the network architecture, which allows allows training of the entire network in a single multi-task training session. When Faster R-CNN <ref type="bibr" target="#b30">[31]</ref> was introduced State-of-the-art object detection networks used region proposal algorithms to generate object location hypotheses. In this architecture a Region Proposal Network (RPN) concept was used instead of the Selective Search algorithm. RPN is integrated into Fast R-CNN architecture and uses shared weights. Many researchers used Faster R-CNN while replacing its backbone (feature extraction CNN) with newer architectures in the following years. Faster R-CNN is like its' predecessors, a two-stage object detection method.</p><p>Single-stage detection was popularized in Deep Learning mainly by two detector architectures: Single Shot MultiBox Detector (SSD) <ref type="bibr" target="#b31">[32]</ref> and You Only Look Once (YOLO v1, 9000, v3, v4, scaled V4: <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>). Those networks perform both, object detection and object classification in a single step. Therefore the detection time can be reduced. Such a result is obtained by generating a constant number of predictions per class per image. For SSD it was 8732 detections for each class for a 300x300px image. Each detection is characterized by a category score for each category and 4 offsets for a fixed set of default bounding boxes. Default boxes have different aspect ratios at each location in several feature maps with different scales. In this type of methods, many false detections need to be removed considering objectness or classification score and overlaps between detections. YOLOv1 used a single scale image for prediction with 98 detections per class and did not use default boxes YOLOv2 (YOLO9000) predicts nine bounding boxes at each cell from a 13x13 grid, resulting in in 1521 boxes. YOLOv3 introduces a multi-label approach to classification since, in datasets such as Open Images <ref type="bibr" target="#b37">[38]</ref>, class labels do not have a guaranteed hierarchy level. It uses image 3 image pyramid levels for detection. YOLOv4 introduces a new backbone network while using the same detector as YOLOv3. This paper studies data augmentation, post-processing methods. A month after YOLOv4 <ref type="bibr" target="#b35">[36]</ref> was proposed, a new approach to object detection tasks was introduced in <ref type="bibr" target="#b38">[39]</ref>. Detection Transformer (DETR) introduced Transformers known from Natural Language Processing tasks <ref type="bibr" target="#b39">[40]</ref> into object detection while preserving image processing with CNNs. DETR predicts a sparse, fixed number of objects which are in training matched with ground truth labels using bipartite matching. DETR's main limitations were slow convergence and limited feature spatial resolution. It required 10 times more epochs than other approaches to converge to a similar error level. Limited spatial resolution lowered the Average Precision metric for small objects' sizes. The following year, the main issues were mitigated in Deformable DETR <ref type="bibr" target="#b40">[41]</ref> thanks to the introduced deformable multi-head attention module.</p><p>EfficientDet, a single-stage object detector, was introduced in [42] a month after <ref type="bibr" target="#b38">[39]</ref>. The main goal of the authors was to improve the efficiency of object detection models. The goal was separated into two challenges, namely efficient multi-scale feature fusion and model scaling. They introduced a weighted bi-directional feature pyramid network (BiFPN) to address the first challenge. BiFPN adds a bottom-up pathway to fuse multi-scale features similarly to Neural Architecture Search Feature Pyramid Network (NAS-FPN) <ref type="bibr" target="#b42">[43]</ref>, which used NAS to find the best top-down, bottom-up pathways for feature fusion. The second challenge was addressed using a compound scaling method for object detectors inspired by <ref type="bibr" target="#b17">[18]</ref>. It jointly scales up the resolution/depth/width for all backbone, feature network, and box/class prediction network. EfficientDet is proposed in 8 variants D0-D7 with backbone networks EfficientNet B0-B7. The approach shown in <ref type="bibr" target="#b41">[42]</ref> reduced the number of parameters and latency in the object detection network and simultaneously increased the Average Precision metrics. Instance Segmentation is another task for automated image processing. The goal of this task is to provide a segmentation mask for each object instance. Approaches introduced in the pre-Deep Learning Era relied on methods such as edges or superpixels. This changed in Deep Mask <ref type="bibr" target="#b43">[44]</ref> which was one of the first deep neural networks used for the instance segmentation task. The proposed network predicts object proposals for the whole image. Those proposals are class-agnostic segmentation masks, and for each of them, an object likelihood score is computed. Mask RCNN <ref type="bibr" target="#b44">[45]</ref> is an extension of Faster R-CNN, and it ads the instance mask branch in parallel to the classification and bounding box regression branch. In Mask RCNN recognition precedes segmentation, which is faster and more accurate according to authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data collections</head><p>In recent years multiple attempts to detect, classify and segment waste using deep learning have been made. Litter classification of common waste categories based on images has been attempted using few pre-trained convolutional neural networks -AlexNet, MobileNet, InceptionResNetV2, DenseNet, Xception <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48</ref>] -achieving average accuracy in ranges 22% <ref type="bibr" target="#b45">[46]</ref> and 98.2% <ref type="bibr" target="#b47">[48]</ref> for pictures of waste on a plain background. Experiments have also been conducted on the detection of litter on the streets and homes, as well as the segmentation of different types of waste using: Faster R-CNN, Mask R-CNN, SSD, and different types of YOLO (even Tiny-YOLO) <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>. Calculated mAP value for detection task varies between different datasets and architectures from 15.9% for TACO <ref type="bibr" target="#b52">[53]</ref> with Mask-RCNN, up to 81% for Trash-ICRA19 <ref type="bibr" target="#b50">[51]</ref> with Faster R-CNN.</p><p>Nonetheless, very few attempts at the detection and classification into well-recognized recyclable classes have been made. As deep learning requires diverse data to fit the model's parameters optimally, a comprehensive review of existing litter datasets in terms of litter detection is provided. Over ten different data collections and their crucial statistics (e.g. number of images, annotation type) are presented in <ref type="table" target="#tab_0">Table 1</ref> (see <ref type="figure" target="#fig_0">Fig 1 to</ref> visualize their representatives).</p><p>TrashNet. The TrashNet dataset <ref type="bibr" target="#b45">[46]</ref> contains over 2100 labeled images. Each of them belongs to one of the six classes: glass, paper, cardboard, plastic, metal, and trash. The pictures were taken by mobile phone camera using sunlight and/or room lighting. Photographed objects were placed on a white background or fulfill the whole view (cardboard). All images have size a of 512x384px.</p><p>Open Litter Map. Open Litter Map <ref type="bibr" target="#b56">[57]</ref> is a free, open, and crowd-sourced dataset with over 100k images taken by phone cameras. All images are provided with information such as type of presented litter, coordinates, timestamp or phone model. Images come from all over the world, taken by different people. Therefore, they differ significantly from one another.</p><p>Waste Pictures. Waste Pictures [58] contains almost 24000 waste images scraped from Google search, divided into 34 classes. The type of images is very diverse, including even x-rays and drawings of garbage. Sizes also differ significantly. However, most of the photos are below the size of 2000x2000px. Due to the origin of images, they should be manually reviewed for use in a classification task.</p><p>Extended TACO. Trash Annotations in Context (TACO) <ref type="bibr" target="#b52">[53]</ref> is a crowd-sourced dataset of waste in the wild with high-resolution mobile phone images. The TACO dataset contains 1500 annotated images with almost 5000 objects. All trash have been assigned to one of 60 classes that belong to 28 super (top) categories, including the category Unlabeled litter for hard to recognize or heavily obscured objects. The annotations are provided in the well-known COCO format <ref type="bibr" target="#b57">[59]</ref> on the instance segmentation level with an extra background description -Trash, Vegetation, Sand, Water, Indoor, Pavement. Additionally, TACO offers around three thousand of unannotated images, which we have taken advantage of: we provided annotations on the detection level 2 for over 3000 images achieving over 14 000 instances in total. A great advantage is that TACO is characterized by various litter types and high diversity of the backgrounds, from tropical beaches to London streets. However, due to the crowd-sourcing nature of the dataset, labels may contain some user-induced errors and bias, i.e., not all objects in TACO can be categorized strictly as litter as their category is often based on context.</p><p>Wade-AI. The Wade-AI dataset <ref type="bibr" target="#b58">[60]</ref> contains images of waste in the wild environment, provided by Google Street View. It consists of nearly 1400 images with 2200 manually labeled instance masks annotations in COCO format with only one class, called rubbish. The environment and size of the images vary due to the source of the images. Most images are less than 1000x1000.</p><p>UAVVaste. Another publicly available dataset, which also provides instance segmentation masks in the COCO format, is UAVVaste <ref type="bibr" target="#b55">[56]</ref> dataset. It contains 772 hand-labeled aerial images of waste with over 3700 objects of one class -"rubbish". Data was collected in the urban and natural environments e.g. streets, parks and lawns using Unmanned Aerial Vehicles (UAV). The annotated litter is usually relatively small (median of object shape is 76x68px, while median of image shape is 3840x2160px).</p><p>TrashCan and Trash-ICRA. TrashCan <ref type="bibr" target="#b51">[52]</ref> and Trash-ICRA <ref type="bibr" target="#b50">[51]</ref> are datasets both containing underwater images. They are comprised of frames of video showing trash, remotely operated underwater vehicles (ROVs), and undersea flora and fauna. Both datasets are sourced from the JAMSTEC E-Library of Deep-sea Images (J-EDI) dataset [citation], curated by the Japan Agency of Marine-Earth Science and Technology (JAMSTEC) captured from real-world environments, providing a variety of objects. The clarity of the water and quality of the light vary significantly between images creating a diverse dataset. The image sizes in these datasets are 480x270px and 480x360px. Provided annotations are in COCO format. The TrashCan dataset is annotated on the instance segmentation level (7212 images and 6214 annotations) with 16 classes for Material Version (8 classes are trash related and followed trash_ name pattern) or 22 for Instance Version. On the other hand, the Trash-ICRA19 dataset is annotated on the detection level (7668 images and 6706 annotations). It contains seven categories based on the material of the objects.</p><p>Drinking Waste. Drinking Waste <ref type="bibr" target="#b59">[61]</ref> contains over 4800 images of drinking waste belonging to 4 classes: Aluminium Cans, Glass bottles, PET bottles, and HDPE. Provided bounding-box annotations are in YOLO format. The dataset was created with a 12 MP phone camera. Images look similar -there is usually one object in the center on the indoor, plain background. Most of the images have the size of 512x683px.</p><p>MJU-Waste. MJU-Waste dataset <ref type="bibr" target="#b54">[55]</ref> is comprised of 2475 indoor trash images manually annotated in the form of an instance mask in COCO format. It allows two-class semantic segmentation (waste and background). For each color image, the co-registered depth image captured using an RGBD camera is provided. Objects are hand-held and mostly in the center of the image. In most cases there is only one object per image. The only image size is 640x480px.</p><p>Cigarette butt. The cigarette butt dataset <ref type="bibr" target="#b60">[62]</ref> consists of above 2k images, which were synthetically composed photos of small cigarettes lying on the ground. It is an artificial dataset created by applying random scale, rotation or brightness to the foreground cutouts of 25 different cigarette butts, placed in around 300 ground photos. Instance mask annotations are provided for each cigarette object with a median size of 66x65px. Original resolution of images is equal 3024x4032px.</p><p>Places. Places <ref type="bibr" target="#b61">[63]</ref> is a repository of 10 million scene photographs, labeled with 434 scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Images were downloaded by online image search engines (Google Images, Bing Images, and Flickr). Minimal size of images is 200x200px. Despite the fact that this is not a trash dataset, it can be used to identify natural and urbanized places without trash.  3 Two-stage framework to detect and sort litter</p><p>Litter sorting methods around the world differ significantly. On the one hand, this provides a large variety of objects (litter), as well as diverse backgrounds -waste can be commonly found indoor and outdoor, and in environments such as households, offices, roads and pavement scenes, and sometimes even under water. On the other hand, this affects available datasets, which do not provide a large enough number of annotated images. Moreover, a huge variety of categories and annotation levels (from classification to instance segmentation) resulted in the need for well-defined waste categories that could be applied in the industry. Hence, seven litter categories are provided based on a real-world waste segregation system in Gda?sk (Poland): bio, glass, metal and plastic, non-recyclable, other (e.g. batteries, large household appliances, tires), paper, and unknown (old, degraded litter).</p><p>To utilize all of the available data we tackle the problem by dividing the detection into two separate steps: litter localization and litter classification. The localization model is used to find regions in the image containing garbage. Then, each region is extracted and passed to the classification model, that assigns its category. The proposed pipeline is presented in the <ref type="figure">Fig. 2</ref>.</p><p>This section outlines the details behind the training procedure, the methodology, and our experiments. We compare various architectures, as well as numerous hyperparameters, to ensure the efficiency of the solution. Additionally, we present how each dataset can be used in the training process -separately for waste localization, classification or both, depending on the annotation type. <ref type="figure">Figure 2</ref>: A pipline of a two-stage framework to detect and sort litter</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Litter in images</head><p>During Detect Waste in Pomerania project, twelve publicly available datasets and an additional data collected using Google Images Download <ref type="bibr" target="#b62">[64]</ref> were used. Segmentation and detection datasets combined resulted in a new detectwaste dataset <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b59">61,</ref><ref type="bibr" target="#b58">60]</ref> (with the addition of cigarette butt dataset [62] -detect-waste+ dataset) with a single class -Litter -used in the first stage of our framework: Localization (see <ref type="figure">Fig. 2 top panel)</ref>. For the purpose of the second stage -Classification -additional images of bio, glass, other, and paper <ref type="bibr" target="#b45">[46,</ref><ref type="bibr">58,</ref><ref type="bibr" target="#b62">64]</ref> waste were added to the set of clipping images created by cutting out litter instances from some photos used in the detection stage <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b59">61,</ref><ref type="bibr" target="#b56">57]</ref>. These images form a final classification dataset named classify-waste. Furthermore, the dataset was supplemented with more than a thousand images containing garbage-free backgrounds <ref type="bibr" target="#b61">[63]</ref> and around 55k pseudolabeled images <ref type="bibr" target="#b56">[57]</ref>, creating the classify-waste+ dataset.</p><p>The localization task was mainly performed on a detect-waste dataset with a single class called litter. In the last experiments, the dataset was extended to the detect-waste+ dataset, which is previous detect-waste dataset with additional pictures presenting cigarette butts <ref type="bibr" target="#b60">[62]</ref> to boost the detection of small objects. <ref type="figure" target="#fig_1">Fig. 3</ref> shows the datasets included in the detect-waste+ dataset along with the number of used images, divided into groups depending on the trash environment (in the wild, underwater, inside homes and artificial). Additional experiments were performed for multi-class detection on Extended TACO. The division of this dataset into Gda?sk segregation categories is presented in the <ref type="figure" target="#fig_2">Fig. 4</ref>.  In the classification task, in addition to instances cut-out from Extended TACO and Drinking waste, some classes from TrashNet (paper and glass) and Waste Pictures (bio and other) datasets have been added. Additionally, we scrapped remaining data from the web. The Google Images Download <ref type="bibr" target="#b62">[64]</ref> software was used to search and collect more images with bio and other waste like batteries, medicines or rubble, and also images that depicted scenes without the presence of garbage (background) from Places dataset <ref type="bibr" target="#b61">[63]</ref>. Semi-labeled (with predicted localization) images from Open Litter Map <ref type="bibr" target="#b56">[57]</ref> were also utilized. <ref type="figure" target="#fig_3">Fig. 5</ref> shows number of used images per category (according to the obligatory segregation rules of the city of Gda?sk) without images from Open Litter Map, while <ref type="table" target="#tab_1">Table 2</ref> provides information from which dataset the images come from, for which category. <ref type="table" target="#tab_2">Table 3</ref> presents numbers of images from semi-labeled Open Litter Map with some established seven litter class pre-assignment.   Most of the trash found in the classification dataset is metal and plastic. Unfortunately, the second numerously represented category is unknown -the litter that has probably decomposed so much that it is hard to classify it. This makes our dataset highly imbalanced and requires special attention in the following steps. Another problem is related to annotations errors coming from datasets without strict annotation rules (especially when it comes to the trash label) -unfortunately some annotations are of poor quality. Moreover, we manually rejected some of the images that were malformed or mislabeled.</p><p>To ensure that the training and test data distributions approximately match, we randomly selected 80% of the original images as the training set. We kept the rest as validation/testing set for each part of the used dataset separately. The split was done by preserving the percentage of samples for each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Litter Detection</head><p>In the first step, the proposed framework localizes litter in the image without recognizing its class. Three different neural networks popular in common object detection tasks were analyzed, namely EfficientDet <ref type="bibr" target="#b63">[65]</ref>, DETR <ref type="bibr" target="#b64">[66]</ref>, and Mask R-CNN <ref type="bibr" target="#b30">[31]</ref>. The first two architectures allow for object detection, whereas the third one also implements segmentation. All models were trained to predict trash at each location and scale (the solution is not targeted at a specific object size). To properly evaluate each proposed architecture, a set of tests was conducted using a wide range of datasets altogether and individually. Defined classes varied depending on the used dataset, as described in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Training details</head><p>To ensure one-stage detector efficient and fast performance, as EfficientDet <ref type="bibr" target="#b41">[42]</ref>, was utilized to localize litter in images. The used model was initialized with pre-trained weights. In the final experiments decay rate was set to 0.95, the learning rate to 1e-3, and the number of epochs equaled 20. Also, several approaches to data augmentation <ref type="bibr" target="#b65">[67]</ref> were used. During training, Gaussian blur, random brightness, rain, fog, and snow were added. Additionally, images were rotated and cropped around the annotated bounding boxes with padding if needed. However, the best results were obtained for simple resize (to match default EfficientDet's input size), and normalization using means and standard deviation values per channel as for the COCO dataset <ref type="bibr" target="#b57">[59]</ref>. We hypothesize that augmentation did not significantly improve the results because used data was naturally diversified by mixing a wide range of datasets. In conducted studies, the Pytorch implementation of EfficientDet <ref type="bibr" target="#b63">[65]</ref>.</p><p>Also, Transformers approach was applied to the waste detection task. DETR (which stands for DEtection TRansformer) with ResNet-50 and ResNet-101 backbone was initialized with pre-trained weights. In all experiments AdamW <ref type="bibr" target="#b66">[68]</ref> optimizer was used, with a learning rate set to 1e-4 in the transformer and 1e-5 in the backbone. Selected number of queries, that represents the maximum number of instances that could be found in image, equaled 100, as by default. In final experiments, after about 63 epochs loss values saturate, and longer training reduce neither the training nor the validation error. The DETR litter detection model is based on Facebook team implementation <ref type="bibr" target="#b64">[66]</ref>.</p><p>In the case of instance segmentation, Mask R-CNN <ref type="bibr" target="#b44">[45]</ref> with Resnet-50 backbone was used. In final experiments, the model was trained for 26 epochs with learning rate set to 1e-3 and decay set to 1e-4. The transfer learning technique was also used in this study, as each of the models was initially pre-trained on the COCO 2017 [69] subset. Implementation of the model is based on standard Pytorch library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Results</head><p>Conducted experiments were divided into two phases. In the first phase selected architectures were tested on the detect-waste dataset. The model, which exhibit the best performance, was selected for further training. The second phase showed how the results are distributed on the subpart datasets. This allowed for a deeper analysis of the effectiveness of the selected architecture and better preparation to the final training. As a basic evaluation metric, Average Precision (AP) for intersection over union (IoU) equaled 0.50 was used (AP@05 <ref type="bibr" target="#b67">[70]</ref>). In the case of multi-label detection, this metric is averaged over all categories, giving mAP@0.50. As presented in <ref type="table" target="#tab_3">Table 4</ref>, average precision of litter detection, using detect-waste dataset with one class, varied in the range of 28.0% for Mask R-CNN with ResNet-50 backbone to 65.5% for EfficientDet-D2. In general, due to a significant gap between EfficientDets and other tested architectures the rest of experiments were limited to this network family. Additionally, more complex networks from this family (i.e. EfficientDet-D3) were also tested, and exhibited a similar performance. As EfficientDet-D2 network reached the best evaluation results, it is smaller (considering the number of parameters) and requires less computing power, we decided to proceed with it exclusively.</p><p>Results of a comprehensive study using EfficientDet-D2 and selected datasets separately are presented in <ref type="table" target="#tab_4">Table 5</ref>. The mAP@0.50 calculated per dataset is the highest for images presenting indoor scenario, namely Drink-waste (cans, plastic and glass bottles) and MJU-Waste (one hand-held waste object per image) datasets. However, EfficientDet-D2 also reached very high score, mAP@0.50 above 90%, for TrashCan 1.0 with selected 8 underwater waste categories. The worst result, reaching mAP@0.50 below 10%, was achieved for the underwater images from Trash-ICRA19 dataset, which could be related with poor quality of the photos (blurred movie frames). On the other hand, detection performance for images taken in natural or urban background was in range of 56.8% for Extended TACO (trash in various environments) to 74.1 for UAVVaste (small objects constituting over 80% of the dataset shown from a bird's eye view), which proved that precise detection of garbage in the different environment is possible. Corresponding sample predictions are shown in the <ref type="figure">Fig. 6</ref>.</p><p>Achieved results proved importance of data quality in the learning process of DL-based system, but apart from different quality of photos and environments of waste occurrence in which they were taken, also the number and kind of waste classes varied depending on the analyzed dataset. Moreover, annotated trash objects differ in shape and size. This suggests that one-class detection (with one litter category) using EfficientDet-B2 network might lead to much better performance. To provide more details, in final experiments with EfficientDet-D2, AP score at different IoU thresholds levels, along with AP@[0.50:0.95] <ref type="bibr" target="#b57">[59]</ref> (AP integrated over IoUs in range from 0.5 to 0.95, and step 0.05) were calculated. This results are presented in <ref type="table" target="#tab_5">Table 6</ref>. Selected neural network was tested in four different scenarios. In two of them the Extended TACO dataset was used, while the third and fourth experiments were conducted using different type of detect-waste dataset instead.  <ref type="figure">Figure 6</ref>: Example EfficientDet-D2 predictions for the diverse waste datasets. Selected images were taken in different locations such as a beach, pavement, indoor, and underwater. Detected objects vary in size and number -images show from one to five small, medium or large objects. EfficientDet-D2 trained on the detect-waste+ dataset reached the biggest mAP for IoU equal both 0.50 (66.4%) and 0.75 (51.3%). Solution effectiveness in respect of detected object size demonstrated better precision for large (59.8%) and medium (51.3%) objects, than for small (9.3%) instances. Moreover, as expected, the test conducted using the detect-waste+ dataset, which boosted the evaluation result for small objects from 5.9% to 9.3%, and at the same time increased effectiveness in terms of AP@0.50 from 65.5% to 66.4%. In the case of a models trained on the Extended TACO dataset with one class, AP@0.50 reached 9.6 percentage points less (56.8%) than the one that was trained on the detect-waste+ dataset (66.4%). However, in respect to detected objects size, among all conducted experiments, it reached the best results for small (19.8%) objects.</p><p>On the other hand, experiments in which detected trash was divided into seven classes (bio, glass, metals and plastic, non-recyclable, other, paper, unknown) resulted in significantly reduced mAP in all analyzed evaluation metrics, reaching almost 4 times smaller value for both IoU equal 0.50 (16.2%) and 0.75 (13.0%) than the best solution. Regarding detected objects size, for tiny objects multi-label detector trained on Extended TACO dataset reached better results (6.4%) than the one that was trained on detect-waste dataset with one class (5.9%). This may be due to the fact that in Extended TACO, approximately 45% of instances are small (area &lt; 32 2 ), while for detect-waste it is only almost 25% of whole dataset. For that reason, feeding the detect-waste dataset with cigarettes butt improved the quality of prediction. It is worth emphasizing that a neural network that provides an ability to detect directly into 7 waste categories demonstrated average precision similar to one-class based detectors only for one category -metals and plastic -reaching AP@0.50 = 43.3%. Results obtained for the remaining six classes varied in range of AP@0.50 = 0.1% for bio to AP@0.50 = 8.9% for unknown litter. For that reason, it was decided to perform classification in a separate stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Litter Classification</head><p>At the second stage of our approach, we performed multi-class classification for seven waste categories. Due to the imbalanced classify-waste dataset with a significant dominance of metals and plastic class, classification networks were trained on cut out trash with some additions of underrepresented classes from other classification datasets as presented in <ref type="figure" target="#fig_3">Fig. 5</ref> and described in Section 3.1. Boundaries of cropped litter were established using bounding boxes for annotated images and objects detected by EfficientDet-D2 <ref type="bibr" target="#b41">[42]</ref> in case of unlabeled data. These all combined waste instances were applied as an input images to solve the classification problem.</p><p>To take advantage of the semi-labelled Open Litter Map dataset <ref type="bibr" target="#b56">[57]</ref> pseudo-labeling <ref type="bibr" target="#b68">[71]</ref> technique was applied. Pseudo-labeling is a kind of semi-supervised teaching method. The main concept of this approach is to teach a network to detect objects on data with and without annotation. Firstly unlabeled data is preassigned to some category by pretrained model, and afterwards used in further training. The process is repeated during the training every batch or epoch, as is presented in <ref type="figure">Fig. 7</ref>. This provides considerably larger, yet only partially annotated, dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Training details</head><p>In our research we have tested two architectures dedicated to object classification problem -ResNet-50 <ref type="bibr" target="#b11">[12]</ref> and EfficientNet-B2 <ref type="bibr" target="#b17">[18]</ref> to chose the one that achieves better performance for waste classification task. We have thoroughly examined number of training hyperparameters. During all experiments, we set the learning rate to 1e-4, batch size to 16, and trained each network for 20 epochs. The output of our network differed between 7 and 8 classes. The first 7 classes refer to previously mentioned waste categories, while the 8th was a background class in order to reduce number of false-positives. Also, two kinds of samplers -random and weighted -were used to reduce the impact of data imbalance. Therefore it was crucial to investigate results on each class, not only on a whole dataset. The other hyperparameter was the "pseudo-labelling type" that shows if the pseudo-labels update was performed every batch or every epoch, or not at all.</p><p>Moreover, to additionally ensure our training dataset's diversity, we applied data augmentation <ref type="bibr" target="#b65">[67]</ref>. At first, we extended the dimensions of our images to randomly crop parts of them. Horizontal/vertical flip and shift scale rotation, which does not change the image's content but only the position of an object in the image, were also applied. On the other hand, a random brightness contrast and cutout were used to change the image's content slightly. Finally, both training and testing sets were normalized and resized to 224x224, as it was an EfficientNet-B2 <ref type="bibr" target="#b17">[18]</ref> input size. The second stage of our approach was self-implemented in Pytorch Lightning and based on the repository <ref type="bibr" target="#b69">[72]</ref>. <ref type="figure">Figure 7</ref>: Flowchart for pseudo-labeling semi-supervised learning technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Results</head><p>A comparison of the performance of two different neural networks is presented in <ref type="table" target="#tab_6">Table 7</ref>. One of them is EfficientNet-B2 -the backbone of the network used in the detection task. The results achieved using this classifier exceeded markedly higher values (over 10 percentage points) than ResNet-50. EfficientDet-B2 being state-of-the-art architecture comparing to ResNet-50 provides better performance in the classification task, similary as in detection, where backbones are EfficientNet-B2 for EfficientDet-D2 and ResNet-50 for Mask RCNN or DETR (see <ref type="table" target="#tab_3">Table 4</ref>). It leads to the conclusion, that the EfficientNet networks family is a preferred choice to deal with waste images.</p><p>The experiments showed that updating pseudo-labels every batch can slightly raise accuracy. Although the bestachieved accuracy was 74.6%, it was performed for a random sampler. While analyzing the confusion matrices of each training, we have noticed that applying a weighted sampler provides more balanced results for each class. Therefore we have achieved an accuracy of 73% (and 86.7% on the training set), while almost 25% of our dataset were test images.</p><p>Although the confusion matrix clearly shows that most of the predictions are accurate (see <ref type="figure" target="#fig_4">Fig. 8</ref>, and <ref type="table" target="#tab_7">Table 8</ref>), it indicates a significant data imbalance. The metals and plastic class was predicted with the highest precision of 87%, which is connected to the large representatives of this class. Still, it also results in a relatively low recall, which means that many objects are classified incorrectly as metals and plastic. There was a noticeable problem with identifying the unknown and non-recyclable classes, which precision was equal 52%. The other classes were recognized with the higher precision but still, due to the data imbalance, not fully correct.</p><p>The biggest confusion was remarked between metals and plastic and unknown category. Probably it was because of partly degraded or destroyed trash. All the classes were rarely misclassified with the glass, as evidenced by a high recall value -82%. The F 1 -score metric for this class achieved the utmost result -83%, which shows the balance between false positives and false negatives. It is worth noticing, that eliminating background from the rest of the waste was extremely successful -at the level of 97% for precision and 97% for recall. Adding a separate class for background improved the performance and as it was assumed, reduced the number of false-positives.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>There is a visibly increasing demand for artificial intelligence in numerous human activities. Following that, we proposed a DL-based framework that can localize trash in the image and then identify its class using two separate neural networks. Datasets used in this study were created by using various publicly available data of waste collected in diverse environments: inside houses, in the natural or urban environment and even underwater.</p><p>Firstly, three neural networks architectures were adapted to conduct the garbage detection. Used data was naturally augmented, which allowed for a precise location of the waste with AP@0.5 equals 66.4% for EfficientDet-D2 model. This is an excellent result comparing to other recent reports, that were presented for some of used datasets separately (15.9% for TACO with Mask R-CNN <ref type="bibr" target="#b52">[53]</ref>, or 55.4% for TrashCan 1.0 with Mask R-CNN <ref type="bibr" target="#b51">[52]</ref>). Qualitatively good results were also observed for the semi-labeled OpenLitterMap dataset, allowing for its further use in the next stage.</p><p>In the case of classification, litter was divided into seven categories that imitate the sorting policy introduced in the city of Gda?sk with extra background class to eliminate false positives coming from the detector. Additionally, during training pseudo-labeling technique was applied, which allowed to utilize an unlabeled data. However, this gave only a slight performance boost in case of litter classification, which can be related to high imbalance and small amounts of labeled data for specific litter categories, especially in the case of bio or other waste. In the end, the accuracy up to 75% was achieved for the EfficientNet-B2 network. To the best of our knowledge, we present one of the first results to classify litter in wild.</p><p>As Artificial Intelligence is required to be more accurate than a human, the main future direction for the proposed system will be to improve its performance. Selected detectors works well when localizing medium and large objects, but recognition of small litter is still challenging. For that reason, exploring different state-of-the-art approaches, such as Deformable DETR <ref type="bibr" target="#b40">[41]</ref>, seems to be a good idea. On the other hand, a more balanced dataset and the use of the latest EfficientNetv2 <ref type="bibr" target="#b18">[19]</ref>, could also boost the classification accuracy.</p><p>Despite this, the presented framework showed the great potential of the DL-based methodology for waste management.</p><p>In the future with the assistance of DL models, it would be possible to mount robotic arms in waste management plants to automatically distinguish between different classes of objects and sort garbage without human intervention. Additionally, the high precision of litter localization in a large variety of the environments, shows the possibility of using neural networks for waste monitoring in cities or detecting of illegal dumps in nature, for example with the use of drones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgements</head><p>MF, ZK, SM and AM contributed to the final version open-source code. MF, ZK and SM analyzed and interpreted the waste datasets regarding the waste sorting rules in Gda?sk (Poland). AM and SM planned the experiments and adjusted the models. SM prepared the results of the experiments. AM lead the team during the project. SM took the lead in the preparation of the manuscript. AM prepared the illustrations. AK and KM helped supervise the project.</p><p>SM wrote the Introduction. SM and AM conducted the researched on available datasets. AK conducted and written the review on the deep learning classification. KM conducted and written the review on the deep learning object detection. ZK analyzed the data and written the datasets sections. MP and MF described our approach and experiments. SM concluded our project and proposed future directions.</p><p>All authors provided critical feedback and helped shape the research, analysis and manuscript. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Representative pictures from each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Datasets included in detect-waste+ dataset used for detection task provided with number of images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Annotations per category for Extended TACO dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Images per category for classify-waste + Places.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Evaluation of the accuracy of a classification in form of confusion matrix for EfficientNet-B2, Weighted Sampler, and Pseudolabelling per batch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics for selected public waste datasets for classification and object detection purposes.</figDesc><table><row><cell>Dataset</cell><cell cols="4"># classes # images # instances annotation type</cell></row><row><cell>TrashNet</cell><cell>5</cell><cell>2194</cell><cell>2194</cell><cell>labels</cell></row><row><cell>Waste Pictures</cell><cell>34</cell><cell>23633</cell><cell>23633</cell><cell>labels</cell></row><row><cell cols="2">Open Litter Map &gt;100</cell><cell>&gt;100k</cell><cell>&gt;100k</cell><cell>multilabels</cell></row><row><cell cols="2">Extended TACO 7</cell><cell>4562</cell><cell>14286</cell><cell>bounding box</cell></row><row><cell>Wade-AI</cell><cell>1</cell><cell>1396</cell><cell>2247</cell><cell>instance masks</cell></row><row><cell>UAVVaste</cell><cell>1</cell><cell>772</cell><cell>3718</cell><cell>instance masks</cell></row><row><cell>TrashCan 1.0</cell><cell>8</cell><cell>7212</cell><cell>6214</cell><cell>instance masks</cell></row><row><cell>Trash-ICRA19</cell><cell>7</cell><cell>7668</cell><cell>6706</cell><cell>bounding box</cell></row><row><cell>Drinking waste</cell><cell>4</cell><cell>4810</cell><cell>5058</cell><cell>bounding box</cell></row><row><cell>MJU-Waste</cell><cell>1</cell><cell>2475</cell><cell>2532</cell><cell>instance masks</cell></row><row><cell>Cigarette butt</cell><cell>1</cell><cell>2200</cell><cell>2200</cell><cell>instance masks</cell></row><row><cell>Places</cell><cell>205</cell><cell>2.5M</cell><cell>2.5M</cell><cell>labels</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Number of images and their origin for a given category for the classification task.</figDesc><table><row><cell>Dataset</cell><cell cols="8">background bio glass metals &amp; plastic paper non-recyclable other unknown</cell></row><row><cell cols="2">Extended TACO -</cell><cell>69</cell><cell>592</cell><cell>6057</cell><cell>601</cell><cell>2802</cell><cell>154</cell><cell>3258</cell></row><row><cell>Drinking waste</cell><cell>-</cell><cell>-</cell><cell cols="2">1162 3604</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>waste-pictures &amp;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Google search</cell><cell>-</cell><cell>92</cell><cell>49</cell><cell>-</cell><cell>203</cell><cell>-</cell><cell>366</cell><cell>-</cell></row><row><cell>TrashNet</cell><cell>-</cell><cell>-</cell><cell>501</cell><cell>-</cell><cell>801</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Places</cell><cell>1017</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Number of images from Open Litter Map with pre-assignment of litter class.</figDesc><table><row><cell>Category</cell><cell># pseudo-labels</cell></row><row><cell>bio</cell><cell>-</cell></row><row><cell>glass</cell><cell>3136</cell></row><row><cell cols="2">metals and plastic 29219</cell></row><row><cell>non-recyclable</cell><cell>1971</cell></row><row><cell>other</cell><cell>900</cell></row><row><cell>paper</cell><cell>819</cell></row><row><cell>unknown</cell><cell>19701</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of results of selected architectures on detect-waste dataset with one class -litter.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell>AP@0.5</cell></row><row><cell>DETR</cell><cell>ResNet-50</cell><cell>50.7</cell></row><row><cell>DETR</cell><cell>ResNet-101</cell><cell>51.6</cell></row><row><cell>Mask R-CNN</cell><cell>ResNet-50</cell><cell>28.0</cell></row><row><cell cols="3">EfficientDet-D2 EfficientNet-B2 65.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results on different datasets achieved using EfficientDet-D2.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Classes mAP@0.50</cell></row><row><cell cols="2">Extended TACO 1</cell><cell>56.8</cell></row><row><cell>Wade-AI</cell><cell>1</cell><cell>71.5</cell></row><row><cell>UAVVaste</cell><cell>1</cell><cell>74.1</cell></row><row><cell>TrashCan 1.0</cell><cell>8</cell><cell>91.3</cell></row><row><cell>Trash-ICRA19</cell><cell>7</cell><cell>7.3</cell></row><row><cell>Drink-waste</cell><cell>4</cell><cell>99.4</cell></row><row><cell>MJU-Waste</cell><cell>1</cell><cell>97.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>EfficientDet-D2 final evaluation results.DatasetClasses mAP@0.50:0.95 mAP@0.50 mAP@0.75 AP S AP M AP L</figDesc><table><row><cell cols="2">Extended TACO 7</cell><cell>11.9</cell><cell>16.2</cell><cell>13.0</cell><cell>6.4</cell><cell>9.4</cell><cell>15.0</cell></row><row><cell cols="2">Extended TACO 1</cell><cell>40.4</cell><cell>56.8</cell><cell>43.5</cell><cell>19.8</cell><cell>37.2</cell><cell>51.6</cell></row><row><cell>detect-waste</cell><cell>1</cell><cell>45.8</cell><cell>65.5</cell><cell>50.2</cell><cell>5.9</cell><cell>49.1</cell><cell>59.7</cell></row><row><cell>detect-waste+</cell><cell>1</cell><cell>46.9</cell><cell>66.4</cell><cell>51.3</cell><cell>9.3</cell><cell>51.3</cell><cell>59.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison of accuracy of selected classifiers.</figDesc><table><row><cell>Model</cell><cell cols="3"># classes Accuracy Sampler</cell><cell>Pseudo-labeling type</cell></row><row><cell cols="2">EfficientNet-B2 8</cell><cell>73.02</cell><cell cols="2">Weighted per batch</cell></row><row><cell cols="2">EfficientNet-B2 8</cell><cell>74.6</cell><cell>Random</cell><cell>per epoch</cell></row><row><cell cols="2">EfficientNet-B2 8</cell><cell>72.8</cell><cell cols="2">Weighted per epoch</cell></row><row><cell cols="2">EfficientNet-B4 7</cell><cell>71.0</cell><cell>Random</cell><cell>per epoch</cell></row><row><cell cols="2">EfficientNet-B4 7</cell><cell>67.6</cell><cell cols="2">Weighted per epoch</cell></row><row><cell cols="2">EfficientNet-B2 7</cell><cell>72.7</cell><cell>Random</cell><cell>per epoch</cell></row><row><cell cols="2">EfficientNet-B2 7</cell><cell>68.3</cell><cell cols="2">Weighted per epoch</cell></row><row><cell cols="2">EfficientNet-B2 7</cell><cell>74.4</cell><cell>Random</cell><cell>None</cell></row><row><cell>ResNet-50</cell><cell>8</cell><cell>60.6</cell><cell cols="2">Weighted None</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Summary of the precision, recall, and F 1 -score for each class of waste.</figDesc><table><row><cell>Class name</cell><cell cols="3">Precision Recall F 1 -score</cell></row><row><cell>background</cell><cell>0.97</cell><cell>0.97</cell><cell>0.97</cell></row><row><cell>bio</cell><cell>0.62</cell><cell>0.51</cell><cell>0.56</cell></row><row><cell>glass</cell><cell>0.83</cell><cell>0.82</cell><cell>0.83</cell></row><row><cell cols="2">metals and plastic 0.87</cell><cell>0.70</cell><cell>0.78</cell></row><row><cell>non-recyclable</cell><cell>0.52</cell><cell>0.65</cell><cell>0.58</cell></row><row><cell>other</cell><cell>0.71</cell><cell>0.74</cell><cell>0.72</cell></row><row><cell>paper</cell><cell>0.62</cell><cell>0.76</cell><cell>0.68</cell></row><row><cell>unknown litter</cell><cell>0.52</cell><cell>0.65</cell><cell>0.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>The 5-month (October 2020 -February 2021) lasting project "Detect Waste in Pomerania" was organized and lead by Agnieszka Miko?ajczyk, Magdalena Kortas and Ewa Marczewska from Women in Machine Learning &amp; Data Science Trojmiasto. A team of carefully selected members, nine female data scientists, analysts and machine learning engineers supported by five industry mentors studied and worked together on developing a model for trash detection. This solution would be applicable for video and photography. The authors acknowledge other team members: Anna Brodecka, Katarzyna ?agocka, Ewa Marczewska, Pedro F. Proen?a, Adam Kaczmarek and Iwona Sobieraj who contributed to the project.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Only the primary part (1.5k images) of the dataset is annotated with instance masks, we provided bounding box annotation for the rest.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors of <ref type="bibr" target="#b10">[11]</ref> proposed a family of architectures called VGG -very deep networks consisting of 16 to 19 layers. The authors observed that stacking of several 3x3 layers could imitate the kernels of bigger sizes achieving greater effective receptive fields while reducing the number of parameters. This strategy provides an additional benefit of a more discriminative decision function caused by more nonlinear layers in the structure.</p><p>Further increase in the number of layers leads to deterioration of training and decrease in neural network performance. To alleviate the problem ResNet family <ref type="bibr" target="#b11">[12]</ref> was proposed. The architecture is based on residual connections, that introduce no extra parameters or computational effort. Instead, this connectivity pattern facilitates gradient flow, enabling effective training of networks consisting of as many as 200 layers.</p><p>The authors acknowledge the infrastructure and support of Digital Innovation Hub dih4.ai in Gda?sk. The authors wish to express their thanks to Voicelab.ai for the financial support and Epinote for data annotation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Global waste to grow by 70 percent by 2050 unless urgent action is taken</title>
		<ptr target="https://www.worldbank.org/en/news/press-release/2018/09/20/global-waste-to-grow-by-70-percent-by-2050-unless-urgent-action-is-taken-world-bank-report" />
	</analytic>
	<monogr>
		<title level="m">World bank report</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">No plastic in nature: Assessing plastic ingestion from nature to people</title>
		<ptr target="https://www.newcastle.edu.au/newsroom/featured/plastic-ingestion-by-people-could-be-equating-to-a-credit-card-a-week" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<ptr target="http://www3.weforum.org/docs/WEF_The_New_Plastics_Economy.pdf" />
		<title level="m">The new plastics economy rethinking the future of plastics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sorting of plastic waste for effective recycling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biswajit</forename><surname>Ruj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyajit</forename><surname>Jash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Appl. Sci. Eng. Res</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Wastenet: Waste classification at the edge for smart bins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Palade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siobhan</forename><surname>Clarke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An internet of things based smart waste management system using lora and tensorflow deep learning model</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<editor>Teoh Ji Sheng, Mohammad Shahidul Islam, Norbahiah Misran, Mohd Hafiz Baharuddin, Haslina Arshad, Md. Rashedul Islam, Muhammad E. H. Chowdhury, Hatem Rmili, and Mohammad Tariqul Islam</editor>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="148793" to="148811" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The internet of wasted things (iowt)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Ashrafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Heydarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madhur</forename><surname>Behl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IOT &apos;18: Proceedings of the 8th International Conference on the Internet of Things, IOT &apos;18</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A smart waste management with self-describing objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Glouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Couderc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Second International Conference on Smart Systems, Devices and Technologies (SMART&apos;13)</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI&apos;17</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence, AAAI&apos;17</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnetv2</surname></persName>
		</author>
		<title level="m">Smaller models and faster training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/2103.15808</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Object detection in 20 years: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxia</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05055</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE computer society conference on computer vision and pattern recognition</title>
		<meeting>the 2001 IEEE computer society conference on computer vision and pattern recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE computer society conference on computer vision and pattern recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recognition using regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1030" to="1037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08036</idno>
		<title level="m">Scaled-yolov4: Scaling cross stage partial network</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The open images dataset v4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10778" to="10787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7036" to="7045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doll?r</surname></persName>
		</author>
		<idno>abs/1506.06204</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trashnet</surname></persName>
		</author>
		<ptr target="https://github.com/garythung/trashnet" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A comparison of activation functions in artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cenk</forename><surname>Bircanoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nafiz</forename><surname>Ar?ca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th Signal Processing and Communications Applications Conference (SIU)</title>
		<imprint>
			<date type="published" when="2018-05" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multilayer hybrid deep-learning method for waste classification and recycling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohai</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Kamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence and Neuroscience</title>
		<imprint>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Smart trash net: Waste localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oluwasanya</forename><surname>Awe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robel</forename><surname>Mengistu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Sreedhar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Research on automatic garbage detection system based on deep learning and narrowband internet of things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhishan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyun</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikai</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics: Conference Series</title>
		<imprint>
			<biblScope unit="volume">1069</biblScope>
			<biblScope unit="page">12032</biblScope>
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Robotic detection of marine litter using deep visual detection models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sattar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5752" to="5758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Trashcan: A semantically-segmented dataset towards visual detection of marine debris</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungseok</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junaed</forename><surname>Sattar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Proen?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sim?es</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06975</idno>
		<title level="m">Taco: Trash annotations in context for litter detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Yolo trashnet: Garbage detection in video streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Carolis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ladogana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Macchiarulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Evolving and Adaptive Intelligent Systems (EAIS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A multi-level approach to waste object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzheng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyi</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Autonomous, onboard vision-based trash and litter detection in low altitude aerial images collected by an unmanned aerial vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Piechocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartosz</forename><surname>Ptak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Walas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">com -open data on plastic pollution with blockchain rewards (littercoin)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openlittermap</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>Open geospatial data, softw. stand</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Let&apos;s Do It Foundation. wade-ai</title>
		<ptr target="https://github.com/letsdoitworld/wade-ai" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Drinking waste classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadiy</forename><surname>Serezhkin</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/arkadiyhacks/drinking-waste-classification" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Cigarette butt dataset &amp; trained weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kelly</surname></persName>
		</author>
		<ptr target="https://www.immersivelimit.com/datasets/cigarette-butts" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Hardik Vasa. google-images-download</title>
		<ptr target="https://github.com/hardikvasa/google-images-download" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientdet</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/efficientdet-pytorch" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>pytorch</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Albumentations: Fast and flexible image augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Druzhinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandr</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei Ba Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980v9</idno>
		<title level="m">A method for stohastic optimisation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mark Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams Luc Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Comput Vis</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2013 Workshop : Challenges in Representation Learning (WREPL)</title>
		<imprint>
			<biblScope unit="volume">07</biblScope>
			<biblScope unit="page">2013</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<ptr target="https://github.com/lukemelas/EfficientNet-PyTorch" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Efficientnet-pytorch</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
