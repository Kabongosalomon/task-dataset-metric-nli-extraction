<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TLDR9+: A Large Scale Resource for Extreme Summarization of Social Media Posts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajad</forename><surname>Sotudeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IRLab</orgName>
								<orgName type="institution">Georgetown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanieh</forename><surname>Deilamsalehy</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
							<email>franck.dernoncourt@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IRLab</orgName>
								<orgName type="institution">Georgetown University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TLDR9+: A Large Scale Resource for Extreme Summarization of Social Media Posts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent models in developing summarization systems consist of millions of parameters and the model performance is highly dependent on the abundance of training data. While most existing summarization corpora contain data in the order of thousands to one million, generation of large-scale summarization datasets in order of couple of millions is yet to be explored. Practically, more data is better at generalizing the training patterns to unseen data. In this paper, we introduce TLDR9+ -a largescale summarization dataset-containing over 9 million training instances extracted from Reddit discussion forum (https://github. com/sajastu/reddit_collector). This dataset is specifically gathered to perform extreme summarization (i.e., generating onesentence summary in high compression and abstraction) and is more than twice larger than the previously proposed dataset. We go one step further and with the help of human annotations, we distill a more finegrained dataset by sampling High-Quality instances from TLDR9+ and call it TLDRHQ dataset. We further pinpoint different state-ofthe-art summarization models on our proposed datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text summarization is defined as generating a concise sequence of text as summary, given relatively a longer document as source. A high-quality summary conveys the most important points of its associated source. The task is generally performed in two ways: 1) extractive in which salient sentences are identified and concatenated to form the final summary <ref type="bibr" target="#b25">(Nallapati et al., 2017;</ref><ref type="bibr" target="#b7">Dong et al., 2018;</ref><ref type="bibr" target="#b32">Sotudeh et al., 2021a;</ref><ref type="bibr" target="#b28">Narayan et al., 2020;</ref><ref type="bibr" target="#b5">Cho et al., 2020)</ref>; and 2) abstractive that produces a paraphrasing of the main contents of the given text. <ref type="bibr" target="#b30">(See et al., 2017;</ref><ref type="bibr" target="#b8">Gehrmann et al., 2018</ref>; MacAvaney *Work done during the internship at Adobe Research.</p><p>We go to school together, we have three lessons a week together. She normally sits at the front and I sit at the back, but recently the person I sit next to has been struggling with mental health and hasn't been in, so I moved and sit next to her most lessons. We also do this engineering scheme together, so we have maybe half an hour a week with two other people working on that. For a while now we 've texted each other a few times a week with pictures of our cats, since we both love them. Outside of that, we don't really hang out at all. I see a lot of theatre, and about a week ago she said she wanted to come see something with me. So I agree, I love showing people theatre. When we find our seats, mine has a pole in the way so I can't see a section of the stage unless I lean away from her, but her view is perfect. About half an hour in, she leans on my shoulder. Halfway through act 2 she starts hugging my arm, while still leaning on my shoulder. She was kind of cuddling all day, we went to an arcade earlier as well. She doesn't seem like the cuddling type of friend, and I'm very worried she has a crush on me. I don't want to ruin a friendship, I don't like her back. Should I just ignore it until she asks me? What if she thinks that was a date? TL;DR I took my friend to see a show, she leant on my shoulder the whole time. I 'm not into her but I think she has a crush on me? <ref type="figure">Figure 1</ref>: An example Reddit post with TLDR summary. As seen, the TLDR summary is extremely short, and highly abstractive. <ref type="bibr" target="#b40">Zhang et al., 2019;</ref><ref type="bibr" target="#b34">Sotudeh et al., 2020a;</ref><ref type="bibr" target="#b20">Lewis et al., 2020;</ref><ref type="bibr" target="#b19">Lebanoff et al., 2020)</ref> and is considered more challenging as the model needs to deal with novel words generation beyond sentence extraction.</p><p>Over the past few years, different neural models including RNN <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997)</ref> and Transformer-based <ref type="bibr">(Vaswani et al., 2017)</ref> networks have been proposed to facilitate the summarization task. While promising, the performance of such models is bound to the abundance of training data due to the massive model complexity <ref type="bibr" target="#b39">(Ying, 2019)</ref>. Lack of sufficient training data worsens the model's ability to generalize patterns in training data to unseen data <ref type="bibr">(Althnian et al., 2021)</ref>. In addition, overfitting will be likely inevitable as the model is forced to learn from a limited set of data; hence, hindering the generalization. This justifies the necessity of large-scale corpora for training large and complex models.</p><p>Prevalence of social media platforms has provided communities with an opportunity to ex- change different types of data while interacting with each other. Reddit 1 is one of such popular platforms where users post their content of interest in a variety of domains. TLDR, the acronym for "Too Long; Didn't Read", is a common practice that aims at removing unnecessary information from the lengthy post, and presenting its gist information in a few words. <ref type="figure">Figure 1</ref> shows a sample of Reddit post with its TLDR, which aims at abstracting post with extreme compression. Abundance of posts that contain such TLDRs during recent years has given rise to generation of data collections that can be utilized for training deep neural networks; hence, addressing the challenge of largescale datasets' scarcity. Despite the possibility of acquiring large-scale datasets from social media platforms, training deep neural networks on such datasets is yet challenging. This might be due to the specific writing style of social media content such as informal language and massive noise within such content <ref type="bibr" target="#b35">(Sotudeh et al., 2020b)</ref>. <ref type="table">Table 1</ref> shows some of the existing summarization datasets in social and non-social media domains. These datasets are specifically proposed for extreme summarization task, where the aim is to produce one to two summary sentences in extreme compression and high abstraction. In this paper, we introduce our dataset, TLDR9+ with over 9 million instances which is more than twice larger than the previous dataset <ref type="bibr" target="#b37">(V?lske et al., 2017)</ref>. We further sample high-quality instances in virtue of human annotations from TLDR9+ to construct TLDRHQ yielding 1.7 million instances in the hope of providing firm grounds for future work. Owing to extremely short length of TLDR summaries (less than 40 words), our datasets are rather suitable for extreme summarization task than for longer ones.</p><p>In this research, we aim at harvesting instances that include TLDRs written by the Reddit users spanning the period of 2005-2021. Our early attempt at gathering such instances yields over 9 million instances with TLDRs as the initial set (i.e., TLDR9+). Since social media posts are inherently noisy, we consider applying a heuristic method to cut out low-quality instances from the initial set, which ultimately results in 1.7 million high-quality instances. For deciding such heuristic, we employ human annotators to help to obtain a more finegrained dataset (i.e., TLDRHQ). Furthermore, we establish various state-of-the-art extractive and abstractive summarization models on our proposed datasets. Finally, we carry out an analysis over the results on both datasets to shed light on future direction. We believe that our datasets can be utilized to pave the path for future research. Our miner code and data are made publicly available at https:// github.com/sajastu/reddit_collector, along with the licensing details included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Over the past few years, summarization community has witnessed a variety of summarization datasets in different domains <ref type="bibr" target="#b30">(See et al., 2017;</ref><ref type="bibr" target="#b6">Cohan et al., 2018;</ref><ref type="bibr" target="#b17">Kornilova and Eidelman, 2019;</ref><ref type="bibr" target="#b12">Grusky et al., 2018;</ref><ref type="bibr" target="#b33">Sotudeh et al., 2021b)</ref>. While these collections have provided a fair basis to perform different neural text summarization models, the necessity of introducing large-scale collections, in magnitude of over 4 million, has not been much explored.</p><p>Among the first attempts on this track, <ref type="bibr" target="#b29">Rush et al. (2015)</ref> gathered the English Gigaword corpus <ref type="bibr" target="#b10">(Graff et al., 2003)</ref> which contains around 4 million article-headline pairs for the task of news headline generation. Researchers have noted that lead bias is the common phenomenon in most news datasets, where early parts of the article generally include the most important information <ref type="bibr" target="#b14">(Kedzie et al., 2018;</ref><ref type="bibr" target="#b41">Zhu et al., 2019;</ref><ref type="bibr" target="#b11">Grenander et al., 2019)</ref>. To alleviate the lead bias for training summarization models, there have been recent efforts to propose summarization datasets, where the lead bias phenomenon is mitigated and summaries are sampled from diverse source regions. Amongst those, <ref type="bibr" target="#b31">Sharma et al. (2019)</ref>  written abstractive summaries. <ref type="bibr" target="#b15">Kim et al. (2019)</ref> proposed Reddit TIFU in which the abstractive gold summaries are sampled from diverse regions of the source document, rather than lead regions. Our proposed datasets are more suited for the task of extreme summarization <ref type="bibr" target="#b27">(Narayan et al., 2018;</ref><ref type="bibr" target="#b4">Cachola et al., 2020)</ref>, where the task is to create a short one-sentence summary. To this end, <ref type="bibr" target="#b27">Narayan et al. (2018)</ref> proposed XSUM dataset which is a real-word dataset compiling online articles from the British Broadcasting Corporation (BBC). TLDR generation task is also a new form of extreme summarization. <ref type="bibr" target="#b15">Kim et al. (2019)</ref> collected Reddit-TIFU dataset, consisting of 120K posts from the online discussions from Reddit. Recent efforts have mined around 4 million Reddit posts along with their TLDR summaries <ref type="bibr" target="#b37">(V?lske et al., 2017)</ref> which resulted in Webis-TLDR-17 dataset. While our work is similar to theirs, our collected dataset is more than twice larger than the one previously proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Reddit Collection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>Reddit is a social news aggregation and discussion website platform that has been officially launched since June 2005. It supports some features specific to social platforms such as web content rating through up-voting, and discussion topics via subreddits. The user-created content can be of any domain such as News, Politics, Science, Sport and etc. Users can post or comment on a specific topic that falls into a specific subreddit. Within subreddits, users submit their post as submission, and others can react through commenting under the posted submission. Each submission and comment has a text body/selftext which reflects the users' information exchange regarding a specific topic. The existence of social platforms such as Reddit has provided the research community with an opportunity to experiment with resources that use informal language, rather than those in news, scientific or legal documents which use formal language. TLDR-Too Long; Didn't Read-is a common practice in Reddit that often appears at the end of long Reddit posts. It is denoted as an extremely short summary that urges users to read a shorter version of a longer text when they do not have time to read the entire post. <ref type="figure" target="#fig_0">Figure 2</ref> shows the ratio of posts containing such TLDR summaries over the entire submitted posts (and comments) across different years. It is observable that although we see an ascending trend since 2005, the number of TLDRs remains fixed (see Section 3.4) while the number of posts increases drastically.</p><p>Pushshift 2 is a social media data repository platform that has been recently made available to NLP researchers <ref type="bibr" target="#b1">(Baumgartner et al., 2020)</ref>. It contains recent and historical dumps of Reddit posts that are updated in real-time. In order to create the TLDR dataset, we downloaded the whole data dumps (submissions and comments) which cover the period of 2005-2021, and extracted instances that contain TLDRs within the posted source text. This mining process resulted in TLDR9+ dataset, which contains over 9 million instances. To acquire a more fine-grained dataset, with the help of human annotations, we obtained TLDRHQ dataset, consisting of 1.7 million high-quality instances. The datasets' construction details are discussed in what follows. <ref type="table">Construction: TLDR9+ and  TLDRHQ   TLDR9+</ref>. After downloading Reddit data dumps, we extract posts in which a mention of TLDR-style keywords is found. To find TLDR-style keywords within a given text, we declare a regular expression that matches words starting with "TL" and ending with "DR", with permission of having up to three characters in-between as also done by <ref type="bibr" target="#b37">V?lske et al. (2017)</ref>. This stage yields the TLDR9+ dataset as the full corpus. At the next filtering stage, we utilize a heuristic method along with human supervision to narrow it down to a more fine-grained dataset that contain high-quality instances. TLDRHQ. A few studies have noted that usergenerated content in social media platforms is noisy <ref type="bibr" target="#b21">(Liu and Inkpen, 2015)</ref> in terms of having spam, bad grammar, and spelling errors. To filter out such noisy instances from the TLDR9+ dataset, we use a heuristic method to drop low-quality instances while retaining high-quality ones. To be more specific, given a post-TLDR pair, we firstly identify the highest score source sentence in terms of ROUGE-2 and ROUGE-L mean scores (i.e., oracle sentence). The choice of oracle sentence lies in the fact that we postulate to extract a sentence from the longer post that has the highest similarity with the TLDR summary as the gold standard. We then decide to either drop or retain the instance if the score surpasses a pre-defined threshold. We experiment with different thresholds of 0.15, 0.17, 0.20, 0.22 and 0.25, and choose one considering the annotations done by human annotators. The details of human annotation process is discussed in what follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Human Annotation</head><p>As mentioned earlier, we first define 5 fixed thresholds including 0.15, 0.17, 0.20, 0.22, and 0.25 to create 5 data subsets from TLDR9+ dataset. Specifically, we take TLDR9+ as the initial seed, from which 5 subsets is created as follows. To gather instances for each of the pre-defined thresholds, we check if the oracle sentence's score in the given instance surpasses the experimented threshold. If it does so, we add it to the subset, otherwise, it is dropped. We then randomly sample 20 cases from each of these subsets with their oracle sentence and TLDR summaries, yielding 100 cases for annotation in total. We have four human annotators from our NLP group either confirm (labeling with 1) or reject (labeling with 0) if the oracle sentence validates the TLDR summary. By definition, the sentence validates the TLDR summary if at least one fragment can be found within the sentence that semantically occurs in TLDR summary. We further provide the instances' text (i.e., source) as the "Context" for the oracle sentence, and ask the annotators to confirm or reject if the context also validates the TLDR summary. Context is specifically important for the cases where the oracle sentence does not validate the TLDR summary. In fact, by providing context, we aspire to verify if an ideal summarizer is able to generate the TLDR using the context when the oracle sentence is not much informative. For tie cases 3 , we employ a fifth annotator to make the final decision.   <ref type="table" target="#tab_2">Table 2</ref> presents the average decision score assigned to the samples on each threshold. The decision score for a given sample is defined as the annotators' average confidence at giving label 1 to that specific sample. If the average confidence score surpasses 0.50, we assign 1 and if it is below 0.50, the sample is annotated with 0. Otherwise, the fifth annotator decides the label. As shown, threshold 0.22 attains the full score in the presence and absence of the context. Overall, this shows that most of the annotators believe the TLDR can be distilled considering both oracle sentence and the entire source. <ref type="figure" target="#fig_1">Figure 3</ref> shows pair-wise inter-rater S score agreement <ref type="bibr" target="#b2">(Bennet et al., 1954</ref>) throughout the annotation process on threshold 0.22, denoting that annotators have mostly slight or fair agreement in labeling process. Specifically, when the context is not provided (i.e., merely with consideration of oracle sentence), raters (2, 4), (2, 3), and (1, 3) have quite a high rate of agreement. On the other hand,  TLDRs per year (Submissions) 2 0 0 9 2 0 1 0 2 0 1 1 2 0 1 2 2 0 1 3 2 0 1 4 2 0 1 5 2 0 1 6 2 0 1 7 2 0 1 8 2 0 1 9 2 0 2 0  most pairs of annotators including (1, 2), (1, 4), and (2, 4) achieve a high agreement rate when the context is given. As the given decision scores -either only with oracle sentence or provided contextsum up to 1.0, and considering moderately high agreement rate between the annotators, we decide to sample our TLDRHQ dataset from the instances in that was in threshold 0.22's subset. This leads us to choose human-decided threshold 0.22 as our ground to sample High-Quality TLDRs for constructing TLDRHQ dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dataset Analysis</head><p>In this section, we give statistics, along with analyses on the proposed datasets. <ref type="table">Table 3</ref> shows general statistics of datasets in terms of post and TLDR length. As shown, the compression rate 4 is 8.7 and 12.5 in TLDR9+, and TLDRHQ datasets, respectively. This shows that authors generally tend to write much shorter TLDRs that highly shortens the post's text, which is expected due to the nature of TLDR summaries.  <ref type="table">Table 3</ref>: Average words length and number of sentences per instance along with the compression ratio in our proposed datasets. <ref type="figure" target="#fig_2">Figure 4</ref> demonstrates the number of TLDR pairs in TLDR9+ across different years. As observed, 83.65% of these TLDRs occur after 2013 which shows the popularity of this writing style among Reddit users. We also see a similar trend for years after 2013, each of which constitutes a fixed amount (10%-12%) of the dataset. <ref type="table" target="#tab_7">Table 4</ref> demonstrates the detailed information including data size, sentence length and vocabulary statistics of TLDRHQ dataset.</p><p>As mentioned earlier, we define the oracle sentence to be the one within the longer post that has the highest overlap with TLDR summary in terms of ROUGE-2 and ROUGE-L mean scores. The oracle sentence's relative position in post's text along with its importance is shown in <ref type="figure" target="#fig_3">Figure 5 (a)</ref>. We   </p><formula xml:id="formula_0">s i ?D RG 2+L</formula><p>where D is the set of all sentences within the post, and s i denotes the ith sentence. RG 2+L (.) is a function that takes in a post's sentence, and outputs the mean of its ROUGE-2 and ROUGE-L score with respect to TLDR summary. Intuitively, the oracle importance score can be framed as the attention score over the oracle sentences when the scoring function is ROUGE. Observing <ref type="figure" target="#fig_3">Figure 5</ref>, while more of the oracle sentences occur in early parts of the post's text (&lt; 0.10) with importance score of less than 0.30, it appears that the oracle sentences are spread out across the post's text overall. This observation is substantial, justifying the usability of this dataset for extractive summarization task.</p><p>To analyze the abstraction level of TLDRHQ dataset, we plot the percentage of novel n-grams within the TLDR summary <ref type="bibr" target="#b30">(See et al., 2017)</ref> in <ref type="figure" target="#fig_3">Figure 5 (b)</ref>, as well as the TLDR's n-gram abstractiveness <ref type="bibr" target="#b9">(Gehrmann et al., 2019)</ref> in <ref type="figure" target="#fig_3">Figure 5</ref> (c) over the all instances in TLDRHQ dataset. As indicated, there are quite a large proportion of novel n-gram words appeared in the TLDR summary as the heat extent is mostly concentrated in the upper half of the y-axis. These plots show the promising capability and challenges of this dataset to be used for abstractive summarization models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>We benchmark several extractive and abstractive summarization baselines over our two proposed datasets. BERTSUMEXT. <ref type="bibr" target="#b22">(Liu and Lapata, 2019</ref>) Bert-SumExt model is the extractive variant of BERT-SUM which is the BERT Model fine-tuned on text summarization task. In this regard, <ref type="bibr">BERT [CLS]</ref> tokens are appended to the start of each input sentence, and their associated representations are used to predict if the sentence should be included in the final summary or not. BERTSUMABS. <ref type="bibr" target="#b20">(Lewis et al., 2020)</ref> BERTSUM-ABS is the abstractive model of BERTSUM, where a Transformers-based decoder is added to the BERT Encoder. BART. <ref type="bibr" target="#b20">(Lewis et al., 2020)</ref> BART is a regressive autoencoder model that is pre-trained by first corrupting the text with an arbitrary noising function, and secondly, trying to reconstruct the original input text. BART is particularly effective when fine-tuned on text generation tasks such as summarization. As BART has both encoder and decoder pre-trained, it can be perceived as an extension to general BERT models in which only encoder is pre-trained.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset</head><p>We randomly split our datasets to construct training, validation, and test sets. Specifically, for TLDR9+, we use 99-0.5-0.5 split which results in 9, <ref type="bibr">139,935 (train), 43,753 (validation), and 43,749 (test)</ref> instances. To split TLDRHQ, we use 95-2.5-2.5 division yielding 1, <ref type="bibr">590,132 (train), 40,481 (validation), and 40,486 (test)</ref> pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training and Hyper-parameters</head><p>To train the summarization models, we utilize Hug-gingFace's Transformers <ref type="bibr" target="#b38">(Wolf et al., 2020)</ref> for BART, and the open implementation 5 of BERT-SUMEXT, BERTSUMABS. We use warm-up steps of 32K, and 20K for BART and BERTSUM variants, respectively. The AdamW optimizer <ref type="bibr" target="#b23">(Loshchilov and Hutter, 2019)</ref> is used with learning rate of 3e ? 5, beta parameter of 0.98, and weight decay of 0.01 for BART model. For BERTSUM variants, we use the default Adam (Kingma and Ba, 2015) optimizer with learning rates of 2e ? 3 for the encoder, and 1e ? 2 for the decoder as suggested by the main paper <ref type="bibr" target="#b22">(Liu and Lapata, 2019)</ref>. For all models, we use cross-entropy loss function. We train the models on 8 Nvidia Tesla V100 GPUs for 5 epochs with early stopping of the training when the validation loss does not decrease for 3 consecutive validation steps. The validation step is done every 25K training steps. To visualize and keep track of the learning process, we use Weight and Biases <ref type="bibr" target="#b3">(Biewald, 2020)</ref> toolkit. This is expected as BART's both encoder and decoder have been pre-trained on a large amount of unlabelled data, unlike BERTSUM variants that only have pre-trained encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>Comparing abstractive models with BERT-SUMEXT, we observe relatively large performance gap. This might be due to the fact that TLDRs in both TLDR9+ and TLDRHQ datasets are rather abstractive than extractive as also shown in Section 3.4. Yet with the existence of such a huge gap, the ORACLE-EXT (i.e., upper bound of an extractive summarizer) scores prove that more developed extractive summarizers can perform outof-the-box and mitigate this gap. The performance gap on TLDR9+ brings various challenges to develop summarization models that better fit on the larger dataset that include noisy data <ref type="bibr" target="#b18">(Kumar et al., 2020)</ref>. This noise might be handled via methods such as noise-aware training models <ref type="bibr" target="#b26">(Namysl et al., 2020)</ref>, while enabling the models to benefit from the large-scale TLDR9+ dataset. We leave this part for future work. It has to be mentioned that automatic evaluation of summarization continues to be an issue and while this dataset does not solve that, instead can be used with any evaluation metric as they evolve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>To gain insights into the qualities of summarization model, we analyze the outputs generated by the models. The diagrams demonstrating n-gram abstractiveness and percentage of novel n-grams, generated by BART and BERTSUMABS, are plotted in <ref type="figure">Figure 6</ref>. As observed, BART model appears to have a similar trend to the ground truth TLDRs. On the other hand, BERTSUMABS model has increasing n-gram abstractiveness, and novel n-gram percentage with increasing n. It is also interesting that after 6-gram, BERTSUMEXT model reaches a  <ref type="figure">Figure 6</ref>: The n-gram abstractiveness and percentage of novel n-gram metrics across different n-grams on TLDRHQ's test set. As seen, BART generates more abstractive summaries than BERTSUMABS as it mitigates the gap between BERTSUMABS and ground truth summary.</p><p>plateau when generating novel n-grams, but we a drop after 3-grams for BART and the ground truth TLDRs. This shows that from 1-gram to 3-gram, there are increasing number of novel words appeared in the ground-truth and BART, but after that, they both tend to copy n-grams rather than generating those.</p><p>To understand the limitation and qualities of current state-of-the-art summarization models, we conduct a qualitative analysis on several samples from TLDRHQ dataset, of which one is shown in <ref type="figure">Figure  7</ref>. Analyzing this sample, we observe that BART generated a better summary in terms of faithfulness to the ground truth TLDR. On the other hand, while BERTSUMABS could identify the important region of the source document, it has produced a longer TLDR with additional information that is present in the source, but not in the ground truth TLDR summary. BERTSUMEXT model could have identified a source sentence that is partly in connection with the ground truth TLDR, but it leaves out the most important sentence as the oracle to be extracted. Considering the upper performance of extractive summarizers (i.e., ORACLE-EXT score in <ref type="table" target="#tab_9">Table 5</ref>), we believe that there is a large room for improvement on this dataset. Investigations of more advanced models remain for future work.</p><p>Let me start this off by saying I'm not sure if this is the right spot to ask , and matching is not really my forte. I have my nostril pierced , as well as my septum. I got them done earlier this year and I've been playing around with different jewelry .all my jewelry has been white gold / silver... until now (edit -I originally had a silver hoop in my nostril and it was constantly irritated so I read up on it and found that silver is not good for piercings so I only use 14k white gold currently ). I purchased a 14k solid rose gold nose hoop (20g). I'm curious if it would look weird wearing a rose gold nose hoop with a white gold seamless septum ring (16g) ?? or any white gold septum jewelry? I don't want to look like a fool who can't match her facial jewelry! BertSumAbs. would it look weird wearing a rose gold nose hoop with a white gold seamless septum ring (16g) ?? or any white gold septum jewelry ? I don't want to look like a fool who can't match her facial jewelry .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BART.</head><p>would it look weird to wear a rose gold nosering with a white gold hoop septum ring?</p><p>Ground truth. would it look weird to wear a rose gold hoop in my nostril with a white gold hoop in my septum?</p><p>BertSumExt.</p><p>I purchased a 14k solid rose gold nose hoop (20g). <ref type="figure">Figure 7</ref>: A sample from TLDRHQ test set along with the model generated summaries. Underlined text in source shows the important regions of the source for generating TLDR summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we proposed two large-scale summarization datasets called TLDR9+, and TLDRHQ. The TLDR9+ dataset contains over 9 millions Reddit post-TLDR instances. To distill a more finegrained dataset out of TLDR9+, we sample highquality instances with the help of human annotations to construct TLDRHQ. Our analyses over TLDR9+ and TLDRHQ datasets show its usability for performing both extractive and abstractive summarization tasks. We further establish extractive and abstractive baseline results using state-of-theart summarization models on both datasets. We hope our datasets can pave the path for future studies in this direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>proposed BIGPATENT, consisting 1.3 million patent documents, collected from Google Patents Public Datasets, with human-The proportion of TLDRs over entire posts (submissions and comments) submitted per year (Figures (c) and (d)). At the time of writing this paper, submissions dumps are partly uploaded for 2021 (until 2021-06), while there is no comments dumps uploaded for 2021.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>S score inter-rater agreement for annotation without context (left), and annotation with context (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The proportion of instances containing TLDR in TLDR9+ dataset. As seen, the number of TLDRs is increasing each year. At the time of conducting this research, the submission data dumps are partially uploaded for 2021 (until 2021-06), while there is no comments uploaded for 2021 in the Pushshift repository.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Heatmaps of TLDRHQ showing (a) the oracle sentence's importance to its relative position; (b) percentage of novel n-grams; and (c) n-gram abstractiveness. The heat extent shows the number of the instances within the specific bin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2110.01159v2 [cs.CL] 5 Oct 2021</figDesc><table><row><cell>Dataset</cell><cell>Domain</cell><cell># instances</cell></row><row><cell>Non-social media</cell><cell></cell><cell></cell></row><row><cell>SCITLDR</cell><cell>Scientific</cell><cell>3.2K</cell></row><row><cell>XSUM</cell><cell>News</cell><cell>227K</cell></row><row><cell>Social media</cell><cell></cell><cell></cell></row><row><cell>Reddit TIFU</cell><cell>Social Media</cell><cell>120K</cell></row><row><cell>Webis-TLDR-17</cell><cell>Social Media</cell><cell>4M</cell></row><row><cell>TLDRHQ (ours)</cell><cell>Social Media</cell><cell>1.7M</cell></row><row><cell>TLDR9+ (ours)</cell><cell>Social Media</cell><cell>9.2M</cell></row><row><cell cols="3">Table 1: Overview of extreme summarization datasets</cell></row><row><cell cols="3">across different social and non-social domains with</cell></row><row><cell>number of instances.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average decision scores given by the annotators for each threshold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">: Detailed statistics of TLDRHQ dataset</cell></row><row><cell cols="2">define the oracle importance score as follows:</cell></row><row><cell>oracle importance =</cell><cell>max RG 2+L (s i )</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>ROUGE (F1) results of the state-of-the-art summarization models on the test sets of the proposed TLDR summarization datasets (TLDR9+, and TLDRHQ).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>presents the performance of the state-of-the-</cell></row><row><cell>art summarization models on our proposed datasets</cell></row><row><cell>in terms of ROUGE-1, ROUGE-2, and ROUGE-L</cell></row><row><cell>scores. As indicated, BART outperforms all other</cell></row><row><cell>models across all ROUGE variants in both datasets.</cell></row></table><note>5 https://github.com/nlpyang/PreSumm</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.reddit.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://files.pushshift.io/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Suppose a case where two annotators confirm (label 1), while the other two reject (label 0).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We warmly thank the anonymous reviewers as well as Tracy King for their helpful feedback and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Alanoud Bin Dris, Najla Alzakari, A. A. Elwafa, and H. Kurdi. 2021. Impact of dataset size on classification performance: An empirical evaluation in the medical domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Althnian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alsaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Heyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amani</forename><forename type="middle">K</forename><surname>Al-Baity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">796</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The pushshift reddit dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Savvas</forename><surname>Zannettou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Keegan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blackburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Communications Through Limited-Response Questioning*</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Bennet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="DOI">10.1086/266520</idno>
	</analytic>
	<monogr>
		<title level="j">Public Opinion Quarterly</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="308" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Experiment tracking with weights and biases. Software available from wandb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Biewald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tldr: Extreme summarization of scientific documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Cachola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FINDINGS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Better highlighting: Creating sub-sentence summary highlights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A discourse-aware attention model for abstractive summarization of long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokhwan</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Banditsum: Extractive summarization as a contextual bandit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Hoof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bottom-up abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sebastian Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating abstractive summaries with finetuned language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INLG</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">English gigaword. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">34</biblScope>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Countering the effects of lead bias in news summarization via multi-stage training and auxiliary losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Grenander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C K</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Grusky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Content selection in deep learning models of summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Kedzie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Abstractive summarization of reddit posts with multi-level memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongchang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Billsum: A corpus for automatic summarization of us legislation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kornilova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<idno>abs/1910.00523</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Noisy text data: Achilles&apos; heel of bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Makhija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>In WNUT</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to fuse sentences with transformers for summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Lebanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimating user location in social media with stacked denoising auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VS@HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ontology-aware clinical abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajad</forename><surname>Sotudeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">W</forename><surname>Talati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Filice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nat: Noise-aware training for robust neural sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Namysl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stepwise extractive summarization and planning with structured transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Ad?mek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blavz</forename><surname>Bratanivc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">T</forename><surname>Mc-Donald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A neural attention model for sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harvard</forename><surname>Seas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bigpatent: A large-scale dataset for abstractive and coherent summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On generating extended summaries of long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajad</forename><surname>Sotudeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI-21 Workshop on Scientific Document Understanding (SDU)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">On generating extended summaries of long documents. SDU@AAAI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajad</forename><surname>Sotudeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
		<idno>abs/2012.14136</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attend to medical ontologies: Content selection for clinical abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajad</forename><surname>Sotudeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Filice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Guir at semeval-2020 task 12: Domain-tuned contextualized models for offensive language detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajad</forename><surname>Sotudeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao-Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ophir</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Illia Polosukhin. 2017. Attention is all you need. ArXiv, abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tl;dr: Mining reddit to learn automatic summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>V?lske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahbaz</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NFiS@EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An overview of overfitting and its solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Ying</surname></persName>
		</author>
		<idno type="DOI">10.1088/1742-6596/1168/2/022022</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics: Conference Series</title>
		<imprint>
			<biblScope unit="volume">1168</biblScope>
			<biblScope unit="page">22022</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gmyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<title level="m">Make lead bias in your favor: Zero-shot abstractive news summarization. arXiv: Computation and Language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
