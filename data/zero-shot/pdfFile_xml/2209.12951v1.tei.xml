<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Liquid Structural State-Space Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Hasani</surname></persName>
							<email>rhasani@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Science and Artificial Intelligence Lab (CSAIL)</orgName>
								<orgName type="institution">Massachusetts Institute of Technology (MIT) Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Lechner</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Science and Artificial Intelligence Lab (CSAIL)</orgName>
								<orgName type="institution">Massachusetts Institute of Technology (MIT) Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-Hsuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Science and Artificial Intelligence Lab (CSAIL)</orgName>
								<orgName type="institution">Massachusetts Institute of Technology (MIT) Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makram</forename><surname>Chahine</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Science and Artificial Intelligence Lab (CSAIL)</orgName>
								<orgName type="institution">Massachusetts Institute of Technology (MIT) Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Amini</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Science and Artificial Intelligence Lab (CSAIL)</orgName>
								<orgName type="institution">Massachusetts Institute of Technology (MIT) Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Rus</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Science and Artificial Intelligence Lab (CSAIL)</orgName>
								<orgName type="institution">Massachusetts Institute of Technology (MIT) Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Liquid Structural State-Space Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>DRL * indicates authors with equal contributions Correspondence to</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-ofthe-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) statespace model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical timeseries, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition dataset Liquid-S4 achieves 96.78% accuracy with 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference. * Code Repository:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning representations from sequences of data requires expressive temporal and structural credit assignment. In this space, the continuous-time neural network class of liquid time-constant networks (LTC) <ref type="bibr" target="#b40">(Hasani et al., 2021b)</ref> has shown theoretical and empirical evidence for their expressivity and their ability to capture the cause and effect of a given task from high-dimensional sequential demonstrations <ref type="bibr" target="#b56">(Lechner et al., 2020a;</ref><ref type="bibr">Vorbach et al., 2021)</ref>. Liquid networks are nonlinear state-space models (SSMs) with an input-dependent state transition module that enables them to learn to adapt the dynamics of the model to incoming inputs, at inference, as they are dynamic causal models <ref type="bibr" target="#b21">(Friston et al., 2003)</ref>. Their complexity, however, is bottlenecked by their differential equation numerical solver that limits their scalability to longer-term sequences. How can we take advantage of LTC's generalization and causality capabilities and scale them to competitively learn long-range sequences without gradient issues, compared to advanced recurrent neural networks (RNNs) <ref type="bibr" target="#b19">(Erichson et al., 2021;</ref><ref type="bibr" target="#b29">Gu et al., 2020a;</ref><ref type="bibr" target="#b80">Rusch and Mishra, 2021)</ref>, convolutional neural networks (CNNs) <ref type="bibr" target="#b12">(Cheng et al., 2022;</ref><ref type="bibr" target="#b53">Lea et al., 2016;</ref><ref type="bibr" target="#b76">Romero et al., 2021b)</ref>, and attention-based models <ref type="bibr" target="#b87">(Vaswani et al., 2017)</ref>?</p><p>In this work, we set out to leverage the elegant formulation of structural state-space models (S4) <ref type="bibr" target="#b32">(Gu et al., 2022a)</ref> to obtain linear liquid network instances that possess the approximation capabilities of both S4 and LTCs. This is because structural SSMs are shown to largely dominate advanced RNNs, CNNs, and Transformers across many data modalities such as text, sequence of pixels, audio, and time series <ref type="bibr">(Gu et al., 2021</ref><ref type="bibr" target="#b32">(Gu et al., , 2022a</ref><ref type="bibr" target="#b35">Gupta, 2022)</ref>. Structural SSMs achieve such impressive performance by using three main mechanisms: 1) High-order polynomial projection operators (HiPPO) <ref type="bibr" target="#b29">(Gu et al., 2020a)</ref> that are applied to state and input transition matrices to memorize signals' history, 2) diagonal plus low-rank parametrization of the obtained HiPPO <ref type="bibr" target="#b32">(Gu et al., 2022a)</ref>, and 3) an efficient (convolution) kernel computation of an SSM's transition matrices in the frequency domain, transformed back in time via an inverse Fourier transformation <ref type="bibr" target="#b32">(Gu et al., 2022a)</ref>.</p><p>To combine S4 and LTCs, instead of modeling sequences by linear state-space models of the form? = A x + B u, y = C x, (as done in structural and diagonal SSMs <ref type="bibr">(Gu et al., 2022a,b)</ref>), we propose to use a linearized LTC state-space model <ref type="bibr" target="#b40">(Hasani et al., 2021b)</ref>, given by the following dynamics:? = (A + B u) x + B u, y = C x. We show that this dynamical system can also be efficiently solved via the same parametrization of S4, giving rise to an additional convolutional Kernel that accounts for the similarities of lagged signals. We call the obtained model Liquid-S4. Through extensive empirical evaluation, we show that Liquid-S4 consistently leads to better generalization performance compared to all variants of S4, CNNs, RNNs, and Transformers across many time-series modeling tasks. In particular, we achieve SOTA performance on the Long Range Arena benchmark <ref type="bibr" target="#b85">(Tay et al., 2020b)</ref> with an average of 87.32%. To sum up, we make the following contributions:</p><p>1. We introduce Liquid-S4, a new state-space model that encapsulates the generalization and causality capabilities of liquid networks as well as the memorization, efficiency and scalability of S4. 2. We achieve State-of-the-art performance on pixel-level sequence classification, text, speech recognition and all six tasks of the long-range arena benchmark with an average accuracy of 87.32%. On the full raw Speech Command recognition dataset Liquid-S4 achieves 96.78% accuracy with 30% reduction in parameter. Finally on the BIDMC vital signs dataset Liquid-S4 achieves SOTA in all modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Learning Long-Range Dependencies with RNNs. Sequence modeling can be performed autoregressively with RNNs which possess persistent states <ref type="bibr" target="#b62">(Little, 1974)</ref> originated from Ising <ref type="bibr" target="#b6">(Brush, 1967)</ref> and Hopfield networks <ref type="bibr" target="#b44">(Hopfield, 1982;</ref><ref type="bibr" target="#b74">Ramsauer et al., 2020)</ref>. Discrete RNNs approximate continuous dynamics stepby-steps via dependencies on the history of their hidden states, and continuous-time (CT) RNNs use ordinary differential equation (ODE) solvers to unroll their dynamics with more elaborate temporal steps <ref type="bibr" target="#b23">(Funahashi and Nakamura, 1993)</ref>. CT-RNNs can perform remarkable credit assignment in sequence modeling problems both on regularly sampled, irregularly-sampled data <ref type="bibr" target="#b1">(Amig? et al., 2012;</ref><ref type="bibr" target="#b3">Belletti et al., 2016;</ref><ref type="bibr" target="#b20">Foster, 1996;</ref><ref type="bibr" target="#b52">Kowal et al., 2019;</ref><ref type="bibr" target="#b60">Li and Marlin, 2016;</ref><ref type="bibr" target="#b70">Pearson et al., 2003;</ref><ref type="bibr" target="#b77">Roy and Yan, 2020)</ref>, by turning the spatiotemproal dependencies into vector fields <ref type="bibr" target="#b11">(Chen et al., 2018)</ref>, enabling better generalization and expressivity <ref type="bibr" target="#b40">(Hasani et al., 2021b;</ref><ref type="bibr" target="#b64">Massaroli et al., 2020)</ref>. Numerous works have studied their characteristics to understand their applicability and limitations in learning sequential data and flows <ref type="bibr" target="#b17">(Dupont et al., 2019;</ref><ref type="bibr" target="#b18">Durkan et al., 2019;</ref><ref type="bibr" target="#b27">Gruenbacher et al., 2022;</ref><ref type="bibr" target="#b28">Grunbacher et al., 2021;</ref><ref type="bibr" target="#b36">Hanshu et al., 2020;</ref><ref type="bibr" target="#b38">Hasani et al., 2020;</ref><ref type="bibr" target="#b43">Holl et al., 2020;</ref><ref type="bibr" target="#b45">Jia and Benson, 2019;</ref><ref type="bibr" target="#b61">Liebenwein et al., 2021;</ref><ref type="bibr" target="#b73">Quaglino et al., 2020)</ref>.</p><p>However, when these RNNs are trained by gradient descent (Allen- <ref type="bibr" target="#b0">Zhu and Li, 2019;</ref><ref type="bibr" target="#b79">Rumelhart et al., 1986;</ref><ref type="bibr" target="#b81">Sherstinsky, 2020)</ref>, they suffer from the vanishing/exploding gradients problem, which makes difficult the learning of long-term dependencies in sequences <ref type="bibr" target="#b5">(Bengio et al., 1994;</ref><ref type="bibr" target="#b41">Hochreiter, 1991)</ref>. This issue happens in both discrete RNNs such as GRU-D with its continuous delay mechanism <ref type="bibr" target="#b9">(Che et al., 2018)</ref> and Phased-LSTMs <ref type="bibr" target="#b68">(Neil et al., 2016)</ref>, and continuous RNNs such as ODE-RNNs <ref type="bibr" target="#b78">(Rubanova et al., 2019)</ref>, <ref type="bibr">GRU-ODE (De Brouwer et al., 2019)</ref>, Log-ODE methods  which compresses the input time-series by time-continuous path signatures <ref type="bibr" target="#b22">(Friz and Victoir, 2010)</ref>, and neural controlled differential equations , and liquid time-constant networks (LTCs) <ref type="bibr" target="#b40">(Hasani et al., 2021b)</ref>.</p><p>Numerous solutions have been proposed to resolve these gradient issues to enable long-range dependency learning. Examples include discrete gating mechanisms in LSTMs <ref type="bibr" target="#b26">(Greff et al., 2016;</ref><ref type="bibr" target="#b42">Hochreiter and Schmidhuber, 1997)</ref>, GRUs <ref type="bibr" target="#b15">(Chung et al., 2014)</ref>, continuous gating mechanisms such as CfCs <ref type="bibr" target="#b39">(Hasani et al., 2021a)</ref>, hawks LSTMs <ref type="bibr" target="#b65">(Mei and Eisner, 2017)</ref>, IndRNNs <ref type="bibr" target="#b59">(Li et al., 2018)</ref>, state regularization <ref type="bibr" target="#b90">(Wang and Niepert, 2019)</ref>, unitary RNNs <ref type="bibr" target="#b46">(Jing et al., 2019)</ref>, dilated RNNs <ref type="bibr" target="#b7">(Chang et al., 2017)</ref>, long memory stochastic processes (Greaves-Tunnell and Harchaoui, 2019), recurrent kernel networks , Lipschitz RNNs <ref type="bibr" target="#b19">(Erichson et al., 2021)</ref>, symmetric skew decomposition <ref type="bibr" target="#b92">(Wisdom et al., 2016)</ref>, infinitely many updates in iRNNs <ref type="bibr" target="#b47">(Kag et al., 2019)</ref>, coupled oscillatory RNNs (coRNNs) <ref type="bibr" target="#b80">(Rusch and Mishra, 2021)</ref>, mixedmemory RNNs , and Legendre Memory Units <ref type="bibr" target="#b88">(Voelker et al., 2019)</ref>. Learning Long-range Dependencies with CNNs and Transformers. RNNs are not the only solution to learning long-range dependencies. Continuous convolutional kernels such as CKConv <ref type="bibr" target="#b76">(Romero et al., 2021b)</ref> and <ref type="bibr" target="#b75">(Romero et al., 2021a)</ref>, and circular dilated CNNs <ref type="bibr" target="#b12">(Cheng et al., 2022)</ref> have shown to be efficient in modeling long sequences faster than RNNs. There has also been a large series of works showing the effectiveness of attention-based methods for modeling spatiotemporal data. A large list of these models is listed in <ref type="table" target="#tab_2">Table 1</ref>. These baselines have recently been largely outperformed by the structural state-space models <ref type="bibr" target="#b32">(Gu et al., 2022a)</ref>. State-Space Models. SSMs are well-established frameworks to study deterministic and stochastic dynamical systems <ref type="bibr" target="#b48">(KALMAN, 1960)</ref>. Their state and input transition matrices can be directly learned by gradient descent to model sequences of observations <ref type="bibr">(Gu et al., 2021;</ref><ref type="bibr" target="#b40">Hasani et al., 2021b;</ref><ref type="bibr" target="#b57">Lechner et al., 2020b)</ref>. In a seminal work, <ref type="bibr" target="#b32">Gu et al. (2022a)</ref> showed that with a couple of fundamental algorithmic methods on memorization and computation of input sequences, SSMs can turn into the most powerful sequence modeling framework to-date, outperforming advanced RNNs, temporal and continuous CNNs <ref type="bibr" target="#b12">(Cheng et al., 2022;</ref><ref type="bibr">Romero et al., 2021a,b)</ref> and a wide variety of Transformers <ref type="bibr" target="#b87">(Vaswani et al., 2017)</ref>, available in <ref type="table" target="#tab_2">Table 1</ref> by a significant margin.</p><p>The key to their success is their diagonal plus-low rank parameterization of the transition matrix of SSMs via higher-order polynomial projection (HiPPO) matrix <ref type="bibr" target="#b29">(Gu et al., 2020a)</ref> obtained by a scaled Legendre measure (LegS) inspired by the Legendre Memory Units <ref type="bibr" target="#b88">(Voelker et al., 2019)</ref> to memorize input sequences, a learnable input transition matrix, and an efficient Cauchy Kernel algorithm, results in obtaining structural SSMs named S4. It was also shown recently that diagonal SSMs (S4D) <ref type="bibr" target="#b35">(Gupta, 2022)</ref> could be as performant as S4 in learning long sequences when parametrized and initialized properly <ref type="bibr">(Gu et al., 2022b,c)</ref>. There was also a new variant of S4 introduced as simplified-S4 (S5) <ref type="bibr" target="#b82">Smith et al. (2022)</ref> that tensorizes the 1-D operations of S4 to gain a more straightforward realization of SSMs. Here, we introduce Liquid-S4, which is obtained by a more expressive SSM, namely liquid time-constant (LTC) representation <ref type="bibr" target="#b40">(Hasani et al., 2021b)</ref> which achieves SOTA performance across many benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Setup and Methodology</head><p>In this section, we first revisit the necessary background to formulate our Liquid Structural State-Space Models. We then set up and sketch our technical contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background</head><p>We aim to design an end-to-end sequence modeling framework built by SSMs. A continuous-time SSM representation of a linear dynamical system is given by the following set of equations:</p><p>Liquid Structural State-Space Model?</p><formula xml:id="formula_0">x(t) = A x(t) + B u(t), y(t) = C x(t) + D u(t).</formula><p>(1)</p><p>Here, x(t) is an N-dimensional latent state, receiving a 1-dimensional input signal u(t), and computing a 1-dimensional output signal y(t). A (N?N) , B (N?1) , C (1?N) and D (1?1) are system's parameters. For the sake of brevity, throughout our analysis, we set D = 0 as it can be added eventually after construction of our main results in the form of a skip connection <ref type="bibr" target="#b32">(Gu et al., 2022a)</ref>. Discretization of SSMs. In order to create a sequence-to-sequence model similar to a recurrent neural network (RNN), we discretize the continuous-time representation of SSMs by the trapezoidal rule (bilinear transform) ? as follows (sampling step = ?t) <ref type="bibr" target="#b32">(Gu et al., 2022a</ref>):</p><formula xml:id="formula_1">x k = A x k?1 + B u k , y k = C x k (2)</formula><p>This is obtained via the following modifications to the transition matrices:</p><formula xml:id="formula_2">A = (I ? ?t 2 A) ?1 (I + ?t 2 A), B = (I ? ?t 2 A) ?1 ?t B, C = C<label>(3)</label></formula><p>With this transformation, we constructed a discretized seq-2-seq model that can map the input u k to output y k , via the hidden state x k ? R N . A is the hidden transition matrix, B and C are input and output transition matrices, respectively. Creating a Convolutional Representation of SSMs. The system described by <ref type="formula">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref>, can be trained via gradient descent to learn to model sequences, in a sequential manner which is not scalable. To improve this, we can write the discretized SSM in (2) as a discrete convolutional kernel. To construct the convolutional kernel, let us unroll the system (2) in time as follows, assuming a zero initial hidden states x ?1 = 0:</p><formula xml:id="formula_3">x 0 = Bu 0 , x 1 = ABu 0 + Bu 1 , x 2 = A 2 Bu 0 + ABu 1 + Bu 2 , . . .<label>(4)</label></formula><formula xml:id="formula_4">y 0 = CBu 0 , y 1 = CABu 0 + CBu 1 , y 2 = CA 2 Bu 0 + CABu 1 + CBu 2 , . . .</formula><p>The mapping u k ? y k can now can be formulated into a convolutional kernel explicitly:</p><formula xml:id="formula_5">y k = CA k Bu 0 + CA k?1 Bu 1 + . . . CABu k?1 + CBu k , y = K * u (5) K ? R L := K L (C, A, B) := CA i B i?[L] = CB, CAB, . . . , CA L?1 B<label>(6)</label></formula><p>Equation <ref type="formula">(5)</ref> is a non-circular convolutional kernel. <ref type="bibr" target="#b32">Gu et al. (2022a)</ref> showed that under the condition that K is known, it could be solved very efficiently by a black-box Cauchy kernel computation pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Liquid Structural State-Space Models</head><p>In this work, we construct a convolutional kernel corresponding to a linearized version of LTCs <ref type="bibr" target="#b40">(Hasani et al., 2021b)</ref>; an expressive class of continuous-time neural networks that demonstrate attractive generalizability outof-distribution and are dynamic causal models <ref type="bibr" target="#b21">(Friston et al., 2003;</ref><ref type="bibr" target="#b38">Hasani et al., 2020;</ref><ref type="bibr">Vorbach et al., 2021)</ref></p><formula xml:id="formula_6">. ? s ? 2 ?t 1?z ?1 1+z ?1</formula><p>In their general form, the state of a liquid time-constant network at each time-step is given by the set of ODEs described below <ref type="bibr" target="#b40">(Hasani et al., 2021b)</ref>:</p><formula xml:id="formula_7">dx(t) dt = ? A + B f (x(t), u(t), t, ?) Liquid time-constant x(t) + B f (x(t), u(t), t, ?).<label>(7)</label></formula><p>In this expression, x (N?1) (t) is the vector of hidden state of size N, u (m?1) (t) is an input signal with m features, A (N?1) is a time-constant state-transition mechanism, B (N?1) is a bias vector, and represents the Hadamard product. f (.) is a bounded nonlinearity parametrized by ?.</p><p>Our objective is to show how the liquid time-constant (i.e., an input-dependent state transition mechanism in state-space models can enhance its generalization capabilities by accounting for the covariance of the input samples. To do this, we linearize the LTC formulation of Eq. 7 in the following to better connect the model to SSMs. Let's dive in: Linear Liquid Time-Constant State-Space Model. A Linear LTC SSM can be presented by the following coupled bilinear (first order bilinear Taylor approximation <ref type="bibr" target="#b71">(Penny et al., 2005)</ref>) equation:</p><formula xml:id="formula_8">x(t) = A + B u(t)] x(t) + B u(t), y(t) = C x(t)<label>(8)</label></formula><p>Similar to (1), x(t) is an N-dimensional latent state, receiving a 1-dimensional input signal u(t), and computing a 1-dimensional output signal y(t). A (N?N) , B (N?1) , and C (1?N) . Note that D is set to zero for simplicity. In <ref type="formula" target="#formula_8">(8)</ref>, the first B u(t) is added element-wise to A. This dynamical system allows the coefficient (state transition compartment) of state vector x(t) to be input dependent which, as a result, allows us to realize more complex dynamics. Discretization of Liquid-SSMs. Similar to SSMs, Liquid-SSMs can also be discretized by a bilinear transform (trapezoidal rule) to construct a sequence-to-sequence model as follows:</p><formula xml:id="formula_9">x k = A + B u k x k?1 + B u k , y k = C x k<label>(9)</label></formula><p>The discretized parameters A, B, and C are identical to that of (3), which are function of the continuous-time coefficients A, B, and C, and the discretization step ?t. Creating a Convolutional Representation of Liquid-SSMs. Similar to (4), we first unroll the Liquid-SSM in time to construct a convolutional kernel of it. By assuming x ?1 = 0, we have:</p><formula xml:id="formula_10">x 0 = Bu 0 , y 0 = CBu 0 x 1 = ABu 0 + Bu 1 + B 2 u 0 u 1 , y 1 = CABu 0 + CBu 1 +CB 2 u 0 u 1 (10) x 2 = A 2 Bu 0 + ABu 1 + Bu 2 + AB 2 u 0 u 1 + AB 2 u 0 u 2 + B 2 u 1 u 2 + B 3 u 0 u 1 u 2 y 2 = CA 2 Bu 0 + CABu 1 + CBu 2 + CAB 2 u 0 u 1 + CAB 2 u 0 u 2 + CB 2 u 1 u 2 + CB 3 u 0 u 1 u 2 , . . .</formula><p>The resulting expressions of the Liquid-SSM at each time step consist of two types of weight configurations: 1. Weights corresponding to the mapping of individual time instances of inputs independently, shown in black in (10), and 2. Weights associated with all orders of auto-correlation of the input signal, shown in violet in (10). The first set of weights corresponds to the convolutional kernel of the simple SSM, shown by Eq. 5 and Eq. 6, whereas the second set leads to the design of an additional input correlation kernel, which we call the liquid kernel. These kernels generate the following input-output mapping:</p><formula xml:id="formula_11">y k = CA k Bu 0 + CA k?1 Bu 1 + . . . CABu k?1 + CBu k + P ? p=2 ? k + 1 p of u i u i+1 . . . u p CA (k+1?p?i) B p u i u i+1 . . . u p<label>(11)</label></formula><p>for i ? Z and i ? 0, ?</p><formula xml:id="formula_12">y = K * u + K liquid * u correlations</formula><p>For instance, let us assume we have a 1-dimensional input signal u(t) of length L = 100 on which we run the liquid-SSM kernel. We set the hyperparameters P = 4. This value represents the maximum order of the correlation terms we would want to take into account to output a decision. This means that the signal u correlations in (11) will contain all combinations of 2 order correlation signals ( L+1 2 ), u i u j , 3 order ( L+1 3 ), u i u j u k and 4 order signals ( L+1 4 ), u i u j u k u l . The kernel weights corresponding to this auto-correlation signal would be:</p><formula xml:id="formula_13">K liquid * u correlations = CA (k?1) B 2 , . . . , CB 2 , . . . , CA (k?2) B 3 , . . . , CB 3 , . . . , CA (k?3) B 4 , . . . , CB 4 * (12) u 0 u 1 , . . . , u k?1 u k , . . . , u 0 u 1 u 2 , . . . , u k?2 u k?1 u k , . . . , u 0 u 1 u 2 u 3 , . . . , u k?3 u k?2 u k?1 u k T Here, u correlations is a vector of length ( k+1 2 ) + ( k+1 3 ) + ( k+1 4 ), and the kernel K liquid ? R ( k+1 2 )+( k+1 3 )+( k+1 4 )</formula><p>. This additional kernel takes the temporal correlation of incoming input samples into consideration. This way Liquid-SSM give rise to a more general sequence modeling framework. The liquid convolutional kernel, K liquid is as follows:</p><formula xml:id="formula_14">K liquid ? R?L := K L (C, A, B) := CA (L?i?p) B p i?[L], p?[P ] = CAL ?2 B 2 , . . . , CB p<label>(13)</label></formula><p>How to compute Liquid-S4 kernel efficiently? <ref type="bibr" target="#b32">Gu et al. (2022a)</ref> showed that the S4 convolution kernel could be computed efficiently using the following elegant parameterization tricks:</p><p>? To obtain better representations in sequence modeling schemes by SSMs, instead of randomly initializing the transition matrix A, we can use the Normal Plus Low-Rank (NPLR) matrix below, called the Hippo Matrix <ref type="bibr" target="#b29">(Gu et al., 2020a)</ref> which is obtained by the Scaled Legendre Measure (LegS) <ref type="bibr">(Gu et al., 2021</ref><ref type="bibr" target="#b32">(Gu et al., , 2022a</ref>:</p><formula xml:id="formula_15">(HiPPO Matrix) A nk = ? ? ? ? ? ? (2n + 1) 1/2 (2k + 1) 1/2 if n &gt; k n + 1 if n = k 0 if n &lt; k<label>(14)</label></formula><p>? The NPLR representation of this matrix is the following <ref type="bibr" target="#b32">(Gu et al., 2022a)</ref>:</p><formula xml:id="formula_16">A = V ?V * ? PQ = V (? ? (V * P) (V * Q) * ) V *<label>(15)</label></formula><p>Here, V ? C N?N is a unitary matrix, ? is diagonal, and P, Q ? R N?r are the low-rank factorization.</p><p>Eq. 14 is Normal plus low rank with r = 1 <ref type="bibr" target="#b32">(Gu et al., 2022a)</ref>. With the decomposition 15, we can obtain A over complex numbers in the form of Diagonal plus low-rank (DPLR) <ref type="bibr" target="#b32">(Gu et al., 2022a)</ref>.</p><p>? Vectors B n and P n are initialized by B n = (2n + 1) 1 2 and P n = (n + 1/2) 1 2 <ref type="bibr" target="#b33">(Gu et al., 2022b)</ref>. Both vectors are trainable.</p><p>? Furthermore, it was shown in <ref type="bibr" target="#b33">Gu et al. (2022b)</ref> that with Decomposition 15, the eigenvalues of A might be on the right half of the complex plane, thus, result in numerical instability. To resolve this, <ref type="bibr" target="#b33">Gu et al. (2022b)</ref> recently proposed to use the parametrization ? ? PP * instead of ? ? PP * .</p><p>Algorithm 1 LIQUID-S4 KERNEL -The S4 convolution kernel (highlighted in black) is used from <ref type="bibr" target="#b32">Gu et al. (2022a)</ref> and <ref type="bibr" target="#b33">Gu et al. (2022b)</ref>. Liquid kernel computation is highlighted in purple.</p><p>Input: S4 parameters ?, P, B, C ? C N , step size ?, liquid kernel order P, inputs seq length L, liquid kernel sequence lengthL Output: SSM convolution kernel K = K L <ref type="figure">(A, B, C)</ref> and SSM liquid kernel K liquid <ref type="figure">= K?L(A, B, C)</ref>  for p in {2, . . . , P } do 8: for p in {2, . . . , P } do 13:</p><formula xml:id="formula_17">for A = ? ? PP * (Eq. 6) 1: C ? I ? A L * C Truncate SSM generating function (SSMGF) to length L 2: k 00 (?) k 01 (?) k 10 (?) k 11 (?) ? C P * 2 ? 1?? 1+? ? ? ?1 [B P] Black-box Cauchy kernel 3:K(?) ? 2 1+? k 00 (?) ? k 01 (?)(1 + k 11 (?)) ?1 k 10 (?)<label>Woodbury</label></formula><formula xml:id="formula_18">K liquid=p = K (L?L,L) B p?1 (L?L,L) * J?L J?L</formula><formula xml:id="formula_19">K liquid=p = C B p?1 (L?L,L) 14: K liquid .append(K liquid=p ) 15:</formula><p>end for 16: end if ? Computing the powers of A in direct calculation of the S4 kernel K is computationally expensive. S4 computes the spectrum of K instead of direct computations, which reduces the problem of matrix powers to matrix inverse computation <ref type="bibr" target="#b32">Gu et al. (2022a)</ref>. S4 then computes this convolution kernel via a black-box Cauchy Kernel efficiently, and recovers K by an inverse Fourier Transform (iFFT) <ref type="bibr" target="#b32">(Gu et al., 2022a)</ref>.</p><p>K liquid possess similar structure to the S4 kernel. In particular, we have:</p><p>Proposition 1. The liquid-S4 kernel for each order p ? P, K liquid , can be computed by the anti-diagonal transformation (flip operation) of the product of the S4 convolution kernel, K = CB, CAB, . . . , CA L?1 B , and a vector B</p><formula xml:id="formula_20">p?1 ? R N .</formula><p>The proof is given in Appendix. Proposition 1 indicates that the liquid-s4 kernel can be obtained from the precomputed S4 kernel and a Hadamard product of that kernel with the transition vector B powered by the chosen liquid order. This is illustrated in Algorithm 1, lines 6 to 10, corresponding to a mode we call KB, which stands for Kernel ? B.</p><p>Additionally, we introduce a simplified Liquid-S4 kernel that is easier to compute while being as expressive as or even better performing than the KB kernel. To obtain this, we set the transition matrix A in Liquid-S4 of Eq. 13, with an identity matrix, only for the input correlation terms. This way, the liquid-s4 Kernel for a given liquid order p ? P reduces to the following expression:  <ref type="bibr" target="#b85">(Tay et al., 2020b)</ref>. Methods denoted by ** are reported from <ref type="bibr" target="#b32">(Gu et al., 2022a)</ref>. The rest of the models' performance results are reported from the cited paper. Liquid-S4 is used with its PB kernel. </p><p>We call this kernel Liquid-S4 -PB, as it is obtained by powers of the vector B. The computational steps to get this kernel is outlined in Algorithm 1 lines 11 to 15. Computational Complexity of the Liquid-S4 Kernel. The computational complexity of the S4-Legs Convolutional kernel solved via the Cauchy Kernel is?(N + L), where N is the state-size, and L is the sequence length <ref type="bibr" target="#b32">[Gu et al. (2022a)</ref>, Theorem 3]. Liquid-S4 both in KB and PB modes can be computed i? O(N + L + p maxL ). The added time complexity in practice is tractable. This is because we usually select the liquid orders, p, to be less than 10 (typically p max = 3, andL which is the number of terms we use to compute the input correlation vector, u correlation , is typically two orders of magnitude smaller than the seq length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments with Liquid-S4</head><p>In this section, we present an extensive evaluation of Liquid-S4 on sequence modeling tasks with very long-term dependencies and compare its performance to a large series of baselines ranging from advanced Transformers  <ref type="bibr" target="#b33">(Gu et al., 2022b)</ref>. The rest of the models' performance results are reported from the cited paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BIDMC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>HR RR SPO2</p><p>UnICORNN <ref type="bibr" target="#b80">(Rusch and Mishra, 2021)</ref> 1.39 1.06 0.869 coRNN <ref type="bibr" target="#b80">(Rusch and Mishra, 2021)</ref> 1.81 1.45 -CKConv * 2.05 1.214 1.051 NRDE <ref type="bibr" target="#b67">(Morrill et al., 2021)</ref> 2.97 1.49 1.29 LSTM * 10.7 2.28 -Transformer * 12.2 2.61 3.02 XGBoost  4.72 1.67 1.52 Random Forest  5.69 1.85 1.74 Ridge Regress.  17.3 3.86 4.16 S4-LegS * <ref type="bibr" target="#b33">(Gu et al., 2022b)</ref> 0.332 (0.013) 0.247 (0.062) 0.090 (0.006) S4-FouT * <ref type="bibr" target="#b33">(Gu et al., 2022b)</ref> 0.339 (0.020) 0.301 (0.030) 0.068 (0.003) S4D-LegS * <ref type="bibr" target="#b33">(Gu et al., 2022b)</ref> 0.367 (0.001) 0.248 (0.036) 0.102 (0.001) S4-(LegS/FouT) * <ref type="bibr" target="#b33">(Gu et al., 2022b)</ref> 0.344 (0.032) 0.163 (0.008) 0.080 (0.007) S4D-Inv * <ref type="bibr" target="#b33">(Gu et al., 2022b)</ref> 0.373 (0.024) 0.254 (0.022) 0.110 (0.001) S4D-Lin * <ref type="bibr" target="#b33">(Gu et al., 2022b)</ref> 0.379 (0.006) 0.226 (0.008) 0.114 (0.003)</p><p>Liquid-S4 (ours) 0.303 (0.002) 0.158 (0.001) 0.066 (0.002) p=3 p=2 p=4</p><p>and Convolutional networks to many variants of State-space models. In the following, we first outline the baseline models we compare against. We then list the datasets we evaluated these models on and finally present results and discussions.</p><p>Baselines. We consider a broad range of advanced models to compare liquid-S4 with. These baselines include transformer variants such as vanilla Transformer <ref type="bibr" target="#b87">(Vaswani et al., 2017)</ref>, Sparse Transformers <ref type="bibr" target="#b13">(Child et al., 2019</ref>), a Transformer model with local attention <ref type="bibr" target="#b85">(Tay et al., 2020b)</ref>, Longformer <ref type="bibr" target="#b4">(Beltagy et al., 2020)</ref>, Linformer , Reformer <ref type="bibr" target="#b51">(Kitaev et al., 2019)</ref>, Sinkhorn Transformer <ref type="bibr" target="#b84">(Tay et al., 2020a)</ref>, BigBird <ref type="bibr" target="#b94">(Zaheer et al., 2020)</ref>, Linear Transformer <ref type="bibr" target="#b49">(Katharopoulos et al., 2020)</ref>, and Performer <ref type="bibr" target="#b14">(Choromanski et al., 2020)</ref>. We also include architectures such as FNets (Lee-Thorp et al., 2021), Nystromformer <ref type="bibr" target="#b93">(Xiong et al., 2021)</ref>, Luna-256 <ref type="bibr" target="#b63">(Ma et al., 2021)</ref>, H-Transformer-1D <ref type="bibr" target="#b95">(Zhu and Soricut, 2021)</ref>, and Circular Diluted Convolutional neural networks (CDIL) <ref type="bibr" target="#b12">(Cheng et al., 2022)</ref>. We then include a full series of state-space models and their variants such as diagonal SSMs (DSS) <ref type="bibr" target="#b35">(Gupta, 2022)</ref>, S4 <ref type="bibr" target="#b32">(Gu et al., 2022a)</ref>, S4-legS, S4-FouT, S4-LegS/FouT <ref type="bibr" target="#b34">(Gu et al., 2022c)</ref>, S4D-LegS <ref type="bibr" target="#b33">(Gu et al., 2022b)</ref>, S4D-Inv, S4D-Lin and the Simplified Structural State-space models (S5) <ref type="bibr" target="#b82">(Smith et al., 2022)</ref>. Datasets. We first evaluate Liquid-S4's performance on the well-studied Long Range Arena (LRA) benchmark <ref type="bibr" target="#b85">(Tay et al., 2020b)</ref>, where Liquid-S4 outperforms other S4 and S4D variants in every task pushing the state-of-the-art further with an average accuracy of 87.32%. LRA dataset includes six tasks with sequence lengths ranging from 1k to 16k. We then report Liquid-S4's performance compared to other S4, and S4D variants as well as other models, on the BIDMC Vital Signals dataset <ref type="bibr" target="#b24">(Goldberger et al., 2000;</ref><ref type="bibr" target="#b72">Pimentel et al., 2016)</ref>. BIDMC uses bio-marker signals of length 4000 to predict Heart rate (HR), respiratory rate (RR), and blood oxygen saturation (SpO2).</p><p>We also experiment with the sCIFAR dataset that consists of the classification of flattened images in the form of 1024-long sequences into 10 classes.</p><p>Finally, we perform RAW Speech Command (SC) recognition with FULL 35 LABELS as conducted very recently in the updated S4 article <ref type="bibr" target="#b32">(Gu et al., 2022a)</ref>. ? SC dataset contains sequences of length 16k to be classified into 35 commands. <ref type="bibr" target="#b32">Gu et al. (2022a)</ref> introduced a new test case setting to assess the performance of models (trained on 16kHz sequences) on sequences of length 8kHz. S4 and S4D perform exceptionally well in this zero-shot test scenario. <ref type="table" target="#tab_2">Table 1</ref> depicts a comprehensive list of baselines benchmarked against each other on six long-range sequence modeling tasks in LRA. We observe that Liquid-S4 instances (all use the PB kernel with a scaled Legendre (LegS) configuration) with a small liquid order, p, ranging from 2 to 6, consistently outperform all baselines in all six tasks, establishing the new SOTA on LRA with an average performance of 87.32%. In particular, on ListOps, Liquid-S4 improves S4-LegS performance by more than 3%, on character-level IMDB by 2.2%, and on 1-D pixel-level classification (CIFAR) by 0.65%, while establishing the-state-of the-art on the hardest LRA task by gaining 96.54% accuracy. Liquid-S4 performs on par with improved S4 and S4D instances on both AAN and Pathfinder tasks. The performance of SSM models is generally well-beyond what advanced Transformers, RNNs, and Convolutional networks achieve on LRA tasks, with the Liquid-S4 variants standing on top. It is worth noting that Liquid-S4 kernels perform better with smaller kernel sizes (See more details on this in Appendix); For instance, on ListOps and IMDB, their individual liquid-S4 kernel state-size could be as small as seven units. This significantly reduces the parameter count in Liquid-S4 in comparison to other variants. The impact of increasing Liquid Order p. <ref type="figure">Figure 1</ref> illustrates how increasing the liquid order, p, can consistently improve performance on ListOps and IMDB tasks from LRA. <ref type="table" target="#tab_4">Table 2</ref> demonstrates the performance of a variety of classical and advanced baseline models on the BIDMC dataset for all three heart rate (HR), respiratory rate (RR), and blood oxygen saturation (SpO2) level prediction tasks. We observe that Liquid-s4 with a PB kernel of order p = 3, p = 2, and p = 4, perform better than all S4 and S4D variants. It is worth denoting that Liquid-S4 is built by the same parametrization as S4-LegS (which is the official S4 model reported in the updated S4 report <ref type="bibr" target="#b32">(Gu et al., 2022a)</ref>). In RR, Liquid-S4 outperforms S4-LegS by a significant margin of 36%. On SpO2, Liquid-S4 performs 26.67% better than S4-Legs. On ? It is essential to denote that there is a modified speech command dataset that restricted the dataset to only 10 output classes and is used in a couple of works (see for example <ref type="bibr">(Gu et al., 2021;</ref><ref type="bibr">Romero et al., 2021a,b)</ref>). Aligned with the updated results reported in <ref type="bibr" target="#b32">(Gu et al., 2022a)</ref> and <ref type="bibr" target="#b33">(Gu et al., 2022b)</ref>, we choose not to break down this dataset and use the full-sized benchmark.  <ref type="bibr" target="#b33">Gu et al. (2022b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on Long Range Arena</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on BIDMC Vital Signs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Accuracy</head><p>Transformer <ref type="bibr" target="#b86">(Trinh et al., 2018)</ref> 62.2 FlexConv <ref type="bibr" target="#b75">(Romero et al., 2021a)</ref> 80.82 TrellisNet <ref type="bibr" target="#b2">(Bai et al., 2018)</ref> 73.42 LSTM <ref type="bibr" target="#b42">(Hochreiter and Schmidhuber, 1997)</ref> 63.01 r-LSTM <ref type="bibr" target="#b86">(Trinh et al., 2018)</ref> 72.2 UR-GRU <ref type="bibr" target="#b30">(Gu et al., 2020b)</ref> 74.4 HiPPO-RNN <ref type="bibr" target="#b29">(Gu et al., 2020a)</ref> 61.1 LipschitzRNN <ref type="bibr" target="#b19">(Erichson et al., 2021)</ref> 64.2 S4-LegS <ref type="bibr" target="#b33">(Gu et al., 2022b)</ref> 91.80 (0.43) S4-FouT <ref type="bibr" target="#b33">(Gu et al., 2022b)</ref> 91.22 (0.25) S4-(LegS/FouT) <ref type="bibr" target="#b33">(Gu et al., 2022b)</ref> 91.58 (0.17) S4D-LegS <ref type="bibr" target="#b33">(Gu et al., 2022b)</ref> 89.92 (1.69) S4D-Inv <ref type="bibr" target="#b33">(Gu et al., 2022b)</ref> 90.69 (0.06) S4D-Lin <ref type="bibr" target="#b33">(Gu et al., 2022b)</ref> 90.42 (0.03) S5 <ref type="bibr" target="#b82">Smith et al. (2022)</ref> 89.66</p><p>Liquid-S4 (ours) 92.02 (0.14) p=3</p><p>HR, Liquid-S4 outperforms S4-Legs by 8.7% improvement in performance. The hyperparameters are given in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on 1-D Pixel-level Image Classification</head><p>Similar to the previous tasks, a Liquid-S4 network with PB kernel of order p = 3 outperforms all variants of S4 and S4D while being significantly better than Transformer and RNN baselines as summarized in <ref type="table" target="#tab_6">Table 3</ref>. The hyperparameters are given in Appendix. <ref type="table" target="#tab_7">Table 4</ref> demonstrates that Liquid-S4 with liquid order p = 2 achieves the best performance amongst all benchmarks on the 16KHz testbed with full dataset. Liquid-S4 also performs competitively on the half-frequency zero-shot experiment, while it does not realize the best performance. Although the task is solved to a great degree, the reason could be that liquid kernel accounts for covariance terms. This might influence the learned representations in a way that hurts performance by a small margin in this zero-shot experiment. The hyperparameters are given in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on Speech Commands</head><p>It is essential to denote that there is a modified speech command dataset that restricts the dataset to only ten output classes, namely SC10, and is used in a couple of works (see for example <ref type="bibr">(Gu et al., 2021;</ref><ref type="bibr">Romero et al., 2021a,b)</ref>). Aligned with the updated results reported in <ref type="bibr" target="#b32">(Gu et al., 2022a)</ref> and <ref type="bibr" target="#b33">(Gu et al., 2022b)</ref>, we choose not to break down this dataset and report the full-sized benchmark in the main paper. Nevertheless, we conducted an experiment with SC10 and showed that even on the reduced dataset, with the same hyperparameters, we solved the task with a SOTA accuracy of 98.51%. The results are presented in <ref type="table" target="#tab_4">Table  S2</ref>.  <ref type="bibr" target="#b33">(Gu et al., 2022b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SC FULL Labels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Parameters 16kHz 8kHz</p><p>InceptionNet <ref type="formula">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We showed that structural state-space models could be considerably improved in performance if they are formulated by a linear liquid time-constant kernel, namely Liquid-S4. Liquid-S4 kernels are obtainable with minimal effort with their kernel computing the similarities between time-lags of the input signals in addition to the main S4 diagonal plus low-rank parametrization. Liquid-S4 kernels with Smaller parameter counts achieve SOTA performance on all six tasks of the Long-range arena dataset, on BIDMC heart rate, respiratory rate, and blood oxygen saturation, on sequential 1-D pixel-level image classification, and on Speech command recognition.  <ref type="table" target="#tab_4">Table S2</ref>: Performance on RAW Speech Command dataset with the reduced ten classes (SC10) dataset.Numbers indicate validation accuracy. The accuracy of baseline models is reported from <ref type="table">Table 5</ref> of <ref type="bibr" target="#b32">(Gu et al., 2022a</ref>). x stands for infeasible computation on a single GPU or not applicable as stated in <ref type="table" target="#tab_2">Table 10</ref> of <ref type="bibr" target="#b32">(Gu et al., 2022a)</ref>. The hyperparameters for Liquid-S4 are the same as the ones reported for Speech Commands Full Dataset reported in <ref type="table" target="#tab_2">Table S1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SC10</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model 16kHz 8kHz</head><p>Transformer  <ref type="bibr">(Gu et al., 2021)</ref> x x S4-LegS <ref type="bibr">(Gu et al., 2022a) 98.32</ref> 96.30</p><p>Liquid-S4 (ours) 98.51 95.9 p=2 p=2</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Performance on Long Range Arena Tasks. Numbers indicate validation accuracy (standard deviation). The accuracy of models denoted by * are reported from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performance on BIDMC Vital Signs dataset. Numbers indicate RMSE on the test set. The accuracy of models denoted by * is reported from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Performance on sCIFAR dataset. Numbers indicate Accuracy (standard deviation). The accuracy of baseline models is reported fromTable 9 of</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Performance on RAW Speech Command dataset with FULL 35 Labels and with the reduced ten classes.Numbers indicate validation accuracy. The accuracy of baseline models is reported fromTable 11 of</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table S1 :</head><label>S1</label><figDesc>Hyperparameters for obtaining best performing models. BN= Batch Normalization, LN = Layer normalization, WD= Weight decay. Features H State Size Norm Pre-norm Dropout LR Batch Size Epochs WD</figDesc><table><row><cell cols="2">Depth ListOps 9</cell><cell>128</cell><cell>7</cell><cell>BN</cell><cell>True</cell><cell>0.01</cell><cell>0.002</cell><cell>12</cell><cell>30</cell><cell>0.03</cell></row><row><cell>Text (IMDB)</cell><cell>4</cell><cell>128</cell><cell>7</cell><cell>BN</cell><cell>True</cell><cell>0.1</cell><cell>0.003</cell><cell>8</cell><cell>50</cell><cell>0.01</cell></row><row><cell>Retrieval (AAN)</cell><cell>6</cell><cell>256</cell><cell>64</cell><cell>BN</cell><cell>False</cell><cell>0.2</cell><cell>0.005</cell><cell>16</cell><cell>20</cell><cell>0.05</cell></row><row><cell>Image (CIFAR)</cell><cell>6</cell><cell>512</cell><cell>512</cell><cell>LN</cell><cell>False</cell><cell>0.1</cell><cell>0.01</cell><cell>16</cell><cell>200</cell><cell>0.03</cell></row><row><cell>Pathfinder</cell><cell>6</cell><cell>256</cell><cell>64</cell><cell>BN</cell><cell>True</cell><cell>0.0</cell><cell cols="2">0.0004 4</cell><cell>200</cell><cell>0.03</cell></row><row><cell>Path-X</cell><cell>6</cell><cell>320</cell><cell>64</cell><cell>BN</cell><cell>True</cell><cell>0.0</cell><cell>0.001</cell><cell>8</cell><cell>60</cell><cell>0.05</cell></row><row><cell cols="2">Speech Commands 6</cell><cell>128</cell><cell>7</cell><cell>BN</cell><cell>True</cell><cell>0.0</cell><cell>0.008</cell><cell>10</cell><cell>50</cell><cell>0.05</cell></row><row><cell>BICMD (HR)</cell><cell>6</cell><cell>128</cell><cell>256</cell><cell>LN</cell><cell>True</cell><cell>0.0</cell><cell>0.005</cell><cell>32</cell><cell>500</cell><cell>0.01</cell></row><row><cell>BICMD (RR)</cell><cell>6</cell><cell>128</cell><cell>256</cell><cell>LN</cell><cell>True</cell><cell>0.0</cell><cell>0.01</cell><cell>32</cell><cell>500</cell><cell>0.01</cell></row><row><cell>BICMD (SpO2)</cell><cell>6</cell><cell>128</cell><cell>256</cell><cell>LN</cell><cell>True</cell><cell>0.0</cell><cell>0.01</cell><cell>32</cell><cell>500</cell><cell>0.01</cell></row><row><cell>sCIFAR</cell><cell>6</cell><cell>512</cell><cell>512</cell><cell>LN</cell><cell>False</cell><cell>0.1</cell><cell>0.01</cell><cell>50</cell><cell>200</cell><cell>0.03</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by The Boeing Company and the Office of Naval Research (ONR) Grant N00014-18-1-2830.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1. Proof of Proposition 1</head><p>Proof. This can be shown by unrolling the S4 convolution kernel and multiplying its components with B p?1 , performing an anti-diagonal transformation to obtain the corresponding liquid S4 kernel:</p><p>For p = 2 (correlations of order 2), S4 kernel should be multiplied by B. The resulting kernel would be:</p><p>We obtain the liquid kernel by flipping the above kernel to be convolved with the 2-term correlation terms (p=2):</p><p>Similarly, we can obtain liquid kernels for higher liquid orders and obtain the statement of the proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2. Hyperparameters</head><p>Learning Rate. Liquid-S4 generally requires a smaller learning rate compared to S4 and S4D blocks.</p><p>Setting ?t max and ?t min We set ?t max for all experiments to 0.2, while the ?t min was set based on the recommendations provided in <ref type="bibr" target="#b34">(Gu et al., 2022c)</ref> to be proportional to ? 1 seq length . Causal Modeling vs. Bidirectional Modeling Liquid-S4 works better when it is used as a causal model, i.e., with no bidirectional configuration. d s tate We observed that liquid-S4 PB kernel performs best with smaller individual state sizes d s tate. For instance, we achieve SOTA results in ListOps, IMDB, and Speech Commands by a state size set to 7, significantly reducing the number of required parameters to solve these tasks. Choice of Liquid-S4 Kernel In all experiments, we choose our simplified PB kernel over the KB kernel due to the computational costs and performance. We recommend the use of PB kernel. Choice of parameter p in liquid kernel. In all experiments, start off by setting p or the liquidity order to 2. This means that the liquid kernel is going to be computed only for correlation terms of order 2. In principle, we observe that higher p values consistently enhance the representation learning capacity of liquid-S4 modules, as we showed in all experiments. We recommend p = 3 as a norm to perform experiments with Liquid-S4. The kernel computation pipeline uses the PyKeops package <ref type="bibr" target="#b8">(Charlier et al., 2021)</ref> for large tensor computations without memory overflow. All reported results are validation accuracy (similar to <ref type="bibr" target="#b32">Gu et al. (2022a)</ref>) performed with 2 to 3 different random seeds, except for the BIDMC dataset, which reports accuracy on the test set.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Can sgd learn recurrent neural networks with provable generalization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10331" to="10341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transcripts: An algebraic approach to coupled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Amig?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aschenbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos: An Interdisciplinary Journal of Nonlinear Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13105</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Trellis networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Scalable linear causal inference for irregularly sampled time series with long range dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W</forename><surname>Belletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Sparks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bayen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03336</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">History of the lenz-ising model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Brush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews of modern physics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">883</biblScope>
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dilated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Kernel operations on the gpu, with autodiff, without memory overflows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Charlier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feydy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Glaun?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-D</forename><surname>Collin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Durif</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v22/20-275.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">74</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13431" to="13442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Classification of long sequential data using circular dilated convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Khalitov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02143</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Q</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gru-ode-bayes: Continuous modeling of sporadicallyobserved time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">De</forename><surname>Brouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Moreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7377" to="7388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Augmented neural odes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3134" to="3144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural spline flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bekasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7509" to="7520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lipschitz recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Erichson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Azencot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Queiruga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hodgkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=-N7PBXqOUJZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wavelets for period analysis of unevenly sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Astronomical Journal</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page">1709</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic causal modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Penny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1273" to="1302" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Multidimensional stochastic processes as rough paths: theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Friz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Victoir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">120</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Approximation of dynamical systems by continuous time recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-I</forename><surname>Funahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="801" to="806" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hausdorff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Mietus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Stanley</surname></persName>
		</author>
		<title level="m">Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals. circulation</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="215" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A statistical investigation of long memory in language and music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Greaves-Tunnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2394" to="2403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lstm: A search space odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutn?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gotube: Scalable statistical verification of continuous-depth models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Gruenbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Smolka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="6755" to="6764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the verification of neural odes with stochastic guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grunbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cyranka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Smolka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11525" to="11535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hippo: Recurrent memory with optimal polynomial projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving the gating mechanism of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3800" to="3809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Combining recurrent, convolutional, and continuous-time models with linear state space layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=uYLFoz1vlAC" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">On the parameterization and initialization of diagonal state space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.11893</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">How to train your hippo: State space models with generalized orthogonal basis projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Timalsina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.12037</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Diagonal state spaces are as effective as structured state spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.14343</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On robustness of neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hanshu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiawei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jiashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Response characterization for auditing cell dynamics in long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Naser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The natural lottery ticket winner: Reinforcement learning with ordinary neural circuits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Machine Learning. JMLR. org</title>
		<meeting>the 2020 International Conference on Machine Learning. JMLR. org</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liebenwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschaikowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Teschl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13898</idno>
		<title level="m">Closed-form continuous-depth models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Liquid time-constant networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021-05" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="7657" to="7666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Untersuchungen zu dynamischen neuronalen netzen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>TU M?nich</orgName>
		</respStmt>
	</monogr>
	<note>in german] diploma thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Holl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07457</idno>
		<title level="m">Learning to control pdes with differentiable physics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="2554" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Neural jump stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9843" to="9854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gated orthogonal recurrent units: On learning to forget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peurifoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soljacic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="765" to="783" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rnns incrementally evolving on an equilibrium manifold: A panacea for vanishing and exploding gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Kalman</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Basic Eng., Trans. ASME, D</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Neural controlled differential equations for irregular time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6696" to="6707" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Functional autoregression for sparsely sampled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Kowal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Matteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ruppert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Business &amp; Economic Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks: A unified approach to action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Mixed-memory rnns for learning long-term dependencies in irregularly sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>OpenReview</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Designing worm-inspired neural networks for interpretable robotic control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Neural circuit policies enabling auditable autonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="642" to="652" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Gershgorin loss stabilizes the recurrent neural network compartment of an end-to-end robot learning scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03824</idno>
		<title level="m">Fnet: Mixing tokens with fourier transforms</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (indrnn): Building a longer and deeper rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A scalable end-to-end gaussian process adapter for irregularly sampled time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1804" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Sparse flows: Pruning continuous-depth models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liebenwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="22628" to="22642" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The existence of persistent states in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical biosciences</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="101" to="120" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Linear unified nested attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2441" to="2453" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Dissecting neural odes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Asama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3952" to="3963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The neural hawkes process: A neurally self-modulating multivariate point process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6754" to="6764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Neural cdes for long time series via the log-ode method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08295</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Neural rough differential equations for long time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7829" to="7838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Phased lstm: Accelerating recurrent network training for long or event-based sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3882" to="3890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">In-depth benchmarking of deep neural network architectures for ecg diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nonaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="414" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Imbalanced clustering for microarray time-series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shwaber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML</title>
		<meeting>the ICML</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Bilinear dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Penny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">360</biblScope>
			<biblScope unit="page" from="983" to="993" />
			<date type="published" when="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Toward a robust estimation of respiratory rate from pulse oximeters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Charlton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Birrenkott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Watkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tarassenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Clifton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1914" to="1923" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Snode: Spectral discretization of neural odes for system identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quaglino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gallieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutn?k</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Hopfield networks is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?fl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kreil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Kopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Flexconv: Continuous kernel convolutions with differentiable kernel sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-J</forename><surname>Bruintjes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoogendoorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Ckconv: Continuous kernel convolution for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuzina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoogendoorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Robust landsat-based crop time series modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">238</biblScope>
			<biblScope unit="page">110810</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Latent ordinary differential equations for irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5321" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Coupled oscillatory recurrent neural network (co{rnn}): An accurate and (gradient) stable architecture for learning long time dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=F3s69XzWOia" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm) network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sherstinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">404</biblScope>
			<biblScope unit="page">132306</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Simplified state space layers for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Warrington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Linderman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.04933</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Time series extrinsic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bergmeir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1032" to="1060" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Sparse sinkhorn attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9438" to="9447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Long range arena: A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Learning longer-term dependencies in rnns with auxiliary losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4965" to="4974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Legendre memory units: Continuous-time representation in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kaji?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Causal navigation by continuous-time neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vorbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">State-regularized recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6596" to="6606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Full-capacity unitary recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4880" to="4888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Nystr?mformer: A nystr?m-based algorithm for approximating self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="14138" to="14148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17283" to="17297" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">H-transformer-1d: Fast one-dimensional hierarchical attention for sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3801" to="3815" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
