<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SUPERVISED VIDEO SUMMARIZATION VIA MULTIPLE FEATURE SETS WITH PARALLEL ATTENTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junaid</forename><forename type="middle">Ahmed</forename><surname>Ghauri</surname></persName>
							<email>junaid.ghauri@tib.eu</email>
							<affiliation key="aff0">
								<orgName type="department">TIB -Leibniz Information Centre for Science and Technology</orgName>
								<address>
									<settlement>Hannover</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherzod</forename><surname>Hakimov</surname></persName>
							<email>sherzod.hakimov@tib.eu</email>
							<affiliation key="aff0">
								<orgName type="department">TIB -Leibniz Information Centre for Science and Technology</orgName>
								<address>
									<settlement>Hannover</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Ewerth</surname></persName>
							<email>ralph.ewerth@tib.eu</email>
							<affiliation key="aff0">
								<orgName type="department">TIB -Leibniz Information Centre for Science and Technology</orgName>
								<address>
									<settlement>Hannover</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">L3S Research Center</orgName>
								<orgName type="institution">Leibniz University Hannover</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SUPERVISED VIDEO SUMMARIZATION VIA MULTIPLE FEATURE SETS WITH PARALLEL ATTENTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-supervised video summarization</term>
					<term>visual attention</term>
					<term>attention mechanism</term>
					<term>motion features</term>
					<term>video analy- sis</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The assignment of importance scores to particular frames or (short) segments in a video is crucial for summarization, but also a difficult task. Previous work utilizes only one source of visual features. In this paper, we suggest a novel model architecture that combines three feature sets for visual content and motion to predict importance scores. The proposed architecture utilizes an attention mechanism before fusing motion features and features representing the (static) visual content, i.e., derived from an image classification model. Comprehensive experimental evaluations are reported for two well-known datasets, SumMe and TVSum. In this context, we identify methodological issues on how previous work used these benchmark datasets, and present a fair evaluation scheme with appropriate data splits that can be used in future work. When using static and motion features with parallel attention mechanism, we improve state-of-the-art results for SumMe, while being on par with the state of the art for the other dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In the current information age, the enormous amount of available informative or entertaining multimedia content has increased the need for methods to detect important and thus relevant content. The number of available videos is also growing rapidly <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and the velocity highlights that the detection of important video segments is an essential and crucial task for the field of computer vision. Every minute, hundreds <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> or even a thousand hours of videos are being uploaded to video or social media platforms. It can be observed that skipping forward to a desired or interesting part of a video is widespread and this behavior is also denoted as "skim through" <ref type="bibr" target="#b4">[4]</ref>. No doubt this is subjective behavior but similar behavior of various humans can highlight the importance of a particular video segment. Overall, it is obvious that the fine-grained identification of important segments in a video is an important task. Video summarization can be defined as the conversion of a (potentially long) video into a shorter video that contains essential segments and thus allows a viewer to understand the video content. One of the main challenges is to identify and select important frames and segments in the original video that can be reused in the video summary. The generation of summaries in the form of selected frames from a video is useful when the main task is to get shorter videos with possible important parts. Many methods apply video segmentation as an initial pre-processing step for video summarization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b6">6]</ref>. For example, finegrained approaches at the video segment level of a second can open doors to potential applications such as live video stream from surveillance, learning, or the entertainment sector. Although the problem of assigning importance scores to frames for video summarization has been studied before <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8]</ref>, little attention has been paid to the impact of core system components, that is the role of different types of visual features and how to combine them. In this regard, most previous work just relied on content-based features for image classification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12]</ref>.</p><p>In this paper, we address this research gap and investigate how different feature types, i.e., static and motion features, can be integrated in a model architecture for video summarization. How a fusion of different types of features affects the overall performance by incorporating them with an attention mechanism similar as used by previous work <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b13">13]</ref>. For this task, we propose a novel deep learning model for supervised video summarization called Multi-Source Visual Attention (MSVA). The model fuses image and motion features based on a self-attention mechanism <ref type="bibr" target="#b10">[10]</ref> in a parallel fashion. Our comprehensive analysis on two benchmark datasets shows that our model outperforms other systems on the SumMe dataset <ref type="bibr" target="#b1">[2]</ref>, while obtaining performance similar to the state of the art on the TVSum dataset <ref type="bibr" target="#b0">[1]</ref>. In addition, we uncover issues in the experimental evaluation of previous methods: in some cases, videos were either excluded from the evaluation or reused in multiple splits, which makes it difficult to compare the systems in a fair manner and hinders the reproducibility of results. The crucial aspect of cross-fold validation on both benchmark datasets, where previous meth-ods either excluded some data points from evaluation or some data points were repeated in multiple splits, which makes it difficult to compare the published systems fairly. Therefore, we present a revised version of both benchmark datasets by providing five new non-overlapping splits and evaluating previous approaches on them and share the source code of our methodology and the evaluation 1 . We share the source code for the proposed model and the new non-overlapping splits for the TVSum and SumMe datasets with the research community. Our main contributions can be summarized as follows:</p><p>1.) We introduce a novel architecture based on multisource visual features with an attention mechanism. Track changes is off 2.) We identify issues in previous experimental setups and reproduce the experimental results for some approaches on valid cross-validation folds for two benchmark datasets.</p><p>3.) State-of-the-art results are improved for the SumMe dataset, while achieving similar results in comparison with other models on the TVSum dataset.</p><p>The rest of the paper is structured as follows. In Section 2, we review previous work on supervised video summarization. Section 3 describes the different feature sets, the proposed model architecture, and the attention mechanism. Experimental results and the comparison with other state-ofthe-art methods are reported in Section 4. We conclude the paper with a summary in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>To solve the task of video summarization, both supervised and unsupervised machine learning approaches have been suggested in the literature. Supervised methods train classifiers to learn the importance of a frame or segment for a summary. The process starts with the segmentation of videos, either uniformly into equally sized chunks, as done by Gygli et al. <ref type="bibr" target="#b5">[5]</ref>, or using algorithms like kernel temporal segmentation (KTS) by Potapov et al. <ref type="bibr" target="#b4">[4]</ref>. Gygli et al. <ref type="bibr" target="#b5">[5]</ref> computed an interestingness score for each segment using a weighted sum of features by combining low-level spatio-temporal salience or high-level motion information, while Song et al. <ref type="bibr" target="#b0">[1]</ref> measure frame-level importance using learned factorization. Another approach is suggested by Potapov et al. <ref type="bibr" target="#b4">[4]</ref> to train SVMs to classify frames in segments obtained through KTS.</p><p>Recurrent Neural Networks (RNN) or specifically long short-term memory (LSTM) and bidirectional LSTM (BiL-STM) have been also proposed for video summarization, where a BiLSTM model is stacked with Determinantal Point Process (DPP) <ref type="bibr" target="#b1">[2]</ref>, weighted memory layers with LSTM <ref type="bibr" target="#b2">[3]</ref>. In these approaches, either model helps to avoid similar frames in the final selection of a summary or solves this problem by encoding long video sequences to short sequences. As mentioned before, attention mechanisms are widely used 1 https://github.com/TIBHannover/MSVA in video summarization and combined with different neural architectures <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b13">13]</ref> where promising or even best results have been achieved recently.</p><p>Our model MSVA differs from approaches like MAVS <ref type="bibr" target="#b12">[12]</ref>, M-AVS <ref type="bibr" target="#b11">[11]</ref> and MC-VSA <ref type="bibr" target="#b9">[9]</ref> as follows. The proposed MSVA model has multiple sources of visual features where attention is applied to each source in a parallel fashion. The MAVS system <ref type="bibr" target="#b12">[12]</ref> is a memory augmented video summarizer with global attention, M-AVS <ref type="bibr" target="#b11">[11]</ref> considers multiplicative attention for video summarization with encoder-decoder, and MC-VSA [9] a multi-concept video self-attention where attention is applied to multiple layers of encoder and decoder. The majority of previous work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">11]</ref> use pre-trained image features from GoogleNet <ref type="bibr" target="#b14">[14]</ref> to encode video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SUPERVISED VIDEO SUMMARIZATION WITH MULTI-SOURCE FEATURES</head><p>In this section, we describe the overall architecture of the Multi-Source Visual Attention (MSVA) model and details about the different building blocks. First, we define some variables of the target problem. A video can be represented</p><formula xml:id="formula_0">as a sequence V = (v 1 , v 2 , . . . , v t , . . . , v T ), where t ? 1, 2, . . . , T and v t is the visual frame at time t. The sequence V of frames can be represented by different visual features as X = (x 1 , x 2 , . . . , x t , . . . , x T ), where x t ? R d</formula><p>is a vector that represents the extracted features from the t-th frame based on a specific feature encoder with a dimension d. The task of the model is to produce Y = {y 1 , y 2 , . . . , y t , . . . , y T } as an output that represents the importance score of frames. The feature encoders can be based on pre-trained models like GoogleNet <ref type="bibr" target="#b14">[14]</ref>, or content-based image features as mentioned in related work, in order to extract features to represent frames in videos. In contrast to prior work, we exploit additional models to enhance the representation of visual information in frames. For instance, different actions such as bungee jumping or hiking need to have different representations and rely on motion and temporal aspects, and not just on the static image content and object categories present in the ImageNet dataset. Therefore, we propose the use of content-based image features in combination with motion-related features to capture a richer representation within our model.</p><p>Once the features are extracted, we employ an attention mechanism followed by a couple of linear layers with different features and fuse them to obtain a common embedding space to represent frames. After fusion, we apply linear layers, normalization, activation functions, and predict the importance score of given input frames. The overall architecture of the MSVA model is shown in <ref type="figure">Figure 1</ref>. Next, we describe visual feature extraction, the attention mechanism, and fusion techniques to incorporate visual features from multiple sources for video summarization.  <ref type="figure">Fig. 1</ref>. The neural network architecture for the Multi-Source Visual Attention (MSVA) model with parallel self-attention mechanism based on multiple feature sets</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extracted Visual Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Content-based Image and Motion Features</head><p>We propose to combine three different feature types to provide a richer representation of frames in videos. After applying uniform sub-sampling to all frames of a video (two frames per second), the selected frames are passed as input to the pre-trained models to extract the following features. Image content: A deep neural network is trained on the Ima-geNet dataset for an object classification task <ref type="bibr" target="#b14">[14]</ref>. The most common pre-trained model to extract visual features for the video summarization domain is GoogleNet <ref type="bibr" target="#b14">[14]</ref>. We use the same model to extract content-based frame features from the pool5 layer (1024 dimensions), and represent them as X o . Motion: To leverage motion-related features, we use the pretrained I3D (Inflated 3D ConvNet) <ref type="bibr" target="#b15">[15]</ref> model on Kinetics dataset <ref type="bibr" target="#b16">[16]</ref>, which is composed of human actions such as drawing, drinking, laughing, hugging, opening present. From this model, we can extract two types of features: RGB (red, green, blue) and optical flow. RGB features, denoted as X r , capture the channel-wise color information with regard to scene changes, while the optical flow, denoted as X f , represents the motion in consecutive frames. The features are extracted from the second last layer of the pre-trained I3D model (1024 dimensions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention Module</head><p>The attention module used in our architecture is based on Fajtl et al.'s approach <ref type="bibr" target="#b10">[10]</ref>. It is shown in <ref type="figure">Figure 1 (b)</ref>. Each type of feature is fed into a separate attention mechanism followed by two linear and single normalization layers. An important aspect of the attention mechanism is the aperture p that defines the aperture window with a range [t-p, t+p]; the normalized attention vectors for a window are (? t?p , . . . , ? t?1 , ? t , ? t+1 , . . . , ? t+p ). The attention weights at time index t with a subset from the feature set X are calculated as follows.</p><formula xml:id="formula_1">e t,i = s[(U x i T )(V x t )], t ? [0, T -1], i ? [0, T -1]<label>(1)</label></formula><p>Here, T is the number of video frames, U and V are learnable weight matrices, s ? [0, 1] is a scale parameter that reduces the value of the dot product, x i is the i-th feature vector from the entire input sequence.</p><formula xml:id="formula_2">? t,i = exp(e t,i ) T -1 k=0 exp(e t,k )<label>(2)</label></formula><p>? t,i is a pairwise attention weight of the input vector x t with respect to a vector from the entire input sequence x i . The vector ? t contains attention weights for the target frame at time t based on the other vectors from the input sequence.</p><formula xml:id="formula_3">? t = (? t?p,i , . . . , ? t?1,i , ? t,i , ? t+1,i , . . . , ? t+p,i ) (3)</formula><p>The feature vector of a visual descriptor is multiplied with attention weights and fed into two linear layers (L 1 , L 2 ) and then into a normalization layer to obtain a latent representation Z of each type of features (as shown in <ref type="figure">Figure 1</ref> (c)):</p><formula xml:id="formula_4">Z = Norm(L 2 (L 1 (? t X)))<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-source Fusion</head><p>As mentioned above, our proposed model incorporates different types of features. In this stage, all latent representations Z after attention and the following linear transformation layers are fused to obtain a single vector representation of input frames. We use an addition function to fuse three vectors Z o , Z f , Z r corresponding to the latent representations of input features X o , X f , X r (image, optical flow, RGB). The result is passed to a linear layer (L 3 ), followed by ReLU activation, dropout, normalization and another linear layer (L 4 ). Finally, the output vector is fed into a sigmoid function that outputs importance scores? for the input frames:</p><formula xml:id="formula_5">h = L 4 (Norm(Drop(ReLU(L 3 (Z o + Z f + Z r ))))) (5) y = sigmoid(h)<label>(6)</label></formula><p>The formula given in Equation 5 and the model architecture depicted in <ref type="figure">Figure 1</ref> can be considered as an intermediate fusion, since the combination of latent representations of different feature types is performed in the intermediate layers of the neural network. Additionally, we experiment with other techniques such as early and late fusion. Early fusion is realized by combining the input features X after the encoding of input frames, then it is followed by a single attention mechanism, linear layers, normalization, dropouts, activation functions, and the classification layer. For the late fusion, we combine the latent representations in the last linear layer (L 4 ), which indicates that latent representations of different types of features are processed by different layers in parallel until L 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL SETUP AND RESULTS</head><p>In this section, we present details about benchmark datasets, evaluation protocols, ablation studies, comparison with stateof-the-art baselines, and video-wise qualitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>We use the following benchmark datasets to evaluate our approach and compare it with state-of-the-art approaches:</p><p>1.) TVSum <ref type="bibr" target="#b0">[1]</ref>: 50 videos with the length of 2-10 minutes, annotated by 20 users.</p><p>2.) SumMe <ref type="bibr" target="#b1">[2]</ref>: 25 videos with the length of 1-6 minutes, annotated by 18 users.</p><p>The evaluation of previous approaches on these datasets is based on 5-fold cross-validation and the reported results are F 1 scores averaged across the five splits of the corresponding datasets. The splits for TVSum and SumMe datasets are provided by Zhang et al. <ref type="bibr" target="#b1">[2]</ref>. When reproducing the results of previous work, we have observed methodological issues in the evaluation for both benchmark datasets. Some videos are dropped from certain splits, and are never part of the validation set. For instance, "video 5" and "video 8" in SumMe as well as "video 21" and "video 28" in TVSum are not part of any validation split. In total, eight videos from SumMe and 19 videos from TVSum are dropped from certain splits and were not evaluated. Another issue is that some videos are repeated in multiple splits. To fix the mentioned problems and to provide a fair comparison for future research, we release new versions of the two benchmark datasets by providing five non-overlapping splits where videos are equally divided across splits without any repetition or exclusion.</p><p>Regarding the evaluation, Otani et al. <ref type="bibr" target="#b17">[17]</ref> argue that the F 1 score for the task of video summarization has certain limitations and proposed to measure the correlation between predicted and human-annotations. In particular, they suggested Spearman's ? and Kendall's ? as correlation coefficients to evaluate models on how close the summaries predicted by models are to human annotations. Following these arguments, we evaluate our model architecture on the original splits and on the new non-overlapping splits for both datasets in terms of F 1 scores and correlation coefficients. The corresponding non-overlapping splits and the source code for evaluation metrics will be available to enable fair comparisons and reproducibility of future research 2 . We have reproduced work from Fajtl et al. <ref type="bibr" target="#b10">[10]</ref> with correlation coefficients according to <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b17">17]</ref> for the respective experiments by evaluating on both F 1 measure and correlation coefficients to compare against previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>Ablation study: We evaluated different building blocks of our proposed MSVA model. We performed a grid search on model hyper-parameters such as the aperture size of an attention mechanism (50-1000), fusion techniques (early, intermediate, late), and combination of feature types (object, RGB, optical flow) using random 20% of data. The linear layers are of size 1024, Adam (Adaptive Moment Estimation) is used as the optimizer, and each variation is trained for 300 epochs with a stopping criterion of 50 epochs when the loss is static.</p><p>The results are given in <ref type="table">Table 1</ref>. The reported score is the average F 1 of 5-fold cross-validation on the nonoverlapping splits for both benchmark datasets that we provide. We include only the best performing combinations of hyper-parameters (including aperture window size 250). It can be concluded that the model with an intermediate fusion of three features gives the best performance for both datasets with an aperture size of 250.</p><p>Overall comparison: We compared our proposed MSVA model with other state-of-the-art models that were evaluated on both benchmark datasets.</p><p>The results are given in <ref type="table" target="#tab_1">Table 2</ref> for both benchmark datasets. We report F 1 score, Kendall's ? and Spear- <ref type="table">Table 1</ref>. Ablation study with different feature types, fusion techniques with best aperture size (250) for the MSVA model. F 1 is the average score calculated on the newly provided five non-overlapping splits.</p><p>Dataset</p><formula xml:id="formula_6">Fusion Features F 1 - X o 50.5 early X o +X r +X f 46.7 X o +X r 44.5 SumMe X o +X f 44.8 intermediate X o +X r +X f 53.4 X o +X r 50.9 X o +X f 51.5 late X o +X r +X f 51.0 X o +X r 50.1 X o +X f 50.8 - X o 60.1 early X o +X r +X f 57.3 X o +X r 56.7</formula><p>TVSum</p><formula xml:id="formula_7">X o +X f 56.3 intermediate X o +X r +X f 61.5 X o +X r 61.1 X o +X f 61.2 late X o +X r +X f 60.1 X o +X r 58.9 X o +X f 59.7</formula><p>man's ? correlation coefficients on the newly provided nonoverlapping splits (denoted as F 1 ) along with F 1 on the original splits (denoted as F * 1 ). The results of previous work that did not share source code are reported only for the original splits. We evaluated VASNet <ref type="bibr" target="#b10">[10]</ref> on all evaluation metrics as the source code of the model is available <ref type="bibr" target="#b2">3</ref> . Based on the given results, we can see that our model improves the state-of-theart results for SumMe dataset and achieves comparable results for the TVSum dataset, while outperforming multiple systems including VASNet <ref type="bibr" target="#b10">[10]</ref>. A similar pattern is seen for both correlation coefficients, where our model obtains the best results for both datasets. Another observation is that the performance of VASNet is reduced by 1-2 points in terms of F 1 score when evaluated on non-overlapping splits in comparison to the original splits of both datasets. It can be explained by the fact that some videos were not part of any splits for 5-fold cross-validation and some videos were repeated across multiple splits. The MAVS approach <ref type="bibr" target="#b12">[12]</ref> is the best performing model on the TVSum from the compared models, while it has poor performance on SumMe. On the contrary, our proposed model outperforms all baselines on SumMe, while still achieving comparable results for TVSum as well, and best results in terms of correlations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative analysis</head><p>In <ref type="figure" target="#fig_0">Figure 2</ref>, we plot the importance score predictions of MSVA model compared to the average ground truth labels assigned by the annotators. Our model achieved lower performance for the video on the left, while it achieved a higher performance for the video on the right. The video on the left is called "Playing on water slide" with main content being recorded next to a water slide and a number of kids are playing around it. The difficulty lies in selecting different importance scores for frames that look similar, i.e., the background is still a water slide and children. Such confusion can be observed for ground truth labels assigned in the middle of the video where different scores are assigned to visually similar frames. The video on the right is called "Playing ball" with the main content being a dog and a bird playing with a white ball. One analysis in this video is that there are few objects appearing at a particular time, i.e., three objects, where one is prominent and the others are small like a bird and a ball.</p><p>To understand the impact of splitting the dataset, we plot F 1 scores for all videos in SumMe in <ref type="figure" target="#fig_1">Figure 3</ref>. The scores are computed when an individual video was part of the test set across five folds. There is a big difference in the F1 score across videos. Thus, the exclusion of certain videos affects the overall comparison across different models as well as the repetition of the same videos in multiple splits has an impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion</head><p>The experimental results for both datasets demonstrate the importance of exploiting multiple sources of feature types, particularly with regard to motion. Similarly, the attention mechanism on each source in a parallel fashion plays a vital role to provide decisive power to the model. Although the  supervised method has the advantage to learn from annotated labels, the model also incorporates bias from datasets based on (partially disagreeing) annotations from multiple users, as shown by the qualitative analysis mentioned above. Lastly, it is shown that the fusion strategy has a noticeable impact on the performance when utilizing multiple feature sets. Model performance varies a lot on different videos for which the reasons ca reasons can be given by observing the visual content of the video. This also demands to get features from the visual contextual domain to fill the gap on these difficult kinds of videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we have proposed a model architecture that utilizes visual features from multiple sources, i.e., static object and motion features, with an attention mechanism and dif-ferent fusion techniques. The intermediate fusion of object and motion features appeared to be the best configuration as shown by experiment results. State-of-the-art results were improved on the benchmark dataset SumMe and comparable results were obtained on the other benchmark dataset TVSum. Furthermore, methodological issues have been identified in the evaluation setup of previous work, and we have provided non-overlapping splits for cross-validation for a fair comparison. In the future, we will focus on semantic aspects to enhance the model with additional decisive capabilities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Comparison of predictions and ground truth labels on videos with low (a) "Playing on water slide" and high (b) "Playing ball" score from the SumMe dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>F 1 score analysis for all videos in SumMe. The scores are taken when videos were used for testing across 5-folds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of different methods on benchmark datasets. F 1 is an average score calculated on the newly provided five non-overlapping splits, F * 1 is the reported score by previously systems on original five splits. ? is Spearman's and ? is Kendall's correlation coefficients.</figDesc><table><row><cell cols="2">Dataset Method</cell><cell>F  *  1</cell><cell>F 1</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell>MAVS [12]</cell><cell cols="2">43.1 -</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>M-AVS [11]</cell><cell cols="2">44.4 -</cell><cell>-</cell><cell>-</cell></row><row><cell>SumMe</cell><cell cols="4">re-SEQ2SEQ [3] 44.9 -VASNet [10] 49.7 48.0 0.16 -</cell><cell>-0.17</cell></row><row><cell></cell><cell>MC-VSA [9]</cell><cell cols="2">51.6 -</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>MSVA (ours)</cell><cell cols="3">54.5 53.4 0.20</cell><cell>0.23</cell></row><row><cell></cell><cell>M-AVS [11]</cell><cell cols="2">61.0 -</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>VASNet [10]</cell><cell cols="3">61.4 59.8 0.16</cell><cell>0.17</cell></row><row><cell>TVSum</cell><cell cols="3">MC-VSA [9] re-SEQ2SEQ [3] 63.9 -63.7 -</cell><cell cols="2">0.116 0.142 --</cell></row><row><cell></cell><cell>MAVS [12]</cell><cell cols="2">67.5 -</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>MSVA (ours)</cell><cell cols="3">62.8 61.5 0.19</cell><cell>0.21</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/TIBHannover/MSVA</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/ok1zjf/VASNet</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TVSum: Summarizing web videos using titles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Vallmitjana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Jaimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5179" to="5187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video summarization with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV -14th European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9911</biblScope>
			<biblScope unit="page" from="766" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Retrospective encoders for video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV -15th</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11212</biblScope>
			<biblScope unit="page" from="391" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Category-specific video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danila</forename><surname>Potapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Za?d</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV -13th European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8694</biblScope>
			<biblScope unit="page" from="540" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Creating summaries from user videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayko</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV -13th European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8695</biblScope>
			<biblScope unit="page" from="505" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stacked memory network for video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dagan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="836" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diverse sequential subset selection for supervised video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2069" to="2077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transforming multi-concept attention into video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV -15th Asian Conference on Computer Vision. 2020</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">12626</biblScope>
			<biblScope unit="page" from="498" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Summarizing videos with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Fajtl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajar</forename><surname>Sadeghi Sokeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Argyriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV Workshops -14th Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11367</biblScope>
			<biblScope unit="page" from="39" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video summarization with attention-based encoderdecoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1709" to="1717" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extractive video summarizer with memory augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Conference on Multimedia Conference, MM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="976" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attentive and adversarial learning for video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsu-Jui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Heng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1579" to="1587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian Zhang Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking the evaluation of video summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayu</forename><surname>Otani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Heikkil?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR. 2019</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<biblScope unit="page" from="7596" to="7604" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
