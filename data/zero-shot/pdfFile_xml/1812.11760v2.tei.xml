<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multilingual Constituency Parsing with Self-Attention and Pre-Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
							<email>kitaev@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Cao</surname></persName>
							<email>stevencao@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
							<email>klein@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multilingual Constituency Parsing with Self-Attention and Pre-Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions. We first compare the benefits of no pre-training, fastText <ref type="bibr" target="#b3">(Bojanowski et al., 2017;</ref><ref type="bibr" target="#b15">Mikolov et al., 2018)</ref>, ELMo (Peters  et al., 2018), and BERT (Devlin et al., 2018a)   for English and find that BERT outperforms ELMo, in large part due to increased model capacity, whereas ELMo in turn outperforms the non-contextual fastText embeddings. We also find that pre-training is beneficial across all 11 languages tested; however, large model sizes (more than 100 million parameters) make it computationally expensive to train separate models for each language. To address this shortcoming, we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final model. The 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2% relative error increase in aggregate. We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. Finally, we demonstrate new state-ofthe-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has recently been rapid progress in developing contextual word representations that improve accuracy across a range of natural language tasks <ref type="bibr" target="#b11">Howard and Ruder, 2018;</ref><ref type="bibr" target="#b17">Radford et al., 2018;</ref><ref type="bibr" target="#b6">Devlin et al., 2018a)</ref>. While we have shown in previous work <ref type="bibr" target="#b13">(Kitaev and Klein, 2018</ref>) that such representations are beneficial for constituency parsing, our earlier results only consider the LSTM-based ELMo representations , and only for the English language. In this work, we study a broader range of pre-training conditions and experiment over a variety of languages, both jointly and individually.</p><p>First, we consider the impact on parsing of using different methods for pre-training initial network layers on a large collection of un-annotated text. Here, we see that pre-training provides benefits for all languages evaluated, and that BERT <ref type="bibr" target="#b6">(Devlin et al., 2018a)</ref> outperforms ELMo, which in turn outperforms fastText <ref type="bibr" target="#b3">(Bojanowski et al., 2017;</ref><ref type="bibr" target="#b15">Mikolov et al., 2018)</ref>, which performs slightly better than the non pre-trained baselines. Pre-training with a larger model capacity typically leads to higher parsing accuracies.</p><p>Second, we consider various schemes for the parser fine-tuning that is required after pretraining. While BERT itself can be pre-trained jointly on many languages, successfully applying it, e.g. to parsing, requires task-specific adaptation via fine-tuning <ref type="bibr" target="#b6">(Devlin et al., 2018a)</ref>. Therefore, the obvious approach to parsing ten languages is to fine-tune ten times, producing ten variants of the parameter-heavy BERT layers. In this work, we compare this naive independent approach to a joint fine-tuning method where a single copy of fine-tuned BERT parameters is shared across all ten languages. Since only a small output-specific fragment of the network is unique to each task, the model is 10x smaller while losing an average of only 0.28 F1.</p><p>Although, in general, jointly training multilingual parsers mostly provides a more compact model, it does in some cases improve accuracy as well. To investigate when joint training is helpful, we also perform paired fine-tuning on all pairs of languages and examine which pairs lead to the largest increase in accuracy. We find that larger treebanks function better as auxiliary tasks and that only smaller treebanks see a benefit from joint training. These results suggest that this manner of joint training can be used to provide support for many languages in a resource-efficient man-ner, but does not exhibit substantial cross-lingual generalization except when labeled data is limited. Our parser code and trained models for eleven languages are publicly available. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Our parsing model is based on the architecture described in <ref type="bibr" target="#b13">Kitaev and Klein (2018)</ref>, which is state of the art for multiple languages, including English. A constituency tree T is represented as a set of labeled spans,</p><formula xml:id="formula_0">T = {(i t , j t , l t ) : t = 1, . . . , |T |}</formula><p>where the t th span begins at position i t , ends at position j t , and has label l t . The parser assigns a score s(T ) to each tree, which decomposes as</p><formula xml:id="formula_1">s(T ) = (i,j,l)?T s(i, j, l)</formula><p>The per-span scores s(i, j, l) are produced by a neural network. This network accepts as input a sequence of vectors corresponding to words in a sentence and transforms these representations using one or more self-attention layers. For each span (i, j) in the sentence, a hidden vector v i,j is constructed by subtracting the representations associated with the start and end of the span. An MLP span classifier, consisting of two fullyconnected layers with one ReLU nonlinearity, assigns labeling scores s(i, j, ?) to the span. Finally, the the highest scoring valid tre?</p><formula xml:id="formula_2">T = arg max T s(T )</formula><p>can be found efficiently using a variant of the CKY algorithm. For more details, see <ref type="bibr" target="#b13">Kitaev and Klein (2018)</ref>.</p><p>We incorporate BERT by computing token representations from the last layer of a BERT model, applying a learned projection matrix, and then passing them as input to the parser. BERT associates vectors to sub-word units based on Word-Piece tokenization <ref type="bibr" target="#b20">(Wu et al., 2016)</ref>, from which we extract word-aligned representations by only retaining the BERT vectors corresponding to the last sub-word unit for each word in the sentence. We briefly experimented with other alternatives, such as using only the first sub-word instead, but did not find that this choice had a substantial effect on English parsing accuracy.  <ref type="bibr" target="#b13">Kitaev and Klein (2018)</ref> The fact that additional layers are applied to the output of BERT -which itself uses a selfattentive architecture -may at first seem redundant, but there are important differences between these two portions of the architecture. The extra layers on top of BERT use word-based tokenization instead of sub-words, apply the factored version of self-attention proposed in <ref type="bibr" target="#b13">Kitaev and Klein (2018)</ref>, and are randomly-initialized instead of being pre-trained. We found that passing the (projected) BERT vectors directly to the MLP span classifier hurts parsing accuracies.</p><p>We train our parser with a learning rate of 5 ? 10 ?5 and batch size 32, where BERT parameters are fine-tuned as part of training. We use two additional self-attention layers following BERT. All other hyperparameters are unchanged from <ref type="bibr" target="#b13">Kitaev and Klein (2018)</ref> and <ref type="bibr" target="#b6">Devlin et al. (2018a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Comparison of Pre-Training Methods</head><p>In this section, we compare using BERT, ELMo, fastText, and training a parser from scratch on treebank data alone. Our comparison of the different methods for English is shown in <ref type="table">Table 1</ref>. BERT BASE (?115M parameters) performs comparably or slightly better than ELMo (?107M parameters; 95.32 vs. 95.21 F1), while BERT LARGE (?340M parameters) leads to better parsing accuracy (95.70 F1). Furthermore, both pre-trained contextual embeddings significantly outperform fastText, which performs slightly better than no pre-training (93.72 vs. 93.61 F1). These results show that both the LSTM-based architecture of ELMo and the self-attentive architecture of BERT are viable for parsing, and that pre-training benefits from having a high model capacity. We did not  observe a sizable difference between an "uncased" version of BERT that converts all text to lowercase and a "cased" version of that retains case information.</p><p>We also evaluate an ensemble of four English BERT-based parsers, where the models are combined by averaging their span label scores:</p><formula xml:id="formula_3">s ensemble (i, j, l) = 1 4 4 n=1 s n (i, j, l)</formula><p>The resulting accuracy increase with respect to the best single model (95.87 F1 vs. 95.66 F1) reflects not only randomness during fine-tuning, but also variations between different versions of BERT. When combined with the observation that BERT LARGE outperforms BERT BASE , the ensemble results suggest that empirical gains from pretraining have not yet plateaued as a function of computational resources and model size.</p><p>Next, we compare pre-training on monolingual data to pre-training on data that includes a variety of languages. We find that pre-training on only English outperforms multilingual pretraining given the same model capacity, but the decrease in accuracy is less than 0.3 F1 (95.24 vs. 94.97 F1). This is a promising result because it supports the idea of parameter sharing as a way to provide support for many languages in a resourceefficient manner, which we examine further in Section 4.</p><p>To further examine the effects of pre-training on disparate languages, we consider the extreme case of training an English parser using a version of BERT that was pre-trained on the Chinese Wikipedia. Neither the pre-training data nor the subword vocabulary used are a good fit for the target task. However, English words (e.g. proper names) occur in the Chinese Wikipedia data with sufficient frequency that the model can losslessly represent English text: all English letters are included in its subword vocabulary, so in the worst case it will decompose an English word into its individual letters. We found that this model achieves performance comparable to our earlier parser (Kitaev and Klein, 2018) trained on treebank data alone (93.57 vs. 93.61 F1). These results suggest that even when the pre-training data is a highly imperfect fit for the target application, fine-tuning can still produce results better than or comparable to purely supervised training with randomlyinitialized parameters. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multilingual Model</head><p>We next evaluate how well self-attention and pretraining work cross-linguistically; for this purpose we consider ten languages: English and the nine languages represented in the SPMRL 2013/2014 shared tasks <ref type="bibr" target="#b18">(Seddah et al., 2013)</ref>.</p><p>Our findings from the previous section show that pre-training continues to benefit from larger model sizes when data is abundant. However, as models grow, it is not scalable to conduct separate pre-training and fine-tuning for all languages. This shortcoming can be partially overcome by pre-training BERT on multiple languages, as suggested by the effectiveness of the English parser fine-tuned from multilingual BERT (see <ref type="table">Table 1</ref>). Nevertheless, this straightforward approach also faces scalability challenges because it requires training an independent parser for each language, which results in over 1.8 billion parameters for ten languages. Therefore, we consider a single parser with parameters shared across languages and finetuned jointly. The joint parser uses the same BERT model and self-attention layers for all ten languages but contains one MLP span classifier per language to accommodate the different tree labels (see <ref type="figure" target="#fig_0">Figure 1</ref>). The MLP layers contain 250K-850K parameters, depending on the type of syntactic annotation adopted for the language, which Arabic Basque English French German Hebrew Hungarian Korean Polish Swedish   <ref type="table">Table 3</ref>: Change in development set F1 score due to paired vs. individual fine-tuning. In the "Best" column, starred results are significant at the p &lt; 0.05 level. On average, the three largest treebanks (German, English, Korean) function the best as auxiliaries. Also, the three languages benefitting most from paired training (Swedish, French, Polish) function poorly as auxiliaries.</p><p>is less than 0.5% of the total parameters. Therefore, this joint training entails a 10x reduction in model size.</p><p>During joint fine-tuning, each batch contains sentences from every language. Each sentence passes through the shared layers and then through the MLP span classifier corresponding to its language. To reduce over-representation of languages with large training sets, we follow <ref type="bibr" target="#b7">Devlin et al. (2018b)</ref> and determine the sampling proportions through exponential smoothing: if a language is some fraction f of the joint training set, the probability of sampling examples from that language is proportional to f a for some a. We use the same hyperparameters as in monolingual training but increase the batch size to 256 to account for the increase in the number of languages, and we use a = 0.7 as in <ref type="bibr" target="#b7">Devlin et al. (2018b)</ref>. The individually fine-tuned parsers also use the same hyperparameters, but without the increase in batch size. <ref type="table" target="#tab_3">Table 2</ref> presents a comparison of different parsing approaches across a set of ten languages. Our joint multilingual model outperforms treebankonly models <ref type="bibr" target="#b13">(Kitaev and Klein, 2018)</ref> for each of the languages (88.32 vs 91.12 average F1). We also compare joint and individual fine-tuning. The multilingual model on average degrades perfor-mance only slightly (91.12 vs. 91.40 F1) despite the sharp model size reduction, and in fact performs better for Swedish.</p><p>We hypothesize that the gains/losses in accuracy for different languages stem from two competing effects: the multilingual model has access to more data, but there are now multiple objective functions competing over the same parameters. To examine language compatibility, we also train a bilingual model for each language pair and compare it to the corresponding monolingual model (see <ref type="table">Table 3</ref>). From this experiment, we see that the best language pairs often do not correspond to any known linguistic groupings, suggesting that compatibility of objective functions is influenced more by other factors such as treebank labeling convention. In addition, we see that on average, the three languages with the largest training sets (English, German, Korean) function well as auxiliaries. Furthermore, the three languages that gain the most from paired training (Swedish, French, Polish) have smaller datasets and function poorly as auxiliaries. These results suggest that joint training not only drastically reduces model size, but also gives languages with small datasets a way to benefit from the large datasets of other languages.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We train and evaluate our parsers on treebanks for eleven languages: the nine languages represented in the SPMRL 2013/2014 shared tasks <ref type="bibr" target="#b18">(Seddah et al., 2013</ref>) (see <ref type="table" target="#tab_5">Table 4</ref>), English (see <ref type="table" target="#tab_6">Table 5</ref>), and Chinese (see <ref type="table" target="#tab_7">Table 6</ref>). The English and Chinese parsers use fully monolingual training, while the remaining parsers incorporate a version of BERT pre-trained jointly on 104 languages. For each of these languages, we obtain a higher F1 score than any past systems we are aware of.</p><p>In the case of SPRML, both our single multilingual model and our individual monolingual models achieve higher parsing accuracies than previous systems (none of which made use of pretrained contextual word representations). This result shows that pre-training is beneficial even when model parameters are shared heavily across languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The remarkable effectiveness of unsupervised pretraining of vector representations of language suggests that future advances in this area can continue improving the ability of machine learning methods to model syntax (as well as other aspects of language). As pre-trained models become increasingly higher-capacity, joint multilingual training is a promising approach to scalably providing NLP systems for a large set of languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The architecture of the multilingual model, with components labeled by the number of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Relative ?Error vs. monolingual +4.2%* +10.0%* +5.2%* +0.6% +15.5%* +0.6% +5.6%* -1.5% +2.7% -10.7%* +3.2%*</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Avg</cell><cell>Params</cell></row><row><cell>No pre-training a</cell><cell>85.61</cell><cell>89.71</cell><cell>93.55</cell><cell>84.06</cell><cell>87.69</cell><cell>90.35</cell><cell>92.69</cell><cell>86.59</cell><cell>93.69</cell><cell>84.45</cell><cell>88.32</cell><cell>355M</cell></row><row><cell cols="2">One model per language (this work) 87.97</cell><cell>91.63</cell><cell>94.91</cell><cell>87.42</cell><cell>90.20</cell><cell>92.99</cell><cell>94.90</cell><cell>88.80</cell><cell>96.36</cell><cell>88.86</cell><cell cols="2">91.40 1,851M</cell></row><row><cell cols="2">Joint multilingual model (this work) 87.44</cell><cell>90.70</cell><cell>94.63</cell><cell>87.35</cell><cell>88.40</cell><cell>92.95</cell><cell>94.60</cell><cell>88.96</cell><cell>96.26</cell><cell>89.94</cell><cell>91.12</cell><cell>189M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of monolingual and multilingual training on the SPMRL and WSJ test splits using the version of BERT pre-trained on 104 languages. In the last row, starred differences are significant at the p &lt; 0.05 level using a bootstrap test; see<ref type="bibr" target="#b0">Berg-Kirkpatrick et al. (2012)</ref>. a<ref type="bibr" target="#b13">Kitaev and Klein (2018)</ref> </figDesc><table><row><cell cols="12">Auxiliary Language Arabic Basque English French German Hebrew Hungarian Korean Polish Swedish Average</cell><cell>Best</cell><cell>Best Aux.</cell></row><row><cell># train sentences</cell><cell cols="2">15,762 7,577</cell><cell cols="3">39,831 14,759 40,472</cell><cell>5,000</cell><cell>8,146</cell><cell cols="2">23,010 6,578</cell><cell>5,000</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Language Tested</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Arabic</cell><cell>0</cell><cell>-0.38</cell><cell>-0.20</cell><cell>-0.27</cell><cell>-0.26</cell><cell>-0.14</cell><cell>-0.29</cell><cell>-0.13</cell><cell>-0.31</cell><cell>-0.33</cell><cell>-0.23</cell><cell>+0</cell><cell>None</cell></row><row><cell>Basque</cell><cell>-0.47</cell><cell>0</cell><cell>-0.06</cell><cell>-0.26</cell><cell>0.04</cell><cell>-0.22</cell><cell>-0.27</cell><cell>-0.41</cell><cell>-0.49</cell><cell>-0.34</cell><cell>-0.25</cell><cell>+0.04</cell><cell>German</cell></row><row><cell>English</cell><cell>-0.18</cell><cell>-0.04</cell><cell>0</cell><cell>-0.02</cell><cell>-0.03</cell><cell>-0.07</cell><cell>-0.09</cell><cell>0.05</cell><cell>0.10</cell><cell>-0.05</cell><cell>-0.03</cell><cell>+0.10</cell><cell>Polish</cell></row><row><cell>French</cell><cell>0.42</cell><cell>0.01</cell><cell>0.28</cell><cell>0</cell><cell>0.40</cell><cell>-0.14</cell><cell>0.04</cell><cell>0.27</cell><cell>0.29</cell><cell>-0.10</cell><cell>0.15</cell><cell>+0.42*</cell><cell>Arabic</cell></row><row><cell>German</cell><cell>-0.38</cell><cell>-0.20</cell><cell>0.03</cell><cell>-0.45</cell><cell>0</cell><cell>-0.13</cell><cell>-0.15</cell><cell>-0.13</cell><cell>-0.21</cell><cell>-0.26</cell><cell>-0.19</cell><cell>+0.03</cell><cell>English</cell></row><row><cell>Hebrew</cell><cell>0.13</cell><cell>0.05</cell><cell>-0.27</cell><cell>-0.17</cell><cell>-0.11</cell><cell>0</cell><cell>-0.09</cell><cell>-0.19</cell><cell>-0.30</cell><cell>-0.35</cell><cell>-0.13</cell><cell>+0.13</cell><cell>Arabic</cell></row><row><cell>Hungarian</cell><cell>-0.14</cell><cell>-0.43</cell><cell>-0.29</cell><cell>-0.38</cell><cell>-0.11</cell><cell>-0.39</cell><cell>0</cell><cell>-0.17</cell><cell>-0.28</cell><cell>-0.32</cell><cell>-0.25</cell><cell>+0</cell><cell>None</cell></row><row><cell>Korean</cell><cell>-0.24</cell><cell>-0.25</cell><cell>0.16</cell><cell>-0.27</cell><cell>-0.11</cell><cell>-0.01</cell><cell>0</cell><cell>0</cell><cell>-0.07</cell><cell>-0.17</cell><cell>-0.10</cell><cell>+0.16</cell><cell>English</cell></row><row><cell>Polish</cell><cell>0.25</cell><cell>0.15</cell><cell>0.20</cell><cell>0.24</cell><cell>0.24</cell><cell>0.21</cell><cell>0.14</cell><cell>0.20</cell><cell>0</cell><cell>0.12</cell><cell>0.18</cell><cell>+0.25*</cell><cell>Arabic</cell></row><row><cell>Swedish</cell><cell>0.17</cell><cell>-0.08</cell><cell>0.38</cell><cell>0.54</cell><cell>0.53</cell><cell>-0.11</cell><cell>0.59</cell><cell>0.78</cell><cell>-0.17</cell><cell>0</cell><cell>0.26</cell><cell>+0.78*</cell><cell>Korean</cell></row><row><cell>Average</cell><cell>-0.04</cell><cell>-0.12</cell><cell>0.02</cell><cell>-0.10</cell><cell>0.06</cell><cell>-0.10</cell><cell>-0.01</cell><cell>0.03</cell><cell>-0.14</cell><cell>-0.18</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results on the testing splits of the SPMRL dataset. All values are F1 scores calculated using the version of evalb distributed with the shared task. aBj?rkelund et al. (2013) b  Uses character LSTM, whereas other results from<ref type="bibr" target="#b5">Coavoux and Crabb? (2017)</ref> use predicted part-of-speech tags. c Does not use word embeddings, unlike other results from<ref type="bibr" target="#b13">Kitaev and Klein (2018)</ref>.</figDesc><table><row><cell></cell><cell>LR</cell><cell>LP</cell><cell>F1</cell></row><row><cell>Dyer et al. (2016)</cell><cell>-</cell><cell>-</cell><cell>93.3</cell></row><row><cell>Choe and Charniak (2016)</cell><cell>-</cell><cell>-</cell><cell>93.8</cell></row><row><cell>Liu and Zhang (2017)</cell><cell>-</cell><cell>-</cell><cell>94.2</cell></row><row><cell>Fried et al. (2017)</cell><cell>-</cell><cell>-</cell><cell>94.66</cell></row><row><cell>Joshi et al. (2018)</cell><cell>93.8</cell><cell>94.8</cell><cell>94.3</cell></row><row><cell>Kitaev and Klein (2018)</cell><cell cols="3">94.85 95.40 95.13</cell></row><row><cell>This work (single model)</cell><cell cols="3">95.46 95.73 95.59</cell></row><row><cell cols="4">This work (ensemble of 4) 95.51 96.03 95.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of F1 scores on the WSJ test set.</figDesc><table><row><cell></cell><cell>LR</cell><cell>LP</cell><cell>F1</cell></row><row><cell>Fried and Klein (2018)</cell><cell>-</cell><cell>-</cell><cell>87.0</cell></row><row><cell cols="2">Teng and Zhang (2018) 87.1</cell><cell>87.5</cell><cell>87.3</cell></row><row><cell>This work</cell><cell cols="3">91.55 91.96 91.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison of F1 scores on the Chinese Treebank 5.1 test set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We also attempted to use a randomly-initialized BERT model, but the resulting parser did not train effectively within the range of hyperparameters we tried. Note that the original BERT models were trained on significantly more powerful hardware and for a longer period of time than any of the experiments we report in this paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by DARPA through the XAI program. This work used the Savio computational cluster provided by the Berkeley Research Computing program at the University of California, Berkeley.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An empirical investigation of statistical significance in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Burkett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="995" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The IMS-Wroc?aw-Szeged-CIS entry at the SPMRL 2014 shared task: Reranking and morphosyntax meet unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Bj?rkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozlem</forename><surname>Cetinoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Fale?ska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich?rd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages</title>
		<meeting>the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
	<note>Wolfgang Seeker, and Zsolt Sz?nt?</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Re)ranking meets morphosyntax: State-of-the-art results from the SPMRL 2013 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Bj?rkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozlem</forename><surname>Cetinoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich?rd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Seeker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</title>
		<meeting>the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="135" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00051</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Parsing as language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kook</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1257</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2331" to="2336" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multilingual lexicalized constituency parsing with wordlevel auxiliary tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Crabb?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="331" to="336" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805[cs].ArXiv:1810.04805</idno>
		<title level="m">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://github.com/google-research/bert/blob/master/multilingual.md" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1024</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Policy gradient as a proxy for dynamic oracles in constituency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="469" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving neural parsing by disentangling model combination and reranking effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2025</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="161" to="166" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extending a parser to distant domains using a few dozen partially annotated examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1190" to="1199" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Constituency parsing with a self-attentive encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2676" to="2686" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">In-order transition-based constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="413" to="424" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Advances in pre-training distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Language Resources and Evaluation Conference</title>
		<meeting>the 11th Language Resources and Evaluation Conference<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resource Association</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overview of the SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djam?</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>K?bler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich?rd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iakes</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Koldo Gojenola Galletebeitia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spence</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Przepi?rkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Seeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Versley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Vincze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Woli?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wr?blewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clergerie</forename><surname>Villemonte De La</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</title>
		<meeting>the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="146" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Two local models for neural constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144[cs].ArXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Google&apos;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</title>
		<editor>Greg Corrado, Macduff Hughes, and Jeffrey Dean</editor>
		<meeting><address><addrLine>Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
