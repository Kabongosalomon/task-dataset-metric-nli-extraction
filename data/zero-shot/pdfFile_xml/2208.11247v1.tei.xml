<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SwinFIR: Revisiting the SwinIR with Fast Fourier Convolution and Improved Training for Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dafeng</forename><surname>Zhang</surname></persName>
							<email>dafeng.zhang@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research China -Beijing (SRC-B)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Huang</surname></persName>
							<email>feiyu.huang@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research China -Beijing (SRC-B)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhuo</forename><surname>Liu</surname></persName>
							<email>shizhuo.liu@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research China -Beijing (SRC-B)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research China -Beijing (SRC-B)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhezhu</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research China -Beijing (SRC-B)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SwinFIR: Revisiting the SwinIR with Fast Fourier Convolution and Improved Training for Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer-based methods have achieved impressive image restoration performance due to their capacities to model long-range dependency compared to CNN-based methods. However, advances like SwinIR adopts the window-based and local attention strategy to balance the performance and computational overhead, which restricts employing large receptive fields to capture global information and establish long dependencies in the early layers. To further improve the efficiency of capturing global information, in this work, we propose SwinFIR to extend SwinIR by replacing Fast Fourier Convolution (FFC) components, which have the image-wide receptive field. We also revisit other advanced techniques, i.e., data augmentation, pre-training, and feature ensemble to improve the effect of image reconstruction. And our feature ensemble method enables the performance of the model to be considerably enhanced without increasing the training and testing time. We applied our algorithm on multiple popular large-scale benchmarks and achieved state-of-the-art performance comparing to the existing methods. For example, our SwinFIR achieves the PSNR of 32.83 dB on Manga109 dataset, which is 0.8 dB higher than the state-of-the-art SwinIR method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning has been increasingly used for the image super-resolution in recent years. And the performance has significantly improved as a result of increasing network depth <ref type="bibr" target="#b28">[29]</ref>, recursive learning using ResBlock <ref type="bibr" target="#b20">[21]</ref> and channel attention <ref type="bibr" target="#b46">[47]</ref>. Due to the limitations of the receptive field, Convolutional Neural Network (CNN) concentrates on the limited area of the image. Conversely, the attention module can more effectively combine global information in the early layers, which is why it achieves better performance than CNN. Therefore, the network structure * Equal contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manga109 ?2</head><p>Manga109 ?3 Manga109 ?4</p><p>Urban100 ?2 Urban100 ?3 Urban100 ?4 <ref type="figure">Figure 1</ref>. The comparison results of our SwinFIR with the state-of-the-art methods SwinIR and EDT. Under different scales (?2, ?3, ?4), our SwinFIR achieves the best performance than existing works.</p><p>based on self-attention, especially Transformer <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40]</ref>, can effectively utilize global information from shallow layers to deep. The Vision Transformer (VIT) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42]</ref> has achieved great success in the high-level vision task. Therefore, Liang et al. <ref type="bibr" target="#b27">[28]</ref> explore the potential of VIT in the low-level vision tasks and propose SwinIR. SwinIR, which is based on Swin Transformer <ref type="bibr" target="#b29">[30]</ref>, outperforms the stateof-the-art methods on image restoration tasks such as image super-resolution, and image denoising. It consists of three components: shallow feature extraction, deep feature extraction and high-quality image reconstruction. The fundamental unit of deep feature extraction is called RSTB (Residual Swin Transformer Block), and each RSTB is made up of many Swin Transformer layers and residual connections. In detail, the RSTB performs the self-attention in each window and uses Shift Window strategy to expand the receptive field. However, this limited shift cannot effectively perceive the global information in the early layers.</p><p>Global information is essential for image superresolution (SR) since it can activate more pixels and is beneficial to improve the image reconstruction perfor-mance <ref type="bibr" target="#b14">[15]</ref>. Therefore, in order to utilize global information, we revisit the SwinIR architecture and introduce a new model specifically designed for SR task, called SwinFIR. The Spatial Frequency Block (SFB), which is based on Fast Fourier Convolution (FFC) <ref type="bibr" target="#b4">[5]</ref> and substitutes the convolution layer of the deep feature extraction module of SwinIR, is the essential innovation for SwinFIR. SFB consists of two branches: spatial and frequency model. We employ the FFC to extract the global information in the frequency branch and the hourglass-based residual module in the spatial branch to enhance local feature expression.</p><p>In addition to the SFB module, we also revisit a variety of methods to improve the image super-resolution performance, such as data augmentation, loss function, pretraining strategy, post-processing, etc. Data Augmentation (DA) based on the pixel-domain, which is extensively used and has yielded impressive results in high-level tasks, is rarely studied in SR (super-resolution) tasks. A lot of work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> has proved that effective DA can inhibit overfitting and improve the generalization ability of the model. Therefore, we believe that exploring effective DA will certainly boost the effectiveness of image superresolution. And we demonstrate through experiments that efficient data augmentation approaches, such as channel shuffle and Mixup, can considerably enhance the performance of image super-resolution. At the same time, we propose a brand-new feature-ensemble post-processing technique that is inspired by self-ensemble. The feature ensemble method enables the performance of the model to be considerably enhanced without increasing the training and testing time.</p><p>The comparison results of our SwinFIR with the state-ofthe-art methods SwinIR <ref type="bibr" target="#b27">[28]</ref> and EDT <ref type="bibr" target="#b25">[26]</ref> on Manga109 and Urban100 datasets as shown in <ref type="figure">Figure 1</ref>. Experimental results demonstrate that these strategies can effectively improve the performance of image super-resolution and our SwinFIR achieves state-of-the-art (SOTA) performance on all benchmarks. Specifically, our SwinFIR is 0.30 ? 0.80 dB and 0.24 ? 0.44 dB higher than the SOTA methods of SwinIR and EDT on the Manga109 and the Urban100 dataset, respectively, by using these strategies.</p><p>Our contributions can be summarized as follows:</p><p>? We revisit the SwinIR architecture and introduce the Spatial Frequency Block (SFB) specifically designed for utilizing global information in SR task, called SwinFIR. SFB is based on Fast Fourier Convolution (FFC) and used extract more comprehensive, detailed, and stable features. SFB consists of two branches: spatial and frequency model. We employ the FFC to extract the global information in the frequency branch and the residual module in the spatial branch to enhance local feature expression.</p><p>? We revisit various data augmentation methods in lowlevel tasks and demonstrate that efficient data augmentation approaches, such as channel shuffle and mixup, can considerably boost the performance of image super-resolution. Our method breaks the inertial thinking that data enhancement methods such as inserting new pixels will affect SR performance.</p><p>? We propose a brand-new ensemble strategy called feature ensemble, which integrates multiple trained models to get a better and more comprehensive model without increasing training and testing time, and is a zerocost method to improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Image Super Resolution (SR) is defined as the process of restoring a High Resolution (HR) image from a Low Resolution (LR) image. In recent years, SR models have been actively explored and achieved state-of-the-art performance with the rapid development of deep learning technology. SRCNN <ref type="bibr" target="#b10">[11]</ref> is the pioneering work of deep learning in SR. The network structure is very simple and only three convolutional layers are used. VDSR <ref type="bibr" target="#b19">[20]</ref> uses a deeper network to expedite the rate of convergence. DRRN <ref type="bibr" target="#b37">[38]</ref> proposed the methods of both global and local residual learning and Recursive learning of residual units. EDSR <ref type="bibr" target="#b28">[29]</ref> improves performance by removing unnecessary modules in residual networks and expanding the model size. RCAN <ref type="bibr" target="#b46">[47]</ref> proposed the Channel Attention mechanism to adaptively rescale features of each channel by modeling the interdependencies between feature channels. HAN <ref type="bibr" target="#b33">[34]</ref> further explores the application of attention mechanisms in SR task by modeling the interdependencies between different layers, different channels and different locations. CSNLN <ref type="bibr" target="#b31">[32]</ref> proposed the Cross-Scale Non-Local (CS-NL) attention module and a powerful self-Exemplars Mining (SEM) unit, so that a large amount of information between different scales is given significant attention. NLSA <ref type="bibr" target="#b30">[31]</ref> studied a combination of non-local operations and sparse representation, preserving the remote modeling capability of non-local operations while having the robustness and efficiency of sparse representation. Although all of these CNN-based works achieves excellent performance in SR tasks, CNN suffers from the limitations of the receptive field, and Transformer starts to be acting outstandingly in SR tasks due to its superior remote modeling capability.</p><p>IPT <ref type="bibr" target="#b2">[3]</ref> is a pre-trained Transformer model on the underlying visual task that introduces the Transformer module in the feature map processing stage. HAT <ref type="bibr" target="#b3">[4]</ref> is a Hybrid Attention Transformer that combines multiple attention mechanisms, such as channel attention, self-attention, and overlapping cross-attention. SwinIR <ref type="bibr" target="#b27">[28]</ref> is an image restoration model based on Swin Transformer. However, the potential of the Transformer still cannot be fully exploited by existing work, and our method adapts the SwinIR-based network architecture by introducing the SFB module based on FFC <ref type="bibr" target="#b4">[5]</ref> that can activate more input information. LaMa <ref type="bibr" target="#b36">[37]</ref> proposes a new image restoration network based on FFC. Inspired by LaMa, we propose the SFB, which employs large receptive field operations within early layers of the network, and can take advantage of long dependencies to use more pixels for better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this paper, we revisit the strategies for improving image super-resolution, that involve little or no additional model parameters and calculations. The evolution trajectory from SwinIR to SwinFIR is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. LAM <ref type="bibr" target="#b14">[15]</ref> demonstrate that global information is essential for image super-resolution (SR) since it can activate more pixels and is beneficial to improve the image reconstruction performance. Consequently, we first revisit the SwinIR architecture and introduce the Spatial Frequency Block (SFB) specifically designed for utilizing global information in SR tasks. Then we replace L1 Loss with a more stable Charbonnier Loss <ref type="bibr" target="#b22">[23]</ref>. We also revisit a number of data augmentation techniques that can enhance the effectiveness of image super-resolution. We also examine various popular methods for enhancing image super-resolution performance, such as using more training data, enlarging the window size of Swin Transformer and employing pre-training model. Finally, inspired by self-ensemble, we propose a novel post-processing technique, named feature ensemble, to improve the stability of the model without lengthening the training and testing periods. All models are evaluated on Set5 dataset in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Design</head><p>Inspired by SwinIR, we propose SwinFIR using Swin Transformer and Fast Fourier Convolution, as shown in <ref type="figure">Figure</ref> 3. SwinFIR consists of three modules: shallow feature extraction, deep feature extraction and high-quality (HQ) image reconstruction modules. The shallow feature extraction and high-quality (HQ) image reconstruction modules adopt the same configuration as SwinIR. The residual Swin Transformer block (RSTB) is a residual block with Swin Transformer layers (STL) and convolutional layers in SwinIR. They all have local receptive fields and cannot extract the global information of the input image. The Fast Fourier Convolution has the ability to extract global features, so we replace the convolution (3?3) with Fast Fourier Convolution and a residual module to fuse global and local features, named Spatial Frequency Block (SFB), to improve the representation ability of model.</p><p>The SFB network architecture is shown in <ref type="figure" target="#fig_1">Figure 3</ref>(c) and is composed of two primary components: a spatial  conventional convolution operation on the left and a Fast Fourier convolution (FFC) on the right. We concatenate the left and right outputs, and perform a convolution operation to obtain the final result. The formula is as follows,</p><formula xml:id="formula_0">X SF B = H SF B (X)<label>(1)</label></formula><p>where the X is the feature map from STL. H SF B (?) represents the SFB module and X SF B is the output feature map after various operations of SFB. We send X into two distinct domains, X spatial and X f requency . X spatial is utilized in the spatial domain, and X f requency is intended to capture the long-range context in the frequency domain,</p><formula xml:id="formula_1">X spatial = H spatial (X) (2) X f requency = H f requency (X)<label>(3)</label></formula><p>where H spatial (?) is the spatial convolution module and H f requency (?) represents the frequency FFC module. The left spatial convolution module is a residual module for classical SR and a hourglass residual module for lightweight SR, as shown in 4(b) and 4(c) respectively. Compared to a single-layer convolution, we insert a residual connection and convolution layer to increase the expressiveness of the model. Experiments have shown that this simple modification increases performance dramatically. The X spatial is also represented as,</p><formula xml:id="formula_2">X spatial = H CLC (X) + X<label>(4)</label></formula><p>where H CLC (?) denotes a 3?3 convolution layer at the head and tail, and LeakyReLU operation is conducted between   convolution layers. In the right frequency module, we transform the conventional spatial features into the frequency domain to extract the global information by using the 2-D Fast Fourier Transform (FFT). And we then perform inverse 2-D FFT operation to obtain spatial domain features. The X f requency is also represented as,</p><formula xml:id="formula_3">X = H CL (X) (5) X f requency = H C (H F LF (X) + X)<label>(6)</label></formula><p>where H CL (?) denotes a convolution layer and LeakyReLU. H F LF (?) contains the following series of operations, a 2-D FFT based on a channel-wise, a compilation operation with frequency convolution and LeakeyReLU and an inverse 2-D FFT operation. The number of channels is then reduced in half by a convolution operation,</p><formula xml:id="formula_4">X SF B = H C ([X spatial ||X f requency ]) (7)</formula><p>where H C (?) denotes a convolution layer and || stands for the concatenation operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Loss Function</head><p>In addition to the structure of the neural network, the loss function also determines whether the model can achieve good results. In low level visual tasks, such as super resolution and deblurring, the L2 <ref type="bibr" target="#b11">[12]</ref>, L1 <ref type="bibr" target="#b46">[47]</ref>, perceptual and adversarial <ref type="bibr" target="#b34">[35]</ref> loss functions are often used to optimize neural networks. However, we use Charbonnier loss function <ref type="bibr" target="#b22">[23]</ref> to optimize our SwinFIR to get better performance than other loss functions. In the training phase, the loss function is minimized by training data {I i L , I i H } N i=1 to update the parameters, N represent the numbers of training images. The Charbonnier loss function is,</p><formula xml:id="formula_5">L(?) = 1 N N i=1 (SwinF IR(I i L , ?) ? I i H ) 2 + ? (8)</formula><p>where ? denotes the parameters of SwinFIR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Data Augmentation</head><p>Radu et al. propose rotation and flip data enhancement approaches based on spatial transformation, which is widely used at low-level tasks. However, data augmentation based on the pixel-domain, which is extensively used and has yielded impressive results in high-level tasks, is rarely studied in low-level tasks. In this paper, in addition to flip and rotation, we revisit the effect of data augmentations based on the pixel-domain on image super-resolution, such as RGB channel shuffle, Mixup, Blend, CutMix and Cut-Mixup. RGB channel shuffle randomly shuffles the RGB channels of input images for color augmentation. Mixup randomly mixes the two images according to a certain proportion. Blend randomly adds fixed pixel to input images. CutMix and CutMixup are the combination of Mixup and Cutout. We illustrate in <ref type="figure" target="#fig_0">Figure 2</ref> how various data augmentations affect the performance of image super-resolution on the Set5 dataset. All techniques, except CutMix and Cut-Mixup which destroy visual continuity, are used for data augmentation and achieved performance gains.</p><p>Using more training data, enlarging the window size of Swin Transformer and employing pre-training model have all been demonstrated to be feasible in previous studies, so we won't discuss these here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Feature Ensemble</head><p>From the beginning of training the model to the convergence, there will be many intermediate models. In general, the model with the highest performance on the validation set will be selected as the final one, and other models will be deleted. Multi-model ensemble and self-ensembles are often used to improve the performance of SR. Multi-model ensemble combines the inference results from various models. Self-ensemble averages the transformed outputs from one model and input image. They have the same drawback that it will multiply the inference time. We propose a novel ensemble strategy without lengthening the training and testing periods. Specifically, we select multiple models that performed well on the validation dataset and combine them using the weighted average method. Our feature ensemble strategy can steadily improve the performance of the model and can be applied to any task, including low-level and high-level.</p><formula xml:id="formula_6">SwinF IR(?) = n i=1 SwinF IR(?) i * ? i<label>(9)</label></formula><p>where ? denotes the parameter sets of SwinFIR, n is the numbers of models. ? is the weight of each model and the ? = 1 n in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Following EDT <ref type="bibr" target="#b25">[26]</ref>, we pre-train the SwinFIR on Im-ageNet 2012 <ref type="bibr" target="#b8">[9]</ref>. Additionally, we use BICUBIC to obtain the necessary degradation inputs (?2/3/4) by downsampling the ImageNet 2012 training set. The SwinFIR is then fine-tuned for Classical and Lightweight SR using sub-images (384?384) that were generated by cropping the high-resolution DF2K (DIV2K [29] + Flicker2K <ref type="bibr" target="#b38">[39]</ref>) dataset. We perform validation on Image Super-Resolution benchmark datasets Set5, Set14, BSD100, Urban100 and Manga109 for Classical and Lightweight SR. Similarly, for Stereo Image Super-Resolution, we fine-tune on 800 stereo images of Flickr1024 <ref type="bibr" target="#b42">[43]</ref> training dataset and verify the effectiveness of our SwinFIR on KITTI2012 <ref type="bibr" target="#b13">[14]</ref>, KITTI2015 <ref type="bibr" target="#b32">[33]</ref>, Middlebury <ref type="bibr" target="#b35">[36]</ref> and Flickr1024 testing dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We revisit the long-dependent modeling capabilities of SwinIR and propose an efficient global feature extractor based on Fast Fourier Convolution (FFC). Specifically, we replace the convolution layer in RSTB of SwinIR with Spatial Frequency Block (SFB). For classical image SR, we utilize the same configuration as SwinIR, with the RSTB number, STL number, channel number and attention head number are typically set to 6, 6, 180 and 6, respectively. We also investigate how performance of SR is affected by large window and patch size. As a result, we use a larger window size 12 and patch size 60 in our work. For lightweight image SR, we also decrease RSTB number and channel number to 4 and 60 follow SwinIR, respectively. However, we use 5 STL in the second and third RSTB to accelerate training and inference time.</p><p>We use the Adam with ? 1 = 0.9 and ? 2 = 0.99 and weight decay 0 by default to optimize the Charbonnier loss function for classical image SR, lightweight image SR and Stereo image SR. In the pre-training stage, the initial learning rate is 2e-4 and reduces by 50% in 500,000, 800,000, 900,000, and 950,000 iterations for a total of 1,000,000 iterations, respectively. In the fine-tune stage, the learning rate is decreased to 1e-5 for classical and lightweight image SR. While learning rate is set to 1e-4 for Stereo image SR. Additionally, total training iterations of all tasks are reduced in half. Our implementation is based on PyTorch 1.9.1 and NVIDIA GeForce RTX 3090 GPU with CUDA11.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to state-of-the-arts methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Classical Image Super-Resolution</head><p>Modern algorithms such as EDSR <ref type="bibr" target="#b28">[29]</ref>, RCAN <ref type="bibr" target="#b46">[47]</ref>, SAN <ref type="bibr" target="#b7">[8]</ref>, IGNN <ref type="bibr" target="#b49">[50]</ref>, RNAN <ref type="bibr" target="#b47">[48]</ref>, HAN <ref type="bibr" target="#b33">[34]</ref>, NLSA <ref type="bibr" target="#b30">[31]</ref>, <ref type="table">Table 1</ref>. Quantitative comparison with state-of-the-art methods on benchmark datasets on the Y channel from the YCbCr space for classical image SR. The top two results are marked in red and blue. " ?" indicates that methods adopt pre-training strategy on ImageNet. Visual results are shown in <ref type="figure">Figure 5</ref>, and the images restored by our SwinFIR are clearer. Our SwinFIR can restore high-frequency details based on Fast Fourier Convolution (FFC). Especially, Our SwinFIR performs better when trying to restore images with periodic transformations. It is worth mentioning that the current approaches, whether  based on CNN or Transformer, are inadequate for challenging samples, as shown in <ref type="figure">Figure 5</ref>. And our method addresses this issue by making slight adjustments to the SwinIR, which can significantly improve performance. The LAM results are shown in <ref type="figure" target="#fig_3">Figure 6</ref>. The LAM attribution map and area of contribution accurately depict the significance of each pixel and receptive field size in the input LR image when reconstructing the red box region of the SR images. The Diffusion Index (DI) illustrates the range of relevant and utilised pixels. The higher the DI, the wider range of pixels are used. Furthermore, SwinFIR outperforms SwinIR and EDT in terms of DI, as shown in <ref type="figure" target="#fig_3">Figure 6</ref>. And almost all of the pixels from the input LR image are used to restore the SR image in SwinFIR, according to the LAM attribution map. The qualitative and quantitative results demonstrate that SwinIR does has a imited receptive field, and our SwinFIR based FFC takes advantage of long dependencies to use more pixels for better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Lightweight Image Super-Resolution</head><p>We substitute the Hourglass SFB for SFB in lightweight SwinFIR, named SwinFIR-T. And we also compare SwinFIR-T with the state-of-the-art lightweight super-  resolution methods, including SRCNN <ref type="bibr" target="#b10">[11]</ref>, LapSRN <ref type="bibr" target="#b21">[22]</ref>, DRRN <ref type="bibr" target="#b37">[38]</ref>, CARN-M <ref type="bibr" target="#b0">[1]</ref>, SRFBN-S <ref type="bibr" target="#b26">[27]</ref>, IMDN <ref type="bibr" target="#b17">[18]</ref>, SwinIR (small size) and EDT-T. SwinFIR significantly improves image super-resolution performance and achieves the best results for all metrics, as indicated by the quantitative comparison in <ref type="table">Table.</ref>2. Our SwinFIR achieves 31.50dB PSNR on the Manga109 dataset (?4) when the number of parameters of our method is comparable to the SwinIR and EDT-T, which is 0.58 dB and 0.15 dB higher than them, respectively. Visual results are presented in <ref type="figure" target="#fig_4">Figure 7</ref>, and the images restored by our SwinFIR are sharper and contain more high-frequency detail information. EDT and our SwinFIR all can recover the detailed information of the book on Set14 barbara dataset, but the reconstruction results of EDT are more blurring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Stereo Image Super-Resolution</head><p>We conduct a series of experiments in the stereo image SR and compare SwinFIR to other SR methods. Single image SR methods include EDSR, RCAN, and RDN <ref type="bibr" target="#b48">[49]</ref>, whereas stereo image SR methods include StereoSR <ref type="bibr" target="#b18">[19]</ref>, PASSRnet <ref type="bibr" target="#b40">[41]</ref>, SRRes+SAM <ref type="bibr" target="#b43">[44]</ref>, IMSSRnet <ref type="bibr" target="#b24">[25]</ref>, iPASSR <ref type="bibr" target="#b23">[24]</ref>, SSRDE-FNet <ref type="bibr" target="#b6">[7]</ref> and NAFSSR <ref type="bibr" target="#b5">[6]</ref>. All models are trained on 800 Flickr1024 and 60 Middlebury images. Our Swin-FIR belongs to the single SR method and is trained on Flickr1024 without Middlebury datasets. The SwinFIR be-    <ref type="table" target="#tab_5">Table 3</ref>. Similarly, our SwinFIR outperforms the existing stereo SR methods on Flickr1024 and Middlebury datasets. For example, our SwinFIR performs better for ?4 stereo SR than the prior state-of-the-art models SSRDE-FNet and NAFSSR-B by 0.55 dB and 0.07 dB on the Flickr1024 testing dataset, respectively. The visual comparison results are shown in <ref type="figure" target="#fig_5">Figure 8</ref>. The SR images reconstructed by our SwinFIR are clearer and offer a wealth of details and textures. The quantitative and qualitative results on stereo SR datasets demonstrate the effectiveness and robustness of our SwinFIR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Impact of Data augmentations</head><p>Radu et al. propose seven ways of data augmentations methods without altered content. They contend that other data augmentations can introduce new pixels and degrade superresolution performance. In addition to flip and rotation, we revisit the effect of data augmentations on image superresolution. The channel shuffle and Mixup data augmentations improve the PSNR from 32.78 dB to 32.93 dB, which is 0.15 dB better than flip and rotation, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>The experiments demonstrate that the data augmentations methods based on inserting new pixels sometimes improve the performance of SR, breaking people's previous cognition. However, not all data augmentations can improve image super-resolution performance. For example, CutMix and CutMixup boost the performance of image classification, but they have a drawback in the low-level tasks: they obliterate visual continuity. This drawback makes the image lose semantic information and reduces the available and useful information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Impact of Post Processing</head><p>SwinIR has indicated that post-processing methods such as self-ensemble can considerably improve the performance of image super-resolution. However, self-ensemble has a sub-stantial disadvantage in that it takes longer to do inference. Inspired by self-ensemble, which ensembles the results of image super-resolution, we ensemble the parameters of the trained models. On the one hand, our method plays the role of multiple models in the training process. And on the other hand, it does not increase any training and inference time.</p><p>Our approach steadily improves the super-resolution performance, as shown in the <ref type="table" target="#tab_6">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we revisit the limitations of long-dependent modeling capabilities of SwinIR and propose an efficient global feature extractor based on Fast Fourier Convolution (FFC), named SwinFIR. Furthermore, we also revisit other strategies for improving SR performance, including data augmentation, loss function, pre-training, and feature ensemble. The results demonstrate that our SwinFIR has a wider receptive field than SwinIR, and takes advantage of long dependencies based on FFC to use more pixels for better performance. In particular, our feature ensemble strategy steadily improves performance without lengthening the training and testing periods. Extensive experiments on popular benchmarks show that our SwinFIR surpasses current models and achieves SOTA performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The evolution trajectory from SwinIR to SwinFIR. All models are evaluated on Set5 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The network architecture of SwinFIR.(a) Convolution layer (b) SFB (b) Hourglass SFB</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The Spatial Frequency Block (SFB) is used for classical SR and hourglass SFB for lightweight SR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>LAM comparisons between SwinIR and EDT on Urban100(?4) img 011. The results indicate that SwinFIR utilize more information than SwinIR and EDT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Visual results (?4) achieved by different methods on the Urban100 dataset (lightweight image SR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Visual results (?4) achieved by different methods on the Flickr1024 dataset (stereo image SR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>SwinIR<ref type="bibr" target="#b27">[28]</ref>, and EDT<ref type="bibr" target="#b25">[26]</ref> are compared to our SwinFIR. The DIV2K dataset was used to train the CNNbased methods EDSR, RCAN, SAN, IGNN, RNAN, HAN, and NLSA. Networks based on Vision Transformer include IPT, SwinIR, and EDT. IPT and EDT are trained on the Ima-geNet dataset, while EDT is fine-tuned on the DF2K dataset to get better performance. And SwinIR only is trained on the DF2K dataset. Following EDT, our SwinFIR is first trained on the ImageNet, and then fine-tuned on the DF2K dataset.Table 1displays the quantitative results on benchmark datasets for classical SR. Our SwinFIR achieves the best SR performance on ?2, ?3 and ?4 scales compared with other state-of-the-art methods. Especially, SwinFIR improves the PSNR of SwinIR from 27.45 dB and 32.03 dB to 28.12 dB and 32.83 dB on ?4 scales of Urban100 and Manga109 datasets respectively, 0.77 dB and 0.80 dB higher than its. It demonstrates the effectiveness of our proposed method and represents a major improvement over the image super-resolution task. Even without pre-training, our SwinFIR achieves better or comparable performance than EDT with pre-training, 0.13 dB higher on ?4 scales of Manga109 datasets.</figDesc><table><row><cell></cell><cell>Scale</cell><cell>Training Dataset</cell><cell cols="10">Set5 PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM Set14 BSD100 Urban100 Manga109</cell></row><row><cell>EDSR</cell><cell>?2</cell><cell>DIV2K</cell><cell cols="10">38.11 0.9602 33.92 0.9195 32.32 0.9013 32.93 0.9351 39.10 0.9773</cell></row><row><cell>RCAN</cell><cell>?2</cell><cell>DIV2K</cell><cell cols="10">38.27 0.9614 34.12 0.9216 32.41 0.9027 33.34 0.9384 39.44 0.9786</cell></row><row><cell>SAN</cell><cell>?2</cell><cell>DIV2K</cell><cell cols="10">38.31 0.9620 34.07 0.9213 32.42 0.9028 33.10 0.9370 39.32 0.9792</cell></row><row><cell>IGNN</cell><cell>?2</cell><cell>DIV2K</cell><cell cols="10">38.24 0.9613 34.07 0.9217 32.41 0.9025 33.23 0.9383 39.35 0.9786</cell></row><row><cell>HAN</cell><cell>?2</cell><cell>DIV2K</cell><cell cols="10">38.27 0.9614 34.16 0.9217 32.41 0.9027 33.35 0.9385 39.46 0.9785</cell></row><row><cell>NLSN</cell><cell>?2</cell><cell>DIV2K</cell><cell cols="10">38.34 0.9618 34.08 0.9231 32.43 0.9027 33.42 0.9394 39.59 0.9789</cell></row><row><cell>SwinIR</cell><cell>?2</cell><cell>DF2K</cell><cell cols="10">38.42 0.9623 34.46 0.9250 32.53 0.9041 33.81 0.9427 39.92 0.9797</cell></row><row><cell>EDT</cell><cell>?2</cell><cell>DF2K</cell><cell cols="10">38.45 0.9624 34.57 0.9258 32.52 0.9041 33.80 0.9425 39.93 0.9800</cell></row><row><cell>SwinFIR (Ours)</cell><cell>?2</cell><cell>DF2K</cell><cell cols="10">28.57 0.9630 34.66 0.9263 32.59 0.9049 34.30 0.9459 40.30 0.9809</cell></row><row><cell>IPT  ?</cell><cell>?2</cell><cell cols="2">ImageNet 38.37</cell><cell>-</cell><cell>34.43</cell><cell>-</cell><cell>32.48</cell><cell>-</cell><cell>33.76</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EDT  ?</cell><cell>?2</cell><cell>DF2K</cell><cell cols="10">38.63 0.9632 34.80 0.9273 32.62 0.9052 34.27 0.9456 40.37 0.9811</cell></row><row><cell>SwinFIR  ? (Ours)</cell><cell>?2</cell><cell>DF2K</cell><cell cols="10">38.65 0.9633 34.93 0.9276 32.64 0.9054 34.57 0.9473 40.61 0.9816</cell></row><row><cell>EDSR</cell><cell>?3</cell><cell>DIV2K</cell><cell cols="10">34.65 0.9280 30.52 0.8462 29.25 0.8093 28.80 0.8653 34.17 0.9476</cell></row><row><cell>RCAN</cell><cell>?3</cell><cell>DIV2K</cell><cell cols="10">34.74 0.9299 30.65 0.8482 29.32 0.8111 29.09 0.8702 34.44 0.9499</cell></row><row><cell>SAN</cell><cell>?3</cell><cell>DIV2K</cell><cell cols="10">34.75 0.9300 30.59 0.8476 29.33 0.8112 28.93 0.8671 34.30 0.9494</cell></row><row><cell>IGNN</cell><cell>?3</cell><cell>DIV2K</cell><cell cols="10">34.72 0.9298 30.66 0.8484 29.31 0.8105 29.03 0.8696 34.39 0.9496</cell></row><row><cell>HAN</cell><cell>?3</cell><cell>DIV2K</cell><cell cols="10">34.75 0.9299 30.67 0.8483 29.32 0.8110 29.10 0.8705 34.48 0.9500</cell></row><row><cell>NLSN</cell><cell>?3</cell><cell>DIV2K</cell><cell cols="10">34.85 0.9306 30.70 0.8485 29.34 0.8117 29.25 0.8726 34.57 0.9508</cell></row><row><cell>SwinIR</cell><cell>?3</cell><cell>DF2K</cell><cell cols="10">34.97 0.9318 30.93 0.8534 29.46 0.8145 29.75 0.8826 35.12 0.9537</cell></row><row><cell>EDT</cell><cell>?3</cell><cell>DF2K</cell><cell cols="10">34.97 0.9316 30.89 0.8527 29.44 0.8142 29.72 0.8814 35.13 0.9534</cell></row><row><cell>SwinFIR (Ours)</cell><cell>?3</cell><cell>DF2K</cell><cell cols="10">35.13 0.9328 31.13 0.8556 29.52 0.8161 30.20 0.8885 35.53 0.9554</cell></row><row><cell>IPT  ?</cell><cell>?3</cell><cell cols="2">ImageNet 34.81</cell><cell>-</cell><cell>30.85</cell><cell>-</cell><cell>29.38</cell><cell>-</cell><cell>29.49</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EDT  ?</cell><cell>?3</cell><cell>DF2K</cell><cell cols="10">35.13 0.9328 31.09 0.8553 29.53 0.8165 30.07 0.8863 35.47 0.9550</cell></row><row><cell>SwinFIR  ? (Ours)</cell><cell>?3</cell><cell>DF2K</cell><cell cols="10">35.15 0.9330 31.24 0.8566 29.55 0.8169 30.43 0.8913 35.77 0.9563</cell></row><row><cell>EDSR</cell><cell>?4</cell><cell>DIV2K</cell><cell cols="10">32.46 0.8968 28.80 0.7876 27.71 0.7420 26.64 0.8033 31.02 0.9148</cell></row><row><cell>RCAN</cell><cell>?4</cell><cell>DIV2K</cell><cell cols="10">32.63 0.9002 28.87 0.7889 27.77 0.7436 26.82 0.8087 31.22 0.9173</cell></row><row><cell>SAN</cell><cell>?4</cell><cell>DIV2K</cell><cell cols="10">32.64 0.9003 28.92 0.7888 27.78 0.7436 26.79 0.8068 31.18 0.9169</cell></row><row><cell>IGNN</cell><cell>?4</cell><cell>DIV2K</cell><cell cols="10">32.57 0.8998 28.85 0.7891 27.77 0.7434 26.84 0.8090 31.28 0.9182</cell></row><row><cell>HAN</cell><cell>?4</cell><cell>DIV2K</cell><cell cols="10">32.64 0.9002 28.90 0.7890 27.80 0.7442 26.85 0.8094 31.42 0.9177</cell></row><row><cell>NLSN</cell><cell>?4</cell><cell>DIV2K</cell><cell cols="10">32.59 0.9000 28.87 0.7891 27.78 0.7444 26.96 0.8109 31.27 0.9184</cell></row><row><cell>SwinIR</cell><cell>?4</cell><cell>DF2K</cell><cell cols="10">32.92 0.9044 29.09 0.7950 27.92 0.7489 27.45 0.8254 32.03 0.9260</cell></row><row><cell>EDT</cell><cell>?4</cell><cell>DF2K</cell><cell cols="10">32.82 0.9031 29.09 0.7939 27.91 0.7483 27.46 0.8246 32.05 0.9254</cell></row><row><cell>SwinFIR (Ours)</cell><cell>?4</cell><cell>DF2K</cell><cell cols="10">33.08 0.9048 29.21 0.7971 27.98 0.7508 27.87 0.9348 32.52 0.9292</cell></row><row><cell>IPT  ?</cell><cell>?4</cell><cell cols="2">ImageNet 32.64</cell><cell>-</cell><cell>29.01</cell><cell>-</cell><cell>27.82</cell><cell>-</cell><cell>27.26</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EDT  ?</cell><cell>?4</cell><cell>DF2K</cell><cell cols="10">33.06 0.9055 29.23 0.7971 27.99 0.7510 27.75 0.8317 32.39 0.9283</cell></row><row><cell>SwinFIR  ? (Ours)</cell><cell>?4</cell><cell>DF2K</cell><cell cols="10">33.20 0.9068 29.36 0.7993 28.03 0.7520 28.12 0.8393 32.83 0.9314</cell></row><row><cell>IPT [3],</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Visual results (?4) achieved by different methods on the Urban100 dataset (classical image SR).</figDesc><table><row><cell></cell><cell></cell><cell>EDSR</cell><cell>IGNN</cell><cell>HAN</cell><cell>IPT</cell><cell>RNAN</cell></row><row><cell cols="2">Urban100(?4): img 004</cell><cell>RCAN</cell><cell>SwinIR</cell><cell>EDT</cell><cell>SwinFIR(ours)</cell><cell>Reference</cell></row><row><cell></cell><cell></cell><cell>EDSR</cell><cell>IGNN</cell><cell>HAN</cell><cell>IPT</cell><cell>RNAN</cell></row><row><cell cols="2">Urban100(?4): img 044</cell><cell>RCAN</cell><cell>SwinIR</cell><cell>EDT</cell><cell>SwinFIR(ours)</cell><cell>Reference</cell></row><row><cell></cell><cell></cell><cell>EDSR</cell><cell>IGNN</cell><cell>HAN</cell><cell>IPT</cell><cell>RNAN</cell></row><row><cell cols="2">Urban100(?4): img 073</cell><cell>RCAN</cell><cell>SwinIR</cell><cell>EDT</cell><cell>SwinFIR(ours)</cell><cell>Reference</cell></row><row><cell>HR Image</cell><cell>DI: 4.38 Figure 5. 17.22 dB LAM Attribution Area of Contribution</cell><cell>19.21 dB DI: 11.95</cell><cell>21.78 dB DI: 19.31</cell><cell></cell><cell></cell></row><row><cell>LR Image</cell><cell>SR Results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SwinIR</cell><cell>EDT</cell><cell>SwinFIR</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison with state-of-the-art methods on benchmark datasets on the Y channel from the YCbCr space for lightweight image SR. The top two results are marked in red and blue. ) PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM ?2 LAPAR 548 38.01 0.9605 33.62 0.9183 32.19 0.8999 32.10 0.9283 38.67 0.9772</figDesc><table><row><cell cols="3">#Param. (?10 3 LatticeNet Scale Method 756</cell><cell cols="4">Set5 38.15 0.9610 33.78 0.9193 32.25 0.9005 32.43 0.9302 Set14 BSDS100 Urban100</cell><cell>Manga109 --</cell></row><row><cell></cell><cell>SwinIR</cell><cell>878</cell><cell cols="4">38.14 0.9611 33.86 0.9206 32.31 0.9012 32.76 0.9340 39.12 0.9783</cell></row><row><cell></cell><cell>EDT-T</cell><cell>917</cell><cell cols="4">38.23 0.9615 33.99 0.9209 32.37 0.9021 32.98 0.9362 39.45 0.9789</cell></row><row><cell></cell><cell>SwinFIR-T(Ours)</cell><cell>891</cell><cell cols="4">38.26 0.9616 34.08 0.9221 32.38 0.9024 33.14 0.9374 39.55 0.9790</cell></row><row><cell></cell><cell>LAPAR</cell><cell>594</cell><cell cols="4">34.36 0.9267 30.34 0.8421 29.11 0.8054 28.15 0.8523 33.51 0.9441</cell></row><row><cell></cell><cell>LatticeNet</cell><cell>765</cell><cell cols="4">34.53 0.9281 30.39 0.8424 29.15 0.8059 28.33 0.8538</cell><cell>-</cell><cell>-</cell></row><row><cell>?3</cell><cell>SwinIR</cell><cell>886</cell><cell cols="4">34.62 0.9289 30.54 0.8463 29.20 0.8082 28.66 0.8624 33.98 0.9478</cell></row><row><cell></cell><cell>EDT-T</cell><cell>919</cell><cell cols="4">34.73 0.9299 30.66 0.8481 29.29 0.8103 28.89 0.8674 34.44 0.9498</cell></row><row><cell></cell><cell>SwinFIR-T(Ours)</cell><cell>891</cell><cell cols="4">34.75 0.9300 30.68 0.8489 29.30 0.8106 29.04 0.8697 34.60 0.9506</cell></row><row><cell></cell><cell>LAPAR</cell><cell>659</cell><cell cols="4">32.15 0.8944 28.61 0.7818 27.61 0.7366 26.14 0.7871 30.42 0.9074</cell></row><row><cell></cell><cell>LatticeNet</cell><cell>777</cell><cell cols="4">32.30 0.8962 28.68 0.7830 27.62 0.7367 26.25 0.7873</cell><cell>-</cell><cell>-</cell></row><row><cell>?4</cell><cell>SwinIR</cell><cell>897</cell><cell cols="4">32.44 0.8976 28.77 0.7858 27.69 0.7406 26.47 0.7980 30.92 0.9151</cell></row><row><cell></cell><cell>EDT-T</cell><cell>922</cell><cell cols="4">32.53 0.8991 28.88 0.7882 27.76 0.7433 26.71 0.8051 31.35 0.9180</cell></row><row><cell></cell><cell>SwinFIR-T(Ours)</cell><cell>891</cell><cell cols="4">32.62 0.9002 28.95 0.7898 27.79 0.7440 26.85 0.8088 31.50 0.9199</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SRCNN</cell><cell>LapSRN</cell><cell>DRRN</cell><cell>CARN-M</cell><cell>SRFBN-S</cell></row><row><cell></cell><cell>Set14(?4): barbara</cell><cell></cell><cell>IMDN</cell><cell>SwinIR</cell><cell>EDT</cell><cell>SwinFIR(ours)</cell><cell>Reference</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SRCNN</cell><cell>LapSRN</cell><cell>DRRN</cell><cell>CARN-M</cell><cell>SRFBN-S</cell></row><row><cell></cell><cell>Urban100(?4): img 092</cell><cell></cell><cell>IMDN</cell><cell>SwinIR</cell><cell>EDT</cell><cell>SwinFIR(ours)</cell><cell>Reference</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Quantitative results achieved by different methods on the KITTI 2012<ref type="bibr" target="#b13">[14]</ref>, KITTI 2015<ref type="bibr" target="#b32">[33]</ref>, Middlebury<ref type="bibr" target="#b35">[36]</ref>, and Flickr1024<ref type="bibr" target="#b40">[41]</ref> datasets on the RGB space for stereo image SR. #P represents the number of parameters of the networks. Here, PSNR/SSIM values achieved on both the left images (i.e., Left) and a pair of stereo images (i.e., (Left + Right) /2) are reported. The top two results are marked in red and blue.42M  24.49/0.7502 23.67/0.7273 27.70/0.8036 24.53/0.7555 24.21/0.7511 27.64/0.8022 21.70/0.6460 PASSRnet ?4 1.42M 26.26/0.7919 25.41/0.7772 28.61/0.8232 26.34/0.7981 26.08/0.8002 28.72/0.8236 23.31/0.7195 SRRes+SAM ?4 1.73M 26.35/0.7957 25.55/0.7825 28.76/0.8287 26.44/0.8018 26.22/0.8054 28.83/0.8290 23.27/0.7233</figDesc><table><row><cell>Method</cell><cell>Scale</cell><cell>#P</cell><cell></cell><cell>Left</cell><cell></cell><cell></cell><cell cols="2">(Left + Right) /2</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>KITTI 2012</cell><cell>KITTI 2015</cell><cell>Middlebury</cell><cell>KITTI 2012</cell><cell>KITTI 2015</cell><cell>Middlebury</cell><cell>Flickr1024</cell></row><row><cell>VDSR</cell><cell>?2</cell><cell cols="8">0.66M 30.17/0.9062 28.99/0.9038 32.66/0.9101 30.30/0.9089 29.78/0.9150 32.77/0.9102 25.60/0.8534</cell></row><row><cell>EDSR</cell><cell>?2</cell><cell cols="8">38.6M 30.83/0.9199 29.94/0.9231 34.84/0.9489 30.96/0.9228 30.73/0.9335 34.95/0.9492 28.66/0.9087</cell></row><row><cell>RDN</cell><cell>?2</cell><cell cols="8">22.0M 30.81/0.9197 29.91/0.9224 34.85/0.9488 30.94/0.9227 30.70/0.9330 34.94/0.9491 28.64/0.9084</cell></row><row><cell>RCAN</cell><cell>?2</cell><cell cols="8">15.3M 30.88/0.9202 29.97/0.9231 34.80/0.9482 31.02/0.9232 30.77/0.9336 34.90/0.9486 28.63/0.9082</cell></row><row><cell>StereoSR</cell><cell>?2</cell><cell cols="8">1.08M 29.42/0.9040 28.53/0.9038 33.15/0.9343 29.51/0.9073 29.33/0.9168 33.23/0.9348 25.96/0.8599</cell></row><row><cell>PASSRnet</cell><cell>?2</cell><cell cols="8">1.37M 30.68/0.9159 29.81/0.9191 34.13/0.9421 30.81/0.9190 30.60/0.9300 34.23/0.9422 28.38/0.9038</cell></row><row><cell>IMSSRnet</cell><cell>?2</cell><cell>6.84M</cell><cell>30.90/-</cell><cell>29.97/-</cell><cell>34.66/-</cell><cell>30.92/-</cell><cell>30.66/-</cell><cell>34.67/-</cell><cell>-/-</cell></row><row><cell>iPASSR</cell><cell>?2</cell><cell cols="8">1.37M 30.97/0.9210 30.01/0.9234 34.41/0.9454 31.11/0.9240 30.81/0.9340 34.51/0.9454 28.60/0.9097</cell></row><row><cell>SSRDE-FNet</cell><cell>?2</cell><cell cols="8">2.10M 31.08/0.9224 30.10/0.9245 35.02/0.9508 31.23/0.9254 30.90/0.9352 35.09/0.9511 28.85/0.9132</cell></row><row><cell>NAFSSR-T</cell><cell>?2</cell><cell cols="8">0.45M 31.12/0.9224 30.19/0.9253 34.93/0.9495 31.26/0.9254 30.99/0.9355 35.01/0.9495 28.94/0.9128</cell></row><row><cell>NAFSSR-S</cell><cell>?2</cell><cell cols="8">1.54M 31.23/0.9236 30.28/0.9266 35.23/0.9515 31.38/0.9266 31.08/0.9367 35.30/0.9514 29.19/0.9160</cell></row><row><cell>NAFSSR-B</cell><cell>?2</cell><cell cols="8">6.77M 31.40/0.9254 30.42/0.9282 35.62/0.9545 31.55/0.9283 31.22/0.9380 35.68/0.9544 29.54/0.9204</cell></row><row><cell>SwinFIR-T (Ours)</cell><cell>?2</cell><cell cols="8">0.89M 31.09/0.9226 30.17/0.9258 35.00/0.9491 31.22/0.9254 30.96/0.9359 35.11/0.9497 29.03/0.9134</cell></row><row><cell>SwinFIR (Ours)</cell><cell>?2</cell><cell cols="8">13.99M 31.35/0.9253 30.43/0.9284 35.74/0.9540 31.48/0.9281 31.23/0.9382 35.84/0.9543 29.62/0.9199</cell></row><row><cell>VDSR</cell><cell>?4</cell><cell cols="8">0.66M 25.54/0.7662 24.68/0.7456 27.60/0.7933 25.60/0.7722 25.32/0.7703 27.69/0.7941 22.46/0.6718</cell></row><row><cell>EDSR</cell><cell>?4</cell><cell cols="8">38.9M 26.26/0.7954 25.38/0.7811 29.15/0.8383 26.35/0.8015 26.04/0.8039 29.23/0.8397 23.46/0.7285</cell></row><row><cell>RDN</cell><cell>?4</cell><cell cols="8">22.0M 26.23/0.7952 25.37/0.7813 29.15/0.8387 26.32/0.8014 26.04/0.8043 29.27/0.8404 23.47/0.7295</cell></row><row><cell>RCAN</cell><cell>?4</cell><cell cols="8">15.4M 26.36/0.7968 25.53/0.7836 29.20/0.8381 26.44/0.8029 26.22/0.8068 29.30/0.8397 23.48/0.7286</cell></row><row><cell cols="3">StereoSR 1.IMSSRnet ?4 ?4 6.89M</cell><cell>26.44/-</cell><cell>25.59/-</cell><cell>29.02/-</cell><cell>26.43/-</cell><cell>26.20/-</cell><cell>29.02/-</cell><cell>-/-</cell></row><row><cell>iPASSR</cell><cell>?4</cell><cell cols="8">1.42M 26.47/0.7993 25.61/0.7850 29.07/0.8363 26.56/0.8053 26.32/0.8084 29.16/0.8367 23.44/0.7287</cell></row><row><cell>SSRDE-FNet</cell><cell>?4</cell><cell cols="8">2.24M 26.61/0.8028 25.74/0.7884 29.29/0.8407 26.70/0.8082 26.43/0.8118 29.38/0.8411 23.59/0.7352</cell></row><row><cell>NAFSSR-T</cell><cell>?4</cell><cell cols="8">0.46M 26.69/0.8045 25.90/0.7930 29.22/0.8403 26.79/0.8105 26.62/0.8159 29.32/0.8409 23.69/0.7384</cell></row><row><cell>NAFSSR-S</cell><cell>?4</cell><cell cols="8">1.56M 26.84/0.8086 26.03/0.7978 29.62/0.8482 26.93/0.8145 26.76/0.8203 29.72/0.8490 23.88/0.7468</cell></row><row><cell>NAFSSR-B</cell><cell>?4</cell><cell cols="8">6.80M 26.99/0.8121 26.17/0.8020 29.94/0.8561 27.08/0.8181 26.91/0.8245 30.04/0.8568 24.07/0.7551</cell></row><row><cell>SwinFIR-T (Ours)</cell><cell>?4</cell><cell cols="8">0.89M 26.59/0.8017 25.78/0.7904 29.36/0.8409 26.68/0.8081 26.51/0.8135 29.48/0.8426 23.73/0.7400</cell></row><row><cell>SwinFIR (Ours)</cell><cell>?4</cell><cell cols="8">13.99M 26.83/0.8086 26.00/0.7978 30.01/0.8565 26.92/0.8148 26.74/0.8206 30.14/0.8582 24.14/0.7560</cell></row><row><cell></cell><cell></cell><cell>Bicubic</cell><cell></cell><cell>StereoSR</cell><cell>EDSR</cell><cell></cell><cell>RCAN</cell><cell cols="2">SRRes+SAM</cell></row><row><cell>img 0035 (Left)</cell><cell></cell><cell>iPASSR</cell><cell></cell><cell>SSRDE-FNet</cell><cell cols="2">NAFSSR-B</cell><cell>SwinFIR(ours)</cell><cell cols="2">Reference</cell></row><row><cell></cell><cell></cell><cell>Bicubic</cell><cell></cell><cell>StereoSR</cell><cell>EDSR</cell><cell></cell><cell>RCAN</cell><cell cols="2">SRRes+SAM</cell></row><row><cell>img 0035 (Right)</cell><cell></cell><cell>iPASSR</cell><cell></cell><cell>SSRDE-FNet</cell><cell cols="2">NAFSSR-B</cell><cell>SwinFIR(ours)</cell><cell cols="2">Reference</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Performance with/without feature ensemble (FE). Our approach steadily improves the super-resolution performance with only once inference.</figDesc><table><row><cell cols="5">FE Scale KITTI 2012 Middlebury Flickr1024</cell></row><row><cell>?</cell><cell>?2</cell><cell>31.48</cell><cell>35.84</cell><cell>29.62</cell></row><row><cell>?</cell><cell>?2</cell><cell>31.51</cell><cell>35.89</cell><cell>29.63</cell></row><row><cell>?</cell><cell>?4</cell><cell>26.92</cell><cell>30.14</cell><cell>24.14</cell></row><row><cell>?</cell><cell>?4</cell><cell>26.93</cell><cell>30.16</cell><cell>24.15</cell></row><row><cell cols="5">longs to the single SR method. Our SwinFIR and SwinFIR-</cell></row><row><cell cols="5">T surpass all single image SR methods. In particular, com-</cell></row><row><cell cols="5">pared with EDSR, our SwinFIR-T achieves better perfor-</cell></row><row><cell cols="5">mance, while the number of parameters is only 2.3% of the</cell></row><row><cell cols="3">EDSR, as shown in</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fast, accurate, and lightweight super-resolution with cascading residual network: 15th european conference, munich, germany</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhyuk</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungkon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung</forename><forename type="middle">Ah</forename><surname>Sohn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Activating more pixels in image super-resolution transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.04437</idno>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast fourier convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nafssr: Stereo image super-resolution using nafnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1239" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Feedback network for mutually boosted stereo image super-resolution and disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11065" to="11074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interpreting super-resolution networks with local attribution maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sotr: Segmenting objects with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dantong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liao</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7157" to="7166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Lightweight image super-resolution with information multi-distillation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhancing the spatial resolution of stereo images using a parallax prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeplyrecursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="624" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast and accurate image super-resolution with deep laplacian pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep stereoscopic image super-resolution via interaction module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep stereoscopic image super-resolution via interaction module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoting</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3051" to="3061" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On efficient transformer and image pre-training for lowlevel vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10175</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feedback network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Swinir: Image restoration using swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1833" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image superresolution with non-local sparse attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image super-resolution with cross-scale non-local attention and exhaustive selfexemplars mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5690" to="5699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Single image super-resolution via a holistic attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangde</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4491" to="4500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">High-resolution stereo datasets with subpixel-accurate ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">York</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Krathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nera</forename><surname>Ne?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Porter</forename><surname>Westling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German conference on pattern recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Resolution-robust large mask inpainting with fourier convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Suvorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizaveta</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Mashikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Remizova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Silvestrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naejin</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshith</forename><surname>Goka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiwoong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="2149" to="2159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image superresolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3147" to="3155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="114" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning parallax attention for stereo image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiping</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Flickr1024: A large-scale dataset for stereo image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A stereo attention module for stereo image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="496" to="500" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Residual non-local attention networks for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10082</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cross-scale internal graph neural network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3499" to="3509" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
