<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micael</forename><surname>Carvalho</surname></persName>
							<email>micael.carvalho@lip6.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Cad?ne</surname></persName>
							<email>remi.cadene@lip6.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
							<email>david.picard@lip6.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Soulier</surname></persName>
							<email>laure.soulier@lip6.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
							<email>nicolas.thome@cnam.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<email>matthieu.cord@lip6.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Sorbonne Universit?</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>LIP6, F-75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Sorbonne Universit?</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>LIP6, F-75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Sorbonne Universit?</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>LIP6, F-75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Sorbonne Universit?</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>LIP6, F-75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">CEDRIC -Conservatoire National des Arts et M?tiers</orgName>
								<address>
									<postCode>F-75003</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">Sorbonne Universit?</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>LIP6, F-75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>ACM Reference Format: Micael Carvalho, R?mi Cad?ne, David Picard, Laure Soulier, Nicolas Thome, and Matthieu Cord. 2018. Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings . In Proceedings of The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, Ann Arbor, MI, USA (SIGIR&apos;18). ACM, New York, NY, USA, Article 4, 10 pages.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Information systems ? Multimedia and multimodal retrieval</term>
					<term>? Computer systems organization ? Neural networks</term>
					<term>KEYWORDS Deep Learning, Cross-modal Retrieval, Semantic Embeddings</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Designing powerful tools that support cooking activities has rapidly gained popularity due to the massive amounts of available data, as well as recent advances in machine learning that are capable of analyzing them. In this paper, we propose a cross-modal retrieval model aligning visual and textual data (like pictures of dishes and their recipes) in a shared representation space. We describe an effective learning scheme, capable of tackling large-scale problems, and validate it on the Recipe1M dataset containing nearly 1 million picture-recipe pairs. We show the effectiveness of our approach regarding previous state-of-the-art models and present qualitative results over computational cooking use cases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Designing powerful tools that support cooking activities has become an attractive research field in recent years due to the growing interest of users to eat home-made food and share recipes on social platforms <ref type="bibr" target="#b34">[35]</ref>. These massive amounts of data shared on devoted sites, * Equal contribution Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR <ref type="bibr">'18, 2018 ? 2018</ref> Copyright held by the owner/author(s). such as All Recipes 1 , allow gathering food-related data including text recipes, images, videos, and/or user preferences. Consequently, novel applications are rising, such as ingredient classification <ref type="bibr" target="#b6">[7]</ref>, recipe recognition <ref type="bibr" target="#b38">[39]</ref> or recipe recommendation <ref type="bibr" target="#b34">[35]</ref>. However, solving these tasks is challenging since it requires taking into consideration 1) the heterogeneity of data in terms of format (text, image, video, ...) or structure (e.g., list of items for ingredients, short verbal sentence for instructions, or verbose text for users' reviews); and 2) the cultural factor behind each recipe since the vocabulary, the quantity measurement, and the flavor perception is culturally intrinsic; preventing the homogeneous semantics of recipes.</p><p>One recent approach emerging from the deep learning community aims at learning the semantics of objects in a latent space using the distributional hypothesis <ref type="bibr" target="#b14">[15]</ref> that constrains object with similar meanings to be represented similarly. First used for learning image representations (also called embeddings), this approach has been derived to text-based applications, and recently some researchers investigate the potential of representing multi-modal evidence sources (e.g., texts and images) in a shared latent space <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref>. This research direction is particularly interesting for grounding language with common sense information extracted from images, or viceversa. In the context of computer-aided cooking, we believe that this multi-modal representation learning approach would contribute to solving the heterogeneity challenge, since they would promote a better understanding of each domain-specific word/image/video. In practice, a typical approach consists in aligning text and image representations in a shared latent space in which they can be compared <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>. One direct application in the cooking context is to perform cross-modal retrieval where the goal is to retrieve images similar to a text recipe query or conversely text recipes similar to a image query.</p><p>However, Salvador et al. <ref type="bibr" target="#b33">[34]</ref> highlight that this solution based on aligning matching pairs can lead to poor retrieval performances in a large scale framework. Training the latent space by only matching pairs of the exact same dish is not particularly effective at mapping similar dishes close together, which induces a lack of generalization to newer items (recipes or images). To alleviate this problem, <ref type="bibr" target="#b33">[34]</ref> proposes to use additional data (namely, categories of meals) to train a classifier with the aim of regularizing the latent space embeddings <ref type="figure">(Figure 1a</ref>). Their approach involves adding an extra layer to a deep (a) im2recipe <ref type="bibr" target="#b33">[34]</ref> (b) AdaMine (ours) <ref type="figure">Figure 1</ref>: Comparison between (a) the classification augmented latent space learning of <ref type="bibr" target="#b33">[34]</ref> and (b) our joint retrieval and semantic latent space learning, which combines instance-based (L ins ) and semantic-based (L sem ) losses.</p><p>neural network, specialized in the classification task. However, we believe that such classification scheme is under-effective for two main reasons. First, the classifier adds many parameters that are discarded at the end of the training phase, since classification is not a goal of the system. Second, we hypothesize that given its huge number of parameters, the classifier can be trained with high accuracy without changing much of the structure of the underlying latent space, which completely defeats the original purpose of adding classification information.</p><p>To solve these issues, we propose a unified learning framework in which we simultaneously leverage retrieval and class-guided features in a shared latent space ( <ref type="figure">Figure 1b</ref>). Our contributions are three-fold:</p><p>? We formulate a joint objective function with cross-modal retrieval and classification loss to structure the latent space. Our intuition is that directly injecting the class-based evidence sources in the representation learning process is more effective at enforcing a high-level structure to the latent space as shown in <ref type="figure">Figure 1b</ref>.</p><p>? We propose a double-triplet scheme to express jointly 1) the retrieval loss (e.g., corresponding picture and recipe of a pizza should be closer in the latent space than any other picture -see blue arrows in <ref type="figure">Figure 1b</ref>) and 2) the class-based one (e.g., any 2 pizzas should be closer in the latent space than a pizza and another item from any other class, like salad). This double-loss is capable of taking into consideration both the fine-grained and the high-level semantic information underlying recipe items. Contrary to the proposal of <ref type="bibr" target="#b33">[34]</ref>, our class-based loss acts directly on the feature space, instead of adding a classification layer to the model.</p><p>? We introduce a new scheme to tune the gradient update in the stochastic gradient descent and back-propagation algorithms used for training our model. More specifically, we improve over the usual gradient averaging update by performing an adaptive mining of the most significant triplets, which leads to better embeddings.</p><p>We instantiate these contributions through a dual deep neural network. We thoroughly evaluate our model on Recipe1M <ref type="bibr" target="#b33">[34]</ref>, the only English large-scale cooking dataset available, and show its superior performances compared to the state of the art models.</p><p>The remaining of this paper is organized as follows. In Section 2, we present previous work related to computer-aided cooking and cross-modal retrieval. Then, we present in Section 3 our joint retrieval and classification model as well as our adaptive triplet mining scheme to train the model. We introduce the experimental protocol in Section 4. In Section 5 we experimentally validate our hypotheses and present quantitative and qualitative results highlighting our model effectiveness. Finally, we conclude and discuss perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Computational cooking</head><p>Cooking is one of the most fundamental human activities connected to various aspects of human life such as food, health, dietary, culinary art, and so on. This is more particularly perceived on social platforms in which people share recipes or their opinions about meals, also known as the eat and tweet or food porn phenomenon <ref type="bibr" target="#b1">[2]</ref>. Users' needs give rise to smart cooking-oriented tasks that contribute towards the definition of computational cooking as a research field by itself <ref type="bibr" target="#b0">[1]</ref>. Indeed, the research community is very active in investigating issues regarding food-related tasks, such as ingredient identification <ref type="bibr" target="#b7">[8]</ref>, recipe recommendation <ref type="bibr" target="#b10">[11]</ref>, or recipe popularity prediction <ref type="bibr" target="#b34">[35]</ref>. A first line of work consists in leveraging the semantics behind recipe texts and images using deep learning approaches <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. The objective of such propositions is to generally align different modalities in a shared latent space to perform cross-modal retrieval or recommendation. For instance, Salvador et al. <ref type="bibr" target="#b33">[34]</ref> introduce a dual neural network that aligns textual and visual representations under both the distributional hypothesis and classification constraints <ref type="bibr" target="#b33">[34]</ref>. A second line of work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38]</ref> aims at exploiting additional information (e.g., calories, biological or economical factors) to bridge the gap between computational cooking and healthy issues. For instance, Kusmierczyk et al. <ref type="bibr" target="#b26">[27]</ref> extend the Latent Dirichlet Algorithm (LDA) for combining recipe descriptions and nutritional related meta-data to mine latent recipe topics that could be exploited to predict nutritional values of meals.</p><p>These researches are boosted by the release of food-related datasets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23]</ref>. As a first example, <ref type="bibr" target="#b9">[10]</ref> proposes the Pittsburgh fast-food image dataset, containing 4,556 pictures of fast-food plates. In order to solve more complex tasks, other initiatives provide richer sets of images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref>. For example, <ref type="bibr" target="#b5">[6]</ref> proposes the Food-101 dataset, containing around 101,000 images of 101 different categories. Additional meta-data information is also provided in the dataset of <ref type="bibr" target="#b4">[5]</ref> which involves GPS data or nutritional values. More recently, two very large-scale food-related datasets, respectively in English and Japanese, are released by <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b13">[14]</ref>. The Cookpad dataset <ref type="bibr" target="#b13">[14]</ref> gathers more than 1 million of recipes described using structured information, such as recipe description, ingredients, and process steps as well as images. Salvador et al. <ref type="bibr" target="#b33">[34]</ref> have collected a very large dataset with nearly 1 million recipes, with about associated 800,000 images. They also add extra information corresponding to recipe classes and show how this new semantic information may be helpful to improve deep cross-modal retrieval systems. To the best of our knowledge, this dataset <ref type="bibr" target="#b33">[34]</ref> is the only large-scale English one including a huge pre-processing step for cleaning and formatting information. The strength of this dataset is that it is composed of structured ingredients and instructions, images, and a large number of classes as well. Accordingly, we focus all our experimental evaluation on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross-modal Retrieval</head><p>Cross-modal retrieval aims at retrieving relevant items that are of different nature with respect to the query format; for example when querying an image dataset with keywords (image vs. text) <ref type="bibr" target="#b33">[34]</ref>. The main challenge is to measure the similarity between different modalities of data. In the Information Retrieval (IR) community, early work have circumvented this issue by annotating images to perceive their underlying semantics <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">37]</ref>. However, these approaches generally require a supervision from users to annotate at least a small sample of images. An unsupervised solution has emerged from the deep learning community which consists in mapping images and texts into a shared latent space F in which they can be compared <ref type="bibr" target="#b40">[41]</ref>. In order to align the text and image manifolds in F , the most popular strategies are based either on 1) global alignment methods aiming at mapping each modal manifold in F such that semantically similar regions share the same directions in F ; 2) local metric learning approaches aiming at mapping each modal manifold such that semantically similar items have a short distances in F .</p><p>In the first category of works dealing with global alignment methods, a well-known state-of-the-art model is provided by the Canonical Correlation Analysis (CCA) <ref type="bibr" target="#b17">[18]</ref> which aims at maximizing the correlation in F between relevant pairs from data of different modalities. CCA and its variations like Kernel-CCA <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29]</ref> and Deep-CCA <ref type="bibr" target="#b2">[3]</ref> have been successfully applied to align text and images <ref type="bibr" target="#b42">[43]</ref>. However, global alignment strategies such as CCA do not take into account dissimilar pairs, and thus tend to produce false positives in cross-modal retrieval tasks.</p><p>In the second category of work, local metric learning approaches consider cross-modal retrieval as a ranking problem where items are ranked according to their distance to the query in the latent space. A perfect retrieval corresponds to a set of inequalities in which the distances between the query and relevant items are smaller than the distances between the query and irrelevant items <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref>. Each modal projection is then learned so as to minimize a loss function that measures the cost of violating each of these inequalities <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref>. In particular, <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34]</ref> consider a loss function that minimizes the distance between pairs of matching cross-modal items while maximizing the distance between non-matching cross-modal pairs. However, ranking inequality constraints are more naturally expressed by considering triplets composed of a query, a relevant item, and an irrelevant item. This strategy is similar to the Large Margin Nearest Neighbor loss <ref type="bibr" target="#b39">[40]</ref> in which a penalty is computed only if the distance between the query and the relevant item is larger than the distance between the query and the irrelevant item.</p><p>Our contributions differ from previous work according to three main aspects. First, we propose to model the manifold-alignment issue, which is generally based only on the semantic information <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43]</ref>, as a joint learning framework leveraging retrieval and class-based features. In contrast to <ref type="bibr" target="#b33">[34]</ref> which adds an additional classification layer to a manifold-alignment neural model, we directly integrate semantic information in the loss to refine the structure of the latent space while also limiting the number of parameters to be learned. Second, our model relies on a double-triplet (instead of pairwise learning in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34]</ref> or a single triplet as in <ref type="bibr" target="#b39">[40]</ref>) to fit with the joint learning objectives. Third, we propose a new stochastic gradient descent weighting scheme adapted to such a dual deep embedding architecture, which is computed on minibatches and automatically performs an adaptive mining of informative triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ADAMINE DEEP LEARNING MODEL 3.1 Model Overview</head><p>The objective of our model AdaMine (ADAptive MINing Embeding) is to learn the representation of recipe items (texts and images) through a joint retrieval and classification learning framework based on a double-triplet learning scheme. More particularly, our model relies on the following hypotheses:</p><p>? H1: Aligning items according to a retrieval task allows capturing the fine-grained semantics of items, since the obtained embeddings must rank individual items with respect to each other.</p><p>? H2: Aligning items according to class meta-data allows capturing the high-level semantic information underlying items since it ensures the identification of item clusters that correspond to class-based meta-data.</p><p>? H3: Learning simultaneously retrieval and class-based features allows enforcing a multi-scale structure within the latent space, which covers all aspects of item semantics. In addition, we conjecture that adding a classification layer sequentially to manifold-alignment as in <ref type="bibr" target="#b33">[34]</ref> might be under-effective.</p><p>Based on these hypotheses, we propose to learn the latent space structure (and item embeddings) by integrating both retrieval objective and semantic information in a single cross-modal metric learning problem (see the Latent Space in <ref type="figure" target="#fig_0">Figure 2</ref>). We take inspiration from the learning-to-rank retrieval framework by building a learning schema based on query/relevant item/irrelevant item triplets noted (x q , x p , x n ). Following hypothesis H3, we propose a double-triplet learning scheme that relies on both instance-based and semanticbased triplets, noted respectively (x q , x p , x n ) and (x ? q , x ? p , x ? n ), in order to satisfy the multi-level structure (fine-grained and high-level) underlying semantics. More particularly, we learn item embeddings by minimizing the following objective function:</p><formula xml:id="formula_0">L tot al (? ) = L ins (? ) + ?L sem (? )<label>(1)</label></formula><p>where ? is the network parameter set. L ins is the loss associated with the retrieval task over instance-based triplets (x q , x p , x n ), and L sem is the loss coming with the semantic information over semanticbased triplets (x ? q , x ? p , x ? n ). Unlike <ref type="bibr" target="#b33">[34]</ref> that expresses this second term L sem acting as a regularization over the L ins optimization, in our framework, it is expressed as a joint classification task. This double-triplet learning framework is a difficult learning problem since the trade-off between L ins and L sem is not only influenced by ? but also by the sampling of instance-based and semantic-based triplets and depends on their natural distribution. Furthermore, the sampling of violating triplets can be difficult as the training progresses which usually leads to vanishing gradient problems that are common in triplet-based losses, and are amplified by our double-triplet framework. To alleviate these problems, we propose an adaptive sampling strategy that normalizes each loss allowing to fully control the trade-off with ? alone while also ensuring non-vanishing gradients throughout the learning process.</p><p>In the following, we present the network architecture, each component of our learning framework, and then discuss the learning scheme of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-modal Learning Framework</head><p>3.2.1 Network Architecture. Our network architecture is based on the proposal of <ref type="bibr" target="#b33">[34]</ref>, which consists of two branches based on deep neural networks that map each modality (image or text recipe) into a common representation space, where they can be compared. Our global architecture is depicted in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>The image branch (top-right part of <ref type="figure" target="#fig_0">Figure 2</ref>) is composed of a ResNet-50 model <ref type="bibr" target="#b15">[16]</ref>. It contains 50 convolutional layers, totaling more than 25 million parameters. This architecture is further detailed in <ref type="bibr" target="#b15">[16]</ref>, and was chosen in order to obtain comparable results to <ref type="bibr" target="#b33">[34]</ref> by sharing a similar setup. The ResNet-50 is pretrained on the large-scale dataset of the ImageNet Large Scale Visual Recognition Challenge <ref type="bibr" target="#b32">[33]</ref>, containing 1.2 million images, and is fine-tuned with the whole architecture. This neural network is followed by a fully connected layer, which maps the outputs of the ResNet-50 into the latent space, and is trained from scratch.</p><p>In the recipe branch (top-left part of <ref type="figure" target="#fig_0">Figure 2</ref>), ingredients and instructions are first embedded separately, and their obtained representations are then concatenated as input of a fully connected layer that maps the recipe features into the latent space. For ingredients, we use a bidirectional LSTM <ref type="bibr" target="#b16">[17]</ref> on pretrained embeddings obtained with the word2vec algorithm <ref type="bibr" target="#b31">[32]</ref>. With the objective to consider the different granularity levels of the instruction text, we use a hierarchical LSTM in which the word-level is pretrained using the skip-thought technique <ref type="bibr" target="#b25">[26]</ref> and is not fine-tuned while the sentence-level is learned from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Retrieval loss.</head><p>The objective of the retrieval loss L ins is to learn item embeddings by constraining the latent space according to the following assumptions (Hypothesis H1): 1) ranking items according to a similarity metric so as to gather matching items together and 2) discriminating irrelevant ones. We propose to use a loss function ? ins based on a particular triplet (x q , x p , x n ) consisting of a query x q , its matching counterpart in the other modality x p and a dissimilar item x n . The retrieval loss function L ins is the aggregation of the individual loss ? ins over all triplets. The aim of ? ins is to provide a fine-grained structure to the latent space where the nearest item from the other modality with respect to the query is optimized to be its matching pair. More formally, the individual retrieval loss ? ins (?, x q , x p , x n ) is formalized as follows:</p><formula xml:id="formula_1">? ins (?, x q , x p , x n ) = d(x q , x p ) + ? ? d(x q , x n ) + (2)</formula><p>where d(x, y) expresses the cosine distance between vectors x and y in the latent space F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Semantic loss.</head><p>L sem is acting as a regularizer capable of taking advantage of semantic information in the multi-modal alignment, without adding extra parameters to the architecture nor graph dependencies. To leverage class information (Hypotheses H2), we propose to construct triplets that optimize a surrogate of the k-nearest neighbor classification task. Ideally, for a given query x q , and its corresponding class c(x q ), we want its associated closest sample x ?,q in the feature space to respect c(x q ) = c(x ?,q ). This enforces a semantic structure on the latent space by making sure that related dishes are closer to each other than to non-related ones. To achieve this, we propose the individual triplet loss ? sem :</p><formula xml:id="formula_2">? sem (?, x ? q , x ? p , x ? n ) = d(x ? q , x ? p ) + ? ? d(x ? q , x ? n ) +<label>(3)</label></formula><p>where x ? p belongs to the set of items with the same semantic class c(x ? q ) as the query, and x ? n belongs to the set of items with different semantic classes than the one of the query.</p><p>Contrary to the classification machinery adopted by <ref type="bibr" target="#b33">[34]</ref>, ? sem optimizes semantic relations directly in the latent space without changing the architecture of the neural network, as shown on <ref type="figure">Figure</ref> 1. This promotes a smoothing effect on the space by encouraging instances of the same class to stay closer to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptive Learning Schema</head><p>As commonly used in Deep Learning, we use the stochastic gradient descent (SGD) algorithm which approximates the true gradient over mini-batches. The update term is generally computed by aggregation of the gradient using the average over all triplets in the mini-batch. However, this average strategy tends to produce a vanishing update with triplet losses. This is especially true towards the end of the learning phase, as the few active constraints are averaged with many zeros coming from the many inactive constraints. We believe this problems is amplified as the size of the training set grows. To tackle this issue, our proposed adaptive strategy considers an update term ? adm that takes into account informative triplets only (i.e., non-zero loss). More formally, given a mini-batch B, P r q the set of matching items with respect to a query x q and P s q the set of items with the same class as x q , the update term ? adm is defined by:</p><formula xml:id="formula_3">? adm = x q ?B x p ?B?P r q x n ?B\P r q ?? ins (?, x q , x p , x n ) ? ? r (4) + x p ?B?P s q x n ?B\P s q ? ?? sem (?, x q , x p , x n ) ? ? s</formula><p>with ? ? r and ? ? s being the number of triplets contributing to the cost:</p><formula xml:id="formula_4">? ? r = x q ?B x p ?B?P r q x n ?B\P r q 1 ? ins 0 ? ? s = x q ?B x p ?B?P s q x n ?B\P s q 1 ? s em 0<label>(5)</label></formula><p>At the very beginning of the optimization, all triplets contribute to the cost and, as constraints stop being violated, they are dropped. At the end of the training phase, most of the triplets will have no contribution, leaving the hardest negatives to be optimized without vanishing gradient issues. Remark that this corresponds to a curriculum learning starting with the average strategy and ending with the hard negative strategy like in <ref type="bibr" target="#b35">[36]</ref>, but without the burden of finding the time-step at which to switch between strategies as this is automatically controlled by the weights ? r and ? s .</p><p>Remark also that an added benefit of ? adm is due to the independent normalization of each loss by its number of active triplets. Thus ? adm keeps the trade-off between ? ins and ? sem unaffected by difference between the number of active triplets in each loss and allows ? to be the only effective control parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION PROTOCOL</head><p>The objective of our evaluation is threefold: 1) Analyzing the impact of our semantic loss that directly integrates semantic information in the latent space; 2) Testing the effectiveness of our model; 3) Exploring the potential of our model and its learned latent space for solving smart cooking tasks. All of our experiments are conducted using PyTorch 2 , with our own implementation 3 of the experimental setup (i.e. preprocessing, architecture and evaluation procedures) described by <ref type="bibr" target="#b33">[34]</ref>. We detail the experimental setup in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We use the Recipe1M dataset <ref type="bibr" target="#b33">[34]</ref>, the only large-scale dataset including both English cooking recipes (ingredients and instructions), images, and categories. The raw Recipe1M dataset consists of about 1 million image and recipe pairs. It is currently the largest one in English, including twice as many recipes as <ref type="bibr" target="#b27">[28]</ref> and eight times as many images as <ref type="bibr" target="#b6">[7]</ref>. Furthermore, the availability of semantic information makes it particularly suited to validate our model: around half of the pairs are associated with a class, among 1048 classes parsed from the recipe titles. Using the same preprocessed pairs of recipe-image provided by <ref type="bibr" target="#b33">[34]</ref>, we end up with 238,399 matching pairs of images and recipes for the training set, while the validation and test sets have 51,119 and 51,303 matching pairs, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Methodology</head><p>We carry out a cross-modal retrieval task following the process described in <ref type="bibr" target="#b33">[34]</ref>. Specifically, we first sample 10 unique subsets of 1,000 (1k setup) or 5 unique subsets of 10,000 (10k setup) matching text recipe-image pairs in the test set. Then, we consider each item in a modality as a query (for instance, an image), and we rank items in the other modality (resp. text recipes) according to the cosine distance between the query embedding and the candidate embeddings. The objective is to retrieve the associated item in the other modality at the first rank. The retrieved lists are evaluated using standard metrics in cross-modal retrieval tasks. For each subset (1k and 10k), we estimate the median retrieval rank (MedR), as well as the recall percentage at top K (R@K), over all queries in a modality. The R@K corresponds to the percentage of queries for which the matching item is ranked among the top K closest results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>To test the effectiveness of our model AdaMine, we evaluate our multi-modal embeddings with respect to those obtained by state-ofthe-art (SOTA) baselines:</p><p>? CCA, which denotes the Canonical Correlation Analysis method <ref type="bibr" target="#b17">[18]</ref>. This baseline allows testing the effectiveness of global alignment methods.</p><p>? PWC, the pairwise loss with the classification layer from <ref type="bibr" target="#b33">[34]</ref>. We report their state-of-the-art results for the 1k and 10k setups when available. This baseline exploits the classification task as a regularization of embedding learning.</p><p>? PWC*, our implementation of the architecture and loss described by <ref type="bibr" target="#b33">[34]</ref>. The goal of this baseline is to assess the results of its improved version PWC++, described below.</p><p>? PWC++, the improved version of our implementation PWC*. More particularly, we add a positive margin to the pairwise loss adopted in <ref type="bibr" target="#b33">[34]</ref>, as proposed by <ref type="bibr" target="#b18">[19]</ref>:</p><formula xml:id="formula_5">? pw ++ (?, x q , x) = y d(x q , x) ? ? pos + + (1 ? y) ? ne? ? d(x q , x) +<label>(6)</label></formula><p>with y = 1 (resp. y = 0) for pos. (resp. neg.) pairs. The positive margin ? pos allows matching pairs to have different representations, thus reducing the risk of overfitting. In practice, the positive margin is set to 0.3 and the negative margin to 0.9.</p><p>We evaluate the effectiveness of our model AdaMine, which includes both the triplet loss and the adaptive learning, in different setups, and having the following objectives:</p><p>? Evaluating the impact of the retrieval loss: we run the AdaMine_ins scenario which refers to our model with the instance loss L ins only and the adaptive learning strategy (the semantic loss L sem is discarded);</p><p>? Evaluating the impact of the semantic loss: we run the AdaMine_sem scenario which refers to our model with the semantic loss L sem only and the adaptive learning strategy (the instance loss L ins is discarded);</p><p>? Evaluating the impact of the strategy used to tackle semantic information: we run the AdaMine_ins+cls scenario which refers to our AdaMine model by replacing the semantic loss by the classification head proposed by <ref type="bibr" target="#b33">[34]</ref>;</p><p>? Measuring the impact of our adaptive learning strategy: we run the AdaMine_avg. The architecture and the losses are identical to our proposal, but instead of using the adaptive learning strategy, this one performs the stochastic gradient descent averaging the gradient over all triplets, as is common practice in the literature;</p><p>? Evaluating the impact of the text structure: we run our whole model (retrieval and semantic losses + adaptive SGD) by considering either ingredients only (noted AdaMine_ingr) or instructions only (noted AdaMine_instr).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation details</head><p>Network learning. As adopted by <ref type="bibr" target="#b33">[34]</ref>, we use the Adam <ref type="bibr" target="#b23">[24]</ref> optimizer with a learning rate of 10 ?4 . Besides, we propose a simpler training scheme: At the beginning of the training phase, we freeze the ResNet-50 weights, optimizing only the text-processing branch, as well as the weights of the mapping of the visual processing branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scenarios</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategies</head><p>Image  <ref type="table">Table 1</ref>: Impact of the semantic information. MedR means Median Rank (lower is better). R@K means Recall at K (between 0% and 100%, higher is better). The average value over 5 bags of 10,000 pairs each is reported.</p><p>After 20 epochs, the weights of the ResNet-50 are unfrozen and the whole architecture is fine-tuned for 60 more epochs. For the final model selection, we evaluate the MedR on the validation set at the end of each training epoch, and we keep the model with the best MedR on validation.</p><p>It is worth mentioning that in order to learn our model, a single NVidia Titan X Pascal is used, and the training phase lasts for 30 hours. We also improved the efficiency of the PWC baseline, initially implemented in Torch and requiring 3 days of learning using four NVidia Titan X Pascal to 30 hours on a single NVidia Titan X Pascal. We will release codes for both our model and the PWC* model.</p><p>Parameter choices. Our model AdaMine is a combination of the adaptive bidirectional instance and semantic triplet losses. Its margin ? and the weight ? for the semantic cost L sem are determined using a cross-validation with values varying between 0.1 and 1, and step of 0.1. We finally retained 0.3 for both ? and ?. The parameter ? further analyzed in Section 5.1 and in <ref type="figure">Figure 4</ref>.</p><p>Triplet sampling. As is common with triplet based losses in deep learning, we adopt a per-batch sampling strategy for estimating L ins and L sem (see subsection 3.3). The set of multi-modal (imagerecipe) matching pairs in the train (resp. validation) set are split in 2383 (resp. 513) mini-batches of 100 pairs. Following the dataset structure in which half of the pairs are not labeled by class meta-data, those 100 pairs are split into: 1) 50 randomly selected pairs among those not associated with class information; 2) 50 labeled pairs for which we respect the distribution over all classes in the training set (resp. validation set).</p><p>Within each mini-batch, we then build the set of double-triplets fitting with our joint retrieval and semantic loss functions. Each item in the 100 pairs is iteratively seen as the query. The main issue is to build positive and negative sets with respect to this query. For the retrieval losses, the item in the other modality associated to the query is assigned to the positive set while the remaining items in the other modality (namely, 99 items) are assigned to the negative instance set. For the semantic loss, we randomly select, as the positive set, one item in the other modality that does not belong to the matching pair while sharing the query class. For the negative set, we consider the remaining items in the other modality that do not belong to the query class. For fair comparison between queries over the mini-batch, we limit the size of the negative sets over each query to the smallest negative ensemble size inside the batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Analysis of the semantic contribution</head><p>We analyze our main hypotheses related to the importance of semantic information for learning multi-modal embeddings (see Hypothesis H2 in 3.1). Specifically, in this part we test whether semantic information can help to better structure the latent space, taking into account class information and imposing structural coherence. Compared with <ref type="bibr" target="#b33">[34]</ref> which adds an additional classification layer, we believe that directly injecting this semantic information with a global loss L(? ) (Equation 1) comes as a more natural approach to integrating class-based meta-data (see Hypothesis H3 in 3.1).</p><p>To test this intuition, we start by quantifying, in <ref type="table">Table 1</ref>, the impacts of the semantic information in the learning process. To do so, we evaluate the effectiveness of different scenarios of our model AdaMine with respect to the multi-modal retrieval task (image-totext and text-to-image) in terms of MedR and Recall at ranks 1, 5, and 10. Compared with a retrieval loss alone (AdaMine_ins), we point out that adding semantic information with a classification cost AdaMine_ins+cls or a semantic loss AdaMine improves the results. When evaluating with 10,000 pairs (10k setting), while AdaMine_ins obtains MedRs 15.4 and 15.8, the semantic models (AdaMine_ins+cls and AdaMine) lower these values to <ref type="bibr" target="#b13">14</ref>  <ref type="table">Table 2</ref>: Recipe-to-images visualization. For each recipe, we have the top row, indicating the top 5 images retrieved by our AdaMine model for a given recipe query, and the bottom row, indicating the top 5 images by the triplet loss for the same recipe. In green, the matching image. In blue, images belonging to the same class than the recipe. In red, images belonging to a different class. AM indicates AdaMine, and AM_ins AdaMine_ins.</p><p>15.2, and 13.2 and 12.2, respectively (lower is better) for both retrieval tasks (image-to-text and text-to-image).</p><p>The importance of semantic information becomes clearer when we directly compare the impact of adding the semantic loss to the base model (AdaMine vs AdaMine_ins), since the former obtains the best results for every metric. To better understand this phenomenon, we depict in <ref type="figure">Figure 3</ref> item embeddings obtained by the AdaMine_ins and AdaMine models using a t-SNE visualization. This figure is generated by selecting 400 matching recipe-image pairs (800 data points), which are randomly selected from, and equally distributed among 5 of the most occurring classes of the Recipe1M dataset. Each item is colored according to its category (e.g., blue points for the cupcake class), and items of the same instance are connected with a trace. Therefore, <ref type="figure">Figure 3</ref> allows drawing two conclusions: 1) our model-on the right side of the figure-is able to structure the latent space while keeping items of the same class close to each other (see color clusters); 2) our model reduces the sum of distances between pairs of instances (in the figure, connected with traces), thus reducing the MedR and increasing the recall. We also illustrate this comparison through qualitative examples. In <ref type="table">Table 2</ref>, AdaMine (top row) and AdaMine_ins (bottom row) are compared on four queries, for which both models are able to rank the correct match in the top-5 among 10,000 candidates. For the first and second queries (cucumber salad and roasted chicken, respectively), both models are able to retrieve the matching image in the first position. However, the rest of the top images retrieved by our model are semantically related to the query, by sharing critical ingredients (cucumber, chicken) of the recipe. In the third and fourth queries (pizza and chocolate chip, respectively), our model is able to rank both the matching image and semantically connected samples in a more coherent way, due to a better alignment of the retrieval space produced by the semantic modeling. These results reinforce our intuition that it is necessary to integrate semantic information in addition to item pairwise anchors while learning multi-modal embeddings.</p><p>Second, we evaluate our intuition that classification is undereffective for integrating the semantics within the latent space (see Hypothesis H3 in 3.1). <ref type="table">Table 1</ref> shows that our semantic loss AdaMine, proposed in subsubsection 3.2.3, outperforms our model scenario AdaMine_ins+cls which relies on a classification head as proposed in <ref type="bibr" target="#b33">[34]</ref>. For instance, we obtain an improvement of +9.57% in terms of R@1 with respect to the classification loss setting AdaMine_ins+cls. This result suggests that our semantic loss is more appropriate to organize the latent space so as to retrieve textimage matching pairs. It becomes important, then, to understand the impacts of the weighting factor ? between the two losses L ins and L sem (Equation 1). In <ref type="figure">Figure 4</ref>, we observe a fair level of robustness for lower values of ?, but any value over 0.5 has a hindering effect on the retrieval task, since the semantic grouping starts to be of considerable importance. These experiments confirm the importance of additional semantic clues: despite having one million less parameters than <ref type="bibr" target="#b33">[34]</ref>'s proposal, our approach still achieves better scores, when compared to the addition of the classification head.   <ref type="table">Table 4</ref>: Ingredient-to-Image task. Examples of images in the top 20 results when searching for an ingredient within the class Pizza.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Testing the effectiveness of the model</head><p>In the following, we evaluate the effectiveness of our model, compared to different baseline models. Results are presented in <ref type="table" target="#tab_3">Table 3</ref> for both image-to-text and text-to-image retrieval tasks. We report results on the 1K setup and test the robustness of our model on the 10k setup by reporting only the best state-of-the-art (SOTA) baseline for comparison. From a general point of view, we observe that our model AdaMine overpasses the different baselines and model scenarios. Small values of standard deviation outlines the low variability of experimented models, and accordingly the robustness of obtained results. For instance, our model reaches a value equal to 1 for the Median Rank metric (MedR) for the 1k setting and both retrieval tasks while the well-known SOTA models CCA and PWC++ obtain respectively 15.7 and 3.3. Contrary to PWC, all of our model scenarios, denoted AdaMine_ * , adopt the triplet loss. Ablation tests on our proposals show their effectiveness. This trend is noticed over all retrieval tasks and all metrics. The comparison of the results obtained over 1k and 10k settings outlines the same statement with larger improvements (with similar standard deviation) for our model AdaMine with respect to SOTA models and AdaMine-based scenarios. More particularly, we first begin our discussion with the comparison with respect to SOTA models and outline the following statements:</p><p>? Global alignment models (baseline CCA) are less effective than advanced models (PWC, PWC++, and AdaMine). Indeed, the CCA model obtains a MedR value of 15.7 for the image-to-text retrieval task (1k setting) while the metric range of advanced models  is between 1 and 5.2. This suggests the effectiveness of taking into account dissimilar pairs during the learning process.</p><p>? We observe that our triplet based model AdaMine consistently outperforms pairwise methods (PWC and PWC++). For instance, our model obtains a significant decrease of ?61.84% in terms of MedR with respect to PWC++ for the 10k setting and the image-totext retrieval task. This suggests that relative cosine distances are better at structuring the latent space than absolute cosine distances.</p><p>? Our model AdaMine surpasses the current state-of-the-art results by a large margin. For the 1k setup, it reduces the medR score by a factor of 5-from 5.2 and 5.1 to 1.0 and 1.0-, and by a factor bigger than 3 for the 10k setup. One strength of our model is that it has fewer parameters than PWC++ and PWC, since the feature space is directly optimized with a semantic loss, without the addition a parameter-heavy head to the model. Second, the comparison according to different versions of our model outlines three main statements:</p><p>? The analysis of AdaMine_ins, AdaMine_ins+cls, and AdaMine corroborates the results observed in Section 5.1 dealing with the impact of the semantic loss on the performance of the model. In the 1k setting, the instance-based approach (AdaMine_ins) achieves a MedRs value equal of 1.5 and 1.6 for both tasks (lower is better), while the addition of a classification head (AdaMine_ins+cls), proposed by <ref type="bibr" target="#b33">[34]</ref>, improves these results to 1.1 and 1.2. Removing the classification head and adding a semantic loss (AdaMine) further improves the results to 1 for both retrieval tasks which further validates Hypothesis H3 in 3.1.</p><p>? The adaptive sampling strategy described in subsection 3.3 strongly contributes to the good results of AdaMine. With AdaMine_avg, we test the same setup of AdaMine, replacing the adaptive strategy with the average one. The importance of removing triplets that are not contributing to the loss becomes evident when the scores for both strategies are compared: 24.6 and 24.0 of MedR (lower is better) for AdaMine_avg, and 13.2 and 12.2 for AdaMine, an improvement of roughly 46.34% and 49.17%.</p><p>? AdaMine combines the information coming from the image and all the parts of the recipe (instructions and ingredients), attaining high scores. When compared to the degraded models AdaMine_ingr and AdaMine_instr, we conclude that both textual information are complementary and necessary for correctly identifying the recipe of a plate. While AdaMine achieves MedRs of 13.2 and 12.2 (lower is better), the scenarios without instructions or without ingredients achieve 52.8 and 53.8, and 39.0 and 39.2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative studies on downstream tasks</head><p>In what follows, we discuss the potential of our model for promising cooking-related application tasks. We particularly focus on downstream tasks in which the current setting might be applied. We provide illustrative examples issued from the testing set of our evaluation process. For better readability, we always show the results as images, even for text recipes for which we display their corresponding original picture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ingredient To</head><p>Image. An interesting ability of our model is to map ingredients into the latent space. One example of task is to retrieve recipes containing specific ingredients that could be visually identified. This is particularly useful when one would like to know what they can cook using aliments available in their fridge. To demonstrate this process, we create each recipe query as follows: 1) for the ingredients part, we use a single word which corresponds to the ingredient we want to retrieve; 2) for the instructions part, we use the average of the instruction embeddings over all the training set. Then, we project our query into the multi-modal space and retrieve the nearest neighbors among 10,000 images randomly picked from the testing set. We show on <ref type="table">Table 4</ref> examples of retrieved images when searching for different ingredients while constraining the results to the class pizza. Searching for pineapple or olives results in different types of pizzas. An interesting remark is that searching for strawberries inside the class pizza yields images of fruit pizza containing strawberries, i.e., images that are visually similar to pizzas while containing the required ingredient. This shows the fine-grain structure of the latent space in which recipes and images are organized by visual or semantic similarity inside the different classes.</p><p>Removing ingredients. The capacity of finely model the presence or absence of specific ingredients may be interesting for generating menus, specially for users with dietary restrictions (for instance, peanut or lactose intolerance, or vegetarians and vegans). To do so, we randomly select a recipe having broccoli in its ingredients list ( <ref type="table" target="#tab_5">Table 5</ref>, first column) and retrieve the top 4 closest images in the embedding space from 1000 recipe images ( <ref type="table" target="#tab_5">Table 5</ref>, top row). Then we remove the broccoli in the ingredients and remove the instructions having the broccoli word. Finally, we retrieve once again the top 4 images associated to this "modified" recipe ( <ref type="table" target="#tab_5">Table 5</ref>, bottom row). The retrieved images using the original recipe have broccoli, whereas the retrieved images using the modified recipe do not have broccoli. This reinforces our previous statement, highlighting the ability of our latent space to correctly discriminate items with respect to ingredients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we introduce the AdaMine approach for learning crossmodal embeddings in the context of a large-scale cooking oriented retrieval task (image to recipe, and vice versa). Our main contribution relies on a joint retrieval and classification learning framework in which semantic information is directly injected in the cross-modal metric learning. This allows refining the multi-modal latent space by limiting the number of parameters to be learned. For learning our double-triplet learning scheme, we propose an adaptive strategy for informative triplet mining. AdaMine is evaluated on the very large scale and challenging Recipe1M crossmodal dataset, outperforming the state-of-the-art models. We also outline the benefit of incorporating semantic information and show the quality of the learned latent space with respect to downstream tasks. We are convinced that such very large scale multimodal deep embeddings frameworks offer new opportunities to explore joint combinations of Vision and Language understanding. Indeed, we plan in future work to extend our model by considering hierarchical levels within object semantics to better refine the structure of the latent space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>AdaMine overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2 http://pytorch.org 3 https://github.com/Cadene/recipe1m.bootstrap.pytorch</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>t-SNE visualization. Image (resp. Recipe) points are denoted with the + (resp. ?) symbol. Matching pairs are connected with a trace. Blue points are associated to the cupcake class, orange to hamburger, pink to green beans, green to pork chops, and red to pizza. MedR scores for different values of the ? hyper-parameter, responsible for weighting the semantic regularization cost L s em of AdaMine, calculated over 5 bags of 10.000 validation samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>State-of-the-art comparison. MedR means Median Rank (lower is better). R@K means Recall at K (between 0% and 100%, higher is better). The mean and std values over 10 (resp. 5) bags of 1k (resp. 10k) pairs each are reported for the top (resp. bottom) table. Items marked with a star (*) are our reimplementation of the cited methods.</figDesc><table><row><cell>Mushrooms</cell><cell>Pineapple</cell><cell>Olives</cell><cell>Pepperoni</cell><cell>Strawberries</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Removing ingredient task. Top 4 retrieved images with (top row) and without (bottom row) broccoli in the ingredient and instruction lists w.r.t the original recipe (Tofu Saut?).</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">Proceedings of the 9th Workshop on Multimedia for Cooking and Eating Activities in Conjunction with The 2017 International Joint Conference on Artificial Intelligence</title>
		<meeting>the 9th Workshop on Multimedia for Cooking and Eating Activities in Conjunction with The 2017 International Joint Conference on Artificial Intelligence</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vinicius Monteiro de Lira, Cristina Ioana Muntean, Raffaele Perego, and Chiara Renso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Bolettieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1333" to="1336" />
		</imprint>
	</monogr>
	<note>Social Media Image Recognition for Food Trend Analysis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1247" to="1255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kernel independent component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2002-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Menu-Match: Restaurant-Specific Food Logging from Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saponas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khullar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="844" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Food-101 -Mining Discriminative Components with Random Forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep-based ingredient recognition for cooking recipe retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep-based Ingredient Recognition for Cooking Recipe Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MultiMedia Modeling</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cross-Modal Recipe Retrieval: How to Cook this Dish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MultiMedia Modeling</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="588" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">PFID: Pittsburgh fast-food image dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="289" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploiting Food Choice Biases for Healthier Recipe Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Elsweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Trattner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Harvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A Benchmark Dataset to Study the Representation of Food Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Allegra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Stanco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="584" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction by Learning an Invariant Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVP</title>
		<imprint>
			<biblScope unit="page" from="1735" to="1742" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cookpad Image Dataset: An Image Collection As Infrastructure for Food Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Harashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichiro</forename><surname>Someya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohei</forename><surname>Kikuta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1229" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Distributional structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zellig</forename><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep Residual Learning for Image Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Relations between two sets of variates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="377" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discriminative Deep Metric Learning for Face Verification in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1875" to="1882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic Image Annotation and Retrieval Using Cross-media Relevance Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Food image recognition with deep convolutional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyuki</forename><surname>Kawano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UbiComp &apos;14</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="589" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">FoodCam: A Real-Time Mobile Food Recognition System Employing Fisher Vector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyuki</forename><surname>Kawano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MMM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="369" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unifying visualsemantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<title level="m">Skip-Thought Vectors. In NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Online Food Recipe Title Semantics: Combining Nutrient Facts and Topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Kusmierczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kjetil</forename><surname>N?rv?g</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2013" to="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding and predicting online food recipe production patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Kusmierczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Trattner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kjetil</forename><surname>N?rv?g</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="243" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kernel and nonlinear canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><forename type="middle">Ling</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lai</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Fyfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="365" to="377" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Quadruplet-wise image similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Combining Language and Vision with a Multimodal Skip-gram Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="153" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning Cross-modal Embeddings for Cooking Recipes and Food Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Hynes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferda</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recipe Popularity Prediction with Deep Visual-Semantic Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sanjo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Katsurai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2279" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tag-based Social Image Retrieval: An Empirical Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sourav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khanh</forename><surname>Bhowmick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Tran Nam Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Soc. Inf. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="2364" to="2381" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Investigating the Healthiness of Internet-Sourced Recipes: Implications for Meal Planning and Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Trattner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Elsweiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="489" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Recipe recognition with large multimodal food dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Precioso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distance Metric Learning for Large Margin Nearest Neighbor Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint Latent Subspace Learning and Regression for Cross-Modal Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="917" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distance Metric Learning with Application to Clustering with Side-Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep correlation for matching images and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3441" to="3450" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
