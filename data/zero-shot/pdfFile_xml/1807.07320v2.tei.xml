<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attend and Rectify: a Gated Attention Mechanism for Fine-Grained Recovery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pau</forename><surname>Rodr?guez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><forename type="middle">M</forename><surname>Gonfaus</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Parc de Recerca</orgName>
								<orgName type="institution">Visual Tagging Services</orgName>
								<address>
									<addrLine>Campus UAB</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Xavier</forename><surname>Roca</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Gonz?lez</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Center</orgName>
								<orgName type="institution">Universitat Aut?noma de Barcelona (UAB)</orgName>
								<address>
									<addrLine>Campus UAB</addrLine>
									<postCode>08193</postCode>
									<settlement>Bellaterra</settlement>
									<region>Catalonia Spain</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attend and Rectify: a Gated Attention Mechanism for Fine-Grained Recovery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Convolutional Neural Networks</term>
					<term>Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. It learns to attend to lower-level feature activations without requiring part annotations and uses these activations to update and rectify the output likelihood distribution. In contrast to other approaches, the proposed mechanism is modular, architecture-independent and efficient both in terms of parameters and computation required. Experiments show that networks augmented with our approach systematically improve their classification accuracy and become more robust to clutter. As a result, Wide Residual Networks augmented with our proposal surpasses the state of the art classification accuracies in CIFAR-10, the Adience gender recognition task, Stanford dogs, and UEC Food-100.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Humans and animals process vasts amounts of information with limited computational resources thanks to attention mechanisms which allow them to focus resources on the most informative chunks of information <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> This work is inspired by the advantages of visual and biological attention mechanisms, for tackling fine-grained visual recognition with Convolutional Neural Networks (CNN) <ref type="bibr" target="#b3">[4]</ref>. This is a particularly difficult task since it involves looking for details in large amounts of data (images) while remaining robust to deformation and clutter. In this sense, different attention mechanisms for fine-grained recognition exist in the literature: (i) iterative methods that process images using "glimpses" with recurrent neural networks (RNN) or long short-term memory (LSTM) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, (ii) feed-forward attention mechanisms that augment vanilla CNNs, such as the Spatial Transformer Networks (STN) <ref type="bibr" target="#b6">[7]</ref>, or top-down feed-forward attention mechanisms (FAM) <ref type="bibr" target="#b7">[8]</ref>. Although it is not applied to fine-grained recognition, the Residual Attention introduced by <ref type="bibr" target="#b8">[9]</ref> is another example of feed-forward attention mechanism that takes advantage of residual connections <ref type="bibr" target="#b9">[10]</ref> to enhance or dampen certain regions of the feature maps in an incremental manner.  <ref type="figure">Fig. 1</ref>: The proposed mechanism. The original CNN is augmented with N attention modules at N different depths. Each attention module applies K attention heads to the network feature maps to make a class prediction based on local information. The original network output net is then corrected based on the local features by means of the global attention gates, resulting in the final output.</p><p>Thus, most of the existing attention mechanisms are either limited by having to perform multiple passes through the data <ref type="bibr" target="#b4">[5]</ref>, by carefully designed architectures that should be trained from scratch <ref type="bibr" target="#b6">[7]</ref>, or by considerably increasing the needed amount of memory and computation, thus introducing computational bottlenecks <ref type="bibr" target="#b10">[11]</ref>. Hence, there is still the need of models with the following learning properties: (i) Detect and process in detail the most informative parts of an image for learning models more robust to deformation and clutter <ref type="bibr" target="#b11">[12]</ref>; (ii) feedforward trainable with SGD for achieving faster inference than iterative models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, together with faster convergence rate than Reinforcement Learning-based (RL) methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref>; (iii) preserve low-level detail for a direct access to local low-level features before they are modified by residual identity mappings. This is important for fine-grained recognition, where low-level patterns such as textures can help to distinguish two similar classes. This is not fulfilled by Residual Attention, where low-level features are subject to noise after traversing multiple residual connections <ref type="bibr" target="#b8">[9]</ref>.</p><p>In addition, desirable properties for attention mechanisms applied to CNNs would be: (i) Modular and incremental, since the same structure can be applied at each layer on any convolutional architecture, and it is easy to adapt to the task at hand; (ii) Architecture independent, that is, being able to adapt any pre-trained architecture such as VGG <ref type="bibr" target="#b13">[14]</ref> or ResNet <ref type="bibr" target="#b9">[10]</ref>; (iii) Low computational impact implying that it does not result in a significant increase in memory and computation; and (iv) Simple in the sense that it can be implemented in few lines of code, making it appealing to be used in future work.</p><p>Based on all these properties, we propose a novel attention mechanism that learns to attend low-level features from a standard CNN architecture through a set of replicable Attention Modules and gating mechanisms (see Section 3). Concretely, as it can be seen in <ref type="figure">Figure 1</ref>, any existing architecture can be augmented by applying the proposed model at different depths, and replacing the original loss by the proposed one. It is remarkable that the modules are inde-pendent of the original path of the network, so in practice, it can be computed in parallel to the rest of the network. The proposed attention mechanism has been included in a strong baseline like Wide Residual Networks (WRN) <ref type="bibr" target="#b14">[15]</ref>, and applied on CIFAR-10, CIFAR-100 <ref type="bibr" target="#b15">[16]</ref>, and five challenging fine-grained recognition datasets. The resulting network, called Wide Attentional Residual Network (WARN) systematically enhances the performance of WRNs and surpasses the state of the art in various classification benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are different approaches to fine-grained recognition <ref type="bibr" target="#b16">[17]</ref>: (i) vanilla deep CNNs, (ii) CNNs as feature extractors for localizing parts and do alignment, (iii) ensembles, (iv) attention mechanisms. In this work, we focus on (iv), the attention mechanisms, which aim to discover the most discriminative parts of an image to be processed in greater detail, thus ignoring clutter and focusing on the most distinctive traits. These parts are central for fine-grained recognition, where the inter-class variance is small and the intra-class variance is high.</p><p>Different fine-grained attention mechanisms can be found in the literature. <ref type="bibr" target="#b17">[18]</ref> proposed a two-level attention mechanism for fine-grained classification on different subsets of the ILSVRC <ref type="bibr" target="#b18">[19]</ref> dataset, and the CUB200 2011. In this model, images are first processed by a bottom-up object proposal network based on R-CNN <ref type="bibr" target="#b19">[20]</ref> and selective search <ref type="bibr" target="#b20">[21]</ref>. Then, the softmax scores of another ILSVRC2012 pre-trained CNN, which they call FilterNet, are thresholded to prune the patches with the lowest parent class score. These patches are then classified to fine-grained categories with a DomainNet. Spectral clustering is also used on the DomainNet filters in order to extract parts (head, neck, body, etc.), which are classified with an SVM. Finally, the part-and object-based classifier scores are merged to get the final prediction. The two-level attention obtained state of the art results on CUB200-2011 with only class-level supervision. However, the pipeline must be carefully fine-tuned since many stages are involved with many hyper-parameters.</p><p>Differently from two-level attention, which consists of independent processing and it is not end-to-end, Sermanet et al. proposed to use a deep CNN and a Recurrent Neural Network (RNN) to accumulate high multi-resolution "glimpses" of an image to make a final prediction <ref type="bibr" target="#b4">[5]</ref>. However, reinforcement learning slows down convergence and the RNN adds extra computation steps and parameters.</p><p>A more efficient approach was presented by Liu et al. <ref type="bibr" target="#b12">[13]</ref>, where a fullyconvolutional network is trained with reinforcement learning to generate confidence maps on the image and use them to extract the parts for the final classifiers whose scores are averaged. Compared to previous approaches, in the work done by <ref type="bibr" target="#b12">[13]</ref>, multiple image regions are proposed in a single timestep thus, speeding up the computation. A greedy reward strategy is also proposed in order to increase the training speed. The recent approach presented by <ref type="bibr" target="#b21">[22]</ref> uses a classification network and a recurrent attention proposal network that iteratively refines the center and scale of the input (RA-CNN). A ranking loss is used to enforce incremental performance at each iteration.</p><p>Zhao et al. proposed to enforce multiple non-overlapped attention regions <ref type="bibr" target="#b5">[6]</ref>. The overall architecture consists of an attention canvas generator, which extracts patches of different regions and scales from the original image; a VGG-16 <ref type="bibr" target="#b13">[14]</ref> CNN is then used to extract features from the patches, which are aggregated with a long short-term memory <ref type="bibr" target="#b22">[23]</ref> that attends to non-overlapping regions of the patches. Classification is performed with the average prediction in each region. Similarly, in <ref type="bibr" target="#b23">[24]</ref>, they proposed the Multi-Attention CNN (MA-CNN) to learn to localize informative patches from the output of a VGG-19 and use them to train an ensemble of part classifiers.</p><p>In <ref type="bibr" target="#b10">[11]</ref>, they propose to extract global features from the last layers of a CNN, just before the classifier and use them to attend relevant regions in lower level feature activations. The attended activations from each level are then spatially averaged, channel-wise concatenated, and fed to the final classifier. The main differences with <ref type="bibr" target="#b10">[11]</ref> are: (i) attention maps are computed in parallel to the base model, while the model in <ref type="bibr" target="#b10">[11]</ref> requires output features for computing attention maps; (ii) WARN uses fewer parameters, so dropout is not needed to obtain competitive performance (these two factors clearly reflect in gain of speed); and (iii) gates allow our model to ignore/attend different information to improve the performance of the original model, while in <ref type="bibr" target="#b10">[11]</ref> the full output function is replaced. As a result, WARN obtains 3.44% error on CIFAR10, outperforming <ref type="bibr" target="#b10">[11]</ref> while being 7 times faster w/o parallelization.</p><p>All the previously described methods involve multi-stage pipelines and most of them are trained using reinforcement learning (which requires sampling and makes them slow to train). In contrast, STNs, FAM, the model in <ref type="bibr" target="#b10">[11]</ref>, and our approach jointly propose the attention regions and classify them in a single pass. Moreover, different from STNs and FAM our approach only uses one CNN stream, it can be used on pre-trained models, and it is far more computationally efficient than STNs, FAM, and <ref type="bibr" target="#b10">[11]</ref> as described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our approach</head><p>Our approach consists of a universal attention module that can be added after each convolutional layer without altering pre-defined information pathways of any architecture (see <ref type="figure">Figure 1</ref>). This is helpful since it seamlessly augments any architecture such as VGG and ResNet with no extra supervision, i.e. no part labels are necessary. Furthermore, it also allows being plugged into any existing trained network to quickly perform transfer learning approaches.</p><p>The attention module consists of three main submodules depicted in <ref type="figure" target="#fig_1">Figure  2</ref> (a): (i) the attention heads H, which define the most relevant regions of a feature map, (ii) the output heads O, generate an hypothesis given the attended information, and (iii) the confidence gates G, which output a confidence score for each attention head. Each of these modules is described in detail in the following subsections.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>As it can be seen in <ref type="figure">Figure 1</ref>, a convolution layer is applied to the output of the augmented layer, producing K attentional heatmaps. These attentional maps are then used to spatially average the local class probability scores for each of the feature maps, and produce the final class probability vector. This process is applied to an arbitrary number N of layers, producing N class probability vectors. Then, the model learns to correct the initial prediction by attending the lower-level class predictions. This is the final combined prediction of the network. In terms of probability, the network corrects the initial likelihood by updating the prior with local information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attention head</head><p>Inspired by <ref type="bibr" target="#b5">[6]</ref> and the transformer architecture presented by <ref type="bibr" target="#b24">[25]</ref>, and following the notation established by <ref type="bibr" target="#b14">[15]</ref>, we have identified two main dimensions to define attentional mechanisms: (i) the number of layers using the attention mechanism, which we call attention depth (AD), and (ii) the number of attention heads in each attention module, which we call attention width (AW). Thus, a desirable property for any universal attention mechanism is to be able to be deployed at any arbitrary depth and width.</p><p>This property is fulfilled by including K attention heads H k (width), depicted in <ref type="figure">Figure 1</ref>, into each attention module (depth) 1 . Then, the attention heads at layer l ? [1..L], receive the feature activations Z l ? R c?h?w of that layer as input, and output K attention masks:</p><formula xml:id="formula_0">H l = spatial sof tmax(W H l * Z l ),<label>(1)</label></formula><p>where H l ? R K?h?w is the output matrix of the l th attention module, W H l :</p><p>R c?h?w ? R K?h?w is a convolution kernel with output dimensionality K used to compute the attention masks corresponding to the attention heads H k , and * denotes the convolution operator. The spatial softmax, which performs the softmax operation channel-wise on the spatial dimensions of the input, is used to enforce the model to learn the most relevant region of the image. Sigmoid units could also be used at the risk of degeneration to all-zeros or all-ones. To prevent the attention heads at the same depth to collapse into the same region, we apply the regularizer proposed in <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Output head</head><p>To obtain the class probability scores, the input feature map Z l k is convolved with a kernel:</p><formula xml:id="formula_1">W O l k ? R channels?h?w ? R #labels?h?w ,</formula><p>h, w represent the spatial dimensions, and channels is the number of input channels to the module. This results on a spatial map of class probability scores:</p><formula xml:id="formula_2">O l k = W O l k * Z l .<label>(2)</label></formula><p>Note that this operation can be done in a single pass for all the K heads by setting the number of output channels to #labels ? K. Then, class probability vectors O l k are weighted by the spatial attention scores and spatially averaged:</p><formula xml:id="formula_3">o l k = x,y H l k O l k ,<label>(3)</label></formula><p>where is the element-wise product, and x ? {1..width}, y ? {1..height}. The attention scores H l k are a 2d flat mask and the product with each of the input channels of Z l is done by broadcasting, i.e. repeating H l k for each of the channels of Z l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Layered attention gates</head><p>The final output o l of an attention module is obtained by a weighted average of the K output probability vectors, through the use of head attention gates</p><formula xml:id="formula_4">g H l ? R |H| , k g H l k = 1. o l = k g H l k o l k .<label>(4)</label></formula><p>Where g H is obtained by first convolving Z l with</p><formula xml:id="formula_5">W g l ? R channels?h?w ? R |H|?h?w ,</formula><p>and then performing a spatial weighted average:</p><formula xml:id="formula_6">g H l = sof tmax(tanh( x,y (W g l * Z l ) H l )).<label>(5)</label></formula><p>This way, the model learns to choose the attention head that provides the most meaningful output for a given attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Global attention gates</head><p>In order to let the model learn to choose the most discriminative features at each depth to disambiguate the output prediction, a set of relevance scores c are predicted at the model output, one for each attention module, and one for the final prediction. This way, through a series of gates, the model can learn to query information from each level of the network conditioned to the global context. Note that, unlike in <ref type="bibr" target="#b10">[11]</ref>, the final predictions do not act as a bottleneck to compute the output of the attention modules.</p><p>The relevance scores are obtained with an inner product between the last feature activation of the network Z L and the gate weight matrix W G :</p><formula xml:id="formula_7">c = tanh(W G Z L ).<label>(6)</label></formula><p>The gate values g O are then obtained by normalizing the set of scores by means of a sof tmax function:</p><formula xml:id="formula_8">g O l k = e c l k |G| i=1 e ci ,<label>(7)</label></formula><p>where |G| is the total number of gates, and c i is the i th confidence score from the set of all confidence scores. The final output of the network is the weighted sum of the attention modules:</p><formula xml:id="formula_9">output = g net ? output net + l?{1..|O|} g l O ? o l ,<label>(8)</label></formula><p>where g net is the gate value for the original network output (output net ), and output is the final output taking the attentional predictions o l into consideration. Note that setting the output of G to 1 |O| , corresponds to averaging all the outputs. Likewise, setting {G \ G output } = 0, G output = 1, i.e. the set of attention gates is set to zero and the output gate to one, corresponds to the original pre-trained model without attention.</p><p>It is worth noting that all the operations that use Z l can be aggregated into a single convolution operation. Likewise, the fact that the attention mask is generated by just one convolution operation, and that most masking operations are directly performed in the label space, or can be projected into a smaller dimensionality space, makes the implementation highly efficient. Additionally, the direct access to the output gradients makes the module fast to learn, thus being able to generate foreground masks from the beginning of the training and refining them during the following epochs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We empirically demonstrate the impact on the accuracy and robustness of the different modules in our model on Cluttered Translated MNIST and then compare it with state-of-the-art models such as DenseNets and ResNeXt. Finally, we demonstrate the universality of our method for fine-grained recognition through a set of experiments on five fine-grained recognition datasets, as detailed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Cluttered Translated MNIST 2 Consists of 40 ? 40 images containing a randomly placed MNIST <ref type="bibr" target="#b25">[26]</ref> digit and a set of D randomly placed distractors, see <ref type="figure" target="#fig_6">Figure 5b</ref>. The distractors are random 8 ? 8 patches from other MNIST digits.</p><p>CIFAR <ref type="bibr" target="#b2">3</ref> The CIFAR dataset consists of 60K 32x32 images in 10 classes for CIFAR-10, and 100 for CIFAR-100. There are 50K training and 10K test images.</p><p>Stanford Dogs <ref type="bibr" target="#b26">[27]</ref>. The Stanford Dogs dataset consists of 20.5K images of 120 breeds of dogs, see <ref type="figure" target="#fig_2">Figure 3d</ref>. The dataset splits are fixed and they consist of 12k training images and 8.5K validation images.</p><p>UEC Food 100 <ref type="bibr" target="#b27">[28]</ref>. A Japanese food dataset with 14K images of 100 different dishes, see <ref type="figure" target="#fig_2">Figure 3e</ref>. In order to follow the standard procedure (e.g. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>), bounding boxes are used to crop the images before training.</p><p>Adience dataset <ref type="bibr" target="#b30">[31]</ref>. The adience dataset consists of 26.5 K images distributed in eight age categories (02, 46, 813, 1520, 2532, 3843, 4853, 60+), and gender labels. A sample is shown in <ref type="figure" target="#fig_2">Figure 3a</ref>. The performance on this dataset is measured using 5-fold cross-validation.</p><p>Stanford Cars <ref type="bibr" target="#b31">[32]</ref>. The Cars dataset contains 16K images of 196 classes of cars, see <ref type="figure" target="#fig_2">Figure 3c</ref>. The data is split into 8K training and 8K testing images.</p><p>Caltech-UCSD Birds 200 <ref type="bibr" target="#b32">[33]</ref>. The CUB200-2011 birds dataset (see <ref type="figure" target="#fig_2">Figure  3b</ref>) consists of 6K train and 5.8K test bird images distributed in 200 categories. Although bounding box, segmentation, and attributes are provided, we perform raw classification as done by <ref type="bibr" target="#b6">[7]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation study</head><p>We evaluate the submodules of our method on the Cluttered Translated MNIST following the same procedure as in <ref type="bibr" target="#b11">[12]</ref>. The proposed attention mechanism is used to augment a CNN with five 3 ? 3 convolutional layers and two fullyconnected layers in the end. The three first convolution layers are followed by Batch-normalization and a spatial pooling. Attention modules are placed starting from the fifth convolution (or pooling instead) backward until AD is reached. Training is performed with SGD for 200 epochs, and a learning rate of 0.1, which is divided by 10 after epoch 60. Models are trained on a 200k images train set, validated on a 100k images validation set, and tested on 100k test images. Weights are initialized using He et al. <ref type="bibr" target="#b33">[34]</ref>. <ref type="figure" target="#fig_4">Figure 4</ref> shows the effects of the different hyperparameters of the proposed model. The performance without attention is labeled as baseline. Attention models are trained with softmax attention gates and regularized with <ref type="bibr" target="#b5">[6]</ref>, unless explicitly specified. First, we test the importance of AD for our model by increasingly adding attention layers with AW = 1 after each pooling layer. As it can be seen in <ref type="figure" target="#fig_4">Figure  4b</ref>, greater AD results in better accuracy, reaching saturation at AD = 4, note that for this value the receptive field of the attention module is 5 ? 5 px, and thus the performance improvement from such small regions is limited. <ref type="figure" target="#fig_4">Figure  4c</ref> shows training curves for different values of AW , and AD = 4. As it can be seen, small performance increments are obtained by increasing the number of attention heads even with a single object present in the image.</p><p>Then, we use the best AD and AW , i.e. AD, AW = 4, to verify the importance of using softmax on the attention masks instead of sigmoid (1), the effect of using gates (Eq. 7), and the benefits of regularization <ref type="bibr" target="#b5">[6]</ref>. <ref type="figure" target="#fig_4">Figure 4d</ref> confirms that ordered by importance: gates, softmax, and regularization result in accuracy improvement, reaching 97.8%. In particular, gates play an important role in discarding the distractors, especially for high AW and high AD Finally, in order to verify that attention masks are not overfitting on the data, and thus generalize to any amount of clutter, we run our best model so far <ref type="figure" target="#fig_4">(Figure 4d</ref>) on the test set with an increasing number of distractors (from 4 to 64). For the comparison, we included the baseline model before applying our approach and the same baseline augmented with an STN <ref type="bibr" target="#b6">[7]</ref> that reached comparable performance as our best model in the validation set. All three models were trained with the same dataset with eight distractors. Remarkably, as it can be seen in <ref type="figure" target="#fig_4">Figure 4e</ref>, the attention augmented model demonstrates better generalization than the baseline and the STN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training from scratch</head><p>We benchmark the proposed attention mechanism on CIFAR-10 and CIFAR-100, and compare it with the state of the art. As a base model, we choose Wide Residual Networks, a strong baseline with a large number of parameters so that the additional parameters introduced by our model (WARN) could be considered negligible. The same WRN baseline is used to train an att2 model <ref type="bibr" target="#b10">[11]</ref>, we refer to this model as WRN-att2. Models are initialized and optimized following the same procedure as in <ref type="bibr" target="#b14">[15]</ref>. Attention Modules are systematically placed after each of the three convolutional groups starting by the last one until the attention depth has been reached in order to capture information at different levels of abstraction and fine-grained resolution, this same procedure is followed in <ref type="bibr" target="#b10">[11]</ref>. The model is implemented with pytorch <ref type="bibr" target="#b34">[35]</ref> and run on a single workstation with two NVIDIA 1080Ti. <ref type="bibr" target="#b3">4</ref> First, the same ablation study performed in Section 4.2 is repeated on CI-FAR100. We consistently reached the same conclusions as in Cluttered-MNIST: accuracy improves 1.5% by increasing attention depth from 1 to #residual blocks, and width from 1 to 4. Gating performs 4% better than a simpler linear projection, and 3% with respect to simply averaging the output vectors. A 0.6% improvement is also observed when regularization is activated. Interestingly, we found sigmoid attention to perform similarly to softmax. With this setting, WARN reaches 17.82% error on CIFAR100. In addition, we perform an experiment blocking the gradients from the proposed attention modules to the original network to analyze whether the observed improvement is due to the attention mechanism or an optimization effect due to introducing shortcut paths to the loss function <ref type="bibr" target="#b35">[36]</ref>. Interestingly, we observed a 0.2% drop on CIFAR10, and 0.4% on CIFAR100, which are still better than the baseline. Note that a performance drop should be expected, even without taking optimization into account, since backpropagation makes intermediate layers learn to gather more discriminative features for the attention layers. It is also worth noting that fine-grained accuracy improves even when fine-tuning (gradients are multiplied by 0.1 in the base model), see Section 4.4. In contrast, the approach in <ref type="bibr" target="#b10">[11]</ref> does not converge when gradients are not sent to the base model since classification is directly performed on intermediate feature maps (which continuously shift during training). As seen in <ref type="table" target="#tab_1">Table 1</ref>, the proposed Wide Attentional Residual Network (WARN) improves the baseline model for CIFAR-10 and CIFAR-100 even without the use of Dropout and outperforms the rest of the state of the art in CIFAR-10 while being remarkably faster, as it can be seen in <ref type="table" target="#tab_2">Table 2</ref>. Remarkably, the performance on CIFAR-100 makes WARN competitive when compared with Densenet and Resnext, while being up to 36 times faster. We hypothesize that the increase in accuracy of the augmented model is limited by the base network and even better results could be obtained when applied on the best performing baseline.</p><p>Interestingly, WARN shows superior performance even without the use of dropout; this was not possible with <ref type="bibr" target="#b10">[11]</ref>, which requires dropout to achieve competitive performances, since they introduce more parameters to the augmented  network. The computing efficiency of the top performing models is shown in <ref type="figure" target="#fig_6">Figure 5</ref>. WARN provides the highest accuracy per GFlop on CIFAR-10, and is more competitive than WRN, and WRN-att2 on CIFAR-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Transfer Learning</head><p>We fine-tune an augmented WRN-50-4 pre-trained on Imagenet <ref type="bibr" target="#b18">[19]</ref> and report higher accuracy on five different fine-grained datasets: Stanford Dogs, UEC Food-100, Adience, Stanford Cars, CUB200-2001 compared to the WRN baseline. All the experiments are trained for 100 epochs, with a batch size of 64. The learning rate is first set to 10 ?3 to all layers except the attention modules and the classifier, for which it ten times higher. The learning rate is reduced by a factor of 0.1 every 30 iterations and the experiment is automatically stopped if For the sake of clarity and since the aim of this work is to demonstrate that the proposed mechanism universally improves the baseline CNNs for fine-grained recognition, we follow the same training procedure in all datasets. Thus, we do not use 512 ? 512 images which are central for state-of-the-art methods such as RA-CNNs, or MA-CNNs. Accordingly, we do not perform color jitter and other advanced augmentation techniques such as the ones used by <ref type="bibr" target="#b29">[30]</ref> for food recognition. The proposed method is able to obtain state of the art results in Adience Gender, Stanford dogs and UEC Food-100 even when trained with lower resolution.</p><p>As seen in table 3, WRN substantially increase their accuracy on all benchmarks by just fine-tuning them with the proposed attention mechanism. Moreover, we report the highest accuracy scores on Stanford Dogs, UEC Food, and Gender recognition, and obtain competitive scores when compared with models that use high resolution images, or domain-specific pre-training. For instance, in <ref type="bibr" target="#b38">[39]</ref> a domain-specific model pre-trained on millions of faces is used for age recognition, while our baseline is a general-purpose WRN pre-trained on the Imagenet. It is also worth noting that the performance increase on CUB200-2011 (+1.3%) is higher than the one obtained in STNs with 224?224 images (+0.8%) even though we are augmenting a stronger baseline. This points out that the proposed mechanism might be extracting complementary information that is not extracted by the main convolutional stream. As seen in table 4, WARN not only increases the absolute accuracy, but it provides a high efficiency per introduced parameter.</p><p>A sample of the attention masks for each dataset is shown on <ref type="figure" target="#fig_7">Figure 6</ref>. As it can be seen, the attention heads learn to ignore the background and to attend the most discriminative parts of the objects. This matches the conclusions of Section 4.2.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a novel attention mechanism to improve CNNs. The proposed model learns to attend the most informative parts of the CNN feature maps at different depth levels and combines them with a gating function to update the output distribution. Moreover, we generalize attention mechanisms by defining them in two dimensions: the number of attended layers, and the number of Attention Heads per layer and empirically show that classification performance improves by growing the model in these two dimensions.</p><p>We suggest that attention helps to discard noisy uninformative regions, avoiding the network to memorize them. Unlike previous work, the proposed mechanism is modular, architecture independent, fast, simple, and yet WRN augmented with it obtain state-of-the-art results on highly competitive datasets while being 37 times faster than DenseNet, 30 times faster than ResNeXt, and making the augmented model more parameter-efficient. When fine-tuning on a transfer learning task, the attention augmented model showed superior performance in each recognition dataset. Moreover, state of the art performance is obtained on dogs, gender, and food. Results indicate that the model learns to extract local discriminative information that is otherwise lost when traversing the layers of the baseline architecture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>(a) Attention Module: K attention heads H l k are applied to a feature map Z l , and information is aggregated with the layer attention gates. (b) Global attention: global information from the last feature map Z L is used to compute the gating scores that produce the final output as the weighted average of the outputs of the attention modules and the original network output net</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Samples from the five fine-grained datasets. (a) Adience, (b) CUB200 Birds, (c) Stanford Cars, (d) Stanford Dogs, (e) UEC-Food100</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Softmax, gates, reg.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Ablation experiments on Cluttered Translated MNIST. baseline indicates the original model before being augmented with attention. (a) shows a sample of the cluttered MNIST dataset. (b) the effect of increasing the attention depth (AD), for attention width AW = 1. (c) effect of increasing AW, for AD=4. (d) best performing model (AD, AW = 4, softmax attention gates, and regularization [6]) vs unregularized, sigmoid attention, and without gates. (e) test error of the baseline, attention (AD, AW = 4), and spatial transformer networks (stn), when trained with different amounts of distractors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>.</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Comparison of the best performing Resnext, Densenet, WRN, WRN-att2, and WARN on the CIFAR-10 and CIFAR-100. Validation accuracy is reported as a function of the number of GFLOPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Attention masks for each dataset: (a) Stanford dogs, (b) Stanford cars, (c) Adience gender, (d) CUB birds, (e) Adience age, (f) UEC food. As it can be seen, the masks help to focus on the foreground object. In (c), the attention mask focuses on ears for gender recognition, possibly looking for earrings</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1807.07320v2 [cs.CV] 24 Jul 2018 ...</figDesc><table><row><cell>Attention</cell><cell></cell><cell>...</cell><cell>Attention</cell><cell>Attention</cell></row><row><cell>Conv Block</cell><cell>Conv Block</cell><cell>...</cell><cell>Conv Block</cell></row></table><note>Global Attention Gates</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Error rate on CIFAR-10 and CIFAR-100 (%). Results that surpass all other methods are in blue, results that surpass the baseline are in black bold font. Total network depth, attention depth, attention width, the usage of dropout, and the amount of floating point operations (Flop) are provided in columns 1-5 for fair comparison</figDesc><table><row><cell></cell><cell cols="5">Net Depth AD AW Dropout GFlop CIFAR-10 CIFAR-100</cell></row><row><cell>Resnext [37]</cell><cell>29</cell><cell>--</cell><cell>10.7</cell><cell>3.58</cell><cell>17.31</cell></row><row><cell>Densenet [38]</cell><cell>250 190</cell><cell>----</cell><cell>5.4 9.3</cell><cell>3.62 3.46</cell><cell>17.60 17.18</cell></row><row><cell></cell><cell>28</cell><cell>--</cell><cell>5.2</cell><cell>4</cell><cell>19.25</cell></row><row><cell>WRN [15]</cell><cell>28</cell><cell>--</cell><cell>5.2</cell><cell>3.89</cell><cell>18.85</cell></row><row><cell></cell><cell>40</cell><cell>--</cell><cell>8.1</cell><cell>3.8</cell><cell>18.3</cell></row><row><cell></cell><cell>28</cell><cell>2 -</cell><cell>5.7</cell><cell>4.10</cell><cell>21.20</cell></row><row><cell>WRN-att2 [11]</cell><cell>28</cell><cell>2 -</cell><cell>5.7</cell><cell>3.60</cell><cell>20.00</cell></row><row><cell></cell><cell>40</cell><cell>2 -</cell><cell>8.6</cell><cell>3.90</cell><cell>19.20</cell></row><row><cell></cell><cell>28</cell><cell>2 4</cell><cell>5.2</cell><cell>3.60</cell><cell>18.72</cell></row><row><cell>WARN</cell><cell>28 28</cell><cell>3 4 3 4</cell><cell>5.3 5.3</cell><cell>3.45 3.44</cell><cell>18.61 18.26</cell></row><row><cell></cell><cell>40</cell><cell>3 4</cell><cell>8.2</cell><cell>3.46</cell><cell>17.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Number of parameters, floating point operations (Flop), time (s) per validation epoch, and error rates (%) on CIFAR-10 and CIFAR-100. The "Time" column shows the amount of seconds to forward the validation dataset with batch size 256 on a single GPU Depth Params GFlop Time CIFAR-10 CIFAR-100</figDesc><table><row><cell></cell><cell cols="2">ResNext</cell><cell></cell><cell>29</cell><cell>68M</cell><cell cols="3">10.7 5.02s</cell><cell>3.58</cell><cell></cell><cell cols="2">17.31</cell></row><row><cell></cell><cell cols="2">Densenet</cell><cell cols="2">190</cell><cell>26M</cell><cell>9.3</cell><cell cols="2">6.41s</cell><cell>3.46</cell><cell></cell><cell cols="2">17.18</cell></row><row><cell></cell><cell>WRN</cell><cell></cell><cell></cell><cell>40</cell><cell>56M</cell><cell>8.1</cell><cell cols="2">0.18s</cell><cell>3.80</cell><cell></cell><cell cols="2">18.30</cell></row><row><cell></cell><cell cols="4">WRN-att2 40</cell><cell>64M</cell><cell>8.6</cell><cell cols="2">0.24s</cell><cell>3.90</cell><cell></cell><cell cols="2">19.20</cell></row><row><cell></cell><cell>WARN</cell><cell></cell><cell></cell><cell>28</cell><cell>37M</cell><cell cols="3">5.3 0.17s</cell><cell>3.44</cell><cell></cell><cell cols="2">18.26</cell></row><row><cell></cell><cell>WARN</cell><cell></cell><cell></cell><cell>40</cell><cell>56M</cell><cell>8.2</cell><cell cols="2">0.18s</cell><cell>3.46</cell><cell></cell><cell cols="2">17.82</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>82.75</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>96.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>82.50</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>82.25</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy (\%)</cell><cell>96.3 96.4</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Resnext</cell><cell>Accuracy (\%)</cell><cell>81.50 81.75 82.00</cell><cell></cell><cell></cell><cell></cell><cell>Resnext</cell></row><row><cell></cell><cell>96.2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Densenet WRN-28</cell><cell></cell><cell>81.25</cell><cell></cell><cell></cell><cell></cell><cell>Densenet WRN-40</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">WRN-att2-28</cell><cell></cell><cell>81.00</cell><cell></cell><cell></cell><cell></cell><cell>WRN-att2-40</cell></row><row><cell></cell><cell>96.1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">WARN-28</cell><cell></cell><cell>80.75</cell><cell></cell><cell></cell><cell></cell><cell>WARN-40</cell></row><row><cell></cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8 GFLOPs</cell><cell>9</cell><cell>10</cell><cell></cell><cell>8.0</cell><cell>8.5</cell><cell>9.0</cell><cell>9.5 GFLOPs</cell><cell>10.0</cell><cell>10.5</cell></row><row><cell></cell><cell></cell><cell cols="4">(a) CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) CIFAR-100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on six fine-grained recognition tasks. DSP means that the cited model uses Domain Specific Pre-training. HR means the cited model uses highresolution images. Accuracies that improve the baseline model are in black bold font, and highest accuracies are in blue</figDesc><table><row><cell></cell><cell>Dogs</cell><cell>Food</cell><cell>Cars</cell><cell cols="2">Gender Age</cell><cell>Birds</cell></row><row><cell>SotA</cell><cell cols="6">RA-CNN [22] Inception [30] MA-CNN [24] FAM [8] DEX [39] MA-CNN [24]</cell></row><row><cell>DSP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>87.3</cell><cell>81.5</cell><cell>92.8</cell><cell>93.0</cell><cell>64.0</cell><cell>86.5</cell></row><row><cell>WRN</cell><cell>89.6</cell><cell>84.3</cell><cell>88.5</cell><cell>93.9</cell><cell>57.4</cell><cell>84.3</cell></row><row><cell>WARN</cell><cell>92.9</cell><cell>85.5</cell><cell>90.0</cell><cell>94.6</cell><cell>59.7</cell><cell>85.6</cell></row><row><cell cols="7">a plateau is reached. The network is trained with standard data augmentation,</cell></row><row><cell cols="7">i.e. random 224 ? 224 patches are extracted from 256 ? 256 images with random</cell></row><row><cell cols="2">horizontal flips.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Increment of accuracy (%) per Million of parametersDogs Food Cars Gender Age Birds Average</figDesc><table><row><cell>WRN</cell><cell>1.3</cell><cell>1.2 1.3</cell><cell>1.4</cell><cell>0.8 1.2</cell><cell>1.2</cell></row><row><cell cols="3">WARN 6.9 2.5 3.1</cell><cell>1.5</cell><cell>4.0 2.5</cell><cell>3.4</cell></row><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell><cell>(e)</cell><cell>(f)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Notation: H, O, G are the set of attention heads, output heads, and attention gates respectively. Uppercase letters refer to functions or constants, and lowercase ones to indices. Bold uppercase letters represent matrices and bold lowercase ones vectors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/deepmind/mnist-cluttered 3 https://www.cs.toronto.edu/~kriz/cifar.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/prlz77/attend-and-rectify</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments Authors acknowledge the support of the Spanish project TIN2015-65464-R (MINECO/FEDER), the 2016FI B 01163 grant of Generalitat de Catalunya, and the COST Action IC1307 iV&amp;L Net. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of a Tesla K40 GPU and a GTX TITAN GPU, used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Cognitive psychology and its implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>WH Freeman/Times Books/Henry Holt and Co</publisher>
			<pubPlace>New York, NY, US</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural mechanisms of selective visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Desimone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of neuroscience</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="222" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mechanisms of visual attention in the human cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ungerleider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of neuroscience</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="315" to="341" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Attention for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diversified visual attention networks for fine-grained object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1256" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Spatial transformer networks. In: NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Age and gender recognition in the wild with deep attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Gonfaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Roca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learn to pay attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jetley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="2204" to="2212" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fully convolutional attention localization networks: Efficient attention localization for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06765</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A survey on deep learning-based fine-grained object classification and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJAC</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The application of two-level attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="842" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">The imagenet large scale visual recognition challenge 2012 (ilsvrc2012)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Part-based r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">154</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Look closer to see better: recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Novel dataset for fine-grained image categorization: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Recognition of multiple-food images by detecting candidate regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yanai</surname></persName>
		</author>
		<editor>ICME.</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep-based ingredient recognition for cooking recipe retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Ngo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACM MM, ACM</publisher>
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Food image recognition using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hassannejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Matrella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ciampolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>De Munari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mordonini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cagnoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MADIMA Workshop</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="41" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Age and gender estimation of unfiltered faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eidinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIFS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2170" to="2179" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3d object representations for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Pytorch</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="page" from="562" to="570" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR, IEEE</publisher>
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep expectation of real and apparent age from a single image without facial landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
