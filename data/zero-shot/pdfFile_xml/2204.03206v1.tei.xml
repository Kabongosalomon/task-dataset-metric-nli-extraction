<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">L2G: A Simple Local-to-Global Knowledge Transfer Framework for Weakly Supervised Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Tao</forename><surname>Jiang</surname></persName>
							<email>pt.jiang@mail.nankai.edu.cnandrewhoux@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">TMCC</orgName>
								<orgName type="institution" key="instit2">Nankai University</orgName>
								<address>
									<region>CS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">TMCC</orgName>
								<orgName type="institution" key="instit2">Nankai University</orgName>
								<address>
									<region>CS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">TMCC</orgName>
								<orgName type="institution" key="instit2">Nankai University</orgName>
								<address>
									<region>CS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Jiaotong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">L2G: A Simple Local-to-Global Knowledge Transfer Framework for Weakly Supervised Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mining precise class-aware attention maps, a.k.a, class activation maps, is essential for weakly supervised semantic segmentation. In this paper, we present L2G, a simple online local-to-global knowledge transfer framework for highquality object attention mining. We observe that classification models can discover object regions with more details when replacing the input image with its local patches. Taking this into account, we first leverage a local classification network to extract attentions from multiple local patches randomly cropped from the input image. Then, we utilize a global network to learn complementary attention knowledge across multiple local attention maps online. Our framework conducts the global network to learn the captured rich object detail knowledge from a global view and thereby produces high-quality attention maps that can be directly used as pseudo annotations for semantic segmentation networks. Experiments show that our method attains 72.1% and 44.2% mIoU scores on the validation set of PASCAL VOC 2012 and MS COCO 2014, respectively, setting new state-of-the-art records.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning algorithms <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b63">64]</ref> have promoted the rapid development of the semantic segmentation task in recent years. However, training a deep neural network for semantic segmentation requires a large number of pixel-wise accurate labels, which consume lots of human labors and resources. Recently, to reduce the reliance on accurate annotations, researchers have attempted to study semantic segmentation based on cheap supervisions, such as bounding boxes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b41">42]</ref>, scribbles <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b49">50]</ref>, points <ref type="bibr" target="#b3">[4]</ref>, and image-level labels <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b52">53]</ref>. Among these weak supervisions, imagelevel labels only provide information on the existence of the target object categories, making them more popular than <ref type="bibr">Figure 1</ref>. Conceptual working pipeline of the proposed method. We utilize the attention maps for local views with rich details extracted from the local network to teach the global network. This enables the global network to learn the rich local details knowledge from the local network online and thereby more integral object attentions.</p><p>other supervisions due to the easy way to collect. In this paper, we also focus on weakly supervised semantic segmentation (WSSS) based on image-level labels.</p><p>Speaking of WSSS, one of the most important components should be the class activation map (CAM) <ref type="bibr" target="#b64">[65]</ref> which contains both semantic and location information about the target objects and can be used as pseudo pixel-level annotations for training segmentation networks. Since the quality of CAMs has a great influence on the segmentation results, recently, many strategies have been proposed to advance the original CAM method, including adversarial erasing <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b65">66]</ref>, online attention accumulation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, seed region expansion <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref>, and affinity learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b56">57]</ref>, etc. Despite the good performance, these works mostly take the whole input image as the sole input to the model. However, we empirically observe that classification models can discover more discriminative regions when taking local image patches as input compared to the whole input image. This suggests a proper way to improve the quality of attention maps by making use of local image patches. The second row shows the attention maps generated by CAM <ref type="bibr" target="#b64">[65]</ref>. We can observe that the attention maps of the local views capture more object details compared to that of the global view.</p><p>In this paper, taking the above analysis into account, we present a simple online local-to-global knowledge transfer framework, termed L2G, for generating high-quality object attentions. A conceptual illustration has been depicted in <ref type="figure">Fig. 1</ref>. Different from the aforementioned attention mining strategies, we propose to take advantage of both the global view and the local views randomly cropped from the input image (regions enclosed by the colorful bounding boxes). Specifically, our framework contains a local network that produces local attentions with rich object details for local views as well as a global network that receives the global view as input and aims to distill the discriminative attention knowledge from the local network.</p><p>Our method offers the following advantages. First of all, we produce attention maps from multiple local views of the input image rather than its global view. This allows us to attain more details on undiscovered semantic regions, which are also complementary across different local views, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. Second, by designing a knowledge transfer loss, the complementary attention knowledge can be efficiently transferred to the global network in an online learning manner. This enables the global network to capture pixel-level semantic object details and produce high-quality attention maps in inference. Last but not the least, the overall pipeline is simple and flexible. We can selectively add additional constraints <ref type="bibr" target="#b31">[32]</ref> to the local network to help shape the attained object attentions.</p><p>We evaluate our method on the PASCAL VOC 2012 and MS COCO 2014 datasets. Experiments demonstrate that our method achieves better performance than previous state-of-the-art methods. When using the DeepLab-v2 model <ref type="bibr" target="#b9">[10]</ref> as our segmentation network, we attain 72.1% and 71.7% mIoU scores on the validation set and the test set of PASCAL VOC 2012, and 44.2% on the validation set of MS COCO 2014, setting new state-of-the-art records under the weakly supervised setting. We also conduct a series of ablation experiments to help readers better understand how each component performs in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Weakly Supervised Semantic Segmentation</head><p>One-stage WSSS methods directly utilize the image-level labels as supervision to train an end-to-end segmentation network. Early works <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref> formulate this problem as multiple instance learning. Later, Papandreou et al. <ref type="bibr" target="#b41">[42]</ref> proposed an Expectation-Maximization (EM) method that utilizes the intermediate prediction to supervise the segmentation network. Zhang et al. <ref type="bibr" target="#b61">[62]</ref> utilized the image classification branch to generate attention maps and constructed the pseudo segmentation labels to supervise the parallel segmentation branch. Araslanov et al. <ref type="bibr" target="#b2">[3]</ref> proposed a self-supervised mechanism that applies the image appearance priors to generate pseudo segmentation labels during training. Chen et al. <ref type="bibr" target="#b6">[7]</ref> constructed an end-to-end framework that uses an encoder-decoder network to explore object boundaries. Compared to two-stage WSSS methods, one-stage methods usually have inferior performance and are less attractive.</p><p>Two-stage WSSS methods rely on attention maps <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b64">65]</ref> to generate pseudo segmentation labels, which are then used to train segmentation networks. The core of two-stage WSSS methods is to produce high-quality attention maps <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref>. Towards this goal, a lot of works have been proposed recently. Wei et al. <ref type="bibr" target="#b52">[53]</ref> proposed the adversarial erasing strategy, which iteratively occludes the mined object regions to drive the classification network to discover new object regions. Hou et al. <ref type="bibr" target="#b21">[22]</ref> improved the adversarial erasing strategy by using a self-erasing strategy to prevent attention from spreading to the background. Kolesnikov et al. <ref type="bibr" target="#b27">[28]</ref> introduced the seed-expansion idea, which expands the initial seed regions from the pre-computed attention maps and constrains the expanded regions to align with the object boundaries. Later, Jiang et al. <ref type="bibr" target="#b25">[26]</ref> proposed the online attention accumulation strategy that utilizes the attention maps of different training phases. Chang et al. <ref type="bibr" target="#b5">[6]</ref> exploited the sub-category information to highlight the nondiscriminative semantic regions.</p><p>Another line of works attempts to refine attention maps to obtain integral object regions with precious boundaries. Ahn et al. <ref type="bibr" target="#b1">[2]</ref> learned pixel affinity to propagate the semantics of strong responses in attention maps to the adjacent pixels. Chen et al. <ref type="bibr" target="#b7">[8]</ref> and Ahn et al. <ref type="bibr" target="#b0">[1]</ref> further improved this method by explicitly learning the class boundaries. Lee et al. <ref type="bibr" target="#b31">[32]</ref> utilized the off-the-shelf saliency maps as supervision to guide the region learning to generate high-quality attention maps.</p><p>One common point shared by the aforementioned methods is that they all refine attention maps on the image's global view. Differently, our method takes advantage of both the global view and multiple local views and stud-  <ref type="figure">Figure 3</ref>. Overall framework of the proposed method. The complementary attention maps captured by the local network is distilled into the global network by a knowledge transfer loss.</p><formula xml:id="formula_0">Replace if |S i | ? 0 B i x S i S A i B i G</formula><p>ies how to efficiently transfer the complementary attention knowledge from the local network to the global network to improve the quality of attention maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Knowledge distillation</head><p>Our work is also related to knowledge distillation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b60">61]</ref>, which aims to distill the knowledge from the welltrained teacher model to a student model. For the image classification task, these works focus on improving the student model by imitating the prediction distribution of the teacher model. Moreover, some researchers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39]</ref> also study knowledge distillation for the semantic segmentation task. Differently, we investigate how to transfer the attention knowledge captured by the local views to the global network in an online learning manner to better leverage the complementary information from multiple views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we present the whole framework of our method in detail. Before describing the framework, we first give some fundamental introduction to attention map generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Prerequisites</head><p>We first present the way to generate attention maps. Given an input image I, let y be the image-level label. The output feature F of the last convolutional layer has C channels, identical to the number of classes. The last convolutional layer is followed by a global average pooling layer, where the feature F is pooled to a vector f C of size C. We calculate the classification loss by applying a sigmoid cross-entropy loss function, which is formulated as follows:</p><formula xml:id="formula_1">L ce = ? 1 C C c=1 y c log(?(f c )) + (1 ? y c ) log(1 ? ?(f c )),<label>(1)</label></formula><p>where ? is the sigmoid function. The attention maps can be generated from the output of the last convolutional layer. For some class c, the attention map A c is derived from the c th channel of F , which can be formulated as</p><formula xml:id="formula_2">A c = ReLU(F c ) max(ReLU(F c ))</formula><p>.</p><p>The above method, as pointed out in most previous work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b52">53]</ref>, can only locate the most discriminative regions. It often fails in discovering those non-discriminative object regions that are semantically meaningful as well. In the following, we propose a novel attention generation framework by presenting a new local-to-global knowledge transfer method to capture high-quality object attentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Overall Framework</head><p>As mentioned in Sec. 1, the local network focusing on processing local patch views tends to discover more discriminative object regions. Based on this observation, we propose to leverage the attention maps for local views to aid a global network to locate more integral object regions.</p><p>The overall framework of the proposed approach can be found in <ref type="figure">Fig. 3</ref>. Functionally, there are four components: a global network, a local network, an attention transfer module, and a shape transfer module. The global network and the local network can be any CNN classifier, such as the popular VGGNet <ref type="bibr" target="#b45">[46]</ref> or ResNet-38 <ref type="bibr" target="#b55">[56]</ref>. In the attention transfer module, we optimize two loss functions: a classification loss L cls that is used to recognize the semantic objects and an attention transfer loss L at that encourages the global network to imitate the local network to discover more discriminative regions. In the shape transfer module, we introduce a shape constraint to loss L at , yielding L st , to shape the captured object attentions. Therefore, the overall optimized loss function can be formulated as follows:</p><formula xml:id="formula_4">L = L cls + ? ? L kt ,<label>(3)</label></formula><p>where ? denotes the loss weight for L kt . When no shape constraint is added, L kt = L at . Otherwise, L kt = L st .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Local-to-Global Attention Transfer</head><p>Given an input image I, we transform it into a set of different views V , including a global view V I , and N local views {V 1 , V 2 , ..., V N }, which are randomly cropped from the global view. The local views {V 1 , V 2 , ..., V N } are sent into the local network focusing on generating attention maps that contain rich object details. The global view V I is fed into the global network, which aims to learn the knowledge from the local network and produces object attentions in inference. Let {F 1 , F 2 , ..., F N } be the outputs of the last convolutional layer of the local network and each has C channels corresponding to the number of classes. Let F be the output of the last convolutional layer of the global network that has C + 1 channels. The classification loss and the attention transfer loss can be defined as follows.</p><p>Classification Loss: The classification loss is equipped with on the local network. Specifically, the feature maps {F 1 , F 2 , ..., F N } of the local views are first sent to a global pooling layer, where the features are pooled to a set of 1D feature vectors {f 1 , f 2 , ..., f N }. Given a 1D feature vector f i , the predicted probabilities for all categories can be computed by q i = ?(f i ). Recall that ? is the sigmoid function. Then, the classification loss L cls can be written as</p><formula xml:id="formula_5">L cls = ? 1 N ? C N i=1 C c=1 y c log(q c i ) + (1 ? y c ) log(1 ? q c i ).<label>(4)</label></formula><p>Attention Transfer Loss: We first generate attention maps for the local views from the local network. We use Eqn. <ref type="bibr" target="#b1">(2)</ref> to generate attention maps {A c 1 , A c 2 , ..., A c N } for the c th category if c is in the image-level labels. If c is not in the image-level labels, the attention values in the corresponding attention map will be zeroed. To transfer the attentions attained by the local network to the global network, we adopt the mean squared error loss.</p><p>Given the outputF from the global network, we apply a Softmax function toF along the channel dimension for each location, yielding</p><formula xml:id="formula_6">G c = eF c C+1 i=1 eF i ,<label>(5)</label></formula><p>where the value at each location of G c means the probability of this location being category c. Let {G 1 , G 2 , ..., G N } denote the corresponding regions to {A 1 , A 2 , ..., A N } on the global view, i.e., each pair (G 1 , A 1 ) are cropped from the same coordinate on the global view. The attention transfer loss is formulated by measuring the difference between {A i } and {G i } as follows:</p><formula xml:id="formula_7">L at = 1 N N i=1 ||A i ? G i || 2 .<label>(6)</label></formula><p>During training, we jointly optimize the above two losses. During inference, the attention maps are generated from the global network while the local network can be discarded.</p><p>Discussion: Our method provides an efficient way to leverage the complementary information from the global view and the local views. The local-to-global attention transfer method conducts the global network to absorb the rich object detail knowledge captured by the local network in an online learning manner. Though most previous works also use data augmentations, like random crop, for the inputs, they do not have a component to accumulate the object detail knowledge from the cropped local patches online from a global view. This makes our local-to-global strategy quite different from previous works. We will show more advantages of the proposed approach over other methods in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Local-to-Global Shape Transfer</head><p>The proposed local-to-global attention transfer strategy can already result in more integral object attentions than the original CAM <ref type="bibr" target="#b64">[65]</ref>. However, as the attention transfer process leverages only the image-level labels, the captured attentions around the object boundaries are not sharp enough. To well capture the shape of the localized objects in the attention maps, we attempt to introduce auxiliary salient object information into the attention transfer loss by adding a shape constraint. The saliency model <ref type="bibr" target="#b37">[38]</ref> can serve as a class-agnostic salient object detector, which can segment the foreground objects and provide shape information.</p><p>The shape transfer process is simple, which has been illustrated in the right part of <ref type="figure">Fig. 3</ref>. Given the attention maps {A i } from the local network, we first binarize them with a small threshold (e.g., 0.1), yielding the binary maps {B i }. Then, we utilize the saliency model to generate the saliency map S for the given image I and get the corresponding saliency regions to the attention maps {A i } with the same coordinate on I denoted as {S i }. The attention transfer loss can then be rewritten as</p><formula xml:id="formula_8">L st = 1 N N i=1 ||B i ? S i ? G i || 2 , if |S i | ? = 0 ||A i ? G i || 2 , if |S i | = 0<label>(7)</label></formula><p>where ? denotes element-wise multiplication and |S i | is the cardinality of the saliency map S i . By using B i ?S i , we aim to remove the attention regions outside the salient objects, which belong to the background with high probability. This allows our method to fully leverage the shape information provided by the saliency maps and results in high-quality  attention maps. We will elaborate more on this in our experiment section. Note that as not all the images would have salient objects, it is inappropriate to always use the top part of Eqn. <ref type="bibr" target="#b6">(7)</ref>. Thus, for those images whose saliency maps contain nothing, we utilize the original attention maps as supervision as formulated in the bottom part of Eqn. <ref type="bibr" target="#b6">(7)</ref>. It is worth mentioning that EPS <ref type="bibr" target="#b31">[32]</ref> also uses saliency maps as supervision to provide the network with shape information. Differently, our method focuses more on how to take advantage of multiple local views and how to efficiently transfer the learned knowledge from the local network to the global one. In the following, we will show the advantage of the proposed local-to-global knowledge transfer over EPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The following paper is organized as follows. First, we introduce the experimental setup. Then, we show experimental results on ablation study and analyze the role of each component proposed in our method. Finally, we conduct experiments to compare our method with previous state-ofthe-art WSSS methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets. Experiments are conducted on two publicly available datasets, PASCAL VOC 2012 and MS COCO 2014. The PASCAL VOC 2012 dataset contains 20 semantic categories and the background. It is split into three sets, the training, validation, and test sets, each containing 1464, 1449, and 1456 images, respectively. Following most previous works, we also use the augmented training set <ref type="bibr" target="#b17">[18]</ref>, yielding totally 10582 training images. The MS COCO 2014 dataset has 80 semantic categories. Following <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref>, the images without target categories are excluded from the dataset, remaining 82081 training images and 40137 validation images.</p><p>Evaluation metric. The mean intersection-over-union (mIoU) <ref type="bibr" target="#b40">[41]</ref> is used as the evaluation metric. As the segmentation annotations of the test set in the PASCAL VOC 2012 dataset are not available, we submit the segmentation results to the official PASCAL VOC evaluation server 1 .</p><p>Data augmentation. For data augmentation, the short size of the input image is resized to 512. The global view is with a resolution of 448?448, which is cropped from the input image. The local image patches with resolution 320?320 are cropped from the global view.</p><p>Classification Network. Following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref>, we utilize ResNet-38 <ref type="bibr" target="#b55">[56]</ref> as our classification network. Besides, we also employ a pixel correlation module (PCM) <ref type="bibr" target="#b51">[52]</ref> into the classification network to constrain the shape of the target object. The attention maps are generated from the global network using the multi-scale test strategy <ref type="bibr" target="#b1">[2]</ref>.</p><p>Classification on PASCAL VOC. We train the classification network for 10 epochs and use SGD as the optimizer. The initial learning rate is set to 1e-3, which decays at the <ref type="table">Table 1</ref>. Comparisons of mIoU scores under different network settings. The baseline is the original CAM <ref type="bibr" target="#b64">[65]</ref>. SW: The sliding window strategy is applied to the baseline during inference <ref type="bibr" target="#b64">[65]</ref>. Local: Using multiple local image patches instead of the input image to train the classification network. L2G: Our method with local-to-global attention transfer only. mIoUtrainaug denotes the mIoU score of the pseudo segmentation labels on the augmented training set. Classification on MS COCO. We train the classification network for 15 epochs and use SGD as the optimizer. We set the initial learning rate to 0.1 and use poly as the learning scheduler. The loss weight ? for the attention transfer loss is 30. Other network settings are as follows: batch size: 12, weight decay: 5e-4, patch size: 320?320, patch number: 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No</head><p>Segmentation. We select DeepLab-v1 <ref type="bibr" target="#b8">[9]</ref> and DeepLab-v2 <ref type="bibr" target="#b9">[10]</ref> as our segmentation networks. We report performance based on both VGG-16 <ref type="bibr" target="#b45">[46]</ref> and ResNet-101 <ref type="bibr" target="#b18">[19]</ref>. For VGG-16 based segmentation network, we use the classification model pretrained on ImageNet <ref type="bibr" target="#b12">[13]</ref> for initialization. For ResNet-101, we use the COCO pretrained model. For the experiments on MS COCO dataset, we all utilize ImageNet pretrained model. Following <ref type="bibr" target="#b31">[32]</ref>, we use the same way to generate the pseudo labels. Given the attention maps, we assign a fixed threshold to the background channel and use the argmax function to obtain the label for each pixel. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We design multiple ablation experiments to perform a sanity check for our method. All the ablation experiments are conducted on the PASCAL VOC 2012 dataset. We report the mIoU scores of the pseudo segmentation labels on the augmented training set and the mIoU scores of the segmentation results on the validation set.</p><p>Local view sampling strategy. First, we study the impact of the sampling strategy on the attention maps. We compare two local view sampling strategies. One is the random sampling strategy, and the other is the uniform sampling strategy. We implement the uniform sampling strategy by sliding the window over the global view uniformly. In this way, every pixel can be enclosed within some local view. For the global view with 448?448 resolution, we set the window size to 320?320 and the stride to 64, obtaining 9 local views. For a fair comparison with the uniform sampling strategy, we randomly sample 9 image patches for the random sampling strategy. The qualities of the pseudo segmentation labels using these two strategies are quite close to each other (random 68.8% v.s. uniform 68.5%). To flexibly adjust the local view number N , we choose the random sampling strategy in our method.</p><p>Patch size and patch number N . The patch size controls the spatial size of the local views. The patch number N denotes the number of the local views sent to the local network. To study their impact on the attention quality, we select 5 different patch sizes [240?240, 280?280, 320?320, 360?360, 400?400]. When studying the patch number N , we select the number of local views from the range of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>. As shown in <ref type="figure" target="#fig_2">Fig. 5</ref>, we observe that when N increases, the quality of the pseudo segmentation labels becomes better. The performance tends to be robust when the local view number is larger than 4. For the patch size, we can see that our method achieves the best performance when the local view size takes 320?320. When the size is larger than 320?320, the quality of the pseudo segmentation labels decreases largely.</p><p>Importance of the proposed local-to-global knowledge transfer. When sending the local views to the local network, we can discover more object regions from the result-Image CAM Local L2G Local + Shape L2G + Shape GT <ref type="figure">Figure 6</ref>. Comparison of the segmentation results under different network settings. We can observe that the combining L2G and shape transfer yields the best results, especially on local object details.</p><p>ing attention maps. Here, one may raise a question: "Are the attention maps from the local network good enough so that we do not need the transfer process?". To answer this question, we test the quality of the pseudo segmentation labels using the attention maps from the local network. As shown in Tab. 1, we can see the performance of the local network is slightly better than the baseline CAM <ref type="bibr" target="#b64">[65]</ref>. However, the performance is far lower than L2G (48.5% v.s. 56.8%). We also show some qualitative results of the attention maps in <ref type="figure" target="#fig_1">Fig. 4</ref> and segmentation results in <ref type="figure">Fig. 6</ref>. This indicates the local-to-global attention transfer strategy is a more efficient way to leverage the rich object attention knowledge captured by the local network.</p><p>In addition, we further extend the above experiments by introducing the shape information. The corresponding re-sults can be found in Tab. 2. It has been demonstrated in <ref type="bibr" target="#b31">[32]</ref> that saliency shape information can significantly improve attention quality. However, when the proposed L2G is used, the mIoU scores on both the trainaug and validation sets can be largely improved. We will show more numerical results on segmentation in the next subsection.</p><p>L2G v.s. Sliding window. The key of our method is to leverage the local attention maps to facilitate the global network to discover more integral object regions. One direct way to implement this idea is to utilize the sliding window strategy during inference and aggregate the attention maps from different image patches. We compare our L2G with the sliding window strategy. Specifically, for the sliding window strategy, the window size and the stride are set to 320?320 and 64, respectively. For our L2G, we set the local view size to 320?320 and the number of randomly sampled patches to 4 at a time.</p><p>As shown in Tab. 1, the local-to-global attention transfer strategy achieves much better results than the baseline CAM <ref type="bibr" target="#b64">[65]</ref>, which verifies the effectiveness of our method. However, the results of the sliding window strategy are even worse than the original CAM. We argue that the sliding window strategy is not suitable for mining non-discriminative object regions as the trained model is still based on inputs with the global view. This makes the undiscovered object regions that have different appearances with the distinctive regions hard to respond when processing the global view.</p><p>Classification loss in the global network. The local network is equipped with the classification loss to guide the attention generation. One may ask the question "Does the global network also need the classification loss?" To answer this question, we have attempted to add a classification loss to the global network. We observe that the attention maps generated from the global network locate very small object regions when adding the classification loss. The quality of the pseudo segmentation labels decreases largely from 70.3% to 53.8%. We argue that the classification loss and the attention transfer loss play opposite roles. The classification loss makes the attention be more discriminative. The attention transfer loss aids in transferring the attention on non-discriminative regions to the global network. Thus, the attention maps become worse.</p><p>Local and global backbone sharing. Here, we explore the performance gap between with/without the local and global network backbone sharing. When the local and global networks share the same backbone, the mIoU score of the pseudo segmentation labels on the trainaug set is 69.2%. After training the segmentation network, the mIoU score on the validation set is 70.9%. When the local and global networks utilize different backbones, the mIoU score of the pseudo segmentation labels can be improved by 1.1%. The final segmentation result also attains 1.2% mIoU gains. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with the State-of-the-Arts</head><p>We first compare the quality of our produced attention maps with the previous state-of-the-art WSSS methods. Our attention maps are converted to pseudo segmentation labels. As shown in Tab. 3 and Tab. 4, it is obvious that the attention maps generated by our method are better than other methods no matter whether the saliency maps are used. Without the saliency maps, the mIoU score on the PASCAL VOC train set reaches 56.2%, better than SEAM <ref type="bibr" target="#b51">[52]</ref> by 0.8%. After applying the saliency maps to the transfer process, the mIoU score reaches 71.9%, much better than EPS <ref type="bibr" target="#b31">[32]</ref> (71.9% v.s. 69.4%).</p><p>We use the pseudo segmentation labels to train the DeepLab segmentation model directly. We compare the segmentation performance of our method with previous state-of-the-art methods. Tab. 5 and Tab. 6 list the segmentation results of our method and the recent state-of-the-art methods on the PASCAL VOC dataset. As we can see, compared to the previous WSSS methods, our method achieves the best results on both the validation and test sets. The work most relevant to our method is EPS <ref type="bibr" target="#b31">[32]</ref>, which explicitly uses the saliency maps as supervision. The differences between our method and EPS have been explained in Sec. 3.4. As shown in Tab. 6, we can see that our method can improve the results of EPS by around 1%. Besides, as shown in Tab. 7, our results on the challenging MS COCO dataset are much better than the previous methods, which also demonstrates the effectiveness of our local-to-global strategy. The mIoU of the pseudo labels for our method is 43.4%, much better than that of EPS (37.2%).</p><p>Discussion. It is worthy to note that our local network is just a simple classification model. Because of the flexibility of the proposed framework, we can replace the local network with more complicated attention models to further improve the results. Thus, we believe there is still a large room to improve the proposed framework, and we also hope our local-to-global knowledge transfer method could provide researchers with a new research direction.</p><p>Analysis of failure cases. First, some non-target objects are wrongly recognized as the target classes as shown in the first two rows of <ref type="figure">Fig. 7</ref>. In our L2G, we only use ResNet to <ref type="table">Table 7</ref>. Quantitative comparisons to previous state-of-the-art approaches on MS COCO validation set. All the segmentation results are based on VGGNet backbone <ref type="bibr" target="#b45">[46]</ref> except L2G* using ResNet-101 backbone <ref type="bibr" target="#b18">[19]</ref>. extract attention. Designing more advanced classification models, such as transformers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b59">60]</ref>, could improve the results. Second, the shape of the discovered objects is still being further improved (the last two rows). Using stronger saliency models or over-segmentation methods could, to some extent, solve this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Result GT <ref type="figure">Figure 7</ref>. Two failure segmentation examples of our L2G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel local-to-global attention transfer method to attain object attentions. By leveraging the complementary attention captured by the local network from the local views and introducing the shape constraint to the attention transfer process, our method achieves the best results on both the validation and test sets of PAS-CAL VOC 2012 and the validation set of MS COCO 2014. We hope the proposed approach could facilitate the research on vision tasks relying on high-quality attention maps.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Motivation of our L2G attention knowledge transfer method. The top row shows the original image (global view) and multiple image patches after random crop (local views).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative comparison of attention maps from different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Ablations on the local view size and number N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison of pseudo segmentation labels on the PASCAL VOC train set with no saliency maps.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Table 4. Comparisons of</cell></row><row><cell></cell><cell></cell><cell cols="2">pseudo segmentation labels</cell></row><row><cell></cell><cell></cell><cell cols="2">on the PASCAL VOC train</cell></row><row><cell></cell><cell></cell><cell cols="2">set with saliency maps in-</cell></row><row><cell></cell><cell></cell><cell>corporated.</cell><cell></cell></row><row><cell>Methods</cell><cell>mIoUtrain</cell><cell></cell><cell></cell></row><row><cell>CAM [65]</cell><cell>48.0</cell><cell>Methods</cell><cell>mIoUtrain</cell></row><row><cell>SC-CAM [6] SEAM [52]</cell><cell>50.9 55.4</cell><cell>SGAN [58]</cell><cell>62.8</cell></row><row><cell>ADvCAM [31]</cell><cell>55.6</cell><cell>EPS [32]</cell><cell>69.4</cell></row><row><cell>L2G (ours)</cell><cell>56.2</cell><cell>L2G (ours)</cell><cell>71.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Quantitative comparisons to previous state-of-the-art approaches on PASCAL VOC 2012 validation and test sets. All the segmentation results are based on the DeepLab with VGGNet backbone<ref type="bibr" target="#b45">[46]</ref>. Pub.: Publication, Seg.:Segmentation network, Sup.: Supervision, I.: Image-level label, S.: saliency maps from the off-the-shelf saliency model.</figDesc><table><row><cell>Methods</cell><cell>Pub.</cell><cell cols="4">Seg. Sup. Val (%) Test (%)</cell></row><row><cell>AffinityNet [2]</cell><cell cols="2">CVPR'18 V1</cell><cell>I.</cell><cell>58.4</cell><cell>60.5</cell></row><row><cell>MCOF [51]</cell><cell cols="4">CVPR'18 V1 I.+S. 56.2</cell><cell>57.6</cell></row><row><cell>DSRG [24]</cell><cell cols="4">CVPR'18 V2 I.+S. 59.0</cell><cell>60.4</cell></row><row><cell>SeeNet [22]</cell><cell cols="4">NeurIPS'18 V1 I.+S. 61.1</cell><cell>60.7</cell></row><row><cell>FickleNet [30]</cell><cell cols="4">CVPR'19 V2 I.+S. 61.2</cell><cell>61.9</cell></row><row><cell>OAA + [26]</cell><cell cols="4">ICCV'19 V1 I.+S. 63.1</cell><cell>62.8</cell></row><row><cell>BES [8]</cell><cell cols="2">ECCV'20 V1</cell><cell>I.</cell><cell>60.1</cell><cell>61.1</cell></row><row><cell>MCIS [48]</cell><cell cols="4">ECCV'20 V1 I.+S. 63.5</cell><cell>63.6</cell></row><row><cell>Multi-Est. [16]</cell><cell cols="4">ECCV'20 V1 I.+S. 64.6</cell><cell>64.2</cell></row><row><cell>ICD [15]</cell><cell cols="4">CVPR'20 V1 I.+S. 64.0</cell><cell>63.9</cell></row><row><cell>ECS-Net [49]</cell><cell cols="2">ICCV'21 V1</cell><cell>I.</cell><cell>62.1</cell><cell>63.4</cell></row><row><cell>DRS [27]</cell><cell cols="4">AAAI'21 V1 I.+S. 63.5</cell><cell>64.5</cell></row><row><cell cols="5">Group-WSSS [34] AAAI'21 V2 I.+S. 63.3</cell><cell>63.6</cell></row><row><cell>OAA++ + [25]</cell><cell cols="4">PAMI'21 V1 I.+S. 63.7</cell><cell>63.2</cell></row><row><cell>NSROM [59]</cell><cell cols="4">CVPR'21 V2 I.+S. 65.5</cell><cell>65.3</cell></row><row><cell>EPS [32]</cell><cell cols="4">CVPR'21 V1 I.+S. 66.6</cell><cell>67.9</cell></row><row><cell>EPS [32]</cell><cell cols="4">CVPR'21 V2 I.+S. 67.0</cell><cell>67.3</cell></row><row><cell>L2G (ours)</cell><cell>-</cell><cell cols="3">V1 I.+S. 68.1</cell><cell>68.8</cell></row><row><cell>L2G (ours)</cell><cell>-</cell><cell cols="3">V2 I.+S. 68.5</cell><cell>68.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Quantitative comparisons to previous state-of-the-art approaches on PASCAL VOC 2012 validation and test sets. All the segmentation results are based on the ResNet backbone<ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b55">56]</ref>. Our method achieves the best results.</figDesc><table><row><cell>Methods</cell><cell cols="5">Publication Seg. Sup. Val (%) Test (%)</cell></row><row><cell>AffinityNet [2]</cell><cell cols="2">CVPR'18 V1</cell><cell>I.</cell><cell>61.7</cell><cell>63.7</cell></row><row><cell>MCOF [51]</cell><cell cols="3">CVPR'18 V1 I.+S.</cell><cell>60.3</cell><cell>61.2</cell></row><row><cell>DSRG [24]</cell><cell cols="3">CVPR'18 V2 I.+S.</cell><cell>61.4</cell><cell>63.2</cell></row><row><cell>SeeNet [22]</cell><cell cols="3">NeurIPS'18 V1 I.+S.</cell><cell>63.1</cell><cell>62.8</cell></row><row><cell>IRNet [1]</cell><cell cols="2">CVPR'19 V1</cell><cell>I.</cell><cell>63.5</cell><cell>64.8</cell></row><row><cell>FickleNet [30]</cell><cell cols="3">CVPR'19 V2 I.+S.</cell><cell>64.9</cell><cell>65.3</cell></row><row><cell>OAA + [26]</cell><cell cols="3">ICCV'19 V1 I.+S.</cell><cell>65.2</cell><cell>66.4</cell></row><row><cell>SSDD [45]</cell><cell cols="2">ICCV'19 V1</cell><cell>I.</cell><cell>66.1</cell><cell>66.8</cell></row><row><cell>SEAM [52]</cell><cell cols="2">CVPR'20 V2</cell><cell>I.</cell><cell>64.5</cell><cell>65.7</cell></row><row><cell>SC-CAM [6]</cell><cell cols="2">CVPR'20 V2</cell><cell>I.</cell><cell>66.1</cell><cell>65.9</cell></row><row><cell>ICD [15]</cell><cell cols="3">CVPR'20 V1 I.+S.</cell><cell>67.8</cell><cell>68.0</cell></row><row><cell>BES [8]</cell><cell cols="2">ECCV'20 V2</cell><cell>I.</cell><cell>65.7</cell><cell>66.6</cell></row><row><cell>MCIS [48]</cell><cell cols="3">ECCV'20 V1 I.+S.</cell><cell>66.2</cell><cell>66.9</cell></row><row><cell>Multi-Est. [16]</cell><cell cols="3">ECCV'20 V1 I.+S.</cell><cell>67.2</cell><cell>66.7</cell></row><row><cell>LIID [40]</cell><cell cols="4">PAMI'20 V2 I.+IS. 66.5</cell><cell>67.5</cell></row><row><cell>DRS [27]</cell><cell cols="3">AAAI'21 V2 I.+S.</cell><cell>71.2</cell><cell>71.4</cell></row><row><cell cols="4">Group-WSSS [34] AAAI'21 V2 I.+S.</cell><cell>68.2</cell><cell>68.5</cell></row><row><cell>ECS-Net [49]</cell><cell cols="2">ICCV'21 V1</cell><cell>I.</cell><cell>66.6</cell><cell>67.6</cell></row><row><cell>PMM [35]</cell><cell cols="2">ICCV'21 PSP</cell><cell>I.</cell><cell>68.5</cell><cell>69.0</cell></row><row><cell>CDA [47]</cell><cell cols="2">ICCV'21 V2</cell><cell>I.</cell><cell>66.1</cell><cell>66.8</cell></row><row><cell>CGNet [29]</cell><cell cols="2">ICCV'21 V1</cell><cell>I.</cell><cell>68.4</cell><cell>68.2</cell></row><row><cell>AuxSegNet [57]</cell><cell cols="3">ICCV'21 V1 I.+S.</cell><cell>69.0</cell><cell>68.6</cell></row><row><cell>AdvCAM [31]</cell><cell cols="2">CVPR'21 V2</cell><cell>I.</cell><cell>68.1</cell><cell>68.0</cell></row><row><cell>NSROM [59]</cell><cell cols="3">CVPR'21 V2 I.+S.</cell><cell>70.4</cell><cell>70.2</cell></row><row><cell>EDAM [55]</cell><cell cols="3">CVPR'21 V1 I.+S.</cell><cell>70.9</cell><cell>70.6</cell></row><row><cell>EPS [32]</cell><cell cols="3">CVPR'21 V1 I.+S.</cell><cell>71.0</cell><cell>71.8</cell></row><row><cell>EPS [32]</cell><cell cols="3">CVPR'21 V2 I.+S.</cell><cell>70.9</cell><cell>70.8</cell></row><row><cell>L2G (ours)</cell><cell>-</cell><cell cols="2">V1 I.+S.</cell><cell>72.0</cell><cell>73.0</cell></row><row><cell>L2G (ours)</cell><cell>-</cell><cell cols="2">V2 I.+S.</cell><cell>72.1</cell><cell>71.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://host.robots.ox.ac.uk:8080/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of instance segmentation with inter-pixel relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2209" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4981" to="4990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Single-stage semantic segmentation from image labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4253" to="4262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mixupcam: Weakly-supervised semantic segmentation via uncertainty regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<editor>Brit. Mach. Vis. Conf.</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation via sub-category exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8991" to="9000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end boundary exploration for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shancheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2381" to="2390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation with boundary exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="347" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attentionbased dropout layer for weakly supervised single object localization and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. In Int</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning integral objects with intra-class discriminator for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4283" to="4292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Employing multi-estimations for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="332" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Born again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="page" from="1607" to="1616" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Knowledge adaptation for efficient semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="578" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst. Worksh</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-erasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="549" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bottomup top-down cues for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Massiceti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Kumar Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Worksh. on Energy Minimization Methods in Comput. Vis. Pattern Recog</title>
		<imprint>
			<biblScope unit="page" from="263" to="277" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7014" to="7023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Online attention accumulation for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Hao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Integral object mining via online attention accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Kai</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2070" to="2079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discriminative region suppression for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangeun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1754" to="1761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="695" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unlocking the potential of ordinary classifier: Class-specific adversarial erasing framework for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeokjun</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Hoon</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehee</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6994" to="7003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ficklenet: Weakly and semi-supervised semantic image segmentation using stochastic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5267" to="5276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Antiadversarially manipulated attributions for weakly and semisupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4071" to="4080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Railroad is not a train: Saliency as pseudo-pixel supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5495" to="5505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tell me where to look: Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9215" to="9223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Group-wise semantic mining for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pseudo-mask matters in weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6964" to="6973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for realtime salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3917" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Structured knowledge distillation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2604" to="2613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Leveraging instance-, imageand dataset-level information for weakly supervised instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Huan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Song</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fully convolutional multi-class multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1713" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-supervised difference detection for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wataru</forename><surname>Shimoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5208" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Context decoupling augmentation for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mining cross-image semantics for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="347" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ecs-net: Improving weakly supervised semantic segmentation by using connections between class activation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7283" to="7292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning randomwalk label propagation for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7158" to="7166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation by iteratively mining common object features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1354" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yude</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12275" to="12284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7268" to="7277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Embedded discriminative attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Harold</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16765" to="16774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Leveraging auxiliary tasks with affinity learning for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farid</forename><surname>Boussaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferdous</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6984" to="6993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Saliency guided self-attention network for weakly and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE Access</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="14413" to="14423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Nonsalient region object mining for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhou</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Sen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenmin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2623" to="2632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3903" to="3911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Reliability does matter: An end-toend weakly supervised semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12765" to="12772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Multiminer: Object-adaptive region mining for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuangqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07834</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
