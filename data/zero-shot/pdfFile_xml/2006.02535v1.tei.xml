<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A SURVEY ON DEEP LEARNING TECHNIQUES FOR STEREO-BASED DEPTH ESTIMATION 1 A Survey on Deep Learning Techniques for Stereo-based Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Laga</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><forename type="middle">Valentin</forename><surname>Jospin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farid</forename><surname>Boussaid</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
						</author>
						<title level="a" type="main">A SURVEY ON DEEP LEARNING TECHNIQUES FOR STEREO-BASED DEPTH ESTIMATION 1 A Survey on Deep Learning Techniques for Stereo-based Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-CNN</term>
					<term>Deep Learning</term>
					<term>3D Reconstruction</term>
					<term>Stereo Matching</term>
					<term>Multi-view Stereo</term>
					<term>Disparity Estimation</term>
					<term>Feature Leaning</term>
					<term>Feature Matching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating depth from RGB images is a long-standing ill-posed problem, which has been explored for decades by the computer vision, graphics, and machine learning communities. Among the existing techniques, stereo matching remains one of the most widely used in the literature due to its strong connection to the human binocular system. Traditionally, stereo-based depth estimation has been addressed through matching hand-crafted features across multiple images. Despite the extensive amount of research, these traditional techniques still suffer in the presence of highly textured areas, large uniform regions, and occlusions. Motivated by their growing success in solving various 2D and 3D vision problems, deep learning for stereo-based depth estimation has attracted a growing interest from the community, with more than 150 papers published in this area between 2014 and 2019. This new generation of methods has demonstrated a significant leap in performance, enabling applications such as autonomous driving and augmented reality. In this article, we provide a comprehensive survey of this new and continuously growing field of research, summarize the most commonly used pipelines, and discuss their benefits and limitations. In retrospect of what has been achieved so far, we also conjecture what the future may hold for deep learning-based stereo for depth estimation research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>D EPTH estimation from one or multiple RGB images is a long standing ill-posed problem, with applications in various domains such as robotics, autonomous driving, object recognition and scene understanding, 3D modeling and animation, augmented reality, industrial control, and medical diagnosis. This problem has been extensively investigated for many decades. Among all the techniques that have been proposed in the literature, stereo matching is traditionally the most explored one due to its strong connection to the human binocular system.</p><p>The first generation of stereo-based depth estimation methods relied typically on matching pixels across multiple images captured using accurately calibrated cameras. Although these techniques can achieve good results, they are still limited in many aspects. For instance, they are not suitable when dealing with occlusions, featureless regions, or highly textured regions with repetitive patterns. Interestingly, we, as humans, are good at solving such illposed inverse problems by leveraging prior knowledge. For example, we can easily infer the approximate sizes of objects, their relative locations, and even their approximate relative distance to our eye(s). We can do this because all the previously seen objects and scenes have enabled us to build prior knowledge and develop mental models of how the 3D world looks like. The second generation of methods tries to leverage this prior knowledge by formulating the problem as a learning task. The advent of deep learning techniques in computer vision <ref type="bibr" target="#b0">[1]</ref> coupled with the increasing availability of large training datasets, have led to a third generation of methods that are able to recover the lost dimension. Despite being recent, these methods have demonstrated exciting and promising results on various tasks related to computer vision and graphics. In this article, we provide a comprehensive and structured review of the recent advances in stereo image-based depth estimation using deep learning techniques. These methods use two or more images captured with spatiallydistributed RGB cameras <ref type="bibr" target="#b0">1</ref> . We have gathered more than 150 papers, which appeared between January 2014 and December 2019 in leading computer vision, computer graphics, and machine learning conferences and journals 2 . The goal is to help the reader navigate in this emerging field, which has gained a significant momentum in the past few years.</p><p>The major contributions of this article are as follows;</p><p>? To the best of our knowledge, this is the first article that surveys stereo-based depth estimation using deep learning techniques. We present a comprehensive review of more than 150 papers, which appeared in the past six years in leading conferences and journals. <ref type="bibr">?</ref> We provide a comprehensive taxonomy of the stateof-the-art. We first describe the common pipelines <ref type="bibr" target="#b0">1</ref>. Deep learning-based depth estimation from monocular images and videos is an emerging field and requires a separate survey.</p><p>2. At the time of writing this article.</p><p>arXiv:2006.02535v1 [cs.CV] 1 Jun 2020 and then discuss the similarities and differences between methods within each pipeline. <ref type="bibr">?</ref> We provide a comprehensive review and an insightful analysis on all the aspects of the problem, including the training data, the network architectures and their effect on the reconstruction performance, the training strategies, and the generalization ability. <ref type="bibr">?</ref> We provide a comparative summary of the properties and performances of some key methods using publicly available datasets and in-house images. The latter have been chosen to test how these methods would perform on completely new scenarios.</p><p>The rest of this article is organized as follows; Section 2 formulates the problem and lays down the taxonomy. Section 3 surveys the various datasets which have been used to train and test stereo-based depth reconstruction algorithms. Section 4 focuses on the works that use deep learning architectures to learn how to match pixels across images. Section 5 reviews the end-to-end methods for stereo matching, while Section 6 discusses how these methods have been extended to the multi-view stereo case. Section 7 focuses on the training procedures including the choice of the loss functions and the degree of supervision. Section 8 discusses the performance of key methods. Finally, Section 9 discusses the potential future research directions, while Section 10 summarizes the main contributions of this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SCOPE AND TAXONOMY</head><p>Let I = {I k , k = 1, . . . , n} be a set of n ? 1 RGB images of the same 3D scene, captured using cameras whose intrinsic and extrinsic parameters can be known or unknown. The goal is to estimate one or multiple depth maps, which can be from the same viewpoint as the input <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, or from a new arbitrary viewpoint <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. This article focuses on deep learning methods for stereo-based depth estimation, i.e., n = 2 in the case of stereo matching, and n &gt; 2 for the case of Multi-View Stereo (MVS). Monocular and video-based depth estimation methods are beyond the scope of this article and require a separate survey. Learning-based depth reconstruction can be summarized as the process of learning a predictor f ? that can infer from the set of images I, a depth mapD that is as close as possible to the unknown depth map D. In other words, we seek to find a function f ? such that L(I) = d (f ? (I), D) is minimized. Here, ? is a set of parameters, and d(?, ?) is a certain measure of distance between the real depth map D and the reconstructed depth map f ? (I). The reconstruction objective L is also known as the loss function.</p><p>We can distinguish two main categories of methods. Methods in the first class (Section 4) mimic the traditional stereo-matching techniques <ref type="bibr" target="#b10">[11]</ref> by explicitly learning how to match, or put in correspondence, pixels across the input images. Such correspondences can then be converted into an optical flow or a disparity map, which in turn can be converted into depth at each pixel in the reference image. The predictor f is composed of three modules: a feature extraction module, a feature matching and cost aggregation module, and a disparity/depth estimation module. Each module is trained independently from the others.</p><p>The second class of methods (Section 5) solves the stereo matching problem using a pipeline that is trainable endto-end. Two main classes of methods have been proposed. Early methods formulated the depth estimation as a regression problem. In other words, the depth map is directly regressed from the input without explicitly matching features across the views. While these methods are simple and fast at runtime, they require a large amount of training data, which is hard to obtain. Methods in the second class mimic the traditional stereo matching pipeline by breaking the problem into stages composed of differentiable blocks and thus allowing end-to-end training. While a large body of the literature focused on pairwise stereo methods, several papers have also addressed the multi-view stereo case and these will be reviews in Section 6.</p><p>In all methods, the estimated depth maps can be further refined using refinement modules <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> and/or progressive reconstruction strategies where the reconstruction is refined every time new images become available.</p><p>Finally, the performance of deep learning-based stereo methods depends not only on the network architecture but also on the datasets on which they have been trained (Section 3) and on the training procedure used to optimise their parameters (Section 7). The latter includes the choice of the loss functions and the supervision mode, which can be fully supervised with 3D annotations, weakly supervised, or self-supervised. We will discuss all these aspects in the subsequent sections. <ref type="table">Table 1</ref> summarizes some of the datasets that have been used to train and test deep learning-based depth estimation algorithms. Below, we discuss these datasets based on their sizes, their spatial and depth resolution, the type of depth annotation they provide, and the domain gap (or shift) issue faced by many deep learning-based algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATASETS</head><p>(1) Dataset size. The first datasets, which appeared prior to 2016, are of small scale due to the difficulty of creating ground-truth 3D annotations. An example is the two KITTI datasets <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b20">[21]</ref>, which contain 200 stereo pairs with their corresponding disparity ground-truth. They have been extensively used to train and test patch-based CNNs for stereo matching algorithms (see <ref type="bibr">Section 4)</ref>, which have a small receptive field. As such a single stereo pair can result in thousands of training samples. However, in end-to-end architectures (Sections 5 and 6), a stereo pair corresponds to only one sample. End-to-end networks have a large number of parameters, and thus require large datasets for efficient training. While collecting large image datasets is very easy, e.g., by using video sequences as in e.g., NYU2 <ref type="bibr" target="#b16">[17]</ref>, ETH3D <ref type="bibr" target="#b24">[25]</ref>, SUN3D <ref type="bibr" target="#b18">[19]</ref>, and ETH3D <ref type="bibr" target="#b24">[25]</ref>, annotating them with 3D labels is time consuming. Recent works, e.g., the AppoloScape <ref type="bibr" target="#b33">[34]</ref> and A2D2 <ref type="bibr" target="#b34">[35]</ref>, use LIDAR to acquire dense 3D annotations.</p><p>Data augmentation strategies, e.g., by applying geometric and photometric transformations to the images that are available, have been extensively used in the literature. There are, however, a few other strategies that are specific to depth estimation. This includes artificially synthesizing and rendering from 3D CAD models 2D and 2.5D views TABLE 1: Datasets for depth/disparity estimation. "GT": ground-truth, "Tr.": training, "Ts.": testing, "fr.": frames, "Vol.": volumetric, "Eucl": Euclidean, "Ord": ordinal, "Int.": intrinsic, "Ext.": extrinsic. from various (random) viewpoints, poses, and lighting conditions. One can also overlay rendered 3D models on the top of real images. This approach has been used to generate the FlyingThings3D, Monkaa, and Driving datasets of <ref type="bibr" target="#b21">[22]</ref>, and the OmniThings and OmniHouse datasets for benchmarking MVS for omnidirectional images <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>.</p><p>Huang et al. <ref type="bibr" target="#b26">[27]</ref> followed a similar idea but used scenes from video games to generate MVS-Synth, a photo-realistic synthetic dataset prepared for learning-based Multi-View Stereo algorithms.</p><p>The main challenge is that generating large amounts of synthetic data containing varied real-world appearance and motion is not trivial <ref type="bibr" target="#b35">[36]</ref>. As a result, a number of works overcome the need for ground-truth depth information by training their deep networks without 3D supervision, see Section 7.1. Others used traditional depth estimation and structure-from-motion (SfM) techniques to generate 3D annotations. For example, Li et al. <ref type="bibr" target="#b27">[28]</ref> used modern structurefrom-motion and multiview stereo (MVS) methods together with multiview Internet photo collections to create the largescale MegaDepth dataset providing improved depth estimation accuracy via bigger training dataset sizes. This dataset has also been automatically augmented with ordinal depth relations generated using semantic segmentation.</p><p>(2) Spatial and depth resolutions. The disparity/depth information can be either in the form of maps of the same or lower resolution than the input images, or in the form of sparse depth values at some locations in the reference image. Most of the existing datasets are of low spatial resolution. In recent years, however, there has been a growing focus on stereo matching with high-resolution images. An example of a high-resolution dataset is the HR-VS and HR-RS of Yang et al. <ref type="bibr" target="#b31">[32]</ref>, where each RGB pair of resolution 1918 ? 2424 is annotated with a depth map of the same resolution. However, the dataset only contains 800 pairs of stereo images, which is relatively small for end-to-end training. Other datasets such as the ApolloScape <ref type="bibr" target="#b33">[34]</ref> and A2D2 <ref type="bibr" target="#b34">[35]</ref> contain very high resolution images, of the order of 3130 ? 960, with more that 100+ hours of stereo driving videos, in the case of ApolloScape, have been specifically designed to test autonomous driving algorithms.</p><p>(3) Euclidean vs. ordinal depth. Instead of manually annotating images with exact, i.e., Euclidean, depth values, some papers, e.g., MegaDepth <ref type="bibr" target="#b27">[28]</ref>, provide ordinal annotations, i.e., pixel x 1 is closer, farther, or at the same depth, as pixel x 2 . Ordinal annotation is simpler and faster to achieve than Euclidean annotation. In fact, it can be accurately obtained using traditional stereo matching algorithms, since ordinal depth is less sensitive to innacuracies in depth estimation (4) Domain gap. While artificially augmenting training datasets allows enriching existing ones, the domain shift caused by the very different conditions between real and synthetic data can result in a lower accuracy when applied to real-world environments. We will discuss, in Section 7.3, how this domain shift issue has been addressed in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DEPTH BY STEREO MATCHING</head><p>Stereo-based depth reconstruction methods take n = 2 RGB images and produce a disparity map D that minimizes an energy function of the form:</p><formula xml:id="formula_0">E(D) = x C(x, d x ) + x y?Nx E s (d x , d y ).<label>(1)</label></formula><p>Here, x and y are image pixels, and N x is the set of pixels that are within the neighborhood of x. The first term of Eqn. (1) is the matching cost. When using rectified stereo pairs, C(x, d x ) measures the cost of matching the pixel x = (i, j) of the left image with the pixel y = (i, j ? d x ) of the right image. In this case,</p><formula xml:id="formula_1">d x = D(x) ? [d min , d max ]</formula><p>is the disparity at pixel x. Depth can then be inferred by  triangulation. When the disparity range is discritized into n d disparity levels, C becomes a 3D cost volume of size W ? H ? n d . In the more general multiview stereo case, i.e., n ? 2, the cost C(x, d x ) measures the inverse likelihood of x on the reference image having depth d x . The second term of Eqn. (1) is a regularization term used to impose constraints such as smoothness and left-right consistency.</p><p>Traditionally, this problem has been solved using a pipeline of four building blocks <ref type="bibr" target="#b10">[11]</ref>, see <ref type="figure" target="#fig_0">Fig. 1</ref>: (1) feature extraction, (2) feature matching across images, (3) disparity computation, and (4) disparity refinement and postprocessing. The first two blocks construct the cost volume C. The third block regularizes the cost volume and then finds, by minimizing Eqn. <ref type="bibr" target="#b0">(1)</ref>, an initial estimate of the disparity map. The last block refines and post-processes the initial disparity map.</p><p>This section focuses on how these individual blocks have been implemented using deep learning-based methods. <ref type="table" target="#tab_2">Table 2</ref> summarises the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning feature extraction and matching</head><p>Early deep learning techniques for stereo matching replace the hand-crafted features (block A of <ref type="figure">Fig. ??)</ref> with learned features <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b41">[42]</ref>. They take two patches, one centered at a pixel x = (i, j) on the left image and another one centered at pixel y = (i, j ? d) on the right image (with d ? {0, . . . , n d }), compute their corresponding feature vectors using a CNN, and then match them (block B of <ref type="figure">Fig. ??)</ref>, to produce a similarity score C(x, d), using either standard similarity metrics such as the L 1 , the L 2 , and the correlation metric, or metrics learned using a top network. The two components can be trained either separately or jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">The basic network architecture</head><p>The basic network architecture, introduced in <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b41">[42]</ref> and shown in <ref type="figure" target="#fig_1">Fig. 2</ref>-(a), is composed of two CNN encoding branches, which act as descriptor computation modules. The first branch takes a patch around a pixel x = (i, j) on the left image and outputs a feature vector that characterizes that patch. The second branch takes a patch around the pixel y =</p><formula xml:id="formula_2">(i, j ? d), where d ? [d min , d max ]</formula><p>is a candidate disparity. Zbontar and LeCun <ref type="bibr" target="#b38">[39]</ref> and later Zbontar et al. <ref type="bibr" target="#b41">[42]</ref> use an encoder composed of four convolutional layers, see <ref type="figure" target="#fig_1">Fig. 2</ref>-(a). Each layer, except the last one, is followed by a ReLU unit. Zagoruyko and Komodakis <ref type="bibr" target="#b36">[37]</ref> and Han et al. <ref type="bibr" target="#b37">[38]</ref> use a similar architecture but add:</p><p>? max-pooling and subsampling after each layer, except the last one, see <ref type="figure" target="#fig_1">Fig. 2</ref> <ref type="bibr">-(b)</ref>. As such, the network is able to account for larger patch sizes and a larger variation in the viewpoint compared to <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b41">[42]</ref>.</p><p>? a Spatial Pyramid Pooling (SPP) module at the end of each feature extraction branch <ref type="bibr" target="#b36">[37]</ref> so that the network can process patches of arbitrary sizes while producing features of a fixed size, see <ref type="figure" target="#fig_1">Fig. 2</ref>-(c). Its role is to aggregate the features of the last convolutional layer, through spatial pooling, into a feature grid of a fixed size. The module is designed in such a way that the size of the pooling regions varies with the size of the input to ensure that the output feature grid has a fixed size independently of the size of the input patch or image. Thus, the network is able to process patches/images of arbitrary sizes and compute feature vectors of the same dimension without changing its structure or retraining.</p><p>The learned features are then fed to a top module, which returns a similarity score. It can be implemented as a standard similarity metric, e.g., the L 2 distance, the cosine distance, and the (normalized) correlation distance (or inner product) as in the MC-CNN-fast (MC-CNN-fst) architecture of <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b41">[42]</ref>. The main advantage of the correlation over the L 2 distance is that it can be implemented using a layer of 2D <ref type="bibr" target="#b50">[51]</ref> or 1D <ref type="bibr" target="#b21">[22]</ref>   <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b41">[42]</ref>. (b) <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b37">[38]</ref>. (c) <ref type="bibr" target="#b36">[37]</ref>. (d) LW-CNN <ref type="bibr" target="#b47">[48]</ref>.</p><p>(e) FED-D2DRR <ref type="bibr" target="#b48">[49]</ref>. (f) <ref type="bibr" target="#b36">[37]</ref>. branch of the network. As such, correlation layers have been extensively used in the literature <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Instead of using hand-crafted similarity measures, recent works use a decision network composed of fully-connected (FC) layers <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b48">[49]</ref>, which can be implemented as 1?1 convolutions, fully convolutional layers <ref type="bibr" target="#b46">[47]</ref>, or convolutional layers followed by fully-connected layers. The decision network is trained jointly with the feature extraction module to assess the similarity between two image patches. Han et al. <ref type="bibr" target="#b37">[38]</ref> use a top network composed of three fully-connected layers followed by a softmax. Zagoruyko and Komodakis <ref type="bibr" target="#b36">[37]</ref> use two linear fully connected layers (each with 512 hidden units) that are separated by a ReLU activation layer while the MC-CNN-acrt network of Zbontar et al. <ref type="bibr" target="#b41">[42]</ref> use up to five fully-connected layers. In all cases, the features computed by the two branches of the feature encoding module are first concatenated and then fed to the top network. Hartmann et al. <ref type="bibr" target="#b46">[47]</ref>, on the other hand, aggregate the features coming from multiple patches using mean pooling before feeding them to a decision network. The main advantage of aggregation by pooling over concatenation is that the former can handle any arbitrary number of patches without changing the architecture of the network or re-training it. As such, it is suitable for computing multipatch similarity.</p><p>Using a decision network instead of hand-crafted similarity measures enables learning, from data, the appropriate similarity measure instead of imposing one at the outset. It is more accurate than using a correlation layer but is significantly slower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Network architecture variants</head><p>Since its introduction, the baseline architecture has been extended in several ways in order to: (1) improve training using residual networks (ResNet) <ref type="bibr" target="#b45">[46]</ref>, (2) enlarge the receptive field of the network without losing in resolution or in computation efficiency <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b51">[52]</ref>, (3) handling multiscale features <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b39">[40]</ref>, (4) reducing the number of forward passes <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b43">[44]</ref>, and (5) easing the training procedure by learning similarity without explicitly learning features <ref type="bibr" target="#b36">[37]</ref>  <ref type="bibr" target="#b45">[46]</ref> add residual blocks with multilevel weighted residual connections to facilitate the training of very deep networks. Its particularity is that the network learns by itself how to adjust the contribution of the added skip connections. It was demonstrated that this architecture outperforms the base network of Zbontar et al. <ref type="bibr" target="#b38">[39]</ref>. 4.1.2.2 Enlarging the receptive field of the network: The scale of the learned features is defined by (1) the size of the input patches, (2) the receptive field of the network, and (3) the kernel size of the convolutional filters and pooling operations used in each layer. While increasing the kernel sizes allows the capture of more global interactions between the image pixels, it induces a high computational cost. Also, the conventional pooling, as used in <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b41">[42]</ref>, reduces resolution and could cause the loss of fine details, which is not suitable for dense correspondence estimation.</p><p>To enlarge the receptive field without losing resolution or increasing the computation time, some techniques, e.g., <ref type="bibr" target="#b51">[52]</ref>, use dilated convolutions, i.e., large convolutional filters but with holes and thus they are computationally efficient. Other techniques, e.g., <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, use spatial pyramid pooling (SPP) modules placed at different locations in the network, see <ref type="figure" target="#fig_1">Fig. 2</ref>-(c-e). For instance, Park et al. <ref type="bibr" target="#b47">[48]</ref>, who introduced FW-CNN for stereo matching, append an SPP module at the end of the decision network, see <ref type="figure" target="#fig_1">Fig. 2</ref></p><formula xml:id="formula_3">-(d).</formula><p>As a result, the receptive field can be enlarged. However, for each pixel in the reference image, both the fully-connected layers and the pooling operations need to be computed n d times where n d is the number of disparity levels. To avoid this, Ye et al. <ref type="bibr" target="#b48">[49]</ref> place the SPP module at the end of each feature computation branch, see Figs. 2-(c) and (e). In this way, it is only computed once for each patch. Also, Ye et al. <ref type="bibr" target="#b48">[49]</ref> employ multiple one-stride poolings, with different window sizes, to different layers and then concatenate their outputs to generate the feature maps, see <ref type="figure" target="#fig_1">Fig. 2</ref>-(e). 4.1.2.3 Learning multiscale features: The methods described so far can be extended to learn features at multiple scales by using multi-stream networks, one stream per patch size <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b39">[40]</ref>, see <ref type="figure" target="#fig_2">Fig. 3</ref>. Zagoruyko and Komodakis <ref type="bibr" target="#b36">[37]</ref> propose a two-stream network, which is essentially a network composed of two siamese networks combined at the output by a top network, see <ref type="figure" target="#fig_2">Fig. 3</ref>-(a). The first siamese network, called central high-resolution stream, receives as in-  put two 32?32 patches centered around the pixel of interest. The second network, called surround low-resolution stream, receives as input two 64 ? 64 patches but down-sampled to 32?32. The output of the two streams are then concatenated and fed to a top decision network, which returns a matching score. Chen et al. <ref type="bibr" target="#b39">[40]</ref> use a similar approach but instead of aggregating the features computed by the two streams prior to feeding them to the top decision network, it appends a top network on each stream to produce a matching score. The two scores are then aggregated by voting, see <ref type="figure" target="#fig_2">Fig. 3</ref></p><formula xml:id="formula_4">-(b).</formula><p>The main advantage of the multi-stream architecture is that it can compute features at multiple scales in a single forward pass. It, however, requires one stream per scale, which is not practical if more than two scales are needed. 4.1.2.4 Reducing the number of forward passes: Using the approaches described so far, inferring the raw cost volume from a pair of stereo images is performed using a moving window-like approach, which would require multiple forward passes, n d forward passes per pixel where n d is the number of disparity levels. However, since correlations are highly parallelizable, the number of forward passes can be significantly reduced. For instance, Luo et al. <ref type="bibr" target="#b43">[44]</ref> reduce the number of forward passes to one pass per pixel by using a siamese network, whose first branch takes a patch around a pixel while the second branch takes a larger patch that expands over all possible disparities. The output is a single 64D representation for the left branch, and n d ? 64 for the right branch. A correlation layer then computes a vector of length n d , where its d?th element is the cost of matching the pixel x on the left image with the pixel x ? d on the rectified right image.</p><p>Zagoruyko and Komodakis <ref type="bibr" target="#b36">[37]</ref> showed that the outputs of the two feature extraction sub-networks need to be computed only once per pixel, and do not need to be recomputed for every disparity under consideration. This can be done in a single forward pass, for the entire image, by propagating full-resolution images instead of small patches. Also, the output of the top network composed of fullyconnected layers in the accurate architecture (i.e., MC-CNN-Accr) can be computed in a single forward pass by replacing the fully-connected layers with convolutional layers of 1 ? 1 kernels. However, it still requires one forward pass for each disparity under consideration. 4.1.2.5 Learning similarity without feature learning: Joint training of feature extraction and similarity computation networks unifies the feature learning and the metric learning steps. Zagoruyko and Komodakis <ref type="bibr" target="#b36">[37]</ref> propose another architecture that does not have a direct notion of features, see <ref type="figure">Fig.</ref> 2-(f). In this architecture, the left and right patches are packed together and fed jointly into a two-channel network composed of convolution and ReLU layers followed by a set of fully connected layers. Instead of computing features, the network directly outputs the similarity between the input pair of patches. Zagoruyko and Komodakis <ref type="bibr" target="#b36">[37]</ref> showed that this architecture is easy to train. However, it is expensive at runtime since the whole network needs to be run n d times per pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Training procedures</head><p>The networks described in this section are composed of a feature extraction block and a feature matching block. Since the goal is to learn how to match patches, these two modules are jointly trained either in a supervised (Section 4.1.3.1) or in a weakly supervised manner (Section 4.1.3.2). 4.1.3.1 Supervised training: Existing methods for supervised training use a training set composed of positive and negative examples. Each positive (respectively negative) example is a pair composed of a reference patch and its matching patch (respectively a non-matching one) from another image. Training either takes one example at a time, positive or negative, and adapts the similarity <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, or takes at each step both a positive and a negative example, and maximizes the difference between the similarities, hence aiming at making the two patches from the positive pair more similar than the two patches from the negative pair <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b44">[45]</ref>. This latter scheme is known as Triplet Contrastive learning.</p><p>Zbontar et al. <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b41">[42]</ref> use the ground-truth disparities of the KITTI2012 <ref type="bibr" target="#b14">[15]</ref> or Middlebury <ref type="bibr" target="#b19">[20]</ref> datasets. For each known disparity, the method extracts one negative pair and one positive pair as training examples. As such, the method is able to extract more than 25 million training samples from KITTI2012 <ref type="bibr" target="#b14">[15]</ref> and more than 38 million from the Middlebury dataset <ref type="bibr" target="#b19">[20]</ref>. This method has been also used by Chen et al. <ref type="bibr" target="#b39">[40]</ref>, Zagoruyku and Komodakis <ref type="bibr" target="#b36">[37]</ref>, and Han et al. <ref type="bibr" target="#b37">[38]</ref>. The amount of training data can be further augmented by using data augmentation techniques, e.g., flipping patches and rotating them in various directions.</p><p>Although the supervised learning works very well, the complexity of the neural network models requires very large labeled training sets, which are hard or costly to collect for real applications (e.g., consider the stereo reconstruction of the Mars landscape). Even when such large sets are available, the ground truth is usually produced from depth sensors and often contains noise that reduces the effectiveness of the supervised learning <ref type="bibr" target="#b52">[53]</ref>. This can be mitigated by augmenting the training set with random perturbations <ref type="bibr" target="#b38">[39]</ref> or synthetic data <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b53">[54]</ref>. However, synthesis procedures are hand-crafted and do not account for the regularities specific to the stereo system and target scene at hand. Loss functions. Supervised stereo matching networks are trained to minimize a matching loss, which is a function that measures the discrepancy between the ground-truth and the predicted matching scores for each training sample. It can be defined using (1) the L 1 distance <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b45">[46]</ref>, (2) the hinge loss <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b45">[46]</ref>, or (3) the cross-entropy loss <ref type="bibr" target="#b43">[44]</ref>. 4.1.3.2 Weakly supervised learning: Weakly supervised techniques exploit one or more stereo constraints to reduce the amount of manual labelling. Tulyakov et al. <ref type="bibr" target="#b49">[50]</ref> consider Multi-Instance Learning (MIL) in conjunction with stereo constraints and coarse information about the scene to train stereo matching networks with datasets for which ground truth is not available. Unlike supervised techniques, which require pairs of matching and non-matching patches, the training set is composed of N triplets. Each triplet is composed of: (1) W reference patches extracted on a horizontal line of the reference image, (2) W positive patches extracted from the corresponding horizontal line on the right image, and (3) W negative patches, i.e., patches that do not match the reference patches, extracted from another horizontal line on the right image. As such, the training set can automatically be constructed from stereo pairs without manual labelling.</p><p>The method is then trained by exploiting five constraints: the epipolar constraint, the disparity range constraint, the uniqueness constraint, the continuity (smoothness) constraint, and the ordering constraint. They then define three losses that use different subsets of these constraints, namely:</p><p>? The Multi Instance Learning (MIL) loss, which uses the epipolar and the disparity range constraints. From these two constraints, we know that every non-occluded reference patch has a matching positive patch in a known index interval, but does not have a matching negative patch. Therefore, for every reference patch, the similarity of the best referencepositive match should be greater than the similarity of the best reference-negative match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>The constractive loss, which adds to the MIL method the uniqueness constraint. It tells us that the matching positive patch is unique. Thus, for every patch, the similarity of the best match should be greater than the similarity of the second best match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>The constractive-DP uses all the constraints but finds the best match using dynamic programming.</p><p>The method has been applied to train a deep siamese neuralnetwork that takes two patches as an input and predicts a similarity measure. Benchmarking on standard datasets shows that the performance is as good or better than the published results on MC-CNN-fst <ref type="bibr" target="#b38">[39]</ref>, which uses the same network architecture but trained using fully labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Regularization and disparity estimation</head><p>Once the raw cost volume is estimated, one can estimate the disparity by dropping the regularization term of Eqn. (1), or equivalently block C of <ref type="figure" target="#fig_0">Fig. 1</ref>, and taking the argmin, the softargmin, or the subpixel MAP approximation (block D of <ref type="figure" target="#fig_0">Fig. 1</ref>). However, the raw cost volume computed from image features could be noise-contaminated, e.g., due to the existence of non-Lambertian surfaces, object occlusions, or repetitive patterns. Thus, the estimated depth maps can be noisy. As such, some methods overcome this problem by using traditional MRF-based stereo framework for cost volume regularization <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b43">[44]</ref>. In these methods, the initial cost volume C is fed to a global <ref type="bibr" target="#b10">[11]</ref> or a semiglobal <ref type="bibr" target="#b54">[55]</ref> matcher to compute the disparity map. Semiglobal matching provides a good tradeoff between accuracy and computation requirements. In this method, the smoothness term of Eqn. (1) is defined as:</p><formula xml:id="formula_5">Es(dx, dy) = ?1?(|dxy = 1) + ?2?(|dxy &gt; 1),<label>(2)</label></formula><p>where d xy = d x ? d y , ? 1 and ? 2 are positive weights chosen such that ? 2 &gt; ? 1 , and ? is the Kronecker delta function, which gives 1 when the condition in the bracket is satisfied, otherwise 0. To solve this optimisation problem, the SGM energy is broken down into multiple energies E s , each one defined along a path s. The energies are minimised independently and then aggregated. The disparity at x is computed using the winner-takes-all strategy of the aggregated costs of all directions:</p><formula xml:id="formula_6">d x = arg min d s E s (x, d).<label>(3)</label></formula><p>This method requires setting the two parameters ? 1 and ? 2 of Eqn. <ref type="bibr" target="#b1">(2)</ref>. Instead of manually setting them, Seki et al. <ref type="bibr" target="#b55">[56]</ref> proposed SGM-Net, a neural network trained to provide these parameters at each image pixel. They obtained better penalties than hand-tuned methods as in <ref type="bibr" target="#b38">[39]</ref>. The SGM method, which uses an aggregated scheme to combine costs from multiple 1D scanline optimizations, suffers from two major issues: (1) streaking artifacts caused by the scanline optimization approach, at the core of this algorithm, may lead to inaccurate results, and (2) the high memory footprint that may become prohibitive with high resolution images or devices with constrained resources. As such Schonberger et al. <ref type="bibr" target="#b56">[57]</ref> reformulate the fusion step as the task of selecting the best amongst all the scanline optimization proposals at each pixel in the image. They solve this task using a per-pixel random forest classifier.</p><p>Poggi et al. <ref type="bibr" target="#b57">[58]</ref> learn a weighted aggregation where the weight of each 1D scanline optimisation is defined using a confidence map computed using either traditional techniques <ref type="bibr" target="#b58">[59]</ref> or deep neural networks, see Section 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">END-TO-END DEPTH FROM STEREO</head><p>Recent works solve the stereo matching problem using a pipeline that is trained end-to-end. Two main classes of methods have been proposed. Early methods, e.g., FlowNet-Simple <ref type="bibr" target="#b50">[51]</ref> and DispNetS <ref type="bibr" target="#b21">[22]</ref>, use a single encoder-decoder, which stacks together the left and right images into a 6D volume, and regresses the disparity map. These methods, which do not require an explicit feature matching module, are fast at runtime. They, however, require a large amount of training data, which is hard to obtain. Methods in the second class mimic the traditional stereo matching pipeline by breaking the problem into stages, each stage is composed of differentiable blocks and thus allowing end-to-end training. Below, we review in details these techniques. <ref type="figure" target="#fig_3">Fig. 4</ref> provides a taxonomy of the state-of-the-art, while <ref type="table">Table 3</ref> compares 28 key methods based on this taxonomy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>to-end methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Feature learning</head><p>Feature learning networks follow the same architectures as the ones described in Figs. 2 and 3. However, instead of processing individual patches, the entire images are processed in a single forward pass producing feature maps of the same or lower resolution as the input images. Two strategies have been used to enable matching features across the images:</p><p>(1) Multi-branch networks composed of n branches where n is the number of input images. Each branch produces a feature map that characterizes its input image <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>. These techniques assume that the input images have been rectified so that the search for correspodnences is restricted to be along the horizontal scanlines.</p><p>(2) Multi-branch networks composed of n d branches where n d is the number of disparity levels. The d-th branch, 1 ? d ? n d , processes a stack of two images, as in <ref type="figure">Fig.</ref> 2-(f); the first image is the reference image. The second one is the right image but re-projected to the d-th depth plane <ref type="bibr" target="#b65">[66]</ref>. Each branch produces a similarity feature map that characterizes the similarity between the reference image and the right image re-projected onto a given depth plane. While these techniques do not rectify the images, they assume that the intrinsic and extrinsic camera parameters are known. Also, the number of disparity levels cannot be varied without updating the network architecture and retraining it. In both methods, the feature extraction module uses either fully convolutional (ConvNet) networks such as VGG, or residual networks such as ResNets <ref type="bibr" target="#b66">[67]</ref>. The latter facilitates the training of very deep networks <ref type="bibr" target="#b67">[68]</ref>. They can also capture and incorporate more global context in the unary features by using either dilated convolutions (Section 4.1.2.2) or multi-scale approaches. For instance, the PSM-Net of Chang and Chen <ref type="bibr" target="#b63">[64]</ref> append a Spatial Pyramid Pooling (SPP) module in order to extract and aggregate features at multiple scales. Nie et al. <ref type="bibr" target="#b64">[65]</ref> extended PSM-Net using a multi-level context aggregation pattern termed Multi-Level Context Ultra-Aggregation (MLCUA). It encapsulates all convolutional features into a more discriminative representation by intra and inter-level features combination. It combines the features at the shallowest, smallest scale with features at deeper, larger scales using just shallow skip connections. This results in an improved performance, compared to PSM-Net <ref type="bibr" target="#b63">[64]</ref>, without significantly increasing the number of parameters in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cost volume construction</head><p>Once the features have been computed, the next step is to compute the matching scores, which will be fed, in the form of a cost volume, to a top network for regularization              and disparity estimation. The cost volume can be three dimensional (3D) where the third dimension is the disparity level (Section 5.2.1), four dimensional (4D) where the third dimension is the feature dimension and the fourth one is the disparity level (Section 5.2.2), or hybrid to benefit from the properties of the 3D and 4D cost volumes (Section 5.2.3). In general, the cost volume is constructed at a lower resolution, e.g., at 1 /8-th, than the input <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b72">[73]</ref>. It is then either subsequently upscaled and refined, or used as is to estimate a low resolution disparity map, which is then upscaled and refined using a refinement module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">3D cost volumes</head><p>5.2.1.1 Construction: A 3D cost volume can be simply built by taking the L 1 , L 2 , or correlation distance between the features of the left image and those of the right image that are within a pre-defined disparity range, see <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b82">[83]</ref>, and the FlowNetCorr of <ref type="bibr" target="#b50">[51]</ref>. The advantage of correlation-based dissimilarities is that they can be implemented using a convolutional layer that does not require training (its filters are the features computed by the second branch of the network). Flow estimation networks such as FlowNetCorr <ref type="bibr" target="#b50">[51]</ref> use 2D correlations. Disparity estimation networks, such as <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b67">[68]</ref>, iResNet <ref type="bibr" target="#b62">[63]</ref>, DispNet3 <ref type="bibr" target="#b74">[75]</ref>, EdgeStereo <ref type="bibr" target="#b75">[76]</ref>, HD 3 <ref type="bibr" target="#b79">[80]</ref>, and <ref type="bibr" target="#b82">[83]</ref>, <ref type="bibr" target="#b83">[84]</ref>, use 1D correlations. 5.2.1.2 Regularization of 3D cost volumes: Once a cost volume is computed, an initial disparity map can be estimated using the argmin, the softargmin, or the subpixel MAP approximation over the depth dimension of the cost volume, see for example <ref type="bibr" target="#b72">[73]</ref> and <ref type="figure" target="#fig_16">Fig. 5-(a)</ref>. This is equivalent to dropping the regularization term of Eqn. <ref type="bibr" target="#b0">(1)</ref>. In general, however, the raw cost volume is noisecontaminated (e.g., due to the existence of non-Lambertian surfaces, object occlusions, and repetitive patterns). The goal of the regularization module is to leverage context along the spatial and/or disparity dimensions to refine the cost volume before estimating the initial disparity map.</p><p>(1) Regularization using traditional methods. Early papers use traditional techniques, e.g., Markov Random Fields (MRF), Conditional Random Fields (CRF), and Semi-Global Matching (SGM), to regularize the cost volume by explicitly incorporating spatial constraints, e.g., smoothness, of the depth maps. Recent papers showed that deep learning networks can be used to fine-tune the parameters of these methods. For example, Kn?belreiter et al. <ref type="bibr" target="#b70">[71]</ref> proposed a hybrid CNN-CRF. The CNN computes the matching term of Eqn. <ref type="bibr" target="#b0">(1)</ref>, which becomes the unary term of a CRF module. The pairwise term of the CRF is parameterized by edge weights computed using another CNN. The end-toend trained CNN-CRF pipeline could achieve a competitive performance using much fewer parameters (thus a better utilization of the training data) than the earlier methods.</p><p>Zheng et al. <ref type="bibr" target="#b88">[89]</ref> provide a way to model CRFs as recurrent neural networks (RNN) for segmentation tasks so that the entire pipeline can be trained end-to-end. Unlike segmentation, in depth estimation, the number of depth samples, whose counterparts are the semantic labels in segmentation tasks, is expected to vary for different scenarios. As such, Xue et al. <ref type="bibr">[</ref> (2) Regularization using 2D convolutions (2DConvNet), Figs. 5-(b) and (c). Another approach is to process the 3D cost volume using a series of 2D convolutional layers producing another 3D cost volume <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>. 2D convolutions are computationally efficient. However, they only capture and aggregate context along the spatial dimensions, see   <ref type="bibr" target="#b71">[72]</ref> use the L 2 distance to compute an initial 3D cost volume and 3D convolutions to regularize it across both the spatial and disparity dimensions, see <ref type="figure" target="#fig_16">Fig. 5</ref>-(d). Due to its memory requirements, the approach first estimates a low-resolution disparity map, which is then progressively improved using residual learning. Zhang et al. <ref type="bibr" target="#b72">[73]</ref> follow the same approach but the refinement block starts with separate convolution layers running on the upsampled disparity and input image respectively, and merge the features later to produce the residual. Chabra et al. <ref type="bibr" target="#b80">[81]</ref> observe that the cost volume regularization step is the one that uses most of the computational resources. They then propose a regularization module that uses 3D dilated convolutions in the width, height, and disparity dimesions, to reduce the computation time while capturing a wider context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">4D cost volumes</head><p>5.2.2.1 Construction: 4D cost volumes to preserve the dimension of the features <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b78">[79]</ref>. The rational behind 4D cost volumes is to let the top network learn the appropriate similarity measure for comparing the features instead of using hand-crafted ones as in Section 5.2.1.</p><p>4D cost volumes can be constructed by feature differences across a pre-defined disparity range <ref type="bibr" target="#b31">[32]</ref>, which results in cost volume of size H ? W ? 2n d ? c, or by concatenating the features computed by the different branches of the network <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b78">[79]</ref>. Using this method, Kendall et al. <ref type="bibr" target="#b60">[61]</ref> build a 4D volume of size H ?W ?(n d +1)?c (c here is the dimension of the features). Zhong et al. <ref type="bibr" target="#b69">[70]</ref> follow the same approach but concatenate the features in an interleaved manner. That is, if f L is the feature map of the left image and f R the feature map of the right image, then the final feature volume is assembled in such a way that its 2i?th slice holds the left feature map while the (2i + 1)?th slice holds the right feature map but at disparity d = i. This results in a 4D cost volume that is twice larger than the cost volume of Kendall et al. <ref type="bibr" target="#b60">[61]</ref>. To capture multi-scale context in the cost volume, Chang and Chen <ref type="bibr" target="#b63">[64]</ref> generate for each input image a pyramid of features, upsamples them to the same dimension, and then builds a single 4D cost volume by concatenation. Wu et al. <ref type="bibr" target="#b78">[79]</ref> build from the multiscale features (four scales) multiscale 4D cost volumes.</p><p>4D cost volumes carry richer information compared to 3D cost volumes. Note, however, that volumes obtained by concatenation contain no information about the feature similarities, so more parameters are required in the subsequent modules to learn the similarity function. 5.2.2.2 Regularization of 4D cost volumes: 4D cost volumes are regularized with 3D convolutions, which exploit the correlation in height, width and disparity dimensions, to produce a 3D cost volume. Kendall et al. <ref type="bibr" target="#b60">[61]</ref> use a U-net encoder-decoder with 3D convolutions and skip connections. Zhong et al. <ref type="bibr" target="#b69">[70]</ref> use a similar approach but add residual connections from the contracting to the expanding parts of the regularization network. To take into account a large context without a significant additional computational burden, Kendall et al. <ref type="bibr" target="#b60">[61]</ref> regularize the cost volume hierarchically, with four levels of subsampling, allowing to explicitly leverage context with a wide field of view. Muliscale 4D cost volumes <ref type="bibr" target="#b78">[79]</ref> are aggregated into a single 3D cost volume using a 3D multi-cost aggregation module, which operates in a pairwise manner starting with the smallest volume. Each volume is processed with an encoder-decoder, upsampled to the next resolution in the pyramid, and then fused using a 3D feature fusion module.</p><p>Also, semi-global matching (SGM) techniques have been used to regularize the 4D cost volume where their parameters are estimated using convolutional networks. In particular, Yu et al. <ref type="bibr" target="#b76">[77]</ref> process the initial 4D cost volume with an encoder-decoder composed of 3D convolutions and upconvolutions, and produces another 3D cost volume. The subsequent aggregation step is performed using an end-toend two-stream network: the first stream generates three cost aggregation proposals C i , one along each of the tree dimensions, i.e., the height, width, and disparity. The second stream is a guidance stream used to select the best proposals. It uses 2D convolutions to produce three guidance (confidence) maps W i . The final 3D cost volume is produced as a weighted sum of the three proposals, i.e., max i (C i * W i ).</p><p>3D convolutions are expensive in terms of memory requirements and computation time. As such, subsequent works that followed the seminal work of Kendall et al. <ref type="bibr" target="#b60">[61]</ref> focused on (1) reducing the number of 3D convolutional layers <ref type="bibr" target="#b84">[85]</ref>, (2) progressively refining the cost volume and the disparity map <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b87">[88]</ref>, and (3) compressing the 4D cost volume <ref type="bibr" target="#b77">[78]</ref>. Below, we discuss these approaches.</p><p>(1) Reducing the number of 3D convolutional layers. Zhang et al. <ref type="bibr" target="#b84">[85]</ref> introduced GANet, which replaces a large number of the 3D convolutional layers in the regularization block with (1) two 3D convolutional layers, (2) a semi-global aggregation layer (SGA), and (3) a local guided aggregation layer (LGA). SGA is a differentiable approximation of the semi-global matching (SGM). Unlike SGM, in SGA the user-defined parameters are learnable. Moreover, they are added as penalty coefficients/weights of the matching cost terms. Thus, they are adaptive and more flexible at different locations for different situations. The LGA layer, on the other hand, is appended at the end and aims to refine the thin structures and object edges. The SGA and</p><p>LGA layers, which are used to replace the costly 3D convolutions, capture local and whole-image cost dependencies. They significantly improve the accuracy of the disparity estimation in challenging regions such as occlusions, large textureless/reflective regions, and thin structures.</p><p>(2) Progressive approaches. Some techniques avoid directly regularizing high resolution 4D cost volumes using the expensive 3D convolutions. Instead, they operate in a progressive manner. For instance, Chang and Chen <ref type="bibr" target="#b63">[64]</ref> introduced PSM-Net, which first estimates a low resolution 4D cost volume, and then regularizes it using stacked hourglass 3D encoder-decoder blocks. Each block returns a 3D cost volume, which is then upsampled and used to regress a high resolution disparity map using additional 3D convolutional layers followed by a softmax operator. As such, the stacked hourglass blocks can be seen as refinement modules.</p><p>Wang et al. <ref type="bibr" target="#b87">[88]</ref> use a three-stage disparity estimation network, called AnyNet, which builds cost volumes in a coarse-to-fine manner. The first stage takes as input low resolution feature maps, builds a low resolution 4D cost volume and then uses 3D convolutions to estimate a low resolution disparity map by searching on a small disparity range. The prediction in the previous level is then upsampled and used to warp the input feature at the higher scale, with the same disparity estimation network used to estimate disparity residuals. The advantage is two-fold; first, at higher resolutions, the network only learns to predict residuals, which reduces the computation cost. Second, the approach is progressive and one can select to return the intermediate disparities, trading accuracy for speed.</p><p>(3) 4D cost volume compression. Tulyakov et al. <ref type="bibr" target="#b77">[78]</ref> reduce the memory usage, without having to sacrify accuracy, by compressing the features into compact matching signatures. As such, the memory footprint is significantly reduced. More importantly, it allows the network to handle an arbitrary number of multiview images and to vary the number of inputs at runtime without having to re-train the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Hybrid 3D-4D cost volumes</head><p>The correlation layer provides an efficient way to measure feature similarities, but it loses much information because it produces only a single-channel map for each dispar-ity level. On the other hand, 4D cost volumes obtained by feature concatenation carry more information but are resource-demanding. They also require more parameters in the subsequent aggregation network to learn the similarity function. To benefit from both, Guo et al. <ref type="bibr" target="#b85">[86]</ref> propose a hybrid approach, which constructs two cost volumes; one by feature concatenation but compressed into 12 channels using two convolutions. The second one is built by dividing the high-dimension feature maps into N g groups along the feature channel, computing correlations within each group at all disparity levels, and finally concatenating the correlation maps forming another 4D volume. The two volumes are then combined together and passed to a 3D regularization module composed of four 3D convolution layers followed by three stacked 3D hourglass networks. This approach results in a significant reduction of parameters compared to 4D cost volumes built by only feature concatenation, without losing too much information like full correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Disparity computation</head><p>The simplest way to estimate the disparity map from the regularized cost volume C is by using the pixel-wise argmin, i.e., d x = arg min d C(x, d) (or equivalently arg max if the volume C encodes the likelihood). However, the agrmin/argmax operator is unable to produce sub-pixel accuracy and cannot be trained with back-propagation due to its non-differentiability. Another approach is the differentiable soft argmin/max over disparity <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b72">[73]</ref>:</p><formula xml:id="formula_7">d * = 1 n d j=0 e ?C(x,j) n d d=0 d ? e ?C(x,d) .<label>(4)</label></formula><p>The soft argmin operator approximates the sub-pixel MAP solution when the distribution is unimodal and symmetric <ref type="bibr" target="#b77">[78]</ref>. When this assumption is not fulfilled, the softargmin blends the modes and may produce a solution that is far from all the modes and may result in over smoothing.</p><p>Chen et al. <ref type="bibr" target="#b86">[87]</ref> observe that this is particularly the case at boundary pixels where the estimated disparities follow multimodal distributions. To address these issues, Chen et al. <ref type="bibr" target="#b86">[87]</ref> only apply a weighted average operation on a window centered around the modal with the maximum probability, instead of using a full-band weighted average on the entire disparity range. Tulyakov et al. <ref type="bibr" target="#b77">[78]</ref> introduced the sub-pixel MAP approximation, which computes a weighted mean around the disparity with the maximum posterior probability as:</p><formula xml:id="formula_8">d * = d:|d?d|?? d ? ?(C(x, d)),<label>(5)</label></formula><p>where ? is a meta parameter set to 4 in <ref type="bibr" target="#b77">[78]</ref>, ? <ref type="figure">(C(x, d)</ref>) is the probability of the pixel x having a disparity d, and d = arg max d C(x, d). The sub-pixel MAP is only used for inference. Tulyakov et al. <ref type="bibr" target="#b77">[78]</ref> also showed that, unlike the softargmin/max, this approach allows changing the disparity range at runtime without re-training the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Variants</head><p>The pipeline described so far infers disparity maps that can be of low-resolution (along the width, height, and disparity dimensions), incomplete, noisy, missing fine details, and suffering from over-smoothing especially at object boundaries. As such, many variants have been introduced to (1) improve their resolution (Section 5.4.1), (2) improve the processing time, especially at runtime (Section 5.4.3), and (3) perform disparity completion and denoising (Section 5.4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Learning to infer high resolution disparity maps</head><p>Directly regressing high-resolution depth maps that contain fine details, e.g., by adding further upconvolutional layers to upscale the cost volume, would require a large number of parameters and thus are computationally expensive and difficult to train. As such, state-of-the-art methods struggle to process high resolution imagery because of memory constraints or speed limitations. This has been addressed by using either bottom-up or top-down techniques.</p><p>Bottom-up techniques operate in a sliding windowlike approach. They take small patches and estimate the refined disparity either for the entire patch or for the pixel at the center of the patch. Lee et al. <ref type="bibr" target="#b93">[94]</ref> follow a splitand-merge approach. The input image is split into regions, and a depth is estimated for each region. The estimates are then merged using a fusion network, which operates in the Fourier domain so that depth maps with different cropping ratios can be handled. While both sliding window and splitand-merge approaches reduce memory requirements, they require multiple forward passes, and thus are not suitable for realtime applications. Also, these methods do not capture the global context, which can limit their performance.</p><p>Top-down techniques, on the other hand, operate on the disparity map estimates in a hierarchical manner. They first estimate a low-resolution disparity map and then upsample them to the desired resolution, e.g., using bilinear upsampling, and further process them using residual learning to recover small details and thin structures <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b80">[81]</ref>. This process can also be run progressively by cascading many of such refinement blocks, each block refines the estimate of the previous block <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b71">[72]</ref>. Unlike upsampling cost volumes, refining disparity maps is computationally efficient since it only requires 2D convolutions. Existing methods mainly differ in the type of additional information that is appended to the upsampled disparity map for refinement. For instance:</p><p>? Khamis et al. <ref type="bibr" target="#b71">[72]</ref> concatenate the upsampled disparity map with the original reference image.</p><p>? Liang et al. <ref type="bibr" target="#b62">[63]</ref> append to the initial disparity map the cost volume and the reconstruction error, defined as the difference between the left image and the right image but warped to the left image using the estimated disparity map.</p><p>? Chabra et al. <ref type="bibr" target="#b80">[81]</ref> take the left image and the reconstruction error on one side, and the left disparity and the geometric error map, defined as the difference between the estimated left disparity and right disparity but warped onto the left view. These are independently filtered using one layer of convolutions followed by batch normalization. The results of the two streams are concatenated and then further processed using a series of convolutional layers to produce the refined disparity map.</p><p>These methods improve the spatial resolution but not the disparity resolution. To refine both the spatial and depth resolution, while operating on high resolution images, Yang et al. <ref type="bibr" target="#b31">[32]</ref> propose to search for correspondences incrementally over a coarse-to-fine hierarchy. The approach constructs a pyramid of four 4D cost volumes, each with increasing spatial and depth resolutions. Each volume is filtered by six 3D convolution blocks, and further processed with a Volumetric Pyramid Pooling block, an extension of Spatial Pyramid Pooling to feature volumes, to generate features that capture sufficient global context for high resolution inputs. The output is then either (1) processed with another conv3D block to generate a 3D cost volume from which disparity can be directly regressed. This allows to report ondemand disparities computed from the current scale, or (2) tri-linearly-upsampled to a higher spatial and disparity resolution so that it can be fused with the next 4D volume in the pyramid. To minimise memory requirements, the approach uses striding along the disparity dimensions in the last and second last volumes of the pyramid. The network is trained end-to-end using a multi-scale loss. This hierarchical design also allows for anytime on-demand reports of disparity by capping intermediate coarse results, allowing accurate predictions for near-range structures with low latency (30ms).</p><p>This approach shares some similarities with the approach of Kendall et al. <ref type="bibr" target="#b60">[61]</ref>, which constructs hierarchical 4D feature volumes and processes them from coarse to fine using 3D convolutions. Kendall et al.'s approach <ref type="bibr" target="#b60">[61]</ref>, however, has been used to leverage context with a wide field of view while Yang et al. <ref type="bibr" target="#b31">[32]</ref> apply coarse-to-fine principles for high-resolution inputs and anytime, on-demand processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Learning for completion and denoising</head><p>Raw disparities can be noisy and incomplete, especially near object boundaries where depth smearing between objects remains a challenge. Several techniques have been developed for denoising and completion. Some of them are ad-hoc, i.e., post-process the noisy and uncomplete initial estimates to generate clean and complete depth maps. Other methods addressed the issue of the lack of training data for completion and denoising. Others proposed novel depth representations that are more suitable for this task, especially for solving the depth smearing between objects.</p><p>Ad-hoc methods process the initially estimated disparities a using variational approaches <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b94">[95]</ref>, Fully-Connected CRFs (DenseCRF) <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b95">[96]</ref>, hierarchical CRFs <ref type="bibr" target="#b1">[2]</ref>, and diffusion processes <ref type="bibr" target="#b39">[40]</ref> guided by confidence maps <ref type="bibr" target="#b96">[97]</ref>. They encourage pixels that are spatially close and with similar colors to have closer disparity predictions. They have been also explored by Liu et al. <ref type="bibr" target="#b4">[5]</ref>. However, unlike Li et al. <ref type="bibr" target="#b1">[2]</ref>, Liu et al. <ref type="bibr" target="#b4">[5]</ref> used a CNN to minimize the CRF energy. Convolutional Spatial Propagation Networks (CSPN) <ref type="bibr" target="#b97">[98]</ref>, <ref type="bibr" target="#b98">[99]</ref>, which implement an anisotropic diffusion process, are particularly suitable for depth completion since they predict the diffusion tensor using a deep CNN. This is then applied to the initial map to obtain the refined one.</p><p>One of the main challenges of deep learning-based depth completion and denoising is the lack of labelled training data, i.e., pairs of noisy, incomplete depth maps and their corresponding clean depth maps. To address this issue, Jeon and Lee <ref type="bibr" target="#b28">[29]</ref> propose a pairwise depth image dataset generation method using dense 3D surface reconstruction with a filtering method to remove low quality pairs. They also present a multi-scale Laplacian pyramid based neural network and structure preserving loss functions to progressively reduce the noise and holes from coarse to fine scales. The approach first predicts the clean complete depth image at the coarsest scale, which has a quarter of the original resolution. The predicted depth map is then progressively upsampled through the pyramid to predict the half and original-sized image. At the coarse level, the approach captures global context while at finer scales it captures local information. In addition, the features extracted during the downsampling are passed to the upsampling pyramid with skip connections to prevent the loss of the original details in the input depth image during the upsampling.</p><p>Instead of operating on the network architecture, the loss function, or the training datasets, Imran et al. <ref type="bibr" target="#b99">[100]</ref> propose a new representation for depth called Depth Coefficients (DC) to address the problem of depth smearing between objects. The representation enables convolutions to more easily avoid inter-object depth mixing. The representation uses a multi-channel image of the same size as the target depth map, with each channel representing a fixed depth. The depth values increase in even steps of size b. (The approach uses 80 bins.) The choice of the number of bins trades-off memory vs. precision. The vector composed of all these values at a given pixel defines the depth coefficients for that pixel. For each pixel, these coefficients are constrained to be non-negative and sum to 1. This representation of depth provides a much simpler way for CNNs to avoid depth mixing. First, CNNs can learn to avoid mixing depths in different channels as needed. Second, since convolutions apply to all channels simultaneously, depth dependencies, like occlusion effects, can be modelled and learned by neural networks. The main limitation, however, is that the depth range needs to be set in advance and cannot be changed at runtime without re-training the network. Imran et al. <ref type="bibr" target="#b99">[100]</ref> also show that the standard Mean Squared Error (MSE) loss function can promote depth mixing, and thus propose to use cross-entropy loss for estimating the depth coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Learning for realtime processing</head><p>The goal is to design efficient stereo algorithms that not only produce reliable and accurate estimations, but also run in realtime. For instance, in the PSMNet <ref type="bibr" target="#b63">[64]</ref>, the cost volume construction and aggregation takes more than 250ms (on nNvidia Titan-Xp GPU). This renders realtime applications infeasible. To speed the process, Khamis et al. <ref type="bibr" target="#b71">[72]</ref> first estimate a low resolution disparity map and then hierarchically refine it. Yin et al. <ref type="bibr" target="#b79">[80]</ref> employ a fixed, coarse-to-fine procedure to iteratively find the match. Chabra et al. <ref type="bibr" target="#b80">[81]</ref> use 3D dilated convolutions in the width, height, and disparity channels when filtering the cost volume. Duggal et al. <ref type="bibr" target="#b82">[83]</ref> combine deep learning with PatchMatch <ref type="bibr" target="#b100">[101]</ref> to adaptively prune out the potentially large search space and significantly speed up inference. PatchMatch-based pruner module is able to predict a confidence range for each pixel, and construct a sparse cost volume that requires significantly less operations. This also allows the model to focus only on regions with high likelihood and save computation and memory. To enable end-to-end training, Duggal et al. <ref type="bibr" target="#b82">[83]</ref> unroll PatchMatch as an RNN where each unrolling step is equivalent to an iteration of the algorithm. This approach achieved a performance that is comparable to the state-ofthe-art, e.g., <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b67">[68]</ref>, while reducing the computation time from 600ms to 60ms per image in the KITTI2015 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Learning confidence maps</head><p>The ability to detect, and subsequently remedy to, failure cases is important for applications such as autonomous driving and medical imaging. Thus, a lot of research has been dedicated to estimating confidence or uncertainty maps, which are then used to sparsify the estimated disparities by removing potential errors and then replacing them from the reliable neighboring pixels. Disparity maps can also be incorporated in a disparity refinement pipeline to guide the refinement process <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b102">[103]</ref>. Seki et al. <ref type="bibr" target="#b101">[102]</ref>, for example, incorporate the confidence map into a Semi-Global Matching (SGM) module for dense disparity estimation. Gidaris et al. <ref type="bibr" target="#b102">[103]</ref> use confidence maps to detect the incorrect estimates, replace them with disparities from neighbouring regions, and then refine the disparity using a refinement network. Jie et al. <ref type="bibr" target="#b73">[74]</ref>, on the other hand, estimate two confidence maps, one for each of the input images, concatenate them with their associated cost volumes, and use them as input to a 3D convolutional LSTM to selectively focus in the subsequent step on the left-right mismatched regions.</p><p>Conventional confidence estimation methods are mostly based on assumptions and heuristics on the matching cost volume analysis, see <ref type="bibr" target="#b58">[59]</ref> for a review and evaluation of the early methods. Recent techniques are based on supervised learning <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b104">[105]</ref>, <ref type="bibr" target="#b105">[106]</ref>, <ref type="bibr" target="#b106">[107]</ref>, <ref type="bibr" target="#b107">[108]</ref>, <ref type="bibr" target="#b108">[109]</ref>. They estimate confidence maps directly from the disparity space either in an ad-hoc manner, or in an integrated fashion so that they can be trained end-to-end along with the disparity/depth estimation. Poggi et al. <ref type="bibr" target="#b109">[110]</ref> provide a quantitative evaluation. Below, we discuss some of these techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Confidence from left-right consistency check</head><p>Left-right consistency is one of the most commonly-used criteria for measuring confidence in disparity estimates. The idea is to estimate two disparity maps, one from the left image (D lef t ), and another from the right image (D right ). An error map can then be computed by taking a pixel-wise difference between D lef t and D right , but warped back onto the left image, and converting them into probabilities <ref type="bibr" target="#b62">[63]</ref>. This measure is suitable for detecting occlusions, i.e., regions that are visible in one view but not in the other.</p><p>Left-right consistency can also be learned using deep or shallow networks composed of fully convolutional layers <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b101">[102]</ref>. Seki et al. <ref type="bibr" target="#b101">[102]</ref> propose a patch-based confidence prediction (PBCP) network, which requires two disparity maps, one estimated from the left image and the other one from the right image. PBCP uses a two-channel network. The first channel enforces left-right consistency while the second one enforces local consistency. The network is trained in a classifier manner. It outputs a label per pixel indicating whether the estimated disparity is correct.</p><p>Instead of treating left-right consistency check as an isolated post-processing step, Jie et al. <ref type="bibr" target="#b73">[74]</ref> perform it jointly with disparity estimation, using a Left-Right Comparative Recurrent (LRCR) model. It consists of two parallel convolutional LSTM networks <ref type="bibr" target="#b110">[111]</ref>, which produce two error maps; one for the left disparity and another for the right disparity. The two error maps are then concatenated with their associated cost volumes and used as input to a 3D convolutional LSTM to selectively focus in the next step on the left-right mismatched regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Confidence from a single raw disparity map</head><p>Left-right consistency checks estimate two disparity maps and thus are expensive at runtime. Shaked and Wolf <ref type="bibr" target="#b45">[46]</ref> train, via the binary cross entropy loss, a network, composed of two fully-connected layers, to predict the correctness of an estimated disparity from only the reference image. Poggi and Mattoccia <ref type="bibr" target="#b106">[107]</ref> pose the confidence estimation as a regression problem and solve it using a CNN trained on small patches. For each pixel, the approach extracts a square patch around the pixel and forwards it to a CNN trained to distinguish between patterns corresponding to correct and erroneous disparity assignments. It is a single channel network, designed for 9 ? 9 image patches. Zhang et al. <ref type="bibr" target="#b72">[73]</ref> use a similar confidence map estimation network, called invalidation network. The key idea is to train the network to predict confidence using a pixel-wise error between the left disparity and the right disparity. At runtime, the network only requires the left disparity. Finally, Poggi and Mattoccia <ref type="bibr" target="#b111">[112]</ref> show that one can improve the confidence maps estimated using previous algorithms by enforcing local consistency in the confidence estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Confidence map from matching densities</head><p>Traditional deep networks represent activations and outputs as deterministic point estimates. Gast and Roth <ref type="bibr" target="#b112">[113]</ref> explore the possibility of replacing the deterministic outputs by probabilistic output layers. To go one step further, they replace all intermediate activations by distributions. As such, the network can be used to estimate the matching probability densities, hereinafter referred to as matching densities, which can then be converted into uncertainties (or confidence) at runtime. The main challenge of estimating matching densities is the computation time. To make it tractable, Gast and Roth <ref type="bibr" target="#b112">[113]</ref> assume parametric distributions. Yin et al. <ref type="bibr" target="#b79">[80]</ref> relax this assumption and propose a pyramidal architecture to make the computation cost sustainable and allow for the estimation of confidence at run time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.4">Local vs. global reasoning</head><p>Some techniques, e.g., Seki et al. <ref type="bibr" target="#b101">[102]</ref>'s, reason locally by enforcing local consistency. Tosi et al. <ref type="bibr" target="#b113">[114]</ref> introduced LGC-Net to move beyond local reasoning. The input reference image and its disparity map are forwarded to a local network, e.g., C-CNN <ref type="bibr" target="#b106">[107]</ref>, and a global network, e.g., an encoder/decoder architecture with a large receptive field. The output of the two networks and the initial disparity, concatenated with the reference image, are further processed with three independent convolutional towers whose outputs are concatenated and processed with three 1 ? 1 convolutional layers to finally infer the confidence map.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw cost volume</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I0 I1</head><p>In I1 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I1 nd</head><p>In-1 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In-1 nd</head><p>Aggregation accross views Aggregation accross depth planes Feature Net</p><formula xml:id="formula_9">I0 I1 1 Feature Net I0 I1 2 .</formula><p>. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I0 I1 nd</head><p>Concatenate .</p><p>. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I0</head><p>In-1 1</p><p>Feature Net</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I0</head><p>In-1 2 . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I0</head><p>In-1 nd  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.5">Combining multiple estimators</head><p>Some papers combine the estimates of multiple algorithms to achieve a better accuracy. Haeusler et al. <ref type="bibr" target="#b103">[104]</ref> fed a random forest with a pool of 23 confidence maps, estimated using conventional techniques, yielding a much better accuracy compared to any confidence map in the pool. Batsos et al. <ref type="bibr" target="#b108">[109]</ref> followed a similar idea but combine the strengths and mitigate the weaknesses of four basic stereo matchers in order to generate a robust matching volume for the subsequent optimization and regularization steps. Poggi and Mattoccia <ref type="bibr" target="#b57">[58]</ref> train an ensemble regression trees classifier. These methods are independent of the disparity estimation module, and rely on the availability of the cost volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LEARNING MULTIVIEW STEREO</head><p>Multiview Stereo (MVS) methods follow the same pipeline as of depth-from-stereo. Early works focused on computing the similarity between multiple patches. For instance, Hartmann et al. <ref type="bibr" target="#b46">[47]</ref>  <ref type="figure" target="#fig_20">(Fig. 6-(a)</ref>) replace the pairwise correlation layer used in stereo matching by an average pooling layer to aggregate the learned features of n ? 2 input patches, and then feed the output to a top network, which returns a matching score. With this method, computing the best match for a pixel on the reference image requires n n?1 d forward passes. (n d is the number of depth levels and n is the number of images.) This is computationally very expensive especially when dealing with high resolution images.</p><p>Techniques that compute depth maps in a single forward pass differ in the way the information from the multiple views is fed to the network and aggregated. We classify them into whether they are volumetric (Section 6.1) or Plane-Sweep Volume (PSV)-based (Section 6.2). The latter does not rely on intermediate volumetric representations of the 3D geometry. The only exception is the approach of Hou et al. <ref type="bibr" target="#b114">[115]</ref>, which performs temporal fusion of the latent representations of the input images. The approach, however, requires temporally-ordered images. <ref type="table" target="#tab_10">Table 4</ref> provides a taxonomy and compares 13 state-of-the-art MVS techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Volumetric representations</head><p>One of the main issues for MVS reconstruction is how to match, in an efficient way, features across multiple images. Pairwise stereo methods rectify the images so that the search for correspondences is restricted to the horizontal epipolar lines. This is not possible with MVS due to the large view angle differences between the images. This has bee addressed using volumetric representations of the scene geometry <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b115">[116]</ref>. Depth maps are then generated by projection from the desired viewpoint. For a given input image, with known camera parameters, a ray from the viewpoint is cast through each image pixel. The voxels intersected by that ray are assigned the color <ref type="bibr" target="#b115">[116]</ref> or the learned feature <ref type="bibr" target="#b59">[60]</ref> of that pixel. Existing methods differ in the way information from multiple views are fused:</p><p>(1) Fusing feature grids. Kar et al. <ref type="bibr" target="#b59">[60]</ref>  <ref type="figure" target="#fig_20">(Fig. 6-(c)</ref>) fuse, recursively, the back-projected 3D feature grids using a recurrent neural network (RNN). The produced 3D grid is regularized using an encoder-decoder. To avoid dependency on the order of the images, Kar et al. <ref type="bibr" target="#b59">[60]</ref> randomly permute the input images during training while constraining the output to be the same.</p><p>(2) Fusing pairwise cost volumes. Choi et al. <ref type="bibr" target="#b116">[117]</ref> fuse the cost volumes, computed from each pair of images, using a weighted sum where the weight of each volume is the confidence map computed from that cost volume.</p><p>(3) Fusing the reconstructed surfaces. Ji et al. <ref type="bibr" target="#b115">[116]</ref> process each pair of volumetric grids using a 3D CNN, which classifies whether a voxel is a surface point or not. To avoid the exhaustive combination of every possible image pairs, Ji et al. <ref type="bibr" target="#b115">[116]</ref> learn their relative importance, using a network composed of fully-connected layers, automatically select a few view pairs based on their relative importance to reconstruct multiple volumetric grids, and take their weighted sum to produce the final 3D reconstruction.</p><p>To handle high resolution volumetric grids, Ji et al. <ref type="bibr" target="#b115">[116]</ref> split the whole space into small Colored Voxel Cubes (CVCs) and regress the surface cube-by-cube. While this reduces the memory requirements, it requires multiple forward passes and thus increases the computation time. Paschalidou et al. <ref type="bibr" target="#b90">[91]</ref> avoid the explicit use of the volumetric representation. Instead, each voxel of the grid is projected onto each of the input views, before computing the pairwise correlation between the corresponding learned features on each pair of views, and then averaging them over all pairs of views.</p><p>Repeating this process for each depth value will result in the depth distribution on each pixel. This depth distribution is regularized using an MRF formulated as a differentiable function to enable end-to-end training.</p><p>In terms of performance, the volumetric approach of Ji et al. <ref type="bibr" target="#b115">[116]</ref> requires 4 hours to obtain a full reconstruction of a typical scene in DTU dataset <ref type="bibr" target="#b23">[24]</ref>. The approach of Paschalidou et al. <ref type="bibr" target="#b90">[91]</ref> takes approximately 25mins, on an Intel i7 computer with an Nvidia GTX Titan X GPU, for the same task. Finally, methods that perform fusion postreconstruction have higher reconstruction errors compared to those that perform early fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Plane-Sweep Volume representations</head><p>These methods directly estimate depth maps from the input without using intermediate volumetric representations of the 3D geometry. As such, they are computationally more efficient. The main challenge to address is how to efficiently match features across multiple views in a single forward pass. This is done by using the Plane-Sweep Volumes (PSV) <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b117">[118]</ref>, <ref type="bibr" target="#b118">[119]</ref>, i.e., they back project the input images <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b117">[118]</ref> or their learned features <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b118">[119]</ref> into planes at different depth values, forming PSVs from which the depth map is estimated. Existing methods differ in the way the PSVs are processed with the feature extraction and feature matching blocks.</p><p>Flynn et al.'s network <ref type="bibr" target="#b65">[66]</ref> ( <ref type="figure" target="#fig_20">Fig. 6-(b)</ref>) is composed of n d branches, one for each depth plane. The d?th branch of the network takes as input the reference image and the planes of the PSVs of the other images which are located at depth d. These are packed together and fed to a two-stage network. The first stage computes matching features between the reference image and all the PSV planes located at depth d. The second stage models interactions across depth planes using convolutional layers. The final block of the network is a per-pixel softmax over depth, which returns the most probable depth value per pixel. The approach requires that the number of views and the camera parameters of each view to be known.</p><p>Huang et al. <ref type="bibr" target="#b26">[27]</ref>'s approach ( <ref type="figure" target="#fig_20">Fig. 6-(d)</ref>) starts with a pairwise matching step where a cost volume is computed between the reference image and each of the input images. For a given pair (I 1 , I i ), i = 2, . . . , n, I i is first backprojected into a PSV. A siamese network then computes a matching cost volume between I 1 and each of the PSV planes. These volumes are aggregated into a single cost volume using an encoder-decoder network. This is referred to as intra-volume aggregation. Finally a max-pooling layer is used to aggregate the multi intra-volumes into a single inter-volume, which is then used to predict the depth map. Unlike Flynn et al. <ref type="bibr" target="#b65">[66]</ref>, Huang et al. <ref type="bibr" target="#b26">[27]</ref>'s approach does not require a fixed number of input views since aggregation is performed using pooling. In fact, the number of views can vary between training and at runtime.</p><p>Unlike <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b65">[66]</ref>, which back-project the input images, the MVSNet of Yao et al. <ref type="bibr" target="#b92">[93]</ref> use the camera parameters to back-project the learned features into a 3D frustum of a reference camera sliced into parallel frontal planes, one for each depth value. The approach then generates the matching cost volume upon a pixel-wise variance-based metric, and finally a generic 3D U-Net is used to regularize the matching cost volume to estimate the depth maps. Luo et al. <ref type="bibr" target="#b118">[119]</ref> extend MVSNet <ref type="bibr" target="#b92">[93]</ref> to P-MVSNet in two ways. First, a raw cost volume is processed with a learnable patch-wise aggregation function before feeding it to the regularization network. This improves the matching robustness and accuracy for noisy data. Second, instead of using a generic 3D-UNet network for regularization, P-MVSNet uses a hybrid isotropic-anisotropic 3D-UNet. The plane-sweep volumes are essentially anisotropic in depth and spatial directions, but they are often approximated by isotropic cost volumes, which could be detrimental. In fact, one can infer the corresponding depth map along the depth direction of the matching cost volume, but cannot get the same information along other directions. Luo et al. <ref type="bibr" target="#b118">[119]</ref> exploit this fact, through the proposed hybrid 3D U-Net with isotropic and anisotropic 3D convolutions, to guide the regularization of matching confidence volume.</p><p>The main advantage of using PSVs is that they eliminate the need to supply rectified images. In other words, the camera parameters are implicitly encoded. However, in order to compute the PSVs, the intrinsic and extrinsic camera parameters need to be either provided in advance or estimated using, for example, Structure-from-Motion techniques as in <ref type="bibr" target="#b26">[27]</ref>. Also, these methods require setting in advance the disparity range and its discretisation. Moreover, they often result in a complex network architecture. Wang et al. <ref type="bibr" target="#b119">[120]</ref> propose a light-weight architecture. It stacks together the reference image and the cost volume, computed using the absolute difference between the reference image and each other image but at different depth planes, and feeds them to an encoder-decoder network, with skip connections, to estimate the inverse depth at three different resolutions. Wang et al. <ref type="bibr" target="#b119">[120]</ref> use a view selection rule, which selects the frames that have enough angle or translation difference and then use the selected frames to compute the cost volume.</p><p>Finally, note that feature back-projection has been also used by Won et al. <ref type="bibr" target="#b29">[30]</ref> for omnidirectional depth estimation from a wide-baseline multi-view stereo setup. The approach uses spherical maps and spherical cost volumes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">TRAINING END-TO-END STEREO METHODS</head><p>The training process aims to find the network parameters W that minimize a loss function L(W ;D, ?) whereD is the estimated disparity, and ? are the supervisory cues. The loss function is defined as the sum of a data term L 1 (D, ?, W ), which measures the discrepancy between the ground-truth and the estimated disparity, and a regularization or smoothness term L 2 (D, W ), which imposes local or global constraints on the solution. The type of supervisory cues defines the degree of supervision (Section 7.1), which can be supervised with 3D groundtruth (Section 7.1.1), selfsupervised using auxiliary cues (Section 7.1.2), or weakly supervised (Section 7.1.3). Some methods use additional cues, in the form of constraints on the solution, to boost the accuracy and performance (Section 7.2). One of the main challenges of deep learning-based techniques is their ability to generalize to new domains. Section 7.3 reviews methods that addressed this issue. Finally, Section 7.4 reviews methods that learn network architectures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Supervision methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">3D supervision methods</head><p>Supervised methods are trained to minimise a loss function that measures the error between the ground truth disparity and the estimated disparity. It is of the form:</p><formula xml:id="formula_10">L = 1 N C(x)H(C(x) ? )D ?(d x ), ?(d x ) ,<label>(6)</label></formula><p>where: d x ad x are, respectively, the groundtruth and the estimated disparity at pixel x. D is a measure of distance, which can be the L 2 , the L 1 <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b98">[99]</ref>, <ref type="bibr" target="#b120">[121]</ref>, the smooth L 1 <ref type="bibr" target="#b63">[64]</ref>, or the smooth L 1 but approximated using the twoparameter robust function ?(?) <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b121">[122]</ref>. C(x) ? [0, 1] is the confidence of the estimated disparity at x. Setting C(x) = 1 and the threshold = 0, ?x is equivalent to ignoring the confidence map. H(x) is the heavyside function, which is equal to 1 if x ? 0, and 0 otherwise. ?(?) is either the identify or the log function. The latter avoids overfitting the network to large disparities. Some papers restrict the sum in Eqn. <ref type="bibr" target="#b5">(6)</ref> to be over only the valid pixels or regions of interest, e.g., foreground or visible pixels <ref type="bibr" target="#b122">[123]</ref>, to avoid outliers. Other, e.g., Yao et al. <ref type="bibr" target="#b92">[93]</ref>, divide the loss into two parts, one over the initial disparity and the other one over the refined disparity. The overall loss is then defined as the weighted sum of the two losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Self-supervised methods</head><p>Self-supervised methods, originally used in optical flow estimation <ref type="bibr" target="#b123">[124]</ref>, <ref type="bibr" target="#b124">[125]</ref>, have been proposed as a possible solution in the absence of sufficient ground-truth training data. These methods mainly rely on image reconstruction losses, taking advantage of the projective geometry, and the spatial and temporal coherence when multiple images of the same scene are available. The rationale is that if the estimated disparity map is as close as possible to the ground truth, then the discrepancy between the reference image and any of the other images but unprojected using the estimated depth map onto the reference image, is also minimized. The general loss function is of the form:</p><formula xml:id="formula_11">L = 1 N x D ? (I ref ) (x) ? ? ? ref (x) ,<label>(7)</label></formula><p>where? ref , which is I right but unwarped onto I ref using the estimated disparity, and D is a measure of distance. The mapping function ? can be:</p><p>? The identity <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b125">[126]</ref>, <ref type="bibr" target="#b126">[127]</ref>. In this case, the loss of Eqn. <ref type="formula" target="#formula_11">(7)</ref> is called a photometric or image reconstruction loss. The distance D can be the L 1 or L 2 distance. Some papers <ref type="bibr" target="#b69">[70]</ref> also use more complex metrics such as the structural dissimilarity <ref type="bibr" target="#b127">[128]</ref> between patches in I ref and in? ref .</p><p>While stereo-based supervision methods do not require ground-truth 3D labels, they rely on the availability of calibrated stereo pairs during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Weakly supervised methods</head><p>Supervised methods for disparity estimation can achieve promising results if trained on large quantities of ground truth depth data. However, manually obtaining groundtruth depth data is extremely difficult and expensive, and is prone to noise and inaccuracies. Weakly supervised methods rely on auxiliary signals to reduce the amount of manual labelling. In particular, Tonioni et al. <ref type="bibr" target="#b128">[129]</ref> used as a supervisory signal the depth estimated using traditional stereo matching techniques to fine-tune depth estimation networks. Since such depth data can be sparse, noisy, and prone to errors, they propose a confidence-guided loss that penalizes ground-truth depth values that are deemed not reliable. It is defined using Eqn. (6) by setting D(?) to be the L 1 distance, and &gt; 0. Kuznietsov et al. <ref type="bibr" target="#b129">[130]</ref> use sparse ground-truth depth for supervised learning, while enforcing the deep network to produce photo-consistent dense depth maps in a stereo setup using a direct image alignment/reprojection loss. These two methods rely on an ad-hoc disparity estimator. To avoid that, Zhou et al. <ref type="bibr" target="#b130">[131]</ref> propose an iterative approach, which starts with a randomly initialized network. At each iteration, it computes matches from the left to the right images, and matches from the right to the left images. It then selects the high confidence matches and adds them as labelled data for further training in the subsequent iterations. The confidence is computed using the left-right consistency of Eqn. (12).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Incorporating additional cues</head><p>Several works incorporate additional cues and constraints to improve the quality of the disparity estimates. Examples include smoothness <ref type="bibr" target="#b69">[70]</ref>, left-right consistency <ref type="bibr" target="#b69">[70]</ref>, maximum depth <ref type="bibr" target="#b69">[70]</ref>, and scale-invariant gradient loss <ref type="bibr" target="#b120">[121]</ref>. Such cues can also be in the form of auxiliary information such as semantic cues used to guide the disparity estimation network. Below, we discuss a number of these works.</p><p>(1) Smoothness. In general, one can assume that neighboring pixels have similar disparity values. Such smoothness constraint can be enforced by minimizing:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>The absolute difference between the disparity predicted at x and those predicted at each pixel y within a certain predefined neighborhood N x around x:</p><formula xml:id="formula_12">L = 1 N x y?Nx |d x ? d y |.<label>(8)</label></formula><p>Here, N is the total number of pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>The magnitude of the first-order gradient ? of the estimated disparity map <ref type="bibr" target="#b67">[68]</ref>:</p><formula xml:id="formula_13">L = 1 N x {(?udx) + (?vdx)} , x = (u, v).<label>(9)</label></formula><p>? The magnitude of the second-order gradient of the estimated disparity <ref type="bibr" target="#b126">[127]</ref>, <ref type="bibr" target="#b131">[132]</ref>:</p><formula xml:id="formula_14">L = 1 N x (? 2 u d x ) 2 + (? 2 v d x ) 2 .<label>(10)</label></formula><p>? The second-order gradient of the estimated disparity map weighted by the image's second-order gradients <ref type="bibr" target="#b69">[70]</ref>:</p><formula xml:id="formula_15">L = 1 N x |? 2 u dx|e ?|? 2 u I(x)| + |? 2 v dx|e ?|? 2 v I(x)| .<label>(11)</label></formula><p>(2) Consistency. Zhong et al. <ref type="bibr" target="#b69">[70]</ref> introduced the loopconsistency loss, which is constructed as follows. Consider the left image I lef t and the synthesized image? lef t obtained by warping the right image to the left image coordinate using the disparity map defined on the right image. A second synthesized left image? lef t can also be generated by warping the left image to the right image coordinates, by using the disparities at the left image, and then warping it back to the left image using the disparity at the right image. The three versions of the left image provide two constraints: I lef t =? lef t and I lef t =? lef t , which can be used to regularize the disparity maps. Godard et al. <ref type="bibr" target="#b132">[133]</ref> introduce the left-right consistency term, which is a linear approximation of the loop consistency. The loss attempts to make the left-view disparity map equal to the projected right-view disparity map. It is defined as:</p><formula xml:id="formula_16">L = 1 N x |d x ?d x |,<label>(12)</label></formula><p>whered is the disparity at the right image but reprojected onto the coordinates of the left image.</p><p>(3) Maximum-depth heuristic. There may be multiple warping functions that achieve a similar warping loss, especially for textureless areas. To provide strong regularization in these areas, Zhong et al. <ref type="bibr" target="#b69">[70]</ref> use the Maximum-Depth Heuristic (MDH) <ref type="bibr" target="#b133">[134]</ref> defined as the sum of all depths/disparities:</p><formula xml:id="formula_17">L = 1 N x |d x |.<label>(13)</label></formula><p>(4) Scale-invariant gradient loss <ref type="bibr" target="#b120">[121]</ref>. It is defined as:</p><formula xml:id="formula_18">L = h?A x g h [D](x) ? g h [D](x) 2 ,<label>(14)</label></formula><p>where</p><formula xml:id="formula_19">A = {1, 2, 4, 8, 16}, x = (i, j), f i,j ? f (i, j), and g h [f ](i, j) = f i+h,j ? fi,j |f i+h,j ? fi,j| , f i,j+h ? fi,j |f i,j+h ? fi,j| .<label>(15)</label></formula><p>This loss penalizes relative depth errors between neighbouring pixels. This loss stimulates the network to compare depth values within a local neighbourhood for each pixel. It emphasizes depth discontinuities, stimulates sharp edges, and increases smoothness within homogeneous regions.</p><p>(5) Incorporating semantic cues. Some papers incorporate additional cues such as normal <ref type="bibr" target="#b134">[135]</ref>, segmentation <ref type="bibr" target="#b67">[68]</ref>, and edge <ref type="bibr" target="#b75">[76]</ref> maps, to guide the disparity estimation. These can be either provided at the outset, e.g., estimated with a separate method as in <ref type="bibr" target="#b75">[76]</ref>, or estimated jointly with the disparity map. Qi et al. <ref type="bibr" target="#b134">[135]</ref> propose a mechanism that uses the depth map to refine the quality of the normal estimates, and the normal map to refine the quality of the depth estimates. This is done using a two-stream network: a depth-to-normal network for normal map refinement using the initial depth estimates, and a normal-to-depth network for depth refinement using the estimated normal map. Yang et al. <ref type="bibr" target="#b67">[68]</ref> and Song et al. <ref type="bibr" target="#b75">[76]</ref> incorporate semantics by stacking semantic maps (segmentation masks in the case of <ref type="bibr" target="#b67">[68]</ref> and edge features in the case of <ref type="bibr" target="#b75">[76]</ref>) with the 3D cost volume. Yang et al. <ref type="bibr" target="#b67">[68]</ref> train jointly a disparity estimation network and a segmentation network by using a loss function defined as a weighted sum of the reconstruction error, a smoothness term, and a segmentation error. Song et al. <ref type="bibr" target="#b75">[76]</ref> further incorporate edge cues in the edge-aware smoothness loss to penalize drastic depth changes in flat regions. Also, to allow for depth discontinuities at object boundaries, the edge-aware smoothness loss is defined based on the gradient map obtained from the edge detection sub-network, which is more semantically meaningful than the variation in raw pixel intensities.</p><p>Wu et al. <ref type="bibr" target="#b78">[79]</ref> introduced an approach that fuses multiscale 4D cost volumes with semantic features obtained using a segmentation sub-network. The approach uses the features of the left and the right images as input to a semantic segmentation network similar to PSPNet <ref type="bibr" target="#b135">[136]</ref>. Semantic features for each image are then obtained from the output of the classification layer of the segmentation network. A 4D semantic cost volume is obtained by concatenating each unary semantic feature with their corresponding unary from the opposite stereo image across each disparity level. Both the spatial pyramid cost volumes and the semantic cost volume are fed into a 3D multi-cost aggregation module, which aggregates them, using an encoder-decoder followed Domain gap <ref type="figure">Fig. 7</ref>: Illustration of the domain gap between synthetic (left) and real (right) images. The left image is from the FlyingThings synthetic dataset <ref type="bibr" target="#b21">[22]</ref>. by a 3D feature fusion module, into a single 3D cost volume in a pairwise manner starting with the smallest volume.</p><p>In summary, appending semantic features to the cost volume improves the reconstruction of fine details, especially near object boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Domain adaptation and transfer learning</head><p>Deep architectures for depth estimation are severely affected by the domain shift issue, which hinders their effectiveness when performing inference on images significantly diverse from those used during the training stage. This can be observed, for instance, when moving between indoor and outdoor environments, from synthetic to real data, see <ref type="figure">Fig. 7</ref>, or between different outdoor/indoor environments, and when changing the camera model/parameters. As such, deep learning networks trained on one domain, e.g., by using synthetic data, suffer when applied to another domain, e.g., real data, resulting in blurry object boundaries and errors in ill-posed regions such as object occlusions, repeated patterns, and textureless regions. These are referred to as generalization glitches <ref type="bibr" target="#b136">[137]</ref>.</p><p>Several strategies have been proposed to adress this domain bias issue. They can be classified into two categories: adaptation by fine-tuning (Section 7.3.1) and adaptation by data transformation (Section 7.3.2). In both cases, the adaptation can be offline or online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">Adaptation by fine-tuning</head><p>Methods in this category perform domain adaptation by first training a network on images from a certain domain, e.g., synthetic images as in <ref type="bibr" target="#b21">[22]</ref>, and then fine-tuning it on images from a target domain. A major difficulty is to collect accurate ground-truth depth for stereo or multiview images from the target domain. Relying on active sensors (e.g., Li-DAR) to obtain such supervised labeled data is not feasible in practical applications. As such, recent works, e.g., <ref type="bibr" target="#b128">[129]</ref>, <ref type="bibr" target="#b136">[137]</ref>, <ref type="bibr" target="#b137">[138]</ref> rely on off-the-shelf stereo algorithms to obtain ground-truth disparity/depth labels in an unsupervised manner, together with state-of-the-art confidence measures to ascertain the correctness of the measurements of the offthe-shelf stereo algorithms. The latter is used in <ref type="bibr" target="#b128">[129]</ref>, <ref type="bibr" target="#b137">[138]</ref> to discriminate between reliable and unreliable disparity measurements, to select the former and fine tune a pretrained model, e.g., DispNet <ref type="bibr" target="#b21">[22]</ref>, using such smaller and sparse set of points as if they were ground-truth labels.</p><p>Pang et al. <ref type="bibr" target="#b136">[137]</ref> also use a similar approach as in <ref type="bibr" target="#b128">[129]</ref>, <ref type="bibr" target="#b137">[138]</ref> to address the generalization glitches. The approach, however, exploits the scale diversity, i.e., up-sampling the stereo pairs enables the model to perform stereo matching in a localized manner with subpixel accuracy, by performing iterative optimisation of predictions obtained at multiple resolutions of the input.</p><p>Note that self-supervised and weakly supervised techniques for disparity estimation, e.g., <ref type="bibr" target="#b132">[133]</ref>, <ref type="bibr" target="#b138">[139]</ref>, <ref type="bibr" target="#b139">[140]</ref>, <ref type="bibr" target="#b140">[141]</ref> can also be used for offline domain adaptation. In particular, if stereo pairs of the target domain are available, these techniques can be fine-tuned, in an unsupervised manner, using reprojection losses, see Sections 7.1.2 and 7.1.3.</p><p>Although effective, these offline adaptation techniques reduce the usability of the methods since users are required to train the models every time they are exposed to a new domain. As a result, several recent papers developed online adaptation techniques. For example, Tonioni et al. <ref type="bibr" target="#b83">[84]</ref> address the domain shift issue by casting adaptation as a continuous learning process whereby a stereo network can evolve online based on the images gathered by the camera during its real deployment. This is achieved in an unsupervised manner by computing error signals on the current frames, updating the whole network by a single back-propagation iteration, and moving to the next pair of input frames. To keep a high enough frame rate, Tonioni et al. <ref type="bibr" target="#b83">[84]</ref> propose a lightweight, fast, and modular architecture, called MADNet, which allows training sub-portions of the whole network independently from each other. This allows adapting disparity estimation networks to unseen environments without supervision at approximately 25 fps, while achieving an accuracy comparable to DispNetC <ref type="bibr" target="#b21">[22]</ref>. Similarly, Zhong et al. <ref type="bibr" target="#b141">[142]</ref> use video sequences to train a deep network online from a random initialization. They employ an LSTM in their model to leverage the temporal information during the prediction.</p><p>Zhong et al. <ref type="bibr" target="#b141">[142]</ref> and Tonioni et al. <ref type="bibr" target="#b83">[84]</ref> consider online adaptation separately from the initial training. Tonioni et al. <ref type="bibr" target="#b142">[143]</ref>, on the other hand, incorporate the adaptation procedure to the learning objective to obtain a set of initial parameters that are suitable for online adaptation, i.e., they can be adapted quickly to unseen environments. This is implemented using the model agnostic meta-learning framework of <ref type="bibr" target="#b143">[144]</ref>, an explicit learn-to-adapt framework that enables stereo methods to adapt quickly and continuously to new target domains in an unsupervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">Adaptation by data transformation</head><p>Methods in this category transform the data of one domain to look similar in style to the data of the other domain. For example, Atapour-Abarghoue et al. <ref type="bibr" target="#b144">[145]</ref> proposed a twostaged approach. The first stage trains a depth estimation model using synthetic data. The second stage is trained to transfer the style of synthetic images to real-world images. By doing so, the style of real images is first transformed to match the style of synthetic data and then fed into the depth estimation network, which has been trained on synthetic data. Zheng et al. <ref type="bibr" target="#b145">[146]</ref> perform the opposite by transforming the synthetic images to become more realistic and using them to train the depth estimation network. Zhao et al. <ref type="bibr" target="#b146">[147]</ref> consider both synthetic-to-real <ref type="bibr" target="#b145">[146]</ref> and realto-synthetic <ref type="bibr" target="#b144">[145]</ref>, <ref type="bibr" target="#b147">[148]</ref> translations. The two translators are trained in an adversarial manner using an adversarial loss and a cycle-consistency loss. That is, a synthetic image when converted to a real image and converted back to the synthetic domain should look similar to the original one.</p><p>Although these methods have been used for monocular depth estimation, they are applicable to (multi-view) stereo matching methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Learning the network architecture</head><p>Much research work in depth estimation is being spent on manually optimizing network architectures, but what about if the optimal network architecture, along with its parameters, could be also learnt from data? Saika et al. <ref type="bibr" target="#b148">[149]</ref> show how to use and extend existing AutoML techniques <ref type="bibr" target="#b149">[150]</ref> to efficiently optimize large-scale U-Netlike encoder-decoder architectures for stereo-based depth estimation. Traditional AutoML techniques have extreme computational demand limiting their usage to small-scale classification tasks. Saika et al. <ref type="bibr" target="#b148">[149]</ref> applies Differentiable Architecture Search (DARTs) <ref type="bibr" target="#b150">[151]</ref> to encoder-decoder architectures. Its main idea is to have a large network that includes all architectural choices and to select the best parts of this network by optimization. This can be relaxed to a continuous optimization problem, which, together with the regular network training, leads to a bilevel optimization problem. Experiments conducted on DispNet of <ref type="bibr" target="#b74">[75]</ref>, an improved version of <ref type="bibr" target="#b21">[22]</ref>, show that the automatically optimized DispNet (AutoDispNet) yields better performance compared to the baseline DispNet of <ref type="bibr" target="#b74">[75]</ref>, with about the same number of parameters. The paper also shows that the benefits of automated optimization carry over to large stacked networks. <ref type="table" target="#tab_10">Tables 3 and 4</ref>, respectively, compare the performance of the methods surveyed in this article on standard datasets such as KITTI2015 for pairwise stereo methods, and DTU, SUN3D and ETH3D for multiview stereo methods. Most of these methods have been trained on subsets of these publicly available datasets. A good disparity estimation method, once properly trained, should achieve good performance not only on publicly available benchmarks but on arbitrary novel images. They should not require re-training or finetuning every time the domain of usage changes. In this section, we will look at how some of these methods perform on novel unseen images. We will first describe in Section 8.1 the evaluation protocol, the images that will be used, and the evaluation metrics. We then discuss the performance of these methods in Sections 8.2 and 8.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION AND COMPARISON</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Evaluation protocol</head><p>We consider several key methods and evaluate their performance on the stereo subset of the ApolloScape dataset <ref type="bibr" target="#b33">[34]</ref>, and on an in-house collected set of four images. The motivation behind this choice is two-fold. First, the ApolloScape dataset is composed of stereo images taken outdoor in autonomous driving setups. Thus, it exhibits several challenges related to uncontrolled complex and varying lighting conditions, and heavy occlusions. Second, the dataset is novel and existing methods have not been trained or exposed to this dataset. Thus, it can be used to assess (a) Baseline: images with good lighting conditions. (b) Challenge: images with challenging lighting conditions. <ref type="figure">Fig. 8</ref>: Examples of stereo pairs and their ground-truth disparity maps from the ApolloScape dataset <ref type="bibr" target="#b33">[34]</ref>.</p><p>how these methods generalize to novel scenarios. In this dataset, ground truth disparities have been acquired by accumulating 3D point clouds from Lidar and fitting 3D CAD models to individually moving cars. We also use four in-house images of size W = 640 and H = 480, see <ref type="figure">Fig. 9</ref>, specifically designed to challenge these methods. Two of the images are of real scenes: a Bicycles scene composed of bicycles in a parking, and an indoor Desk scene composed of office furnitures. We use a moving stereo camera to capture multiple stereo pairs, and Structure-from-Motion (SfM) to build a 3D model of the scenes. We then render depth maps from the real cameras' viewpoints. Regions where depth is estimated with high confidence will be used as groundtruth. The remaining two images are synthetic, but reallooking. They include objects with complex structures, e.g., thin structures such as plants, large surfaces with either uniform colors or textures and repetitive patterns, presenting several challenges to stereo-based depth estimation algorithms.</p><p>We have tested 16 stereo-based methods published in 9 papers (between 2018 and 2019), see below. We use the network weights as provided by the authors.</p><p>(1) AnyNet <ref type="bibr" target="#b87">[88]</ref>: It is a four-stages network, which builds 3D cost volumes in a coarse-to-fine manner. The first stage estimates a low resolution disparity map by searching on a small disparity range. The subsequent stages estimate refined disparity maps using residual learning.</p><p>(2) DeepPruner <ref type="bibr" target="#b82">[83]</ref>: It combines deep learning with Patch-Match <ref type="bibr" target="#b100">[101]</ref> to speed up inference by adaptively pruning out the potentially large search space for correspondences. Two variants have been proposed: DeepPruner (Best), which downsamples the cost volume by a factor of 4, and Deep-Pruner (Fast), which downsamples it by a factor of 8.</p><p>(3) DispNet3 <ref type="bibr" target="#b74">[75]</ref>, an improved version of DispNet <ref type="bibr" target="#b21">[22]</ref> where occlusions and disparity maps are jointly estimated. (c) Right image. (d) Ground-truth disparity maps. <ref type="figure">Fig. 9</ref>: Four images, collected in-house and used to test 16 state-of-the-art methods. The green masks on some of the left images highlight the pixels where the ground-truth disparity is available. The disparity range is shown in pixels while the depth range is in meters.</p><p>dependencies. They are meant to improve the accuracy in challenging regions such as occlusions, large textureless/reflective regions, and thin structures.</p><p>(5) HighResNet <ref type="bibr" target="#b31">[32]</ref>: To refine both the spatial and the depth resolutions while operating on high resolution images, this method searches for correspondences incrementally using a coarse-to-fine hierarchy. Its hierarchical design also allows for anytime on-demand reports of disparity.</p><p>(6) PSMNet <ref type="bibr" target="#b63">[64]</ref>: It progressively regularizes a low resolution 4D cost volume, estimated from a pyramid of features.</p><p>(7) iResNet <ref type="bibr" target="#b62">[63]</ref>: The initial disparity and the learned features are used to calculate a feature constancy map, which measures the correctness of the stereo matching. The initial disparity map and the feature constancy map are then fed into a sub-network for disparity refinement.</p><p>(8) UnsupAdpt <ref type="bibr" target="#b128">[129]</ref>: It is an unsupervised adaptation approach that enables fine-tuning without any ground-truth information. It first trains DispNet-Corr1D <ref type="bibr" target="#b21">[22]</ref> using the KITTI 2012 training dataset and then adapts the network to KITTI2015 and Middlebury 2014.</p><p>(9) SegStereo <ref type="bibr" target="#b67">[68]</ref>: It is an unsupervised disparity estimation method, which uses segmentation masks to guide the disparity estimation. Both segmentation and disparity maps are jointly estimated with an end-to-end network. The methods (1) to <ref type="formula" target="#formula_11">(7)</ref> are supervised with groundtruth depth maps while the methods <ref type="formula" target="#formula_12">(8)</ref> and <ref type="formula" target="#formula_13">(9)</ref> are self-supervised. We compare their accuracy at runtime using the overall Root Mean Square Error (RMSE) defined as:</p><formula xml:id="formula_20">RMSE 2 linear = 1 N N |d i ?d i | 2 ,<label>(16)</label></formula><p>and the Bad-n error defined as the percentage of pixels whose estimated disparity deviates with more than n pixels from the ground truth. We use n ? {0.5, 1, 2, 3, 4, 5}. The Bad-n error considers the distribution and spread of the error and thus provides a better insight on the accuracy of the methods. In addition to accuracy, we also report the computation time and memory footprint at runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Computation time and memory footprint</head><p>From <ref type="table" target="#tab_12">Table 5</ref>, we can distinguish three types of methods; slow methods, e.g., PSMNet <ref type="bibr" target="#b63">[64]</ref>, DeepPruner (Best) and (Fast) <ref type="bibr" target="#b82">[83]</ref>, and GANet <ref type="bibr" target="#b84">[85]</ref>, require more than 1 second to estimate one disparity map. They also require between 3GB and 10GB (for DispNet3 <ref type="bibr" target="#b74">[75]</ref>) of memory at runtime. As such, these methods are very hard to deploy on mobile platforms. Average-speed methods, e.g., AnyNet <ref type="bibr" target="#b87">[88]</ref> and iResNet <ref type="bibr" target="#b62">[63]</ref>, produce a disparity map in around one second. Finally, fast methods, e.g., HighResNet <ref type="bibr" target="#b31">[32]</ref>, require less than 0.1 seconds. In general, methods that use 3D cost volumes are faster and less memory demanding than those that use 4D cost volumes. There are, however, two exceptions: iRes-Net <ref type="bibr" target="#b62">[63]</ref> and DeepPruner <ref type="bibr" target="#b82">[83]</ref>, which use 3D cost volumes but require a large amount of memory at runtime. While iResNet requires less than a second to process images of size W = 640, H = 480, since it uses 2D convolutions to regularize the cost volume, DeepPruner <ref type="bibr" target="#b82">[83]</ref> requires more than 3 seconds. We also observe that HighResNet <ref type="bibr" target="#b31">[32]</ref>, which uses 4D cost volumes but adopts a hierarchical approach to produce disparity on demand, is very efficient in terms of computation time as it only requires 37ms, which is almost 8 times faster than AnyNet <ref type="bibr" target="#b87">[88]</ref>, which uses 3D cost volumes. Note also that AnyNet <ref type="bibr" target="#b87">[88]</ref> can run on mobile devices due to its memory efficiency. <ref type="table" target="#tab_12">Table 5</ref> shows the average RMSE of each of the methods described in Section 8.1. We report the results on a baseline subset composed of 141 images that look more or less like KITTI2012 images, hereinafter referred to as baseline, and on another subset composed of 33 images with challenging lighting conditions, hereinafter referred to as challenge. Here, we focus on the relative comparison across methods since some of the high errors observed might be attributed to the way the ground-truth has been acquired in ApolloScape <ref type="bibr" target="#b33">[34]</ref> dataset, rather than to the methods themselves. We observe that these methods behave almost equally on the two subsets. However, the reconstruction error, is significantly important, &gt; 8 pixels, compared to the errors reported on standard datasets such as KITTI2012 and KITTI2015. This suggests that, when there is a significant domain gap between training and testing then the reconstruction accuracy can be significantly affected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Reconstruction accuracy</head><p>We also observe the same trend on the Bad-n curves of <ref type="figure" target="#fig_0">Fig. 10</ref> where, in all methods, more than 25% of the pixels had a reconstruction error that is larger than 5 pixels. The Bad-n curves show that the errors are large on the foreground pixels, i.e., pixels that correspond to cars, with more than 55% of the pixels having an error that is larger than 3 pixels (against 35% on the background pixels). Interestingly, <ref type="table" target="#tab_12">Table 5</ref> and <ref type="figure" target="#fig_0">Fig. 10</ref> show that most of the methods achieve similar reconstruction accuracies. The only exception is iResNet <ref type="bibr" target="#b62">[63]</ref> trained on Kitti2015 and on ROB <ref type="bibr" target="#b151">[152]</ref>, which had more than 90%, respectively 55%, of pixels with an error that is larger than 5 pixels. In all methods, less than 5% of the pixels had an error that is less than 2 pixels. This suggests that achieving sub-pixel accuracy remains an important challenge for future research. Note that SegStereo <ref type="bibr" target="#b67">[68]</ref>, which is self-supervised, achieves a similar or better performance than many of the supervised methods. Also, the unsupervised self-adaptation method of Tonioni et al. <ref type="bibr" target="#b128">[129]</ref>, which takes the baseline DispNet-Corr1D network <ref type="bibr" target="#b21">[22]</ref> trained on KITTI 2012 and adapts it to KITTI2015 and Middlebury 2014, achieves one of the best performances on the foreground regions.</p><p>In terms of the visual quality of the estimated disparities, see <ref type="figure" target="#fig_0">Fig. 11</ref>, we observe that most of the methods were able to recover the overall shape of trees but fail to reconstruct the details especially the leaves. The reconstruction errors are high in flat areas and around object boundaries. Also, highly reflective materials and poor lighting conditions remain a big challenge to these methods as shown in <ref type="figure" target="#fig_0">Fig. 11-(b)</ref>. The supplementary material provides more results on the four stereo pairs of <ref type="figure">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">FUTURE RESEARCH DIRECTIONS</head><p>Deep learning methods for stereo-based depth estimation have achieved promising results. The topic, however, is still in its infancy and further developments are yet to be expected. In this section, we present some of the current issues and highlight directions for future research.</p><p>(1) Camera parameters. Most of the stereo-based techniques surveyed in this article require rectified images. Multiview stereo techniques use Plane-Sweep Volumes or backprojected images/features. Both image rectification and PSVs require known camera parameters, which are challenging to estimate in the wild. Many papers attempted to address this problem for monocular depth estimation and for 3D shape reconstruction by jointly optimising for the camera parameters and the geometry of the 3D scene <ref type="bibr" target="#b152">[153]</ref>.</p><p>(2) Lighting conditions and complex material properties. Poor lighting conditions and complex materials properties remain a challenge to most of the current methods, see for example <ref type="figure" target="#fig_0">Fig. 11-(b)</ref>. Combining object recognition, highlevel scene understanding, and low-level feature learning can be one promising avenue to address these issues.</p><p>(3) Spatial and depth resolution. Most of the current techniques do not handle high resolution input images and generally produce depth maps of low spatial and depth resolution. Depth resolution is particularly limited, making the methods unable to reconstruct thin structures, e.g., vegetation and hair, and structures located at a far distance from the camera. Although refinement modules can improve the resolution of the estimated depth maps, the gain is still small compared to the resolution of the input images. This has recently been addressed using hierarchical techniques, which allow on-demand reports of disparity by capping the resolution of the intermediate results <ref type="bibr" target="#b31">[32]</ref>. In these methods, low resolution depth maps can be produced in realtime, and thus can be used on mobile platforms, while high resolution maps would require more computation time. Producing, in realtime, accurate maps of high spatial and depth resolutions remains a challenge for future research.</p><p>(4) Realtime processing. Most deep learning methods for disparity estimation use 3D and 4D cost volumes, which are processed and regularized using 2D and 3D convolutions. They are expensive in terms of memory requirements and processing time. Developing lightweight, and subsequently fast, end-to-end deep networks remains a challenging avenue for future research.</p><p>(5) Disparity range. Existing techniques uniformly discretize the disparity range. This results in multiple issues. In particular, although the reconstruction error can be small in the disparity space, it can result in an error of meters in the <ref type="bibr" target="#b24">25</ref>  <ref type="figure" target="#fig_0">Fig. 10</ref>: Overall Bad-n error, n ? [0.5, 5.0] on a selection of 141 (baseline) images from the stereo vision challenge of ApolloScape dataset <ref type="bibr" target="#b33">[34]</ref>. A similar behaviour is observed on the challenge subset, see the supplementary material. The horizontal axis is the error n while the vertical axis is the percentage of pixels whose estimated disparity deviates with more than n pixels from the ground truth.</p><p>depth space, especially at far ranges. One way to mitigate this is by discritizing disparity and depth uniformly in the log space. Also, changing the disparity range requires retraining the networks. Treating depth as a continuum could be one promising avenue for future research.</p><p>(6) Training. Deep networks heavily rely on the availability of training images annotated with ground-truth labels. This is very expensive and labor intensive for depth/disparity reconstruction. As such, the performance of the methods and their generalization ability can significantly be affected including the risk of overfitting the models to specific domains. Existing techniques mitigate this problem by either designing loss functions that do not require 3D annotations, or by using domain adaptation and transfer learning strategies. The former, however, requires calibrated cameras. Domain adaptation techniques, especially unsupervised ones <ref type="bibr" target="#b137">[138]</ref>, are recently attracting more attention since, with these techniques, one can train with both synthetic data, which are easy to obtain, and real-world data. They also adapt, in an unsupervised manner and at run-time to ever-changing environments as soon as new images are gathered. Early results are very encouraging and thus expect in the future to see the emergence of large datasets, similar to ImageNet but for 3D reconstruction.</p><p>(7) Automatically learning the network architecture, its activation functions, and its parameters from data. Most existing research has focused on designing novel network architectures and novel training methods for optimizing their parameters. It is only recently that some papers started to focus on automatically learning optimal architectures. Early attempts, e.g., <ref type="bibr" target="#b148">[149]</ref>, focus on simple architectures. We expect in the future to see more research on automatically learning complex disparity estimation architectures and their activation functions, using, for example, the neuro-evolution theory <ref type="bibr" target="#b153">[154]</ref>, <ref type="bibr" target="#b154">[155]</ref>, which will free the need for manual network design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">CONCLUSION</head><p>This paper provides a comprehensive survey of the recent developments in stereo-based depth estimation using deep learning techniques. Despite their infancy, these techniques are achieving state-of-the-art results. Since 2014, we have entered a new era where data-driven and machine learning techniques play a central role in image-based depth reconstruction. We have seen that, from 2014 to 2019, more than 150 papers on the topic have been published in the major computer vision, computer graphics, and machine learning conferences and journals. Even during the final stages of this submission, more new papers are being published making it difficult to keep track of the recent developments, and more importantly, understand their differences and similarities, especially for new comers to the field. This timely survey can thus serve as a guide to the reader to navigate this fastgrowing field of research. Finally, there are several related topics that have not been covered in this survey. Examples include image-based 3D object reconstruction using deep learning, which has been recently surveyed by Han et al. <ref type="bibr" target="#b152">[153]</ref>, and monocular and video-based depth estimation, which requires a separate survey paper given the large amount of papers that have been published on the topic in the past 5 to 6 years. Other topics include photometric stereo and active stereo <ref type="bibr" target="#b155">[156]</ref>, <ref type="bibr" target="#b156">[157]</ref>, which are outside the scope of this paper.  <ref type="figure">Fig. 8-(b)</ref>. <ref type="figure" target="#fig_0">Fig. 11</ref>: Pixel-wise errors between the ground-truth disparities and the disparities estimated from the images of <ref type="figure">Fig. 8</ref>.</p><p>Mohammed Bennamoun is Winthrop Professor in the Department of Computer Science and Software Engineering at UWA and is a researcher in computer vision, machine/deep learning, robotics, and signal/speech processing. He has published 4 books (available on Amazon, 1 edited book, 1 Encyclopedia article, 14 book chapters, 120+ journal papers, 250+ conference publications, 16 invited and keynote publications. His h-index is 47 and his number of citations is 10,000+ (Google Scholar). He was awarded 65+ competitive research grants, from the Australian Research Council, and numerous other Government, UWA and industry Research Grants. He successfully supervised +26 PhD students to completion. He won the Best Supervisor of the Year Award at QUT (1998), and received award for research supervision at UWA (2008 and 2016) and Vice-Chancellor Award for mentorship (2016). He delivered conference tutorials at major conferences, including: IEEE CVPR 2016, Interspeech 2014, IEEE ICASSP, and ECCV. He was also invited to give a Tutorial at an International Summer School on Deep Learning (DeepLearn 2017).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The building blocs of a stereo matching pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Feature learning architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Multiscale feature learning architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>network -one branch per disparity level Multi-branch network -one branch per image Similarity estimation (Section 5.2.1Taxonomy of the network architectures for stereo-based disparity estimation using end-to-end deep learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 :Zhong</head><label>3</label><figDesc>Taxonomy and comparison of 28 end-to-end deep learning-based disparity estimation techniques. "FCN": Fully-Connected Network, "SPN": Spatial Propagation Network. "LRCR": Left-Right Comparative Recurrent model, "MCUA": Multi-Level Context Ultra-Aggregation for Stereo Matching. "DLA": Deep layer aggregation [69], "VPP": Volumetric Pyramid Pooling. The performance is measured on KITTI2015 test dataset. et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Kendall et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Knobelreiter</head><label></label><figDesc>et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>[</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>66 ?</head><label>66</label><figDesc>Yang et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>aggregation layers Guo et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Wang</head><label></label><figDesc>et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 5 -</head><label>5</label><figDesc>(b), and ignore context along the disparity dimension. Yao et al.<ref type="bibr" target="#b91">[92]</ref> sequentially regularize the 2D cost maps along the depth direction via a Gated Recurrent Unit (GRU), seeFig. 5-(c). This reduces drastically the memory consumption, e.g., from 15.4GB in<ref type="bibr" target="#b92">[93]</ref> to around 5GB, making highresolution reconstruction feasible, while capturing context along both the spatial and the disparity dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>( 3 )</head><label>3</label><figDesc>Regularization using 3D convolutions (3DConvNet), Fig. 5-(d). Khamis et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>.</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>Huang et al.<ref type="bibr" target="#b26">[27]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 6 :</head><label>6</label><figDesc>Taxonomy of multivew stereo methods. (a), (b), and (c) perform early fusion, while (d) performs early fusion by aggregating features across depth plans, and late fusion by aggregating cost volumes across views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>?</head><label></label><figDesc>A mapping to the feature space<ref type="bibr" target="#b67">[68]</ref>, i.e., ? (I ref ) = f where f is the learned feature map. ? The gradient of the image, i.e., ? (I ref ) = ?I ref ,which is less sensitive to variations in lighting and acquisition conditions than the photometric loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>( 4 )</head><label>4</label><figDesc>GANet [85]: It replaces a large number of the 3D convolutional layers in the regularization block with (1) two 3D convolutional layers, (2) a semi-global aggregation layer (SGA), and (3) a local guided aggregation layer (LGA). SGA and LGA layers capture local and whole-image cost (a) Left image. (b) Highlights of regions of interest where ground-truth disparity is estimated with high confidence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Hamid Laga is with the Information Technology, Mathematics and Statistics Discipline, Murdoch University (Australia), and with the Phenomics and Bioinformatics Research Centre, University of South Australia. Email: H.Laga@murdoch.edu.au ? Laurent Valentin Jospin is with the University of Western Australia, Perth, WA 6009, Australia. Email: laurent.jospin@research.uwa.edu.au ? Farid Boussaid is with the University of Western Australia, Perth, WA 6009, Australia. Email: farid.boussaid@uwa.edu.au</figDesc><table /><note>?? Mohammed Bennamoun is with the University of Western Australia, Perth, WA 6009, Australia. Email: mohammed.bennamoun@uwa.edu.au Manuscript received June, 2020; revised June, 2020.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Taxonomy and comparison of deep learning-based stereo matching techniques.</figDesc><table><row><cell>Method</cell><cell>Year</cell><cell cols="4">Feature computation Architectures Dimension</cell><cell cols="2">Similarity</cell><cell>Degree of supervision</cell><cell>Training</cell><cell>Loss</cell><cell>Regularization</cell></row><row><cell cols="2">Zagoruyko [37] 2015</cell><cell></cell><cell>ConvNet</cell><cell cols="2">Multiscale</cell><cell cols="2">FCN</cell><cell>Supervised with positive/negative samples</cell><cell>Hinge and squared L2</cell><cell>NA</cell></row><row><cell>Han [38]</cell><cell>2015</cell><cell></cell><cell>ConvNet</cell><cell cols="2">Fixed scale</cell><cell cols="2">FCN</cell><cell>Supervised</cell><cell>Cross-entropy</cell><cell>NA</cell></row><row><cell>Zbontar [39]</cell><cell>2015</cell><cell></cell><cell>ConvNet</cell><cell cols="2">Fixed scale</cell><cell cols="2">Hand-crafted</cell><cell>Triplet contrastive learning</cell><cell>L1</cell><cell>MRF</cell></row><row><cell>Chen [40]</cell><cell>2015</cell><cell></cell><cell>ConvNet</cell><cell cols="5">Multiscale Correlation + voting Supervised with positive/negative samples</cell><cell>L1</cell><cell>MRF</cell></row><row><cell>Simo [41]</cell><cell>2015</cell><cell></cell><cell>ConvNet</cell><cell cols="2">Fixed scale</cell><cell>L2</cell><cell></cell><cell>Supervised with positive/negative samples</cell><cell>L2</cell><cell>NA</cell></row><row><cell>Zbontar [42]</cell><cell>2016</cell><cell></cell><cell>ConvNet</cell><cell cols="4">Fixed scale Hand-crafted, FCN</cell><cell>Supervised with known disparity</cell><cell>Hinge</cell><cell>Classic stereo</cell></row><row><cell cols="2">Balantas [43] 2016</cell><cell></cell><cell>ConvNet</cell><cell cols="2">Fixe scale</cell><cell>L2</cell><cell></cell><cell>Supervised, triplet contrastive learning</cell><cell>Soft-Positive-Negative (Soft-PN)</cell><cell>?</cell></row><row><cell>Mayer [22]</cell><cell>2016</cell><cell></cell><cell>ConvNet</cell><cell cols="2">Fixed-scale</cell><cell cols="2">Hand-crafted</cell><cell>Supervised</cell><cell>?</cell><cell>Encoder-decoder</cell></row><row><cell>Luo [44]</cell><cell>2016</cell><cell></cell><cell>ConvNet</cell><cell cols="2">Fixed scale</cell><cell cols="2">Correlation</cell><cell>Supervised</cell><cell>Cross-entropy</cell><cell>MRF</cell></row><row><cell>Kumar [45]</cell><cell>2016</cell><cell></cell><cell>ConvNet</cell><cell cols="2">Fixed scale</cell><cell cols="2">ConvNet</cell><cell>Supervised, triplet contrastive learning</cell><cell>Maximise inter-class distance,</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>minimize inter-class distance.</cell></row><row><cell>Shaked [46]</cell><cell cols="5">2017 Highway network with Fixed scale</cell><cell cols="2">FCN</cell><cell>Supervised</cell><cell>Hinge+cross-entropy</cell><cell>Classic+4Conv+</cell></row><row><cell></cell><cell></cell><cell cols="3">multilevel skip connections</cell><cell></cell><cell></cell><cell></cell><cell>5FC</cell></row><row><cell cols="2">Hartmann [47] 2017</cell><cell></cell><cell>ConvNet</cell><cell cols="2">Fixed scale</cell><cell cols="2">ConvNet</cell><cell>Supervised</cell><cell>Croos-entropy</cell><cell>Encoder</cell></row><row><cell>Park [48]</cell><cell>2017</cell><cell></cell><cell>ConvNet</cell><cell cols="2">Fixed scale</cell><cell cols="2">1 ? 1 Convs,</cell><cell>Supervised</cell><cell>?</cell><cell>NA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ReLU, SPP</cell></row><row><cell>Ye [49]</cell><cell>2017</cell><cell></cell><cell>ConvNet</cell><cell cols="2">Fixed scale</cell><cell cols="2">FCN</cell><cell>Supervised</cell><cell>L1</cell><cell>SGM</cell></row><row><cell></cell><cell></cell><cell cols="2">Multisize pooling</cell><cell></cell><cell></cell><cell cols="2">(1 ? 1 convs)</cell></row><row><cell cols="2">Tulyakov [50] 2017</cell><cell cols="6">Generic -independent of the network architecture</cell><cell>Weakly supervised</cell><cell>MIL, Contrastive, Contrastive-DP</cell><cell>?</cell></row><row><cell></cell><cell cols="2">Left feature volume</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Right feature volume</cell><cell>Raw cost volume</cell><cell>Regularized cost volume</cell><cell cols="2">Raw disparity map</cell><cell>Final disparity map</cell></row><row><cell>left and right images (rectified)</cell><cell cols="2">Feature extraction</cell><cell>Feature matching</cell><cell>Cost volume regularization</cell><cell>Argmin</cell><cell></cell><cell>Post-processing and refinement</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ConvNet vs. ResNet: While Zbontar et al. [39], [42] and Han et al. [38] use standard convolutional layers in the feature extraction block, Shaked and Wolf</figDesc><table><row><cell>.</cell></row><row><cell>4.1.2.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>90] re-designed the RNN-formed CRF Fig. 5: Cost volume regularization schemes [92]: (a) does not consider context, (b) captures context along the spatial dimensions using 2D convolutions, (c) captures context along the spatial and disparity dimensions by recurrent regularization using 2D convolutions, and (d) captures context in all dimensions by using 3D convolutions.module so that the model parameters are independent of the number of depth samples. Paschalidou et al.<ref type="bibr" target="#b90">[91]</ref> formulate the inference in a MRF as a differentiable function, hence allowing end-to-end training using back propagation. Note that Zheng et al.<ref type="bibr" target="#b88">[89]</ref> and Paschalidou et al.<ref type="bibr" target="#b90">[91]</ref> focus on multi-view stereo (Section 6). Their approaches, however, are generic and can be used to regularize 3D cost volumes obtained using pairwise stereo networks.</figDesc><table><row><cell>Memory: H x W</cell><cell></cell><cell>H x W</cell><cell></cell><cell>H x W</cell><cell></cell><cell>H x W x D</cell><cell></cell></row><row><cell>dmin</cell><cell>dmax</cell><cell>dmin</cell><cell>dmax</cell><cell>dmin</cell><cell>dmax</cell><cell>dmin</cell><cell>dmax</cell></row><row><cell>(a) Winner-take-all</cell><cell></cell><cell cols="2">(b) Spatial regularization</cell><cell cols="2">(c) Recurrent regularization</cell><cell cols="2">(d) 3D CNN regularization</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Flynn et al. [66]. Kar et al. [60] and Yao et al. [93].</figDesc><table><row><cell>. . . patch 1 patch 2</cell><cell>Encoder Encoder</cell><cell>Feature fusion</cell><cell>(pooling, recurrent net)</cell><cell>Encoder</cell><cell>Cost 0 ... 1</cell><cell>I0 I1</cell><cell>. . . .</cell><cell>I1 1</cell><cell>I1 nd In-1 nd</cell><cell>... ... ...</cell><cell>I0 I0</cell><cell>I1 1 I1 2 I1 nd</cell><cell cols="2">Encoder Encoder</cell><cell>Encoder</cell><cell>Raw cost volume</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>In</cell><cell></cell><cell>In-1 1</cell><cell></cell><cell></cell><cell>I0</cell><cell></cell><cell cols="2">Encoder</cell><cell></cell></row><row><cell></cell><cell>Encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>patch n</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Aggregation accros views</cell><cell cols="3">Aggregation accross depth planes</cell></row><row><cell cols="10">(a) Hartmann et al. [47]. Unprojection Encoder (b) . . . . Unprojection Feature fusion Cost volume Matching features (pooling, recurrent net) Encoder</cell><cell cols="3">Kar et al. Encoder</cell><cell cols="2">Decoder</cell><cell cols="2">Regularized cost volume</cell></row><row><cell></cell><cell>Encoder</cell><cell></cell><cell></cell><cell>Unprojection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">MVSNet of Yao et al.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Feature volumes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">(c) .</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 4 :</head><label>4</label><figDesc>Taxonomy and comparison of 13 deep learning-based MVS techniques.</figDesc><table><row><cell>Method</cell><cell cols="2">Year Representation</cell><cell>Fusion</cell><cell>Training</cell><cell cols="8">Peformance on (DTU, SUN3D, ETH3D) #images Error (mm) % &lt; 1mm % &lt; 2mm # Params Memory Complexity Time (s) Complexity</cell></row><row><cell>Kar et al. [60]</cell><cell cols="2">2017 Volumetric</cell><cell>Recurrent fusion</cell><cell>Supervised</cell><cell>Variable</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>of 3D feature grids</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Hartmann et al. [47] 2017</cell><cell cols="2">Replace correlation by pooling</cell><cell>Supervised</cell><cell>5</cell><cell>(1.356, ?, ?)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(can vary)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ji et al. [116]</cell><cell cols="2">2017 Volumetric</cell><cell>Reconstructed surfaces</cell><cell>Supervised</cell><cell>5</cell><cell>(0.745, ?, ?)</cell><cell>69.95</cell><cell>74.4</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>4 hrs</cell></row><row><cell>Choi et al. [117]</cell><cell cols="2">2018 Volumetric</cell><cell>Pairwise cost volumes</cell><cell>Supervised</cell><cell>5</cell><cell>(0.6511, ?, ?)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>Huang et al. [27]</cell><cell>2018</cell><cell>PSV</cell><cell cols="2">Encoder-decoder for intra-volume, Supervised</cell><cell>Variable</cell><cell>(?, 0.419, 0.412)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Max pooling for inter-volume</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Leroy et al. [118]</cell><cell>2018</cell><cell>PSV</cell><cell>Depth fusion</cell><cell>Supervised</cell><cell>Variable</cell><cell>(0.599, ?, ?)</cell><cell>?</cell><cell>?</cell><cell>72K</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell cols="3">Paschalidou et al. [91] 2018 Depth-based</cell><cell>Avg. pooling over</cell><cell>Supervised</cell><cell>Variable</cell><cell>(?, ?, ?)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>7GB</cell><cell>?</cell><cell>25 mins</cell></row><row><cell></cell><cell></cell><cell></cell><cell>pairwsie correlations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yao et al. [93]</cell><cell>2018</cell><cell>PSV</cell><cell>Feature pooling by variance</cell><cell>Supervised</cell><cell>5</cell><cell cols="2">(0.462, 0.397, 0.470) 75.69</cell><cell>80.25</cell><cell>363K</cell><cell cols="3">5.28GB O(H ? W ? n d ) 0.9s</cell></row><row><cell>Wang et al. [120]</cell><cell cols="2">2018 PSV and abs.</cell><cell>Concatenation of pairwise</cell><cell>Supervised</cell><cell>Variable</cell><cell>(?, 0.114, 0.257)</cell><cell>?</cell><cell>?</cell><cell>33.9M for</cell><cell>?</cell><cell>?</cell><cell>0.04</cell></row><row><cell></cell><cell></cell><cell>difference</cell><cell>cost volumes and ref. image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>n d = 64</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hou et al. [115]</cell><cell>2019</cell><cell>?</cell><cell>Temporal fusion of</cell><cell>Supervised</cell><cell>Variable</cell><cell>(?, 0.101, 0.229)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the latent rep.</cell><cell></cell><cell>(video sequence)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Luo et al. [119]</cell><cell>2019</cell><cell>PSV</cell><cell>Feature pooling by variance</cell><cell>Supervised</cell><cell>Variable</cell><cell>(0.406, ?, ?)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>Xue et al.et al. [90]</cell><cell>2019</cell><cell>PSV</cell><cell>Cost volume</cell><cell>Supervised</cell><cell>5</cell><cell>(0.398, ?, ?)</cell><cell>80.02</cell><cell>83.84</cell><cell>571K</cell><cell cols="3">5.43GB O(H ? W ? n d ) 1.8s</cell></row><row><cell></cell><cell></cell><cell></cell><cell>pooling by variance</cell><cell></cell><cell>(can vary)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Won et al. [30]</cell><cell cols="2">2019 Spherical PSV</cell><cell>Concatenation</cell><cell>Supervised</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Disp. ? [9.3, 34.0] Disp. ? [18.7, 29.9] Disp. ? [5.6, 14.5] Disp. ? [5.5, 13.2] Depth ? [2.1, 7.8] Depth ? [2.4, 3.3] Depth ? [7.8, 25.0] Depth ? [10.8, 25.6]</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 5 :</head><label>5</label><figDesc>Computation time and memory consumption, at runtime, on images of size 640 ? 480. SegStereo<ref type="bibr" target="#b67">[68]</ref> has been tested on a PC equipped with an Nvidia GeForce RTX 2080. The other methods have been tested on a PC equipped with an Nvidia Tesla K40 GPU with a 12 Go graphic memory. See the Supplementary Material for a visual representation.</figDesc><table><row><cell>Method</cell><cell cols="5">Supervision Cost vol. Time (s) Memory (GB) mode</cell><cell>Training set</cell><cell>Bkg</cell><cell cols="2">Baseline Fg Bkg+Fg</cell><cell>Bkg</cell><cell cols="2">Challenge Fg Bkg+Fg</cell></row><row><cell>AnyNet [88]</cell><cell>Supervised</cell><cell>3D</cell><cell>0.285</cell><cell>0.232</cell><cell></cell><cell>KITTI2015</cell><cell>9.46</cell><cell>10.74</cell><cell>10.34</cell><cell>9.83</cell><cell>11.60</cell><cell>11.15</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>KITTI2012</cell><cell>9.80</cell><cell>10.29</cell><cell>10.20</cell><cell>9.34</cell><cell>10.62</cell><cell>10.61</cell></row><row><cell>DeepPruner (Best) [83]</cell><cell>Supervised</cell><cell>3D</cell><cell>8.430</cell><cell>8.845</cell><cell></cell><cell>KITTI2012+2015</cell><cell>9.64</cell><cell>9.43</cell><cell>9.46</cell><cell>12.38</cell><cell>8.74</cell><cell>10.48</cell></row><row><cell>DeepPruner (Fast) [83]</cell><cell>Supervised</cell><cell>3D</cell><cell>3.930</cell><cell>6.166</cell><cell></cell><cell>KITTI2012+2015</cell><cell>9.56</cell><cell>9.90</cell><cell>9.94</cell><cell>8.74</cell><cell>9.75</cell><cell>9.86</cell></row><row><cell>DispNet3 [75]</cell><cell>Supervised</cell><cell>3D</cell><cell>?</cell><cell>10.953</cell><cell></cell><cell>CSS-ft-KITTI</cell><cell>9.68</cell><cell>9.62</cell><cell>9.70</cell><cell>8.38</cell><cell>11.00</cell><cell>11.11</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CSS-FlyingThings3D [22]</cell><cell>9.11</cell><cell>9.64</cell><cell>9.54</cell><cell>8.97</cell><cell>9.91</cell><cell>10.19</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>css-FlyingThings3D [22]</cell><cell>9.29</cell><cell>9.98</cell><cell>9.87</cell><cell>9.66</cell><cell>10.34</cell><cell>10.61</cell></row><row><cell>GANet [85]</cell><cell>Supervised</cell><cell>4D</cell><cell>8.336</cell><cell>3.017</cell><cell></cell><cell>KITTI2015</cell><cell>9.55</cell><cell>9.38</cell><cell>9.39</cell><cell>9.37</cell><cell>9.50</cell><cell>9.89</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>KITTI2012</cell><cell>9.98</cell><cell>10.29</cell><cell>10.25</cell><cell cols="2">10.69 10.95</cell><cell>11.55</cell></row><row><cell>HighResNet [32]</cell><cell>Supervised</cell><cell>4D</cell><cell>0.037</cell><cell>0.474</cell><cell cols="3">Middleburry [20], KITTI2015 [21], 9.47</cell><cell>9.91</cell><cell>9.94</cell><cell>8.58</cell><cell>9.64</cell><cell>9.78</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ETH3D [25], HR-VS [32]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSMNet [64]</cell><cell>Supervised</cell><cell>4D</cell><cell>1.314</cell><cell>1.900</cell><cell></cell><cell>KITTI2015</cell><cell>9.88</cell><cell>9.81</cell><cell>9.80</cell><cell>10.10</cell><cell>9.42</cell><cell>9.93</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>KITTI2012</cell><cell cols="2">10.17 10.24</cell><cell>10.29</cell><cell cols="2">10.66 10.33</cell><cell>11.00</cell></row><row><cell>iResNet [63]</cell><cell>Supervised</cell><cell>3D</cell><cell>0.939</cell><cell>7.656</cell><cell></cell><cell>KITTI2015</cell><cell cols="2">60.04 61.72</cell><cell>60.54</cell><cell cols="2">45.87 46.85</cell><cell>47.86</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ROB [152]</cell><cell cols="2">22.08 17.16</cell><cell>18.08</cell><cell cols="2">23.01 16.51</cell><cell>18.83</cell></row><row><cell>UnsupAdpt [129]</cell><cell>Self-supervised</cell><cell>3D</cell><cell>?</cell><cell>?</cell><cell></cell><cell cols="2">KITTI2012 adapted to KITTI2015 9.44</cell><cell>10.39</cell><cell>10.19</cell><cell cols="2">10.10 10.42</cell><cell>10.78</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Shadow-on-Truck</cell><cell>8.52</cell><cell>10.08</cell><cell>9.58</cell><cell cols="2">10.66 10.88</cell><cell>10.27</cell></row><row><cell>SegStereo [68]</cell><cell>Self-supervised</cell><cell>3D</cell><cell>0.195</cell><cell>? 12.00</cell><cell></cell><cell>CityScapes [23]</cell><cell>9.26</cell><cell>10.30</cell><cell>10.17</cell><cell>9.03</cell><cell>10.49</cell><cell>10.54</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We would like to thank all the authors of the reference papers who have made their codes and datasets publicly available. This work is supported in part by Murdoch University's Vice Chancellor's Small Steps of Innovation Funding Program, and by ARC DP150100294 and DP150104251.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A guide to convolutional neural networks for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="207" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised CNN for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly-supervised disentangling with recurrent transformations for 3D view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1099" to="1107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep convolutional inverse graphics network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">F</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2539" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-view 3D models from single images with a convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="322" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">View synthesis by appearance flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transformation-grounded image generation network for novel 3D view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="702" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="539" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="840" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
	<note type="report_type">ECCV</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A benchmark for the evaluation of rgb-d slam systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Engelhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="573" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sun3d: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1625" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">High-resolution stereo datasets with subpixel-accurate ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ne?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Westling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
	<note>in German conference on pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale data for multiple-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aanaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="168" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A multi-view stereo benchmark with high-resolution images and multi-camera videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sch?ps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3260" to="3269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1746" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DeepMVS: Learning Multi-view Stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="2821" to="2830" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MegaDepth: Learning Single-View Depth Prediction From Internet Photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="2041" to="2050" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reconstruction-based Pairwise Depth Dataset for Depth Image Enhancement Using CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="422" to="438" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">OmniMVS: End-to-End Learning for Omnidirectional Stereo Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8987" to="8996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-End Learning for Omnidirectional Stereo Matching with Uncertainty Prior</title>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical Deep Stereo Matching on High-Resolution Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Manela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Happold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5515" to="5524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Drivingstereo: A large-scale dataset for stereo matching in autonomous driving scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="899" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The apolloscape open dataset for autonomous driving and its application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kassahun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahmudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ricou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Durgesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hauswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?hlegg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dorn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06320</idno>
		<title level="m">A2d2: Audi autonomous driving dataset</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">What makes good synthetic training data for learning disparity and optical flow estimation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="942" to="960" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Match-Net: Unifying feature and metric learning for patch-based matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3279" to="3286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Computing the stereo matching cost with a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1592" to="1599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A deep visual correspondence embedding model for stereo matching costs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="972" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-32</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">PN-Net: Conjoined Triple Deep Network for Learning Local Image Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient deep learning for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5695" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning local image descriptors with deep siamese and triplet convolutional networks by minimising global loss functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5385" to="5394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improved stereo matching with constant highway networks and reflective confidence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4641" to="4650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learned multi-patch similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havlena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1595" to="1603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Look wider to match image patches with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1788" to="1792" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient stereo matching leveraging deep local and context information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="18" to="745" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of deep metrics for stereo reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1339" to="1348" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep Ordinal Regression Network for Monocular Depth Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Descriptor matching with convolutional neural networks: a comparison to sift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="328" to="341" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">SGM-Nets: Semi-global matching with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to Fuse Proposals from Multiple Scanline Optimizations in Semi-Global Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="739" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Learning a general-purpose confidence measure based on o(1) features and a smarter aggregation strategy for semi global matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A quantitative evaluation of confidence measures for stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2121" to="2133" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="364" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICCV</title>
		<imprint>
			<biblScope unit="page" from="66" to="75" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Cascade residual learning: A two-stage convolutional neural network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning for Disparity Estimation Through Feature Constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G H L W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Q L Z J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="2811" to="2820" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Pyramid Stereo Matching Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="5410" to="5418" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Multi-Level Context Ultra-Aggregation for Stereo Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3283" to="3291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">DeepStereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5515" to="5524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">SegStereo: Exploiting semantic information for disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="636" to="651" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="2403" to="2412" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Self-supervised learning for stereo matching with self-improving ability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00930</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">End-to-end training of hybrid CNN-CRF models for stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kn?belreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reinbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shekhovtsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1456" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">StereoNet: Guided hierarchical refinement for real-time edge-aware depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">ActiveStereoNet: end-to-end self-supervised learning for active stereo systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tankovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schoenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fanello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page" from="784" to="801" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Left-Right Comparative Recurrent Model for Stereo Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="3838" to="3846" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Occlusions, Motion and Depth Boundaries with a Generic Network for Disparity, Optical Flow or Scene Flow Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="614" to="630" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">EdgeStereo: A Context Integrated Residual Pyramid Network for Stereo Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Deep Stereo Matching with Explicit Cost Aaggregation Sub-architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Practical Deep Stereo (PDS): Toward applications-friendly deep stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="5871" to="5881" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Semantic Stereo Matching With Pyramid Cost Volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7484" to="7493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Hierarchical Discrete Distribution Decomposition for Match Density Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6044" to="6053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">StereoDRNet: Dilated Residual StereoNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Vortex pooling: Improving context representation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06242</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Deep-Pruner: Learning Efficient Stereo Matching via Differentiable PatchMatch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Duggal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4384" to="4393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Real-time self-adaptive deep stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="195" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">GA-Net: Guided Aggregation Net for End-to-End Stereo Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Group-wise Correlation Stereo Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3273" to="3282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">On the Over-Smoothing Problem of CNN Based Disparity Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8997" to="9005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Anytime stereo image depth estimation on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5893" to="5900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks,&quot; in IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">MVSCRF: Learning Multi-View Stereo With Conditional Random Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4312" to="4321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">RayNet: Learning Volumetric 3D Reconstruction With Ray Potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paschalidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="3897" to="3906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Recurrent MVSNet for High-Resolution Multi-View Stereo Depth Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5525" to="5534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">MVSNet: Depth Inference for Unstructured Multi-view Stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page" from="767" to="783" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Single-Image Depth Estimation Based on Fourier Domain Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="500" to="513" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Realtime local stereo via edge-aware disparity propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="201" to="206" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Learning affinity via spatial propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Depth Estimation via Affinity Learned with Convolutional Spatial Propagation Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="103" to="119" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Depth Coefficients for Depth Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Patch Based Confidence Prediction for Dense Disparity Map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Detect, Replace, Refine: Deep structured prediction for pixel wise labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5248" to="5257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Ensemble learning for confidence measures in stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeusler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kondermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="305" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Learning to detect ground control points for improving the accuracy of stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spyropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1621" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Leveraging stereo matching with learning-based confidence measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="101" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Learning from scratch a confidence measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Probflow: Joint optical flow and uncertainty estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Wannenwetsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1173" to="1182" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">CBMV: A Coalesced Bidirectional Matching Volume for Disparity Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Quantitative evaluation of confidence measures in a machine learning world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5228" to="5237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Learning to predict stereo reliability enforcing local consistency of confidence maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2452" to="2461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Lightweight probabilistic deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="3369" to="3378" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Beyond local reasoning for stereo confidence estimation with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benincasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="319" to="334" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Multi-View Stereo by Temporal Nonparametric Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2651" to="2660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">SurfaceNet: an endto-end 3D neural network for multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICCV</title>
		<imprint>
			<biblScope unit="page" from="2307" to="2315" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Learning Descriptor, Confidence, and Depth Estimation in Multi-view Stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="276" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Shape reconstruction using volume sweeping and learned photoconsistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="781" to="796" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">P-MVSNet: Learning Patch-Wise Matching Confidence Aggregation for Multi-View Stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">461</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">Mvdepthnet: real-time multiview depth estimation neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="248" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">A more general robust loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03077</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Learning dense correspondence via 3D-guided cycle consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Unsupervised convolutional neural networks for motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="1629" to="1633" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Exploiting semantic information and deep matching for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="154" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Unsupervised adaptation for deep stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1614" to="1622" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6647" to="6655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Unsupervised learning of stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1567" to="1575" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07804</idno>
		<title level="m">Sfm-net: Learning of structure and motion from video</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Monocular templatebased reconstruction of inextensible surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perriollat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bartoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="124" to="137" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">GeoNet: Geometric Neural Network for Joint Depth and Surface Normal Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Zoom and learn: Generalizing deep stereo matching to novel domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="2070" to="2079" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for depth prediction from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">Weaklysupervised transfer for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02447</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Deep hierarchical guidance and regularization learning for end-to-end depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="430" to="442" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Towards real-time unsupervised monocular depth estimation on CPU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IROS</title>
		<imprint>
			<biblScope unit="page" from="5848" to="5854" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Open-world stereo video matching with deep RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="101" to="116" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Learning to Adapt for Stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rahnama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9661" to="9670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Real-Time Monocular Depth Estimation Using Synthetic Data With Domain Adaptation via Image Style Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="2800" to="2810" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">T2Net: Synthetic-to-Realistic Translation for Solving Single-Image Depth Estimation Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="767" to="783" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Geometry-Aware Symmetric Domain Adaptation for Monocular Depth Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9788" to="9798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">AdaDepth: Unsupervised Content Congruent Adaptation for Depth Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Krishna</forename><surname>Uppala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="2656" to="2665" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Au-toDispNet: Improving Disparity Estimation With AutoML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1812" to="1823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<title level="m" type="main">Automated machine learning-methods, systems, challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kotthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<title level="m" type="main">Robust vision challenge</title>
		<ptr target="http://www.robustvision.net/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Image-based 3d object reconstruction: State-of-the-art and trends in the deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Laga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Designing neural networks through neuroevolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="35" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">Evolutionary optimization of deep learning activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07224</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Variational Uncalibrated Photometric Stereo Under General Lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haefner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Queau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">SPLINE-Net: Sparse Photometric Stereo Through Lighting Interpolation and Normal Estimation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
