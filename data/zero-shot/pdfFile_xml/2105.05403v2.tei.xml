<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structure Guided Lane Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Su</surname></persName>
							<email>sujinming@meituan.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
							<email>chenchao60@meituan.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhang</surname></persName>
							<email>zhangke21@meituan.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Luo</surname></persName>
							<email>luojunfeng@meituan.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Wei</surname></persName>
							<email>weixiaoming@meituan.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><forename type="middle">Wei</forename><surname>Meituan</surname></persName>
							<email>weixiaolin02@meituan.com</email>
						</author>
						<title level="a" type="main">Structure Guided Lane Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, lane detection has made great progress with the rapid development of deep neural networks and autonomous driving. However, there exist three mainly problems including characterizing lanes, modeling the structural relationship between scenes and lanes, and supporting more attributes (e.g., instance and type) of lanes. In this paper, we propose a novel structure guided framework to solve these problems simultaneously. In the framework, we first introduce a new lane representation to characterize each instance. Then a topdown vanishing point guided anchoring mechanism is proposed to produce intensive anchors, which efficiently capture various lanes. Next, multi-level structural constraints are used to improve the perception of lanes. In the process, pixel-level perception with binary segmentation is introduced to promote features around anchors and restore lane details from bottom up, a lane-level relation is put forward to model structures (i.e., parallel) around lanes, and an image-level attention is used to adaptively attend different regions of the image from the perspective of scenes. With the help of structural guidance, anchors are effectively classified and regressed to obtain precise locations and shapes. Extensive experiments on public benchmark datasets show that the proposed approach outperforms stateof-the-art methods with 117 FPS on a single GPU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Lane detection, which aims to detect lanes in road scenes, is a fundamental perception task and has a wide range of applications (e.g., ADAS [Butakov and <ref type="bibr" target="#b0">Ioannou, 2014]</ref>, autonomous driving <ref type="bibr">[Chen and Huang, 2017]</ref> and high-definition map production <ref type="bibr" target="#b2">[Homayounfar et al., 2019]</ref>). Over the past years, lane detection has made significant progress and it is also used as an important element for tasks of road scene understanding, such as driving area detection <ref type="bibr">[Yu et al., 2020]</ref>.</p><p>To address the task of lane detection, lots of learningbased methods <ref type="bibr">[Pan et al., 2018;</ref><ref type="bibr" target="#b2">Qin et al., 2020]</ref> have been * Co-corresponding author.  <ref type="bibr">[TuSimple, 2017;</ref><ref type="bibr">Pan et al., 2018;</ref><ref type="bibr">Yu et al., 2020;</ref><ref type="bibr">Lee et al., 2017]</ref>, which makes it difficult to characterize lanes in a unified way. (b) Underresearched scene structures. Lane location are strongly dependent on structural information, such as vanishing point (black point), parallelism in bird's eye view and distance attention caused by perspective. (c) More attributes to support. Lanes have more attributes such as instance and type, which should be predicted.</p><p>proposed in recent years, achieving impressive performance on existing benchmarks <ref type="bibr">[TuSimple, 2017;</ref><ref type="bibr">Pan et al., 2018]</ref>. However, there still exist several challenges that hinder the development of lane detection. Frist, there lacks a unified and effective lane representation. As shown in (a) of <ref type="figure" target="#fig_0">Fig. 1</ref>, there exist various definitions including point <ref type="bibr">[TuSimple, 2017]</ref>, mask <ref type="bibr">[Pan et al., 2018]</ref>, marker <ref type="bibr">[Yu et al., 2020]</ref> and grid <ref type="bibr">[Lee et al., 2017]</ref>, which are quite different in form for different scenarios. Second, it is difficult to model the structural relationship between scenes and lanes. As displayed in (b) of <ref type="figure" target="#fig_0">Fig. 1</ref>, the structural information depending on scenes, such as location of vanishing points and parallelism of lanes, is very useful, but there is no scheme to describe it. Last, while predicting lanes, it is also important to predict other attributes including instance and type (see (c) of <ref type="figure" target="#fig_0">Fig. 1</ref>), but it is not easy to extend these for existing methods. These three difficulties are especially difficult to deal with and greatly slow down the development of lane detection. Due to these difficulties, lane detection remains a challenging vision task.</p><p>To deal with the first difficulty, many methods characterize lanes with simple fitted curves or masks. For examples, <ref type="bibr">SCNN [Pan et al., 2018]</ref> treats the problem as a semantic segmentation task, and introduces slice-by-slice convolutions within feature maps, thus enabling message passing. For these methods, lanes are characterized as a special form (e.g., point, curve or mask), so it is difficult to support the format of marker or grid that usually has an uncertain number. Similarly, those who support the latter <ref type="bibr">[Lee et al., 2017]</ref> do not support the former well. To address the second problem, some methods use vanishing point or parallel relation as auxiliary information. For example, a vanishing point prediction task <ref type="bibr">[Lee et al., 2017]</ref> is utilized to implicitly embed a geometric context recognition capability. In these methods, they usually only pay attention to a certain kind of structural information or do not directly use it end-to-end, which leads to the structures not fully functioning and the algorithm complicated. For the last problem, some clustering-or detectionbased methods are used to distinguish or classify instances. Line-CNN <ref type="bibr" target="#b2">[Li et al., 2019]</ref> utilizes line proposals as references to locate traffic curves, which forces the method to learn the feature of lanes. To these methods, they can distinguish instances and even extend to more attributes, but they usually need extra computation and have many manually designed super-parameters, which leads to poor scalability.</p><p>Inspired by these observations and analysis, we propose a novel structure guided framework for lane detection, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. In order to characterize lanes, we propose a box-line based proposal method. In this method, the minimum circumscribed rectangle of the lane is used to distinguish instance, and its center line is used for structured positioning. For the sake of further improving lane detection by utilizing structural information, the vanishing point guided anchoring mechanism is proposed to generate intensive anchors (i.e., as few and accurate anchors as possible). In this mechanism, vanishing point is learned in a segmentation manner and used to produce structural anchors top-down, which can efficiently capture various lanes. Meanwhile, we put forward multi-level structure constraints to improve the perception of lanes. In the process, the pixel-level perception is used to improve lane details with the help of lane binary segmentation, the lane-level relation aims at modeling the parallelism properties of inter-lanes by Inverse Perspective Mapping (IPM) via a neural network, and image-level attention is to attend the image with adaptive weights from the perspective of scenes. Finally, features of lane anchors under structural guidance are extracted for accurate classification, regression and the prediction of other attributes. Experimental results on CULane and Tusimple datasets verify the effectiveness of the proposed method which achieves state-of-theart performance and run efficiently at 117 FPS.</p><p>The main contributions of this paper include: 1) we propose a structure guided framework for lane detection, which characterize lanes and can accurately class, locate and restore the shape of unlimited lanes. 2) we introduce a vanishing point guided anchoring mechanism, in which the vanishing point is predicted and used to produce intensive anchors, which can precisely capture lanes. 3) we put forward the multi-level structural constraints, which are used to sense pixel-level unary details, model lane-level pair-wise relation and adaptively attend image-level global information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review the related works that aim to resolve the challenges of lane detection in two aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Traditional Methods</head><p>To solve the problem of lane detection, traditional methods are usually based on hand-crafted features by detecting shapes of markings and fitting the spline. <ref type="bibr" target="#b3">[Veit et al., 2008]</ref> presents a comprehensive overview of features used to detect road markings. And [Wu and Ranganathan, 2012] uses Maximally Stable Extremal Regions features and performs the template matching to detect multiple road markings. However, there approaches often fail in unfamiliar conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Learning based Methods</head><p>With the development of deep learning, methods [Pizzati and Garc?a, 2019; <ref type="bibr" target="#b3">Van Gansbeke et al., 2019;</ref><ref type="bibr" target="#b2">Guo et al., 2020]</ref> based on deep neural networks achieve progress in lane detection. <ref type="bibr">SCNN [Pan et al., 2018]</ref> generalizes traditional deep layer-by-layer convolutions to enable message passing between pixels across rows and columns. ENet-SAD [Hou et al., 2019] presents a knowledge distillation approach, which allows a model to learn from itself without any additional supervision or labels. PolyLaneNet <ref type="bibr">[Tabelini et al., 2020]</ref> adopts a polynomial representation for the lane markings, and outputs polynomials via the deep polynomial regression. <ref type="bibr">Ul-traFast [Qin et al., 2020]</ref> treats the process of lane detection as a row-based selecting problem using global features. Curve-Lanes <ref type="bibr">[Xu et al., 2020]</ref> proposes a lane-sensitive architecture search framework to automatically capture both long-ranged coherent and accurate short-range curve information.</p><p>In these methods, different lane representations are adopted and some structural information is considered for performance improvement. However, these methods are usually based on the powerful learning ability of neural networks to learn the fitting or shapes of lanes, and the role of scenerelated structural information for lanes has not been paid enough attention to and discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Approach</head><p>To address these difficulties (i.e., characterizing lanes, modeling the relationship between scenes and lanes, and supporting more attributes), we propose a novel structure guided framework for lane detection, denoted as SGNet. In this framework, we first introduce a new lane representation. Then a top-down vanishing point guided anchoring mechanism is proposed, and next multi-level structure constraints is used. Details of the proposed approach are described as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Representation</head><p>For adapting to different styles of lane annotation, we introduce a new box-line based method for lane representation. Firstly, we calculate the minimum circumscribed rectangle R ("box") with the height h and width w for the lane instance L lane . For this rectangle, center line L center ("line") perpendicular to the short side is obtained. And the angle between the positive X-axis and L center in clockwise direction is ?. In this manner, L center provides the position of the lane instance, and h and w restrict the areas involved. Based on R and L center , lane prediction based on points, masks, markers, grids and other formats can be performed. In this paper, the solution based on key points of lane detection is taken just because of the point-based styles of lane annotation in public datasets (e.g., CULane <ref type="bibr">[TuSimple, 2017]</ref> and Tusimple <ref type="bibr">[Pan et al., 2018]</ref>).</p><p>Inspired by existing methods <ref type="bibr" target="#b2">[Li et al., 2019;</ref><ref type="bibr" target="#b2">Chen et al., 2019;</ref><ref type="bibr" target="#b2">Qin et al., 2020]</ref>, we define key points of the lane instance with equally spaced y coordinates Y = {y i } and y i = H P ?1 ? i(i = 1, 2, ..., P ? 1), where P means the number of all key points through image height, which is fixed on images with same height H and width W . Accordingly, the x coordinates of the lane is expressed as X = {x i }. For the convenience of expression, the straight line equation of L center is defined as ax + by + c = 0, a = 0 or b = 0 (1) where a, b and c can be easily computed by ? and any point on L center . Next, when the y coordinate of the center line is y i , we can compute the corresponding x coordinate as</p><formula xml:id="formula_0">x i = L center (y i ) = ?c ? by i a , a = 0.<label>(2)</label></formula><p>Then, we define the offset of x coordinate ?X between the lane L lane and center line L center as</p><formula xml:id="formula_1">?X = {?x i } = {x i ? ?c ? by i a }, X = { ?c ? by i a } + ?X.<label>(3)</label></formula><p>Therefore, based on L center and ?X, we can calculate the lane instance L lane . Usually, it is easier to learn L center and ?X than the directly fitting key points of L lane .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Extractor</head><p>To see <ref type="figure" target="#fig_1">Fig. 2</ref>, SGNet takes ResNet <ref type="bibr" target="#b2">[He et al., 2016]</ref> as the feature extractor, which is modified to remove the last global pooling and fully connected layers for the pixel-level prediction task. Feature extractor has five residual modules for encoding, named as E i (? i ) with parameters ? i (i = 1, 2, ..., 5).</p><p>To obtain larger feature maps, we convolve E 5 (? 5 ) by a convolutional layer with 256 kernels of 3 ? 3 and then ?2 upsample the features, followed by an element-wise summation with E 4 (? 4 ) to obtain E 4 (? 4 ). Finally, for a H ? W input image, a H 16 ? W 16 feature map is output by the feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Vanishing Point Guided Anchoring</head><p>In order to learn the lane representation, there are two main ways to learn the center line L center and x offset ?X. The first way is to learn the determined L center directly with angle, number and position regression, which is usually difficult to achieve precise results because of the inherent difficulty of regression tasks. The second way is based on mature detection tasks, using dense anchors to classify, regress and then obtain proposals representing the lane instance. And the second one has been proved to work well in general object detection tasks, so we choose it as our base model. To learn the center line L center and x offset ?X well, we propose a novel vanishing point guided anchoring mechanism (named as VPG-Anchoring). The vanishing point (VP) provides strong characterization of geometric scene, representing the end of the road and also the "virtual" point where the lanes intersect in the distance. Since VP is the intersection point of lanes, lanes in the scene must pass through VPs, and lines that do not pass through VPs are not lanes in the scene with high probability. Therefore, dense lines radiated from VPs can theoretically cover all lanes in the image, which is equivalent to reducing the generation space of anchors from R H?W ?N proposal to R N proposal . N proposal represents the number of anchors generated at one pixel.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the features map E 4 (? 4 ) is feed to VPG-Anchoring. In the mechanism, VP is predicted by a simple branch, which is implemented by a multi-scale context-aware atrous spatial pyramid pooling (ASPP)  followed by a convolutional layer with 256 kernels of 3 ? 3 and a softmax activation. The VP prediction branch is denoted as ? V (? V ) with parameters ? V .</p><p>Usually, VP is not annotated in lane datasets, such as CU-Lane [Pan et al., 2018], so we average the intersection points of the center lines of all lane instances and get the approximate VP. In addition, a single point is usually difficult to predict, so we expand the area of VP to a radius of 16 pixels and use segmentation algorithm to predict. To achieve this, we expect the output of ? V (? V ) to approximate the ground-truth masks of VP (represented as G V ) by minimizing the loss</p><formula xml:id="formula_2">L V = BCE(? V (? V ), G V ),<label>(4)</label></formula><p>where BCE(?, ?) represents the pixel-level binary crossentropy loss function.</p><p>In order to ensure that generated anchors are dense enough, we choose a W anchor ? W anchor rectangular area with VP as the center, and take one point every S anchor to generate anchors. For each point, anchors are generated every A anchor angle (A anchor ? [0, 180]) as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. In this way, anchors are targeted, intensive and not redundant, compared with general full-scale uniform generation and even specially designed methods for lanes <ref type="bibr" target="#b2">[Li et al., 2019]</ref>. Note that anchors run through the whole image, and only the part below VP is shown for convenient display in Figs. 2 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Classification and Regression</head><p>In order to classify and regress the generated anchors, we extract high-level feature maps based on E 4 (? 4 ) with several convolutional layers. The feature map is named as F A ? R H ?W ?C , where H , W and C are the height, width and channel of F A . For each anchor L lane , the channel-level features of each point on anchors are extracted from F A to obtain lane descriptor D A ? R H ?C , which are used to classify the existence Conf L lane and regress x offsets ?X L lane including the length len of lanes. To learn these, we expect the output to approximate the ground-truth existence GConf L lane and x offsets G?X L lane by minimizing the loss</p><formula xml:id="formula_3">L C = L?1 L lane =0 BCE(Conf L lane , GConf L lane ), L R = L?1 L lane =0 SL1(?X L lane , G?X L lane ),<label>(5)</label></formula><p>where SL1(?, ?) means smooth L1 loss and L means the number of proposals. Finally, Line-NMS [Li et al., 2019] is used to obtain the finally result with confidence thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Multi-level Structure Constraints</head><p>In order to further improve lane perception, we ask for the structural relationship between scenes and lanes, and deeply explore the pixel-level, lane-level and image-level structures.</p><p>Pixel-level Perception. The top-down VPG-Anchoring mechanism covers the structures and distribution of lanes. At the same time, there is a demand of bottom-up detail perception, which ensures that lane details are restored and described more accurately. For the sake of improving the detail perception, we introduce lane segmentation branch to location lane locations and promote pixel-level unary details. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the lane segmentation branch has the same input and similar network structure with the VP prediction branch. The lane segmentation branch is denoted as ? P (? P ) with parameters ? P . To segment lanes, we expect the output of P P = ? P (? P ) to approximate the ground-truth masks of binary lane mask (represented as G P ) by minimizing the loss L P = BCE(P P , G P ).</p><p>To promote the pixel-level unary details, we weight the input features F A by the following operation</p><formula xml:id="formula_5">M A = F A ? P P + F A ,<label>(7)</label></formula><p>where M A are feed to classify and regress instead of F A .</p><p>Lane-level Relation. In fact, lanes conform to certain rules in the construction process, and the most important one is that the lanes are parallel.   two instances can be fitted to the following linear equations: a 1 * x + b 1 * y + c 1 = 0, a 2 * x + b 2 * y + c 2 = 0.</p><p>In these two equations, under the condition that y is equal, the difference of x is always constant. Thus we can get that a 1 * b 2 = a 2 * b 1 . Expanding to all instances, lane-level relation can be formulated as</p><formula xml:id="formula_7">L L = L?1 i=0,j=0,i =j L1(a i b j ? a j b i ).<label>(9)</label></formula><p>Image-level Attention. In the process of camera imaging, distant objects are small after projection. Usually, the distant information of lanes is not prominent visually, but they are equally important. After analysis, it is found that the distance between lanes and VP reflects the inverse proportion to scales in imaging. Therefore, we generate perspective attention map PAM based on VP, which is based on the strong assumption that the attention and distance after imaging satisfies two-dimensional gaussian distribution. PAM ensures the attention of different regions by adaptively restricting the classification and regression loss (from Eq. 5) as follows.</p><formula xml:id="formula_8">L I = L?1 L lane =0 P ?1 p=0 L1(?x L lane p , G?x L lane p ) ? (1 + |E(x L lane p , y L lane p )|),<label>(10)</label></formula><p>where | ? | means normalized to [0, 1]. By taking the losses of Eqs.(4),(5),(6),(9) and (10), the overall learning objective can be formulated as follows:</p><formula xml:id="formula_9">min P L V + L C + L R + L P + L L + L I ,<label>(11)</label></formula><p>where P is the set of {{? i } 5 i=1 , ? 4 , ? V , ? C , ? R , ? P , ? L }, and ? C , ? R and ? L are the parameters of classification, regression and lane-level relation subnetworks, respectively. Metrics. For CULane, we use F1-measure score as the evaluation metric. Following <ref type="bibr">[Pan et al., 2018]</ref>, we treat each lane as a line with 30 pixel width and compute the intersection-over-union (IoU) between groundtruths and predictions with a threshold of 0.5 to For Tusimple, the official metric (Accuracy) is used as the evaluation criterion, which evaluates the correction of predicted lane points.  Training and Inference. We use Adam optimization algorithm to train our network end-to-end by optimizing the loss in Eq. (11). In the optimization process, the parameters of feature extractor are initialized by the pre-trained ResNet-18/34 model and "poly" learning rate policy are employed for all experiments. The training images are resized to the resolution of 360 ? 640 for faster training, and applied affine and flipping. And we train the model for 10 epochs on CULane and 60 epochs on TuSimple. Moreover, we empirically and experimentally set the number of points P = 72, the width of rectangular W anchor = 40, anchor strides S anchor = 5 and anchor angle interval A anchor = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons with State-of-the-art Methods</head><p>We compare our approach with state-of-the-arts including DeeplabV2  We compare our approach with 10 state-of-the-art methods on CULane dataset, as listed in Tab. 1. Comparing our ResNet34-based method with others, we can see that the proposed method consistently outperforms other methods across total and almost all categories. For the total dataset, our method is noticeably improved from 74.80% to 77.27% compared with the second best method. Also, it is worth noting that our method is significantly better on Crowd (+2.31%), Arrow (+2.17%) and Night (+3.79%) compared with second best methods, respectively. In addition, we also obviously lower FP on Cross by 3.78% relative to the second best one. As for Curve, we are slightly below the best method (ERFNet-E2E), which conducts special treatment for curve points while maybe damaging other categories. Moreover, our method has a faster FPS than almost all results. These observations present the efficiency and robustness of our proposed method and validate that VPG-Anchoring and multilevel structures are useful for the task of lane detection. Some examples generated by our approach and other stateof-the-art algorithms are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. We can see that lanes can be detected with accurate location and precise shape by the proposed method, even in complex situations. These visualizations indicate that the proposed lane representation has a good characterization of lanes, and also show the superiority of the proposed method.  <ref type="table">Table 3</ref>: Performance of different settings of the proposed method. "-A" means "Anchoring".</p><p>Moreover, we list the comparisons on Tusimple as shown in Tab. 2. It can be seen that our method is competitive in highway scenes without adjustment, which further proves the effectiveness of structural information for lane detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Analysis</head><p>To validate the effectiveness of different components of the proposed method, we conduct several experiments on CU-Lane to compare the performance variations of our methods. Effectiveness of VPG-Anchoring. To investigate the effectiveness of the proposed VPG-Anchoring, we conduct ablation experiments and introduce three different models for comparisons. The first setting is only the feature extractor and the subnetwork of classification and regression, which is regarded as "Base" model. In Base, anchor is generated uniformly at all positions of the feature map, and A anchor is lowered to ensure the same number with SGNet. In addition, we conduct another model ("Base+V") by adding VPG-Anchor. And we also replace the L center by straight line fitted directly by key points as the "Base+V-F" to explore the importance of VP. The comparisons of above models are listed in Tab. 3. We can observe that the VPG-Anchoring greatly improve the performance of Base model, which verifies the effectiveness of this mechanism. In addition, comparing Base+V with Base+V-F, we find the proposed approximate VP in lane presentation is better than the one by direct fitting. Effectiveness of Multi-level Structures. To explore the effectiveness of the pixel-level, lane-level and image-level structures, we conduct another experiments by combining the pixel-level perception with "Base+V" as "Base+V+P" and adding lane-level relation to "Base+V+P" as "Base+V+P+L". From the last four rows of Tab. 3, we can find that the performance of lane detection can be continuously improved by pixel-, lane-and image-level structures, which validates that the three levels of constrains are compatible with each other, and can be used together to gain performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we rethink the difficulties that hinder the development of lane detection and propose a structure guided framework. In this framework, we introduce a new lane representation to meet the demands of various lane representations. Based on the representation, we propose a novel vanishing point guided anchoring mechanism to generate intensive anchors for efficiently capturing lanes. In addition, multi-level structure constraints is modeled to improve lane perception. Extensive experiments on benchmark datasets validates the effectiveness of the proposed approach with fast inference and shows that the perspective of modeling and utilization of structure information is useful for lane detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Challenges of lane detection. (a) Various representation. There exist many kinds of annotations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Framework of our approach. We first extract the common features by the extractor, which provides features for vanishing point guided anchoring and pixel-level perception. The anchoring produces intensive anchors and perception utilizes binary segmentation to promote features around lanes. Promoted features are used to classify and regress anchors with the aid of lane-level relation and image-level attention. The dashed arrow indicates the supervision, and the supervision of vanishing point and lane segmentation is omitted in the figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Lane representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>VP-guided anchoring mechanism. Anchors (golden lines) generated based on (a) the vanishing point (black point) and (b) the area around vanishing point (black and gray points).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative comparisons of the state-of-the-art algorithms and our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Due to imaging reasons, this relationship is no longer maintained after perspective transformation, but it can be modeled potentially. To model the lane-level relation, we conduct IPM by the H Matrix [Neven et al., 2018] via a neural network. After learning H, the lane instance L lane can be transformed to L lane on bird's eye view, where different instances are parallel. Formally, we define the relationship between lanes as follows. For two lane instances L</figDesc><table /><note>lane1 and L lane2 in the image, they are projected to the bird's-eye view through the learned H matrix, and the corresponding instance L lane1 and L lane2 are obtained. The</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparisons with state-of-the-art methods on CULane dataset. F1-measure score ("%" is omitted) is used to evaluate the results of total and 8 sub-categories. For Cross, only FP are shown. The top three results are in red1, green2 and blue3 fonts with a footnote.</figDesc><table><row><cell></cell><cell>Total</cell><cell cols="8">Normal Crowd Dazzle Shadow No line Arrow Curve Cross</cell><cell>Night</cell><cell>FPS</cell></row><row><cell>DeepLabV2-50</cell><cell>66.70</cell><cell>87.40</cell><cell>64.10</cell><cell>54.10</cell><cell>60.70</cell><cell>38.10</cell><cell>79.00</cell><cell>59.80</cell><cell>2505</cell><cell>60.60</cell><cell>-</cell></row><row><cell>SCNN</cell><cell>71.60</cell><cell>90.60</cell><cell>69.70</cell><cell>58.50</cell><cell>66.90</cell><cell>43.40</cell><cell>84.10</cell><cell>64.40</cell><cell>1990</cell><cell>66.10</cell><cell>8</cell></row><row><cell>FD</cell><cell>-</cell><cell>85.90</cell><cell>63.60</cell><cell>57.00</cell><cell>59.90</cell><cell>40.60</cell><cell>79.40</cell><cell>65.20</cell><cell>7013</cell><cell>57.80</cell><cell>-</cell></row><row><cell>ENet-SAD</cell><cell>70.80</cell><cell>90.10</cell><cell>68.80</cell><cell>60.20</cell><cell>65.90</cell><cell>41.60</cell><cell>84.00</cell><cell>65.70</cell><cell>1998</cell><cell>66.00</cell><cell>75</cell></row><row><cell>PointLane</cell><cell>70.20</cell><cell>88.00</cell><cell>68.10</cell><cell>61.50</cell><cell>63.30</cell><cell>44.00</cell><cell>80.90</cell><cell>65.20</cell><cell>1640</cell><cell>63.20</cell><cell>-</cell></row><row><cell>RONELD</cell><cell>72.90</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PINet</cell><cell>74.40</cell><cell>90.30</cell><cell>72.30</cell><cell>66.30</cell><cell>68.40</cell><cell>49.803</cell><cell>83.70</cell><cell>65.60</cell><cell>14273</cell><cell>67.70</cell><cell>25</cell></row><row><cell>ERFNet-E2E</cell><cell>74.00</cell><cell>91.003</cell><cell>73.103</cell><cell>64.50</cell><cell>74.102</cell><cell>46.60</cell><cell cols="2">85.803 71.901</cell><cell>2022</cell><cell>67.90</cell><cell>-</cell></row><row><cell>IntRA-KD</cell><cell>72.40</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>98</cell></row><row><cell>UltraFast-18</cell><cell>68.40</cell><cell>87.70</cell><cell>66.00</cell><cell>58.40</cell><cell>62.80</cell><cell>40.20</cell><cell>81.00</cell><cell>57.90</cell><cell>1743</cell><cell>62.10</cell><cell>3231</cell></row><row><cell>UltraFast-34</cell><cell>72.30</cell><cell>90.70</cell><cell>70.20</cell><cell>59.50</cell><cell>69.30</cell><cell>44.40</cell><cell>85.70</cell><cell>69.503</cell><cell>2037</cell><cell>66.70</cell><cell>1752</cell></row><row><cell>CurveLanes</cell><cell>74.803</cell><cell>90.70</cell><cell>72.30</cell><cell>67.702</cell><cell>70.10</cell><cell>49.40</cell><cell>85.803</cell><cell>68.40</cell><cell>1746</cell><cell>68.903</cell><cell>-</cell></row><row><cell>Ours-Res18</cell><cell>76.122</cell><cell>91.422</cell><cell>74.052</cell><cell>66.893</cell><cell>72.173</cell><cell>50.162</cell><cell>87.132</cell><cell>67.02</cell><cell cols="3">11641 70.672 1173</cell></row><row><cell>Ours-Res34</cell><cell>77.271</cell><cell>92.071</cell><cell>75.411</cell><cell>67.751</cell><cell>74.311</cell><cell>50.901</cell><cell cols="4">87.971 69.652 13732 72.691</cell><cell>92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparisons with state-of-the-arts on Tusimple.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Chen et al., 2019], RONELD [Chng et al., 2020], PINet [Ko et al., 2020], ERFNet-E2E [Yoo et al., 2020], IntRA-KD [Hou et al., 2020], UltraFast [Qin et al., 2020], CurveLanes [Xu et al., 2020], Cascaded-CNN [Pizzati et al., 2019] and PolyLaneNet [Tabelini et al., 2020].</figDesc><table><row><cell>, SCNN [Pan et al., 2018],</cell></row><row><cell>FD [Philion, 2019], ENet-SAD [Hou et al., 2019] , Point-</cell></row><row><cell>Lane [</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-end learning for lane keeping of self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vadim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Butakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ioannou ; Xinming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE IV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="4422" to="4431" />
		</imprint>
	</monogr>
	<note>Personalized driver/vehicle lane change models for adas</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<editor>Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam</editor>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Davy Neven, Bert De Brabandere, Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool. Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09548</idno>
		<idno>arXiv:2002.06604</idno>
		<ptr target="http://benchmark.tusimple.ai/#/" />
	</analytic>
	<monogr>
		<title level="m">Robust neural network output enhancement for active lane detection</title>
		<editor>Lucas Tabelini, Rodrigo Berriel, Thiago M Paix?o, Claudine Badue, Alberto F De Souza, and Thiago Oliveira-Santos</editor>
		<meeting><address><addrLine>Lee, Junsik Kim, Jae Shin Yoon, Seunghak Shin, Oleksandr Bailo, Namil Kim, Tae-Hee Lee, Hyun Seok Hong</addrLine></address></meeting>
		<imprint>
			<publisher>Pizzati and Garc?a</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="248" to="258" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ECCV. Tabelini et al., 2020. Polylanenet: Lane estimation via deep polynomial regression. arXiv preprint arXiv:2004.10924, 2020. [TuSimple, 2017] TuSimple. Tusimple lane detection challenge</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wu and Ananth Ranganathan. A practical system for road marking detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gansbeke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Intelligent Transportation Systems</title>
		<editor>Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht</editor>
		<meeting><address><addrLine>Hee Seok Lee, Heesoo Myeong, Sungrack Yun, Hyoungwoo Park, Janghoon Cho, and Duck Hoon Kim</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>CVPR Workshops. Yu et al., 2020</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bdd100k: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
