<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Vessel Segmentation By Learning Graphical Connectivity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung</forename><forename type="middle">Yeon</forename><surname>Shin</surname></persName>
							<email>?syshin@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. ECE</orgName>
								<orgName type="institution">ASRI, Seoul Nat&apos;l Univ</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soochahn</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. Electronic Eng</orgName>
								<orgName type="institution">Soonchunhyang Univ</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il</forename><surname>Dong</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Div. Comp. &amp; Elec. Sys. Eng</orgName>
								<orgName type="institution">Hankuk Univ. of Foreign Studies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. ECE</orgName>
								<orgName type="institution">ASRI, Seoul Nat&apos;l Univ</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Vessel Segmentation By Learning Graphical Connectivity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Vessel segmentation</term>
					<term>deep learning</term>
					<term>CNN</term>
					<term>graph convolu- tional network</term>
					<term>retinal image</term>
					<term>coronary artery</term>
					<term>X-ray angiography</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel deep-learning-based system for vessel segmentation. Existing methods using CNNs have mostly relied on local appearances learned on the regular image grid, without considering the graphical structure of vessel shape. To address this, we incorporate a graph convolutional network into a unified CNN architecture, where the final segmentation is inferred by combining the different types of features. The proposed method can be applied to expand any type of CNN-based vessel segmentation method to enhance the performance. Experiments show that the proposed method outperforms the current state-of-the-art methods on two retinal image datasets as well as a coronary artery X-ray angiography dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Observation of blood vessels is crucial in the diagnosis and intervention of many diseases. Clinicians have mainly relied on manual inspections, which can be inaccurate and time-consuming. Over the years, the demand for efficiency has led to the development of numerous methods for automatic vessel segmentation.</p><p>Most methods are based on image-processing <ref type="bibr" target="#b0">[1]</ref>, optimization <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, or their combination. Many optimization methods, including energy minimization methods on a Markov random field <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, aim to determine the best global graph structure based on applied prior knowledge. However, prior knowledge often consists of only simple rules, limiting the modeling capacity. More complex distributions can be modeled using learning-based methods such as boosting <ref type="bibr" target="#b3">[4]</ref> or regression <ref type="bibr" target="#b4">[5]</ref>. However, due to model complexity, only local appearances are mostly learned. Even with deep learning methods based on convolutional neural networks (CNN) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> this limitation persists.</p><p>Thus, we present a novel CNN architecture, the vessel graph network (VGN), that jointly learns the global structure of vessel shape together with local appearances, as shown in <ref type="figure">Fig. 1</ref>. The VGN comprises three components, i) a CNN module for generating pixelwise features and vessel probabilities, ii) a graph convolutional network (GCN) <ref type="bibr" target="#b8">[9]</ref> module to extract features which reflect the <ref type="figure">Fig. 1</ref>: Motivation of the proposed method. Learning about the strong relationship that exists between neighborhoods is not guaranteed in existing CNN-based vessel segmentation methods. The proposed vessel graph network (VGN) utilizes a GCN together with a CNN to address this issue. All figures best viewed in color.</p><p>connectivity of neighboring vertices, and iii) an inference module to produce the final segmentation. The input graph for the GCN is generated in an additional graph construction module. The network architecture is described in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>The technical contributions are as follows. 1) Our work is the first, to our knowledge, method to apply GCN to learn graphical structure of blood vessels.</p><p>2) The VGN combines the GCN within a CNN structure to jointly learn both local appearance and global structure. 3) The VGN structure is widely applicable since it can be combined with any CNN-based method. 4) When extending CNNbased methods to VGN, performance is highly likely to improve, with no risk of degradation. This is because, should the GCN have no positive impact, the VGN will be trained to perform inference only from the CNN features. 5) We perform comparative evaluations on two retinal image datasets and a coronary X-ray angiography dataset, showing that the VGN outperforms current stateof-the-art (SotA) methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of Network Architecture</head><p>The CNN module learns features, on the regular image grid of size h ? w, to infer pixelwise vessel probability</p><formula xml:id="formula_0">P CN N = {p CN N (x i )} h?w i=1 of input image X = {x i } h?w i=1</formula><p>. The GCN module learns features for vertices on an irregular graph constructed from points sampled from vessel centerlines of an initial segmentation P CN N . Due to their interaction in the GCN, the hidden representation of each vertex v j reflects the likelihood of being a vessel, given the likelihood of neighboring vertices, respectively. For instance, when a vertex is surrounded by vessel vertices, it will become more likely to be labeled a vessel based on its GCN features. The combined CNN and GCN features are given to the inference module to compute the final vessel probability map</p><formula xml:id="formula_1">P V GN = {p V GN (x i )} h?w i=1</formula><p>. As the CNN module, we adopt the network of DRIU (deep retinal image understanding) <ref type="bibr" target="#b7">[8]</ref>, based on the VGG-16 network <ref type="bibr" target="#b9">[10]</ref>, due to its SotA performance. We note that any other CNN-based vessel segmentation method can be used. In DRIU, a vessel probability map is inferred from concatenated multi-scale features from the VGG-16. Before the concatenation, feature maps are resized to have identical scale. In our VGN, we adopt the pixelwise cross entropy loss L CN N (X) for this CNN module. Please refer to <ref type="bibr" target="#b7">[8]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Convolutional Network Module</head><p>A graph must be constructed and given as input for both training and testing of the GCN module. We assume a CNN has been pretrained to generate P CN N , on which the following is performed: 1) thresholding, 2) skeletonization by morphological thinning, 3) vertex generation by equidistant sampling, with distance ?, on the skeleton together with skeletal junctions and endpoints, and 4) edge generation between vertices based on the skeletal connectivity or geodesic distances on the vessel probability map.</p><p>We denote the constructed graph from the image X as G X (V, E), where V = {v j } N j=1 and E are sets of vertices and edges, respectively. N is the number of vertices. Input feature vector F j for each vertex is sampled from the intermediate feature map generated from the CNN at the pixel coordinate of each vertex. The matrix of all F j 's is denoted as F ? R N ?C CN N , where C CN N is the feature dimension of the CNN. While the existence and weight of the edge e ij between the i th and j th vertices can be defined in various ways, we empirically use simple binary values based on nearest neighbor connectivity on the skeleton. The adjacency matrix defined by all e ij 's is denoted as A ? R N ?N .</p><p>The GCN operates on the extracted graph as a vertex classifier into vessel or non-vessel. It is defined as a two-layer feed-forward model formulated as:</p><formula xml:id="formula_2">P GCN (V ) = f (F, A) = ?(? ReLU (?F W (0) )W (1) ),<label>(1)</label></formula><formula xml:id="formula_3">with A = A + I N , D rc = c A rc , and? = D ? 1 2 A D ? 1 2 . P GCN (V )</formula><p>is a vessel probability vector for all vertices, which is calculated by applying the sigmoid function ? on the final features.</p><formula xml:id="formula_4">W (0) ? R C CN N ?C GCN and W (1) ? R C GCN ?1</formula><p>are trainable weight matrices, where C GCN is the number of hidden units in the GCN. More layers showed no improvement in our experiments as reported in <ref type="bibr" target="#b8">[9]</ref>. For training the GCN, we use a mean of the vertex-wise cross entropy loss defined as:</p><formula xml:id="formula_5">L GCN (G X ) = ? 1 N j?V l?{BG,F G} p * l (x v2p(vj ) )log p GCN l (v j ),<label>(2)</label></formula><p>where v2p(v j ) returns the pixel index corresponding to v j . p * l (x v2p(vj ) ) and p GCN l (v j ) are the GT label and the vessel probability predicted for v j by the GCN, respectively. BG and F G represent the back/foreground classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inference Module</head><p>To conduct inference on the combined features from the CNN and GCN modules, the spatial dimensions of the features must be normalized. Thus, we reproject the N number of GCN hidden features, only sparsely present on v j 's to the corresponding pixel coordinate in the pixelwise regular grid to coincide with the CNN features. The combined features are represented in a tensor of dimension h ? w ? (C CN N + C GCN ). Since all intermediate layers are ReLU-activated in the VGN, the zero-padding on the non-vertex pixels of the GCN features can be interpreted as those are not activated. P GV N is produced by applying multiple convolution layers on the combined feature tensor. To spread the sparse activations out over the whole image region, the number of the layers and the kernel sizes are determined according to the vertex sampling distance ?. We empirically adopted a plain architecture composed of five convolution layers, all with kernel size 3?3. For training, we again use a mean of the pixelwise cross entropy loss defined as:</p><formula xml:id="formula_6">L IN F ER (X) = ? 1 |X| i ? l?{BG,F G} p * l (x i )log p GV N l (x i ),<label>(3)</label></formula><p>where p GV N l (x i ) is the prediction for each pixel x i from the inference module. The weights for class-balancing are omitted for brevity. Here, ? = ? 2 , if p2v(x i ) ? V 1, otherwise is adopted to prevent trivial solutions which can be inferred only using the CNN features. p2v is the inverse operator of v2p. ? 2 is used since a single pixel is selected as a graph vertex among approximately ? 2 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Network Training</head><p>We adopt a sequential training scheme composed of an initial pretraining of the CNN followed by joint training, including fine-tuning of the CNN module, of the whole VGN. The P CN N inferred from the pretrained CNN is used to construct the training graphs as described in Section 2.2. To maintain efficiency, graph construction is performed only at each K gc training iterations. Compared to when pretraining the CNN, the VGN takes image X as well as G X for joint training. With the assumption of the accuracy of the graph G X constructed from the pretrained CNN module, the proposed network learns the graphical vessel structure while fine-tuning the CNN module end-to-end. The total loss function used for the VGN is defined as:</p><formula xml:id="formula_7">L total (X) = L CN N (X) + L GCN (G X ) + L IN F ER (X).<label>(4)</label></formula><p>When testing the VGN, CNN module feature generation and inference, graph construction, GCN feature generation, and final VGN inference are performed sequentially for each image to generate the final segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>Evaluation Details: We experiment on two retinal image datasets, namely DRIVE <ref type="bibr" target="#b10">[11]</ref> and STARE <ref type="bibr" target="#b11">[12]</ref>, and a coronary artery X-ray angiography dataset (CA-XRA). For the DRIVE and STARE sets, which respectively comprises 40 and 20 images, we followed <ref type="bibr" target="#b7">[8]</ref> for training/test set splits, and human performance measurement using second observer results. CA-XRA was acquired in our cooperating hospital and comprises 3,137 image frames from 85 XRA sequences. All sequences were acquired at 512 ? 512 resolution, 8 bit depth, and 15 fps frame rate. We treated each frame as an independent image, without use of temporal information. Frames of the first 80, and the last 5 sequences were assigned as training and test sets comprising 2,958 and 179 images, respectively.</p><p>Since the authors have not made their training code publicly available, our own implementation of the DRIU <ref type="bibr" target="#b7">[8]</ref> was used as the CNN module. We provide a comparison between results from our implementation and those reported in <ref type="bibr" target="#b7">[8]</ref> for reference. CNN architectures are identical to the original DRIU for the retinal image datasets (C CN N =64=16?4), but slightly modified to include all five stages of VGG-16 (C CN N =80=16?5) to handle the wider variance of vessel width in CA-XRA images. The numbers of hidden units in the GCN were set to be equal to that of the CNN, as C GCN =64 for DRIVE/STARE, and C GCN =80 for CA-XRA. In the inference module, feature depth is halved in layer 1, kept constant in layer 2 ? 4, and reduced to 1 in the final layer 5.</p><p>The details on CNN pretraining mostly followed that of the original DRIU <ref type="bibr" target="#b7">[8]</ref>, but slightly modified the loss function from the sum of pixelwise cross entropy to the mean, and modified the learning rate accordingly. For training the VGN, We use stochastic gradient descent with momentum of 0.9 and a weight decay of 0.0005. The initial learning rates of the pratrained CNN and the remaining modules are 0 and 10 ?2 for DRIVE/STARE, 10 ?6 and 10 ?3 for CA-XRA. We found that not fine-tuning the pretrained CNN shows better results for DRIVE/STARE due to its small number of training images. The learning rate is scheduled to gradually decrease. 50,000 iterations with mini-batch size 1 were run for DRIVE/STARE, while 100,000 iterations with mini-batch size 5 were run for CA-XRA. We applied horizontal flipping, random brightness and contrast adjustment for data augmentation. Precomputed graphs are flipped accordingly.</p><p>The graph update period K gc is set as 10,000 and 20,000 for retinal and CA-XRA datasets, respectively. The vertex sampling rate ? is fixed to 10. We use several thresholds for vessel probability maps during the graph construction, to simulate variability of training graph data. A randomly selected graph is used at each iteration. In test time, the thresholds are all used and the average vessel map is given as the final output.</p><p>Quantitative Evaluation: We compare the proposed VGN with the current SotA <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> as well as several conventional approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref> on precision recall curves. The curve is obtained by computing multiple precision/recall pairs using multiple vessel probability thresholds. We also present the average precisions (AP) and the maximum F1 scores as summary measures.</p><p>The precision recall curves for DRIVE/STARE are summarized in <ref type="figure" target="#fig_1">Fig. 3</ref>. The proposed method shows comparable performance to the original DRIU <ref type="bibr" target="#b7">[8]</ref> for DRIVE and shows the best performance for the STARE dataset. In both datasets, the proposed method achieves the highest AP scores. We note that our implementation of the DRIU method, denoted as DRIU * in <ref type="figure" target="#fig_1">Fig. 3</ref>, gives slightly different performances than the original. Compared to DRIU * , which is the baseline for the proposed VGN, we can clearly see improved performance for both datasets. For the CA-XRA dataset, VGN scored an AP of 0.915 while DRIU scored 0.899, showing a relative improvement of 1.78%.</p><p>Qualitative Evaluation: <ref type="figure">Fig. 4</ref> shows qualitative results from each dataset. Compared to <ref type="bibr" target="#b7">[8]</ref>, VGN reduces false positives, e.g., ribs in CA-XRA, and false negatives. It is interesting that very weak vessels in the first result of STARE are suppressed, rather than enhanced, by considering neighboring vessels. We also note that the proposed VGN seems to perform better for higher quality images such as the STARE dataset since the vessel graph structures become clearer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have proposed a novel CNN architecture that explicitly learns the graphical structure of vessel shape together with local appearance for vessel segmentation. Experiments show the effectiveness on three datasets about two different target organs. For future works, we plan to apply the proposed method to 3D imaging modalities such as the computed tomography angiography or extend it to use the temporal information of video data, e.g., fluoroscopic x-ray sequences. <ref type="figure">Fig. 4</ref>: Qualitative results on the DRIVE, STARE, and CA-XRA dataset. Each of the three block rows represents each dataset in order, all of which with three representative sample results. Each of the six images for a single case represents an input image, result of <ref type="bibr" target="#b7">[8]</ref>, result of VGC, zoomed GT, zoomed result of <ref type="bibr" target="#b7">[8]</ref>, and zoomed result of VGC, from top-left to bottom-right. The results of <ref type="bibr" target="#b7">[8]</ref> for DRIVE/STARE are those provided from the original authors, while our implementation results are shown for CA-XRA.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Overall network architecture of VGN comprising the CNN, graph convolutional network, and inference modules. Refer to text for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Precision recall curves, average precisions (AP), and max F1 scores of the proposed VGN and comparable methods on the DRIVE and STARE dataset. 'Human' indicates the performance of the second annotator. 'DRIU * ' represents our own implementation, which was required as a component of the proposed VGN. Comparison results are thankfully provided by the authors.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Retinal vessel segmentation using the 2-D Gabor wavelet and supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V B</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J G</forename><surname>Leandro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Cesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Cree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-MI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1214" to="1222" />
			<date type="published" when="2006-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning Fully-Connected CRFs for Blood Vessel Segmentation in Retinal Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">P</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barillot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hornegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Extraction of Coronary Vessels in Fluoroscopic X-Ray Sequences Using Vessel Correspondence Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Supervised Feature Learning for Curvilinear Structure Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rigamonti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Projection onto the Manifold of Elongated Structures for Accurate Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<title level="m">4 -Fields: Neural Network Nearest Neighbor Fields for Image Transforms</title>
		<imprint>
			<publisher>ACCV</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">DeepVessel: Retinal Vessel Segmentation via Deep Learning and Conditional Random Field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Kee Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep Retinal Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ridge-based vessel segmentation in color images of the retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Abramoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-MI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="501" to="509" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kouznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-MI</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="210" />
			<date type="published" when="2000-03" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
