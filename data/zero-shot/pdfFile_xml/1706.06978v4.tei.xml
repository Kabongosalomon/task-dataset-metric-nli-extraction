<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Interest Network for Click-Through Rate Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
							<email>guorui.xgr@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengru</forename><surname>Song</surname></persName>
							<email>chengru.scr@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
							<email>xiaoqiang.zxq@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fan</surname></persName>
							<email>fanying.fy@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
							<email>maxiao.ma@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
							<email>yanghui.yyh@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Jin</surname></persName>
							<email>junqi.jjq@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Interest Network for Click-Through Rate Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Information systems ? Display advertising</term>
					<term>Recommender systems</term>
					<term>KEYWORDS Click-Through Rate Prediction, Display Advertising, E-commerce</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embed-ding&amp;MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding&amp;MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In cost-per-click (CPC) advertising system, advertisements are ranked by the eCPM (effective cost per mille), which is the product of the bid price and CTR (click-through rate), and CTR needs to be predicted by the system. Hence, the performance of CTR prediction model has a direct impact on the final revenue and plays a key role in the advertising system. Modeling CTR prediction has received much attention from both research and industry community.</p><p>Recently, inspired by the success of deep learning in computer vision <ref type="bibr" target="#b13">[14]</ref> and natural language processing <ref type="bibr" target="#b0">[1]</ref>, deep learning based methods have been proposed for CTR prediction task <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>These methods follow a similar Embedding&amp;MLP paradigm: large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into fully connected layers (also known as multilayer perceptron, MLP) to learn the nonlinear relations among features. Compared with commonly used logistic regression model <ref type="bibr" target="#b18">[19]</ref>, these deep learning methods can reduce a lot of feature engineering jobs and enhance the model capability greatly. For simplicity, we name these methods Embedding&amp;MLP in this paper, which now have become popular on CTR prediction task.</p><p>However, the user representation vector with a limited dimension in Embedding&amp;MLP methods will be a bottleneck to express user's diverse interests. Take display advertising in e-commerce site as an example. Users might be interested in different kinds of goods simultaneously when visiting the e-commerce site. That is to say, user interests are diverse. When it comes to CTR prediction task, user interests are usually captured from user behavior data. Embedding&amp;MLP methods learn the representation of all interests for a certain user by transforming the embedding vectors of user behaviors into a fixed-length vector, which is in an euclidean space where all users' representation vectors are. In other words, diverse interests of the user are compressed into a fixed-length vector, which limits the expressive ability of Embedding&amp;MLP methods. To make the representation capable enough for expressing user's diverse interests, the dimension of the fixed-length vector needs to be largely expanded. Unfortunately, it will dramatically enlarge the size of learning parameters and aggravate the risk of overfitting under limited data. Besides, it adds the burden of computation and storage, which may not be tolerated for an industrial online system.</p><p>On the other hand, it is not necessary to compress all the diverse interests of a certain user into the same vector when predicting a candidate ad because only part of user's interests will influence his/her action (to click or not to click). For example, a female swimmer will click a recommended goggle mostly due to the bought of bathing suit rather than the shoes in her last week's shopping list. Motivated by this, we propose a novel model: Deep Interest Network (DIN), which adaptively calculates the representation vector of user interests by taking into consideration the relevance of historical behaviors given a candidate ad. By introducing a local activation unit, DIN pays attentions to the related user interests by soft-searching for relevant parts of historical behaviors and takes a weighted sum pooling to obtain the representation of user interests with respect to the candidate ad. Behaviors with higher relevance to the candidate ad get higher activated weights and dominate the representation of user interests. We visualize this phenomenon in the experiment section. In this way, the representation vector of user interests varies over different ads, which improves the expressive ability of model under limited dimension and enables DIN to better capture user's diverse interests.</p><p>Training industrial deep networks with large scale sparse features is of great challenge. For example, SGD based optimization methods only update those parameters of sparse features appearing in each mini-batch. However, adding with traditional ? 2 regularization, the computation turns to be unacceptable, which needs to calculate L2-norm over the whole parameters (with size scaling up to billions in our situation) for each mini-batch. In this paper, we develop a novel mini-batch aware regularization where only parameters of non-zero features appearing in each mini-batch participate in the calculation of L2-norm, making the computation acceptable. Besides, we design a data adaptive activation function, which generalizes commonly used PReLU <ref type="bibr" target="#b11">[12]</ref> by adaptively adjusting the rectified point w.r.t. distribution of inputs and is shown to be helpful for training industrial networks with sparse features.</p><p>The contributions of this paper are summarized as follows:</p><p>? We point out the limit of using fixed-length vector to express user's diverse interests and design a novel deep interest network (DIN) which introduces a local activation unit to adaptively learn the representation of user interests from historical behaviors w.r.t. given ads. DIN can improve the expressive ability of model greatly and better capture the diversity characteristic of user interests. ? We develop two novel techniques to help training industrial deep networks: i) a mini-batch aware regularizer, which saves heavy computation of regularization on deep networks with huge number of parameters and is helpful for avoiding overfitting, ii) a data adaptive activation function, which generalizes PReLU by considering the distribution of inputs and shows well performance. ? We conduct extensive experiments on both public and Alibaba datasets. Results verify the effectiveness of proposed DIN and training techniques. Our code 1 is publicly available. The proposed approaches have been deployed in the commercial display advertising system in Alibaba, one of world's largest advertising platform, contributing significant improvement to the business. In this paper we focus on the CTR prediction modeling in the scenario of display advertising in e-commerce industry. Methods discussed here can be applied in similar scenarios with rich user behaviors, such as personalized recommendation in e-commerce sites, feeds ranking in social networks etc.</p><p>The rest of the paper is organized as follows. We discuss related work in section 2 and introduce the background about characteristic of user behavior data in display advertising system of e-commerce site in section 3. Section 4 and 5 describe in detail the design of DIN model as well as two proposed training techniques. We present experiments in section 6 and conclude in section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATEDWORK</head><p>The structure of CTR prediction model has evolved from shallow to deep. At the same time, the number of samples and the dimension of features used in CTR model have become larger and larger. In order to better extract feature relations to improve performance, several works pay attention to the design of model structure.</p><p>As a pioneer work, NNLM <ref type="bibr" target="#b1">[2]</ref> learns distributed representation for each word, aiming to avoid curse of dimension in language modeling. This method, often referred to as embedding, has inspired many natural language models and CTR prediction models that need to handle large-scale sparse inputs.</p><p>LS-PLM <ref type="bibr" target="#b8">[9]</ref> and FM <ref type="bibr" target="#b19">[20]</ref> models can be viewed as a class of networks with one hidden layer, which first employs embedding layer on sparse inputs and then imposes specially designed transformation functions for target fitting, aiming to capture the combination relations among features.</p><p>Deep Crossing <ref type="bibr" target="#b20">[21]</ref>, Wide&amp;Deep Learning <ref type="bibr" target="#b3">[4]</ref> and YouTube Recommendation CTR model <ref type="bibr" target="#b2">[3]</ref> extend LS-PLM and FM by replacing the transformation function with complex MLP network, which enhances the model capability greatly. PNN <ref type="bibr" target="#b4">[5]</ref> tries to capture high-order feature interactions by involving a product layer after embedding layer. DeepFM <ref type="bibr" target="#b9">[10]</ref> imposes a factorization machines as "wide" module in Wide&amp;Deep <ref type="bibr" target="#b3">[4]</ref> with no need of feature engineering. Overall, these methods follow a similar model structure with combination of embedding layer (for learning the dense representation of sparse features) and MLP (for learning the combination relations of features automatically). This kind of CTR prediction model reduces the manual feature engineering jobs greatly. Our base model follows this kind of model structure. However in applications with rich user behaviors, features are often contained with variable-length list of ids, e.g., searched terms or watched videos in YouTube recommender system <ref type="bibr" target="#b2">[3]</ref>. These models often transform corresponding list of embedding vectors into a fixed-length vector via sum/average pooling, which causes loss of information. The proposed DIN tackles it by adaptively learning the representation vector w.r.t. given ad, improving the expressive ability of model. Attention mechanism originates from Neural Machine Translation (NMT) field <ref type="bibr" target="#b0">[1]</ref>. NMT takes a weighted sum of all the annotations to get an expected annotation and focuses only on information relevant to the generation of next target word. A recent work, Deep-Intent <ref type="bibr" target="#b25">[26]</ref> applies attention in the context of search advertising. Similar to NMT, they use RNN <ref type="bibr" target="#b23">[24]</ref> to model text, then learn one global hidden vector to help paying attention on the key words in each query. It is shown that the use of attention can help capturing the main intent of query or ad. DIN designs a local activation unit to soft-search for relevant user behaviors and takes a weighted sum pooling to obtain the adaptive representation of user interests with respect to a given ad. The user representation vector varies over different ads, which is different from DeepIntent in which there is no interaction between ad and user.</p><p>We make code publicly available, and further show how to successfully deploy DIN in one of the world's largest advertising systems with novel developed techniques for training large scale deep networks with hundreds of millions of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>In e-commerce sites, such as Alibaba, advertisements are natural goods. In the rest of this paper, without special declaration, we regard ads as goods. <ref type="figure" target="#fig_0">Figure 1</ref> briefly illustrates the running procedure of display advertising system in Alibaba, which consists of two main stages: i) matching stage which generates list of candidate ads relevant to the visiting user via methods like collaborative filtering, ii) ranking stage which predicts CTR for each given ad and then selects top ranked ones. Everyday, hundreds of millions of users visit the e-commerce site, leaving us with lots of user behavior data which contributes critically in building matching and ranking models. It is worth mentioning that users with rich historical behaviors contain diverse interests. For example, a young mother has browsed goods including woolen coat, T-shits, earrings, tote bag, leather handbag and children's coat recently. These behavior data give us hints about her shopping interests. When she visits the e-commerce site, system displays a suitable ad to her, for example a new handbag. Obviously the displayed ad only matches or activates part of interests of this mother. In summary, interests of user with rich behaviors are diverse and could be locally activated given certain ads. We show later in this paper making use of these characteristics plays important role for building CTR prediction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DEEP INTEREST NETWORK</head><p>Different from sponsored search, users come into display advertising system without explicitly expressed intentions. Effective approaches are required to extract user interests from rich historical behaviors when building the CTR prediction model. Features that depict users and ads are the basic elements in the CTR modeling of advertisement system. Making use of these features reasonably and mining information from them are critical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Feature Representation</head><p>Data in industrial CTR prediction tasks is mostly in a multi-group categorial form, for example, [weekday=Friday, gender=Female, visited_cate_ids={Bag,Book}, ad_cate_id=Book], which is normally transformed into high-dimensional sparse binary features via encoding <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref>. Mathematically, encoding vector of i-th feature group is formularized as t i ? R K i . K i denotes the dimensionality of feature group i, which means feature group i contains K i unique ids.</p><formula xml:id="formula_0">t i [j] is the j-th element of t i and t i [j] ? {0, 1}. K i j=1 t i [j] = k.</formula><p>Vector t i with k = 1 refers to one-hot encoding and k &gt; 1 refers to multi-hot encoding. Then one instance can be represent as</p><formula xml:id="formula_1">x = [t T 1 , t T 2 , ...t T M ] T in a group-wise manner, where M is num- ber of feature groups, M i=1 K i = K,</formula><p>K is dimensionality of the entire feature space. In this way, the aforementioned instance with four groups of features are illustrated as:</p><formula xml:id="formula_2">[0, 0, 0, 0, 1, 0, 0] weekday=Friday [0, 1] gender=Female [0, .., 1, ..., 1, ...0] visited_cate_ids={Bag,Book} [0, .., 1, ..., 0] ad_cate_id=Book</formula><p>The whole feature set used in our system is described in <ref type="table" target="#tab_0">Table  1</ref>. It is composed of four categories, among which user behavior features are typically multi-hot encoding vectors and contain rich information of user interests. Note that in our setting, there are no combination features. We capture the interaction of features with deep neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Base Model(Embedding&amp;MLP)</head><p>Most of the popular model structures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21]</ref> share a similar Embedding&amp;MLP paradigm, which we refer to as base model, as shown in the left of <ref type="figure" target="#fig_1">Fig.2</ref>. It consists of several parts:</p><p>Embedding layer. As the inputs are high dimensional binary vectors, embedding layer is used to transform them into low dimensional dense representations. For the i-th feature group of t i ,</p><formula xml:id="formula_3">let W i = [w i 1 , ..., w i j , ..., w i K i ] ? R D?K i represent the i-th embed- ding dictionary, where w i j ? R D is an embedding vector with di- mensionality of D.</formula><p>Embedding operation follows the table lookup mechanism, as illustrated in <ref type="figure" target="#fig_1">Fig.2</ref>.</p><formula xml:id="formula_4">? If t i is one-hot vector with j-th element t i [j] = 1, the em- bedded representation of t i is a single embedding vector e i = w i j . ? If t i is multi-hot vector with t i [j] = 1 for j ? {i 1 , i 2 , ..., i k },</formula><p>the embedded representation of t i is a list of embedding vectors:</p><formula xml:id="formula_5">{e i 1 , e i 2 , ...e i k } = {w i i 1 , w i i 2 , ...w i i k }.</formula><p>Pooling layer and Concat layer. Notice that different users have different numbers of behaviors. Thus the number of non-zero values for multi-hot behavioral feature vector t i varies across instances, causing the lengths of the corresponding list of embedding vectors to be variable. As fully connected networks can only handle fixed-length inputs, it is a common practice <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> to transform the list of embedding vectors via a pooling layer to get a fixed-length vector:</p><formula xml:id="formula_6">e i = pooling(e i 1 , e i 2 , ...e i k ).</formula><p>(1) Loss. The objective function used in base model is the negative log-likelihood function defined as:</p><formula xml:id="formula_7">L = ? 1 N (x ,y)?S (y log p(x) + (1 ? y) log(1 ? p(x))),<label>(2)</label></formula><p>where S is the training set of size N , with x as the input of the network and y ? {0, 1} as the label, p(x) is the output of the network after the softmax layer, representing the predicted probability of sample x being clicked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The structure of Deep Interest Network</head><p>Among all those features of <ref type="table" target="#tab_0">Table 1</ref>, user behavior features are critically important and play key roles in modeling user interests in the scenario of e-commerce applications. Base model obtains a fixed-length representation vector of user interests by pooling all the embedding vectors over the user behavior feature group, as Eq.(1). This representation vector stays the same for a given user, in regardless of what candidate ads are. In this way, the user representation vector with a limited dimension will be a bottleneck to express user's diverse interests. To make it capable enough, an easy method is to expand the dimension of embedding vector, which unfortunately will increase the size of learning parameters heavily. It will lead to overfitting under limited training data and add the burden of computation and storage, which may not be tolerated for an industrial online system.</p><p>Is there an elegant way to represent user's diverse interests in one vector under limited dimension? The local activation characteristic of user interests gives us inspiration to design a novel model named deep interest network(DIN). Imagine when the young mother mentioned above in section 3 visits the e-commerce site, she finds the displayed new handbag cute and clicks it. Let's dissect the driving force of click action. The displayed ad hits the related interests of this young mother by soft-searching her historical behaviors and finding that she had browsed similar goods of tote bag and leather handbag recently. In other words, behaviors related to displayed ad greatly contribute to the click action. DIN simulates this process by paying attention to the representation of locally activated interests w.r.t. given ad. Instead of expressing all user's diverse interests with the same vector, DIN adaptively calculate the representation vector of user interests by taking into consideration the relevance of historical behaviors w.r.t. candidate ad. This representation vector varies over different ads.</p><p>The right part of <ref type="figure" target="#fig_1">Fig.2</ref> illustrates the architecture of DIN. Compared with base model, DIN introduces a novel designed local activation unit and maintains the other structures the same. Specifically, activation units are applied on the user behavior features, which performs as a weighted sum pooling to adaptively calculate user representation v U given a candidate ad A, as shown in Eq.</p><formula xml:id="formula_8">(3) v U (A) = f (v A , e 1 , e 2 , .., e H ) = H j=1 a(e j , v A )e j = H j=1 w j e j ,<label>(3)</label></formula><p>where {e 1 , e 2 , ..., e H } is the list of embedding vectors of behaviors of user U with length of H , v A is the embedding vector of ad A. In this way, v U (A) varies over different ads. a(?) is a feed-forward network with output as the activation weight, as illustrated in <ref type="figure" target="#fig_1">Fig.2</ref>. Apart from the two input embedding vectors, a(?) adds the out product of them to feed into the subsequent network, which is an explicit knowledge to help relevance modeling.</p><p>Local activation unit of Eq.(3) shares similar ideas with attention methods which are developed in NMT task <ref type="bibr" target="#b0">[1]</ref>. However different from traditional attention method, the constraint of i w i = 1 is relaxed in Eq.(3), aiming to reserve the intensity of user interests. That is, normalization with softmax on the output of a(?) is abandoned. Instead, value of i w i is treated as an approximation of the intensity of activated user interests to some degree. For example, if one user's historical behaviors contain 90% clothes and 10% electronics. Given two candidate ads of T-shirt and phone, T-shirt activates most of the historical behaviors belonging to clothes and may get larger value of v U (higher intensity of interest) than phone. Traditional attention methods lose the resolution on the numerical scale of v U by normalizing of the output of a(?).</p><p>We have tried LSTM to model user historical behavior data in the sequential manner. But it shows no improvement. Different from text which is under the constraint of grammar in NLP task, the sequence of user historical behaviors may contain multiple concurrent interests. Rapid jumping and sudden ending over these interests causes the sequence data of user behaviors to seem to be noisy. A possible direction is to design special structures to model such data in a sequence way. We leave it for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TRAINING TECHNIQUES</head><p>In the advertising system in Alibaba, numbers of goods and users scale up to hundreds of millions. Practically, training industrial deep networks with large scale sparse input features is of great challenge. In this section, we introduce two important techniques which are proven to be helpful in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Mini-batch Aware Regularization</head><p>Overfitting is a critical challenge for training industrial networks. For example, with addition of fine-grained features, such as features of goods_ids with dimensionality of 0.6 billion (including visited_?oods_ids of user and ?oods_id of ad as described in Table 1), model performance falls rapidly after the first epoch during training without regularization, as the dark green line shown in <ref type="figure" target="#fig_4">Fig.4</ref> in later section 6.5. It is not practical to directly apply traditional regularization methods, such as ? 2 and ? 1 regularization, on training networks with sparse inputs and hundreds of millions of parameters. Take ? 2 regularization as an example. Only parameters of non-zero sparse features appearing in each mini-batch needs to be updated in the scenario of SGD based optimization methods without regularization. However, when adding ? 2 regularization it needs to calculate L2-norm over the whole parameters for each mini-batch, which leads to extremely heavy computations and is unacceptable with parameters scaling up to hundreds of millions. In this paper, we introduce an efficient mini-batch aware regularizer, which only calculates the L2-norm over the parameters of sparse features appearing in each mini-batch and makes the computation possible. In fact, it is the embedding dictionary that contributes most of the parameters for CTR networks and arises the difficulty of heavy computation. Let W ? R D?K denote parameters of the whole embedding dictionary, with D as the dimensionality of the embedding vector and K as the dimensionality of feature space. Expand the ? 2 regularization on W over samples</p><formula xml:id="formula_9">L 2 (W) = ?W ? 2 2 = K j=1 ?w j ? 2 2 = (x ,y)?S K j=1 I (x j 0) n j ?w j ? 2 2 ,<label>(4)</label></formula><p>where w j ? R D is the j-th embedding vector, I (x j 0) denotes if the instance x has the feature id j, and n j denotes the number of occurrence for feature id j in all samples. Eq.(4) can be transformed into Eq.(5) in the mini-batch aware manner</p><formula xml:id="formula_10">L 2 (W) = K j=1 B m=1 (x ,y)?Bm I (x j 0) n j ?w j ? 2 2 ,<label>(5)</label></formula><p>where B denotes the number of mini-batches, B m denotes the m-th mini-batch. Let ? mj = max (x ,y)?B m I (x j 0) denote if there is at least one instance having the feature id j in mini-batch B m . Then Eq.(5) can be approximated by</p><formula xml:id="formula_11">L 2 (W) ? K j=1 B m=1 ? m j n j ?w j ? 2 2 .<label>(6)</label></formula><p>In this way, we derive an approximated mini-batch aware version of ? 2 regularization. For the m-th mini-batch, the gradient w.r.t. the embedding weights of feature j is</p><formula xml:id="formula_12">w j ? w j ? ? ? ? ? ? ? ? 1 | B m | (x ,y)?Bm ?L(p(x ), y) ?w j + ? ? m j n j w j ? ? ? ? ? ? ,<label>(7)</label></formula><p>in which only parameters of features appearing in m-th mini-batch participate in the computation of regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data Adaptive Activation Function</head><p>PReLU <ref type="bibr" target="#b11">[12]</ref> is a commonly used activation function </p><formula xml:id="formula_13">f (s) = s if s &gt; 0 ? s if s ? 0. = p(s) ? s + (1 ? p(s)) ? ? s,<label>(8)</label></formula><p>with the control function to be plotted in the right part of <ref type="figure" target="#fig_2">Fig.3</ref> Dice can be viewed as a generalization of PReLu. The key idea of Dice is to adaptively adjust the rectified point according to distribution of input data, whose value is set to be the mean of input. Besides, Dice controls smoothly to switch between the two channels. When E(s) = 0 and V ar [s] = 0, Dice degenerates into PReLU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In this section, we present our experiments in detail, including datasets, evaluation metric, experimental setup, model comparison and the corresponding analysis. Experiments on two public datasets with user behaviors as well as a dataset collected from the display advertising system in Alibaba demonstrate the effectiveness of proposed approach which outperforms state-of-the-art methods on the CTR prediction task. Both the public datasets and experiment codes are made available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets and Experimental Setup</head><p>Amazon Dataset 2 . Amazon Dataset contains product reviews and metadata from Amazon, which is used as benchmark dataset <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref>. We conduct experiments on a subset named Electronics, which contains 192,403 users, 63,001 goods, 801 categories and 1,689,188 samples. User behaviors in this dataset are rich, with more than 5 reviews for each users and goods. Features include goods_id, cate_id, user reviewed goods_id_list and cate_id_list. Let all behaviors of a user be (b 1 , b 2 , . . . , b k , . . . , b n ), the task is to predict the (k+1)-th reviewed goods by making use of the first k reviewed goods. Training dataset is generated with k = 1, 2, . . . , n ? 2 for each user. In the test set, we predict the last one given the first n ? 1 reviewed goods. For all models, we use SGD as the optimizer with exponential decay, in which learning rate starts at 1 and decay rate is set to 0.1. The mini-batch size is set to be 32.</p><p>MovieLens Dataset 3 . MovieLens data <ref type="bibr" target="#b10">[11]</ref> contains 138,493 users, 27,278 movies, 21 categories and 20,000,263 samples. To make it suitable for CTR prediction task, we transform it into a binary classification data. Original user rating of the movies is continuous value ranging from 0 to 5. We label the samples with rating of 4 and 5 to be positive and the rest to be negative. We segment the data into training and testing dataset based on userID. Among all 138,493 users, of which 100,000 are randomly selected into training set (about 14,470,000 samples) and the rest 38,493 into the test set (about 5,530,000 samples). The task is to predict whether user will rate a given movie to be above 3(positive label) based on historical behaviors. Features include movie_id, movie_cate_id and user rated movie_id_list, movie_cate_id_list. We use the same optimizer, learning rate and mini-batch size as described on Amazon Dataset.</p><p>Alibaba Dataset. We collected traffic logs from the online display advertising system in Alibaba, of which two weeks' samples are used for training and samples of the following day for testing. The size of training and testing set is about 2 billions and 0.14 billion respectively. For all the deep models, the dimensionality of embedding vector is 12 for the whole 16 groups of features. Layers of MLP is set by 192 ? 200 ? 80 ? 2. Due to the huge size of data, we set the mini-batch size to be 5000 and use Adam <ref type="bibr" target="#b14">[15]</ref> as the optimizer. We apply exponential decay, in which learning rate starts at 0.001 and decay rate is set to 0.9. The statistics of all the above datasets is shown in <ref type="table" target="#tab_2">Table 2</ref>. Volume of Alibaba Dataset is much larger than both Amazon and MovieLens, which brings more challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Competitors</head><p>? LR <ref type="bibr" target="#b18">[19]</ref>. Logistic regression (LR) is a widely used shallow model before deep networks for CTR prediction task. We implement it as a weak baseline. ? BaseModel. As introduced in section4.2, BaseModel follows the Embedding&amp;MLP architecture and is the base of most of subsequently developed deep networks for CTR modeling. It acts as a strong baseline for our model comparison. ? Wide&amp;Deep <ref type="bibr" target="#b3">[4]</ref>. In real industrial applications, Wide&amp;Deep model has been widely accepted. It consists of two parts: i) wide model, which handles the manually designed cross product features, ii) deep model, which automatically extracts nonlinear relations among features and equals to the BaseModel. Wide&amp;Deep needs expertise feature engineering on the input of the "wide" module. We follow the practice in <ref type="bibr" target="#b9">[10]</ref> to take cross-product of user behaviors and candidates as wide inputs. For example, in MovieLens dataset, it refers to the cross-product of user rated movies and candidate movies. ? PNN <ref type="bibr" target="#b4">[5]</ref>. PNN can be viewed as an improved version of BaseModel by introducing a product layer after embedding layer to capture high-order feature interactions. ? DeepFM <ref type="bibr" target="#b9">[10]</ref>. It imposes a factorization machines as "wide" module in Wide&amp;Deep saving feature engineering jobs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Metrics</head><p>In CTR prediction field, AUC is a widely used metric <ref type="bibr" target="#b7">[8]</ref>. It measures the goodness of order by ranking all the ads with predicted CTR, including intra-user and inter-user orders. An variation of user weighted AUC is introduced in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref> which measures the goodness of intra-user order by averaging AUC over users and is shown to be more relevant to online performance in display advertising system. We adapt this metric in our experiments. For simplicity, we still refer it as AUC. It is calculated as follows:</p><formula xml:id="formula_15">AUC = n i =1 #impr ession i ? AUC i n i =1 #impr ession i ,<label>(10)</label></formula><p>where n is the number of users, #impression i and AUC i are the number of impressions and AUC corresponding to the i-th user.</p><p>Besides, we follow <ref type="bibr" target="#b24">[25]</ref> to introduce RelaImpr metric to measure relative improvement over models. For a random guesser, the value of AUC is 0.5. Hence RelaImpr is defined as below:    </p><formula xml:id="formula_16">Rel aImpr = AUC(measured model) ? 0.5 AUC(base model) ? 0.5 ? 1 ? 100%.<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Result from model comparison on Amazon Dataset and MovieLens Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Performance of regularization</head><p>As the dimension of features in both Amazon Dataset and Movie-Lens Dataset is not high (about 0.1 million), all the deep models including our proposed DIN do not meet grave problem of overfitting. However, when it comes to the Alibaba dataset from the online advertising system which contains higher dimensional sparse features, overfitting turns to be a big challenge. For example, when training deep models with fine-grained features (e.g., features of ?oods_ids with dimension of 0.6 billion in <ref type="table" target="#tab_0">Table 1</ref>), serious overfitting occurs after the first epoch without any regularization, which causes the model performance to drop rapidly, as the dark green line shown in <ref type="figure" target="#fig_4">Fig.4</ref>. For this reason, we conduct careful experiments to check the performance of several commonly used regularizations.</p><p>? Dropout <ref type="bibr" target="#b21">[22]</ref>. Randomly discard 50% of feature ids in each sample. ? Filter. Filter visited ?oods_id by occurrence frequency in samples and leave only the most frequent ones. In our setting, top 20 million ?oods_ids are left. ? Regularization in DiFacto <ref type="bibr" target="#b15">[16]</ref>. Parameters associated with frequent features are less over-regularized. ? MBA. Our proposed Mini-Batch Aware regularization method (Eq.4). Regularization parameter ? for both DiFacto and MBA is searched and set to be 0.01. <ref type="figure" target="#fig_4">Fig.4</ref> and <ref type="table" target="#tab_5">Table 4</ref> give the comparison results. Focusing on the detail of <ref type="figure" target="#fig_4">Fig.4</ref>, model trained with fine-grained ?oods_ids features brings large improvement on the test AUC performance in the first epoch, compared without it. However, overfitting occurs rapidly in the case of training without regularization (dark green line). Dropout prevents quick overfitting but causes slower convergence. Frequency filter relieves overfitting to a degree. Regularization in DiFacto sets a greater penalty on ?oods_id with high frequency, which performs worse than frequency filter. Our proposed minibatch aware(MBA) regularization performs best compared with all the other methods, which prevents overfitting significantly.</p><p>Besides, well trained models with ?oods_ids features show better AUC performance than without them. This is duo to the richer information that fine-grained features contained. Considering this, although frequency filter performs slightly better than dropout, it throws away most of low frequent ids and may lose room for models to make better use of fine-grained features.  <ref type="bibr" target="#b10">11</ref>.65% RelaImpr and 0.0113 absolute AUC gain over Base-Model. Even compared with competitor DeepFM which performs best on this dataset, DIN still achieves 0.009 absolute AUC gain. It is notable that in commercial advertising systems with hundreds of millions of traffics, 0.001 absolute AUC gain is significant and worthy of model deployment empirically. DIN shows great superiority to better understand and make use of the characteristics of user behavior data. Besides, the two proposed techniques further improve model performance and provide powerful help for training large scale industrial deep networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Result from model comparison on Alibaba Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Result from online A/B testing</head><p>Careful online A/B testing in the display advertising system in Alibaba was conducted from 2017-05 to 2017-06. During almost a month's testing, DIN trained with the proposed regularizer and activation function contributes up to 10.0% CTR and 3.8% RPM(Revenue Per Mille) promotion 4 compared with the introduced BaseModel, the last version of our online-serving model. This is a significant improvement and demonstrates the effectiveness of our proposed approaches. Now DIN has been deployed online and serves the main traffic. It is worth mentioning that online serving of industrial deep networks is not an easy job with hundreds of millions of users visiting our system everyday. Even worse, at traffic peak our system serves more than 1 million users per second. It is required to make realtime CTR predictions with high throughput and low latency. For example, in our real system we need to predict hundreds of ads for each visitor in less than 10 milliseconds. In our practice, several important techniques are deployed for accelerating online serving of industrial deep networks under the CPU-GPU architecture: i) request batching which merges adjacent requests from CPU to take advantage of GPU power, ii) GPU memory optimization which improves the access pattern to reduce wasted transactions in GPU memory, iii) concurrent kernel computation which allows execution of matrix computations to be processed with multiple CUDA kernels concurrently. In all, optimization of these techniques doubles the QPS (Query Per Second) capacity of a single machine practically. Online serving of DIN also benefits from this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8">Visualization of DIN</head><p>Finally we conduct case study to reveal the inner structure of DIN on Alibaba dataset. We first examine the effectiveness of local activation unit. <ref type="figure" target="#fig_5">Fig.5</ref> illustrates the activation intensity of user behaviors with respect to a candidate ad. As expected, behaviors with high relevance to candidate ad are weighted high.</p><p>We then visualize the learned embedding vectors. Taking the young mother mentioned before as example, we randomly select 9 categories (dress, sport shoes, bags, etc) and 100 goods of each category as the candidate ads for her. <ref type="figure" target="#fig_6">Fig.6</ref> shows the visualization of embedding vectors of goods with t-SNE <ref type="bibr" target="#b16">[17]</ref> learned by DIN, in which points with same shape correspond to the same category. We can see that goods with same category almost belong to one cluster, which shows the clustering property of DIN embeddings clearly. Besides, we color the points that represent candidate ads by the prediction value. <ref type="figure" target="#fig_6">Fig.6</ref> is also a heat map of this mother's interest density distribution for potential candidates in embedding space. It shows DIN can form a multimodal interest density distribution in  candidates' embedding space for a certain user to capture his/her diverse interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, we focus on the task of CTR prediction modeling in the scenario of display advertising in e-commerce industry with rich user behavior data. The use of fixed-length representation in traditional deep CTR models is a bottleneck for capturing the diversity of user interests. To improve the expressive ability of model, a novel approach named DIN is designed to activate related user behaviors and obtain an adaptive representation vector for user interests which varies over different ads. Besides two novel techniques are introduced to help training industrial deep networks and further improve the performance of DIN. They can be easily generalized to other industrial deep learning tasks. DIN now has been deployed in the online display advertising system in Alibaba.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of running procedure of display advertising system in Alibaba, in which user behavior data plays important roles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Network Architecture. The left part illustrates the network of base model (Embedding&amp;MLP). Embeddings of cate_id, shop_id and goods_id belong to one goods are concatenated to represent one visited goods in user's behaviors. Right part is our proposed DIN model. It introduces a local activation unit, with which the representation of user interests varies adaptively given different candidate ads.Two most commonly used pooling layers are sum pooling and average pooling, which apply element-wise sum/average operations to the list of embedding vectors.Both embedding and pooling layers operate in a group-wise manner, mapping the original sparse features into multiple fixedlength representation vectors. Then all the vectors are concatenated together to obtain the overall representation vector for the instance.MLP. Given the concatenated dense representation vector, fully connected layers are used to learn the combination of features automatically. Recently developed methods<ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10]</ref> focus on designing structures of MLP for better information extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Control function of PReLU and Dice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where s is one dimension of the input of activation function f (?) and p(s) = I (s &gt; 0) is an indicator function which controls f (s) to switch between two channels of f (s) = s and f (s) = ?s. ? in the second channel is a learning parameter. Here we refer to p(s) as the control function. The left part of Fig.3 plots the control function of PReLU. PReLU takes a hard rectified point with value of 0, which may be not suitable when the inputs of each layer follow different distributions. Take this into consideration, we design a novel data adaptive activation function named Dice, f (s) = p(s) ? s + (1 ? p(s)) ? ?s, p(s) = 1 1 + e ? s ?E[s ] ? V ar [s ]+?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Performances of BaseModel with different regularizations on Alibaba Dataset. Training with fine-grained ?oods_ids features without regularization encounters serious overfitting after the first epoch. All the regularizations show improvement, among which our proposed mini-batch aware regularization performs best. Besides, well trained model with ?oods_ids features gets higher AUC than without them. It comes from the richer information that fine-grained features contained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Illustration of adaptive activation in DIN. Behaviors with high relevance to candidate ad get high activation weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of embeddings of goods in DIN. Shape of points represents category of goods. Color of points corresponds to CTR prediction value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of feature sets used in the display advertising system in Alibaba. Features are composed of sparse binary vectors in the group-wise manner.</figDesc><table><row><cell>Category</cell><cell>Feature Group</cell><cell>Dimemsionality</cell><cell>Type</cell><cell>#Nonzero Ids per Instance</cell></row><row><cell>User Profile Features</cell><cell>gender age_level ...</cell><cell>2 ? 10 ...</cell><cell>one-hot one-hot ...</cell><cell>1 1 ...</cell></row><row><cell></cell><cell>visited_goods_ids</cell><cell>? 10 9</cell><cell>multi-hot</cell><cell>? 10 3</cell></row><row><cell>User Behavior</cell><cell>visited_shop_ids</cell><cell>? 10 7</cell><cell>multi-hot</cell><cell>? 10 3</cell></row><row><cell>Features</cell><cell>visited_cate_ids</cell><cell>? 10 4</cell><cell>multi-hot</cell><cell>? 10 2</cell></row><row><cell></cell><cell>goods_id</cell><cell>? 10 7</cell><cell>one-hot</cell><cell>1</cell></row><row><cell>Ad Features</cell><cell>shop_id cate_id</cell><cell>? 10 5 ? 10 4</cell><cell>one-hot one-hot</cell><cell>1 1</cell></row><row><cell></cell><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell></cell><cell>pid</cell><cell>? 10</cell><cell>one-hot</cell><cell>1</cell></row><row><cell>Context Features</cell><cell>time</cell><cell>? 10</cell><cell>one-hot</cell><cell>1</cell></row><row><cell></cell><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of datasets used in this paper.</figDesc><table><row><cell>Dataset</cell><cell>Users</cell><cell>Goods a</cell><cell>Categories</cell><cell>Samples</cell></row><row><cell>Amazon(Electro).</cell><cell>192,403</cell><cell>63,001</cell><cell>801</cell><cell>1,689,188</cell></row><row><cell>MovieLens.</cell><cell>138,493</cell><cell>27,278</cell><cell>21</cell><cell>20,000,263</cell></row><row><cell>Alibaba.</cell><cell cols="2">60 million 0.6 billion</cell><cell>100,000</cell><cell>2.14 billion</cell></row></table><note>a For MovieLens dataset, goods refer to be movies.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Model Coparison on Amazon Dataset and Movie-Lens Dataset. All the lines calculate RelaImpr by comparing with BaseModel on each dataset respectively.</figDesc><table><row><cell>Model</cell><cell cols="2">MovieLens. AUC RelaImpr</cell><cell cols="2">Amazon(Electro). AUC RelaImpr</cell></row><row><cell>LR</cell><cell>0.7263</cell><cell>-1.61%</cell><cell>0.7742</cell><cell>-24.34%</cell></row><row><cell>BaseModel</cell><cell>0.7300</cell><cell>0.00%</cell><cell>0.8624</cell><cell>0.00%</cell></row><row><cell>Wide&amp;Deep</cell><cell>0.7304</cell><cell>0.17%</cell><cell>0.8637</cell><cell>0.36%</cell></row><row><cell>PNN</cell><cell>0.7321</cell><cell>0.91%</cell><cell>0.8679</cell><cell>1.52%</cell></row><row><cell>DeepFM</cell><cell>0.7324</cell><cell>1.04%</cell><cell>0.8683</cell><cell>1.63%</cell></row><row><cell>DIN</cell><cell>0.7337</cell><cell>1.61%</cell><cell>0.8818</cell><cell>5.35%</cell></row><row><cell cols="2">DIN with Dice a 0.7348</cell><cell>2.09%</cell><cell>0.8871</cell><cell>6.82%</cell></row></table><note>a Other lines except LR use PReLU as activation function.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>shows the results on Amazon dataset and MovieLens dataset. All experiments are repeated 5 times and averaged results are reported. The influence of random initialization on AUC is less than 0.0002. Obviously, all the deep networks beat LR model significantly, which indeed demonstrates the power of deep learning. PNN and DeepFM with specially designed structures preform better than Wide&amp;Deep. DIN performs best among all the competitors. Especially on Amazon Dataset with rich user behaviors, DIN stands out significantly. We owe this to the design of local activation unit structure in DIN. DIN pays attentions to the locally related user interests by soft-searching for parts of user behaviors that are relevant to candidate ad. With this mechanism, DIN obtains an adaptively varying representation of user interests, greatly improving the expressive ability of model compared with other deep networks. Besides, DIN with Dice brings further improvement over DIN, which verifies the effectiveness of the proposed data adaptive activation function Dice.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Best AUCs of BaseModel with different regularizations on Alibaba Dataset corresponding to Fig.4. All the other lines calculate RelaImpr by comparing with first line.</figDesc><table><row><cell>Regularization</cell><cell>AUC RelaImpr</cell></row><row><cell>Without goods_ids feature and Reg.</cell><cell>0.5940 0.00%</cell></row><row><cell>With goods_ids feature without Reg.</cell><cell>0.5959 2.02%</cell></row><row><cell>With goods_ids feature and Dropout Reg.</cell><cell>0.5970 3.19%</cell></row><row><cell>With goods_ids feature and Filter Reg.</cell><cell>0.5983 4.57%</cell></row><row><cell>With goods_ids feature and Difacto Reg.</cell><cell>0.5954 1.49%</cell></row><row><cell>With goods_ids feature and MBA. Reg.</cell><cell>0.6031 9.68%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>shows the experimental results on Alibaba dataset with full feature sets as shown inTable 1. As expected, LR is proven to be much weaker than deep models. Making comparisons among deep models, we report several conclusions. First, under the same activation function and regularization, DIN itself has achieved superior performance compared with all the other deep networks including BaseModel, Wide&amp;Deep, PNN and DeepFM. DIN achieves 0.0059 absolute AUC gain and 6.08% RelaImpr over BaseModel. It validates again the useful design of local activation unit structure. Second, ablation study based on DIN demonstrates the effectiveness of our proposed training techniques. Training DIN with mini-batch aware regularizer brings additional 0.0031 absolute AUC gain over dropout. Besides, DIN with Dice brings additional 0.0015 absolute AUC gain over PReLU.Taken together, DIN with MBA regularization and Dice achieves total</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Model Comparison on Alibaba Dataset with full feature sets. All the lines calculate RelaImpr by comparing with BaseModel. DIN significantly outperforms all the other competitors. Besides, training DIN with our proposed minibatch aware regularizer and Dice activation function brings further improvements.</figDesc><table><row><cell>Model</cell><cell>AUC</cell><cell>RelaImpr</cell></row><row><cell>LR</cell><cell>0.5738</cell><cell>-23.92%</cell></row><row><cell>BaseModel a,b</cell><cell>0.5970</cell><cell>0.00%</cell></row><row><cell>Wide&amp;Deep a,b</cell><cell>0.5977</cell><cell>0.72%</cell></row><row><cell>PNN a,b</cell><cell>0.5983</cell><cell>1.34%</cell></row><row><cell>DeepFM a,b</cell><cell>0.5993</cell><cell>2.37%</cell></row><row><cell>DIN Model a,b</cell><cell>0.6029</cell><cell>6.08%</cell></row><row><cell>DIN with MBA Reg. a</cell><cell>0.6060</cell><cell>9.28%</cell></row><row><cell>DIN with Dice b</cell><cell>0.6044</cell><cell>7.63%</cell></row><row><cell>DIN with MBA Reg. and Dice</cell><cell>0.6083</cell><cell>11.65%</cell></row></table><note>a These lines are trained with PReLU as the activation function.b These lines are trained with dropout regularization.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Experiment code on two public datasets is available on GitHub: https://github.com/zhougr1993/DeepInterestNetwork</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://jmcauley.ucsd.edu/data/amazon/ 3 https://grouplens.org/datasets/movielens/20m/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In our real advertising system, ads are ranked by CTR ? ? bid-price with ? &gt; 1.0, which controls the balance of promotion of CTR and RPM.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bengio</forename><surname>Ducharme R?jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoshua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Product-Based Neural Networks for User Response Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Data Mining</title>
		<meeting>the 16th International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DKN: Deep Knowledge-Aware Network for News Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 26th International World Wide Web Conference</title>
		<meeting>26th International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimized Cost per Click in Taobao Display Advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2191" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An introduction to ROC analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05194</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DeepFM: A Factorization-Machine based Neural Network for CTR Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1725" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The MovieLens Datasets: History and Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Interactive Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="DOI">10.1145/2872427.2883037</idno>
		<ptr target="https://doi.org/10.1145/2872427.2883037" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web</title>
		<meeting>the 25th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Densely connected convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DiFacto: Distributed factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 9th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="377" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image-Based Recommendations on Styles and Substitutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Hengel Anton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ad Click Prediction: a View from the Trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Brendan</forename><surname>Holt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Data Mining</title>
		<meeting>the 10th International Conference on Data Mining</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep Crossing: Web-scale modeling without manually crafted combinatorial features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Hoens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mao</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning Visual Clothing Style With Heterogeneous Dyadic Co-Occurrences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balazs</forename><surname>Kovacs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coupled group lasso for web-scale ctr prediction in display advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Rong</forename><surname>Wu-Jun Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingyi</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepintent: Learning attentions for online advertising with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keng-Hao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei Mark</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1295" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ATRank: An Attention-Based User Behavior Modeling Framework for Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 32th AAAI Conference on Artificial Intelligence</title>
		<meeting>32th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
