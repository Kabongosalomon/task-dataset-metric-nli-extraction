<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MULTI-DISEASE DETECTION IN RETINAL IMAGING BASED ON ENSEMBLING HETEROGENEOUS DEEP LEARNING MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>M?ller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IT-Infrastructure for Translational Medical Research</orgName>
								<orgName type="institution">University of Augsburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?aki</forename><surname>Soto-Rey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IT-Infrastructure for Translational Medical Research</orgName>
								<orgName type="institution">University of Augsburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Medical Data Integration Center</orgName>
								<orgName type="institution">University Hospital Augsburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Kramer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IT-Infrastructure for Translational Medical Research</orgName>
								<orgName type="institution">University of Augsburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MULTI-DISEASE DETECTION IN RETINAL IMAGING BASED ON ENSEMBLING HETEROGENEOUS DEEP LEARNING MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Preprint -March 2021 Page 1 / 6</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Retinal Disease Detection</term>
					<term>Ensemble Learning</term>
					<term>Class Imbalance</term>
					<term>Multi-label Image Classification</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Preventable or undiagnosed visual impairment and blindness affect billion of people worldwide. Automated multi-disease detection models offer great potential to address this problem via clinical decision support in diagnosis. In this work, we proposed an innovative multi-disease detection pipeline for retinal imaging which utilizes ensemble learning to combine the predictive capabilities of several heterogeneous deep convolutional neural network models. Our pipeline includes state-of-the-art strategies like transfer learning, class weighting, real-time image augmentation and Focal loss utilization. Furthermore, we integrated ensemble learning techniques like heterogeneous deep learning models, bagging via 5-fold cross-validation and stacked logistic regression models. Through internal and external evaluation, we were able to validate and demonstrate high accuracy and reliability of our pipeline, as well as the comparability with other stateof-the-art pipelines for retinal disease prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Even if the medical progress in the last 30 years made it possible to successfully treat the majority of diseases causing visual impairment, growing and aging populations lead to an increasing challenge in retinal disease diagnosis <ref type="bibr" target="#b0">[1]</ref>. The World Health Organization (WHO) estimates the prevalence of blindness and visual impairment to 2.2 billion people worldwide, of whom at least 1 billion affections could have been prevented or is yet to be addressed <ref type="bibr" target="#b1">[2]</ref>. Early detection and correct diagnosis are essential to forestall disease course and prevent blindness.</p><p>The use of clinical decision support (CDS) systems for diagnosis has been increasing over the past decade <ref type="bibr" target="#b2">[3]</ref>. Recently, modern deep learning models allow automated and reliable classification of medical images with remarkable accuracy comparable to physicians <ref type="bibr" target="#b3">[4]</ref>. Nevertheless, these models often lack capabilities to detect rare pathologies such as central retinal artery occlusion or anterior ischemic optic neuropathy <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p><p>In this study we push towards creating a highly accurate and reliable multi-disease detection pipeline based on ensemble, transfer and deep learning techniques. Furthermore, we utilize the new Retinal Fundus Multi-Disease Image Dataset (RFMiD) containing various rare and challenging conditions to demonstrate our detection capabilities for uncommon diseases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODS</head><p>The implemented medical image classification pipeline can be summarized in the following core steps and is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>: -Stratified multi-label 5-fold cross-validation -Class weighted Focal loss and up-sampling -Extensive real-time image augmentation -Multiple deep learning model architectures -Ensemble learning strategies: bagging and stacking -Individual training for multi-disease labels and disease risk detection utilizing transfer learning on ImageNet -Stacked binary logistic regression models for distinct classification</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Retinal Imaging Dataset</head><p>The RFMiD dataset consists of 3200 retinal images for which 1920 images were used as training dataset <ref type="bibr" target="#b6">[7]</ref>. The fundus images were captured by three different fundus cameras having a resolution of 4288x2848 (277 images), 2048x1536 (150 images) and 2144x1424 (1493 images), respectively.</p><p>Tab. 1. Annotation frequency for each class in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disease</head><p>Samples Disease Samples Disease Samples D. Risk</p><p>The images were annotated with 46 conditions, including various rare and challenging diseases, through adjudicated consensus of two senior retinal experts. These 46 conditions are represented by the following classes, which are also listed in Tab. 1: An overall normal/abnormal class, 27 specific condition classes and 1 'OTHER' class consisting of the remaining extremely rare conditions. Besides the training dataset, the organizers of the RIADD challenge hold 1280 images back for external validation and testing datasets to ensure robust evaluation <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Preprocessing and Image Augmentation</head><p>In order to simplify the pattern finding process of the deep learning model, as well as to increase data variability, we applied several preprocessing methods.</p><p>We utilized extensive image augmentation for upsampling to balance class distribution and real-time augmentation during training to obtain novel and unique images in each epoch. The augmentation techniques consisted of rotation, flipping, and altering in brightness, saturation, contrast and hue. Through the up-sampling, it was ensured that each label occurred at least 100 times in the dataset which increased the total number of training images from 1920 to 3354.</p><p>Afterwards, all images were square padded in order to avoid aspect ratio loss during posterior resizing. The retinal images were also cropped to ensure that the fundus is center located in the image. The cropping was performed individually for each microscope resolution and resulted in the following image shapes: 1424x1424, 1536x1536 and 3464x3464 pixels. The images were then resized to model input sizes according to the neural network architecture, which was 380x380 for EfficientNetB4, 299x299 for InceptionV3 and 244x244 for all remaining architectures <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b12">[12]</ref>.</p><p>Before feeding the image to the deep convolutional neural network, we applied value intensity normalization as last preprocessing step. The intensities were zero-centered via the Z-Score normalization approach based on the mean and standard deviation computed on the ImageNet dataset <ref type="bibr" target="#b13">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Deep Learning Models</head><p>The state-of-the-art for medical image classification are the unmatched deep convolutional neural network models <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[14]</ref>. Nevertheless, the hyper parameter configuration and architecture selection are highly dependent on the required computer vision task, as well as the key difference between pipelines <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b15">[15]</ref>. Thus, our pipeline combines two different types of image classification models: The disease risk detector for binary classifying normal/abnormal images and the disease label classifier for multi-label annotation of abnormal images.</p><p>Both model types were pretrained on the ImageNet dataset <ref type="bibr" target="#b13">[13]</ref>. For the fitting process, we applied a transfer learning training, with frozen architecture layers except for the classification head, and a fine-tuning strategy with unfrozen layers. Whereas the transfer learning fitting was performed for 10 epochs using the Adam optimization with an initial learning rate of 1-E04, the fine-tuning had a maximal training time of 290 epochs and using a dynamic learning rate for the Adam optimization starting from 1-E05 to a maximum decrease to 1-E07 (decreasing factor of 0.1 after 8 epochs without improvement on the monitored validation loss) <ref type="bibr" target="#b16">[16]</ref>. Furthermore, an early stopping and model checkpoint technique was utilized for the fine-tuning process, stopping after 20 epochs without improvement (after epoch 60) and saving the best model measured according to the validation loss. Instead of defining an epoch as a cycle through the full training dataset, we establish an epoch to have 250 iterations. This allowed to increase the number of seen batches and, thus, to increase the information given to the model during the fitting process of an epoch. As training loss function, we utilized the weighted Focal loss from Lin et al. <ref type="bibr" target="#b17">[17]</ref>.</p><formula xml:id="formula_0">FL( ) = ? (1 ? ) log ( )<label>(1)</label></formula><p>In the above formula, pt is the probability for the correct ground truth class t, ? a tunable focusing parameter (which we set to 2.0) and ?t the associated weight for class t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Disease Risk Detector</head><p>The disease risk detector was established as a binary classifier of the disease risk class for general categorizing between normal and abnormal retinal images. Thus, this model type was trained using only the disease risk class and ignoring all multi-label annotations. Rather than using a single model architecture, we trained multiple models based on the DenseNet201 and EfficientNetB4 architecture <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. For class weight computation, we divided the number of samples by the multiplication of the number of classes (2 for a binary classification) with the number of class occurrences in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Disease Label Classifier</head><p>In contrast, the disease label classifier was established as multi-label classifier of all 28 remaining classes (excluding disease risk) and was trained on the one hot encoded array of the disease labels. Furthermore, we utilized four different architectures for this model type: ResNet152, InceptionV3, DenseNet201 and EfficientNetB4 [9]- <ref type="bibr" target="#b12">[12]</ref>. Identical to class weight computation of the disease risk detector, we computed the weights individually as binary classification for each class. Even if this classifier is provided with all classes, the binary weights balance the decision for each label individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Ensemble Learning Strategy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Bagging</head><p>Next to the utilization of multiple architecture, we also applied a 5-fold cross-validation based as a bagging approach for ensemble learning. Our aim was to create a large variety of models which were trained on different subsets of the training data. This approach not only allowed a more efficient usage of the available training data, but also increased the reliability of a prediction. This strategy resulted in an ensemble of 10 disease risk detector models (2 architectures with each 5 folds) and 20 disease label classifier models (4 architectures with each 5 folds).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Stacking</head><p>For combining the predictions of our, in total, 30 models, we integrated a stacking setup. On top of all deep convolutional neural networks, we applied a binary logistic regression algorithm for each class, individually. Thus, the predictions of all models were utilized as input for computing the classification of a single class. This approach allowed combining the information of all other class predictions to derive an inference for one single class. Overall, this strategy resulted in 29 distinct logistic regression models (1 for disease risk and 28 for each disease-label including the 'other' class). The individual predicted class probabilities are then concatenated to the final prediction. The logistic regression models were also trained with the same 5-fold cross-validation sampling on a heavily augmented version of the training dataset to avoid overfitting as well as avoiding training the logistic regression models on already seen images from the neural network models. As logistic regression solver, we utilized the large-scale boundconstrained optimization (short: 'LBFGS') from Zhu et al. <ref type="bibr" target="#b18">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RESULTS AND DISCUSSION</head><p>The sequential training of a complete cross-validation for one architecture on a single NVIDIA TITAN RTX GPU took around 13.5 hours with 63 epochs on average for each deep convolutional neural network model. Logistic Regression training required less than 30 minutes for all class models combined. No signs of overfitting were observed for the disease label classifiers through validation monitoring, as it can be seen in <ref type="figure" target="#fig_1">Fig. 2</ref>. However, the disease risk detectors showed a strong trend to overfit after the transfer learning phase. Through our strategy to use the model with the best </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Internal Performance Evaluation</head><p>For estimating the performance of our pipeline, we utilized the validation subsets of the 5-fold cross-validation models from the heavily augmented version of our dataset. This approach allowed to obtain testing samples which were never seen in the training process for reliable performance evaluation. For the complex multi-label evaluation, we computed the popular area under the receiver operating characteristic (AUROC) curve, as well as the mean average precision (mAP). Both scores were macro-averaged over classes and cross-validation folds to reduce complexity.</p><p>Our multi-disease detection pipeline revealed a strong and robust classification performance with the capability to also detect rare conditions accurately in retinal images. Whereas the disease label classifier models separately only achieved an AUROC of around 0.97 and a mAP of 0.93, the disease risk detectors demonstrated to have a really strong predictive power of 0.98 up to 0.99 AUROC and mAP. However, for the classifiers the InceptionV3 architecture indicated to have the worst performance compared to the other architectures with only 0.93 AUROC and 0.66 mAP. The associated receiver operating characteristics of the models are illustrated in <ref type="figure">Fig. 3</ref>.</p><p>Training a strong multi-label classifier is in general a complex task, however, the extreme class imbalance between the conditions revealed a hard challenge for building a reliable model <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>. Our applied up-sampling and class weighting technique demonstrated to have a critical boost on the predictive capabilities of the classifier models. Nearly all labels were able to be accurately detected, including the 'OTHER' class consisting of various extremely rare conditions. Nevertheless, the two classes 'EDN' and 'CRS' were the most challenging conditions for all classifier models. Both classes belong to very rare conditions, combined with less than 1.2% occurrence in the original and 2.5% occurrence in the up-sampled dataset. Still, our stacked logistic regression algorithm was able to balance this issue and infer the correct 'EDN' and 'CRS' classifications through context. Overall, our applied ensemble learning strategies resulted in a significant performance improvement compared to the individual deep convolutional neural network models. More details on the internal performance evaluation are listed in Tab. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">External Evaluation through the RIADD Challenge</head><p>Furthermore, we participated at the RIADD challenge which was organized by the authors of the RFMiD dataset <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. The challenge participation allowed not only an independent  evaluation of the predictive power of our pipeline on an unseen and unpublished testing set, but also the comparison with the currently best retinal disease classifiers in the world. In our participation, we were able to reach rank 19 from a total of 59 teams in the first evaluation phase and rank 8 in the final phase. In the independent evaluation from the challenge organizers, we achieved an AUROC of 0.95 for the disease risk classification. For multi-label scoring, they computed the average between the macro-averaged AUROC and the mAP, for which we reached the score 0.70. The top performing ranks shared only a marginal scoring difference which is why we had only a final score difference of 0.05 to the first ranked team. Furthermore, the participation results demonstrated that ensemble learning based classification for deep convolutional neural network models is compatible or even superior to other approaches in the scientific field such as focusing on a single large architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experiments and Improvements</head><p>Additionally, we experimented with using weighted crossentropy loss for training our both model types. This resulted in inferior models for disease label classification, however, the cross-entropy loss fitted disease risk detector models showed less overfitting with equal performance. Further experimentation with loss functions for the disease risk detector models could provide the solution to avoid overfitting.</p><p>An important point for the RIADD challenge participation would be the utilization of more training data, especially for the difficult 'CRS' and 'EDN' classes. According to the challenge rules, other public available datasets like Kaggle DR, IDRiD, Messidor or APTOS are allowed to be used as additional training data <ref type="bibr" target="#b7">[8]</ref>. Our pipeline, which was trained exclusively on the RFMiD dataset, could be further improved with more retinal images of very rare conditions. Besides the training data, more improvement points for further research in retinal disease detection would be the inclusion of image cropping strategies to reduce information loss through resolution resizing, the usage of more architectures (especially with different input resolutions) to increase the model ensemble, and the utilization of specific retinal filters or retinal vessel segmentation as additional information to utilize for the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In this study, we introduced a powerful multi-disease detection pipeline for retinal imaging which exploits ensemble learning techniques to combine the predictions of various deep convolutional neural network models. Next to state-of-the-art strategies, such as transfer learning, class weighting, extensive real-time image augmentation and Focal loss utilization, we applied 5-fold cross-validation as bagging technique and used multiple convolutional neural network architectures to create an ensemble of models. With a stacking approach of class-wise distinct logistic regression models, we combined the knowledge of all neural network models to compute highly accurate and reliable retinal condition predictions. Next to an internal performance evaluation, we also proved the precision and comparability of our pipeline through the participation at the RIADD challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APENDIX</head><p>In order to ensure full reproducibility and to create a base for further research, the complete code of this study, including extensive documentation, is available in the following public Git repository: https://github.com/frankkramer-lab/riadd.aucmedi Furthermore, the trained models, evaluation results and metadata are available in the following public Zenodo repository: https://doi.org/10.5281/zenodo.4573990</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Flowchart diagram of the implemented medical image analysis pipeline for multi-disease detection in retinal imaging. The workflow is starting with the retinal imaging dataset (RFMiD) and ends with computed predictions for novel images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Loss course during the training process for training and validation data. The lines were computed via locally estimated scatterplot smoothing and represent the average loss across all folds. The gray areas around the lines represent the confidence intervals. validation loss, it was still possible to obtain a powerful model for detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Tab. 2 .</head><label>32</label><figDesc>Receiver operating characteristic (ROC) curves for each model type applied in our pipeline. The ROC curves showing the individual model performance measured by the true positive and false positive rate. The cross-validation models were macro-averaged for each model type to reduce illustration complexity. Achieved results of the internal performance evaluation showing the average AUROC and mAP score for each model utilized in our pipeline. The scores were macroaveraged across all cross-validation folds and classes.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We want to thank Dennis Klonnek, Edmund M?ller and Johann Frei for their useful comments and support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COMPLIANCE WITH ETHICAL STANDARDS</head><p>This research study was conducted retrospectively using human subject data made available in open access by Pachade et al. <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Ethical approval was not required as confirmed by the license attached with the open access data.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONFLICT OF INTEREST</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Causes of blindness and vision impairment in 2020 and trends over 30 years, and prevalence of avoidable blindness in relation to VISION 2020: the Right to Sight: an analysis for the Global Burden of Disease Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Adelson</surname></persName>
		</author>
		<idno type="DOI">10.1016/S2214-109X(20</idno>
	</analytic>
	<monogr>
		<title level="j">Lancet Glob. Heal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="30489" to="30496" />
			<date type="published" when="2021-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Blindness and vision impairment</title>
		<ptr target="https://www.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment" />
		<imprint>
			<date type="published" when="2021-02-27" />
		</imprint>
	</monogr>
	<note>World Health Organization</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An overview of clinical decision support systems: benefits, risks, and strategies for success</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pincock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Baumgart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Sadowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Fedorak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kroeker</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-020-0221-y</idno>
	</analytic>
	<monogr>
		<title level="j">npj Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
	<note>Nature Research</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2017.07.005</idno>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-categorical deep learning neural network to classify retinal images: A pilot study employing small database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Um</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Rim</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0187336</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">187336</biblScope>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic detection of rare pathologies in fundus photographs using few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Quellec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lamard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Conze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Massin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cochener</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2020.101660</idno>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">101660</biblScope>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Retinal Fundus Multi-Disease Image Dataset (RFMiD): A Dataset for Multi-Disease Detection Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pachade</surname></persName>
		</author>
		<idno type="DOI">10.3390/data6020014</idno>
	</analytic>
	<monogr>
		<title level="j">Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Home -RIADD (ISBI-2021) -Grand Challenge</title>
		<ptr target="https://riadd.grand-challenge.org/Home/" />
		<imprint>
			<date type="published" when="2021-02-27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1608.06993" />
	</analytic>
	<monogr>
		<title level="m">Proc. -30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017</title>
		<meeting>-30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017</meeting>
		<imprint>
			<date type="published" when="2016-08" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th Int. Conf. Mach. Learn. ICML 2019</title>
		<imprint>
			<date type="published" when="2019-02-27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<ptr target="http://arxiv.org/abs/1905.11946" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016-12" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016-12" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Medical Image Analysis using Convolutional Neural Networks A Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muhammad</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10916-018-1088-1</idno>
	</analytic>
	<monogr>
		<title level="j">J. Med. Syst</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Learning Applications in Medical Image Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2017.2788044</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="9375" to="9379" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1708.02002" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">L-BFGS-B: Fortran Subroutines for Large-Scale Bound-Constrained Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<idno type="DOI">10.1145/279232.279236</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Softw</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="550" to="560" />
			<date type="published" when="1997-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Issues and challenges of class imbalance problem in classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gosain</surname></persName>
		</author>
		<idno type="DOI">10.1007/s41870-018-0251-8</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Inf. Technol</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Handling imbalanced medical image data: A deep-learning-based one-class classification approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artmed.2020.101935</idno>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">101935</biblScope>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
