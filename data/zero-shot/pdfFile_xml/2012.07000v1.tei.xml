<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KVL-BERT: Knowledge Enhanced Visual-and-Linguistic BERT for Visual Commonsense Reasoning 1 st Dandan Song</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cn</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyi</forename><surname>Ma</surname></persName>
							<email>masiyi@bit.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Yang</surname></persName>
							<email>yangsicheng@bit.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lejian</forename><surname>Liao</surname></persName>
							<email>liaolj@bit.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">rd Zhanchen Sun Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">KVL-BERT: Knowledge Enhanced Visual-and-Linguistic BERT for Visual Commonsense Reasoning 1 st Dandan Song</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-visual commonsense reasoning</term>
					<term>multimodal BERT</term>
					<term>commonsense knowledge integration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reasoning is a critical ability towards complete visual understanding. To develop machine with cognition-level visual understanding and reasoning abilities, the visual commonsense reasoning (VCR) task has been introduced. In VCR, given a challenging question about an image, a machine must answer correctly and then provide a rationale justifying its answer. The methods adopting the powerful BERT model as the backbone for learning joint representation of image content and natural language have shown promising improvements on VCR. However, none of the existing methods have utilized commonsense knowledge in visual commonsense reasoning, which we believe will be greatly helpful in this task. With the support of commonsense knowledge, complex questions even if the required information is not depicted in the image can be answered with cognitive reasoning. Therefore, we incorporate commonsense knowledge into the cross-modal BERT, and propose a novel Knowledge Enhanced Visual-and-Linguistic BERT (KVL-BERT for short) model. Besides taking visual and linguistic contents as input, external commonsense knowledge extracted from ConceptNet is integrated into the multi-layer Transformer. In order to reserve the structural information and semantic representation of the original sentence, we propose using relative position embedding and mask-self-attention to weaken the effect between the injected commonsense knowledge and other unrelated components in the input sequence. Compared to other task-specific models and general task-agnostic pre-training models, our KVL-BERT outperforms them by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recently, increasing attention has been focused on visual understanding, and great advances have been achieved in image caption ( [1]- <ref type="bibr" target="#b4">[5]</ref>) and visual question answer (VQA) ( [6]- <ref type="bibr" target="#b8">[9]</ref>). Towards complete visual understanding, artificial intelligence models must perform cognition-level reasoning beyond recognition-level perception. To move towards this goal, the task of visual commonsense reasoning (VCR) <ref type="bibr" target="#b9">[10]</ref> is proposed along with a well-devised new dataset. In VCR, given a challenging question about an image, a machine should answer it correctly and then provide a rationale justifying its answer. Besides detecting objects and their attributes, inferring the likely goals or reasons is needed.</p><p>In recent research, some task-specific models are proposed on the VCR task, such as R2C <ref type="bibr" target="#b9">[10]</ref>, CCN <ref type="bibr" target="#b10">[11]</ref> and HGL <ref type="bibr" target="#b11">[12]</ref>, which achieve good results. The methods adopting the powerful BERT <ref type="bibr" target="#b12">[13]</ref> model as the backbone for learning taskagnostic joint representation of image content and natural language, such as VisualBERT <ref type="bibr" target="#b13">[14]</ref>, ViLBERT <ref type="bibr" target="#b14">[15]</ref>, VL-BERT <ref type="bibr" target="#b15">[16]</ref> and B2T2 <ref type="bibr" target="#b16">[17]</ref>, have shown promising improvements on VCR. However, none of the existing methods have utilized commonsense knowledge in visual commonsense reasoning. In some cases, the explicit recognition results, such as objects or attributes, are not enough for accurate VCR. As not all of the required information is depicted in the image, we need the support of external knowledge to answer complex questions. Moreover, external knowledge supports cognitive reasoning, which is an essential challenge in the VCR task. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the left side of the figure describes an example from the VCR benchmark, the question could not be answered easily because there is no "church" shown in the figure. Based on the detected object "bride", only when the model is equipped with the commonsense knowledge "bride is related to church" and "church is used for getting married", the question could be answered and reasoned correctly.</p><p>Therefore, we incorporate commonsense knowledge into the cross-modal BERT, and propose a novel Knowledge Enhanced Visual-and-Linguistic BERT model in this paper. Specifically, to incorporate commonsense knowledge, we inject relevant entities extracted from ConceptNet <ref type="bibr" target="#b17">[18]</ref> into the input sentence. In this way, the original sentence is transformed into a commonsense-knowledge-enriched sentence. Then, we propose a mechanism for sentence structure and semantic representation reservation. In order to keep the readability and structural information of the original sentence, we employ relative position embedding for the transformed sentence. Furthermore, inspired by <ref type="bibr" target="#b18">[19]</ref>, to weaken the effect between the injected commonsense knowledge and other unrelated components in the input sequence, we make the injected commonsense knowledge visible only to its related entity token, but not to other tokens in the original sentence or visual feature vectors via a visible matrix. We also adopt mask-self-attention mechanism to reserve the semantic and visual representations of the original input. Finally, we feed the token embedding of the commonsense-knowledge-enriched sentence, its special position embedding, segment embedding, visual feature embedding, and the visible matrix to the pretrained Visual-Linguistic BERT <ref type="bibr" target="#b15">[16]</ref> for training and inference.</p><p>Taking <ref type="figure" target="#fig_0">Figure 1</ref> as an example, the object "bride" is the category label of a specific bounding box in the input image. When the model answers question based on the input image and text sequence (the input text sequence consists of question and one of the answers), it first retrieves the tokens contained in the input text sequence from the external knowledge base. For the token "church", the model could query its related entities as shown on the right of <ref type="figure" target="#fig_0">Figure 1</ref>. After the model injects the token "bride" from the external knowledge base into the original sentence, the representation of the token "church" is enriched by the injected token "bride". Then the attention score between the token "church" in the answer and the bounding box where "bride" is detected in the question will be high, which would help the model to choose the correct answer based on the original input and injected commonsense knowledge.</p><p>We conduct comparative experiments on the VCR dataset. Compared to other task-specific models such as R2C <ref type="bibr" target="#b9">[10]</ref>, CCN <ref type="bibr" target="#b10">[11]</ref>, HGL <ref type="bibr" target="#b11">[12]</ref>, and pre-trained task-agnostic multimodal BERT models such as VisualBERT <ref type="bibr" target="#b13">[14]</ref>, ViLBERT <ref type="bibr" target="#b14">[15]</ref>, Unicoder-VL <ref type="bibr" target="#b19">[20]</ref>, B2T2 <ref type="bibr" target="#b16">[17]</ref>, our KVL-BERT outperforms them by a large margin. To find the most effective way to integrate commonsense knowledge, besides our proposed KVL-BERT, we design and evaluate two variants: (1) Extract commonsense knowledge embedding corresponding to each token with transE <ref type="bibr" target="#b20">[21]</ref>, then input the word embedding and commonsense knowledge embedding to the multimodal BERT together. (2) Inject relevant entities extracted from ConceptNet into the input sentence in the same way as the KVL-BERT. Differently, we make the injected knowledge entity tokens share the same position embedding with their related token in the original sentence, and it lacks the mechanism of sentence structure and semantic representation reservation. In short, our contributions can be summarized as:</p><p>? We incorporate commonsense knowledge into the VCR task, and propose a novel KVL-BERT model. To the best of our knowledge, it is the first research to incorporate commonsense knowledge into the VCR task. ? We design and evaluate three architectures of incorporating commonsense knowledge into the cross-modal BERT.</p><p>The experimental results show that injecting commonsense knowledge into the input sentence with sentence structure and semantic representation reservation mechanism is the most effective way. ? Compared to other task-specific models and general taskagnostic pre-training models, our KVL-BERT outperforms them by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Visual commonsense reasoning</head><p>As a critical step towards complete visual understanding, the task of visual commonsense reasoning (VCR) is proposed.</p><p>Beyond recognition-level perception, the model must perform cognition-level reasoning. <ref type="bibr" target="#b9">[10]</ref> introduces Recognition to Cognition Network (R2C) to model the necessary layered inferences for grounding, contextualization, and reasoning. <ref type="bibr" target="#b10">[11]</ref> proposes a Cognition Connectivity Network (CCN) including visual neuron connectivity, contextualized connectivity, and directional connectivity for reasoning. <ref type="bibr" target="#b11">[12]</ref> proposes Heterogeneous Graph Learning (HGL) framework for seamlessly integrating the intra-graph and inter-graph reasoning in order to bridge the vision and language domain. Motivated by the success of BERT <ref type="bibr" target="#b12">[13]</ref> in many natural language processing tasks, several researchers adopt BERT as the backbone for learning task-agnostic joint representation of image content and natural language, such as VisualBERT <ref type="bibr" target="#b13">[14]</ref>, ViLBERT <ref type="bibr" target="#b14">[15]</ref>, VL-BERT <ref type="bibr" target="#b15">[16]</ref>, B2T2 <ref type="bibr" target="#b16">[17]</ref>, Unicoder-VL <ref type="bibr" target="#b19">[20]</ref> and UNITER <ref type="bibr" target="#b21">[22]</ref>, which have shown promising improvements on VCR. However, none of the existing methods have utilized commonsense knowledge in visual commonsense reasoning, which we believe will be greatly helpful in this task. So we propose a novel model to incorporate commonsense knowledge into the cross-modal BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pre-training for visual-linguistic tasks</head><p>After the success of pre-training for computer vision ( <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>) and natural language processing ( <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>) tasks, a series of cross-modal pre-training models are designed. These models utilize self-supervised setting to get joint imagetext embedding, gaining appealing results on various visuallinguistic tasks. Masked Language Model <ref type="bibr" target="#b12">[13]</ref> and similar Masked Region Prediction <ref type="bibr" target="#b14">[15]</ref> tasks are utilized in crossmodal pre-training. And similar to Next-Sentence Prediction <ref type="bibr" target="#b12">[13]</ref>, Image-Text Matching ( <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b21">[22]</ref>) task in also widely used. <ref type="bibr" target="#b21">[22]</ref> also adds extra scene graph prediction tasks (object prediction, attribute prediction and relationship prediction) in the pre-training phase, where the scene graph is constructed by parsing the text sentence into object nodes, attribute nodes and relationship nodes. These latest models are based on different variables of Transformers. VideoBERT <ref type="bibr" target="#b26">[27]</ref> uses off-the-shelf networks to process video clips that are assigned to different clusters, whose ids will be predicted during pre-training. In ViLBERT <ref type="bibr" target="#b14">[15]</ref>, LXMERT <ref type="bibr" target="#b27">[28]</ref> and ERNIE-ViL <ref type="bibr" target="#b28">[29]</ref>, two-stream architecture is introduced. Two single-modal networks process the input image and sentence respectively, then a cross-modal Transformer combines two kinds of information. On the contrary, VisualBERT <ref type="bibr" target="#b13">[14]</ref>, Unicoder-VL <ref type="bibr" target="#b19">[20]</ref>, VL-BERT <ref type="bibr" target="#b15">[16]</ref>, B2T2 <ref type="bibr" target="#b16">[17]</ref> UNITER <ref type="bibr" target="#b21">[22]</ref> and VILLA <ref type="bibr" target="#b29">[30]</ref> propose the single-stream architecture, where a single Transformer is applied to both image and text contents. Compared to the two-stream architecture, it fuses crossmodal information earlier and more flexibly. In our paper, we adopt the single-stream VL-BERT as the backbone to incorporate external commonsense knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. External knowledge integration</head><p>Recent work has confirmed that the machine can become more powerful when incorporating external knowledge in many tasks, such as object detection ( <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>), dialogue generation ( <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>) and cloze style reading comprehension ( <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>). <ref type="bibr" target="#b30">[31]</ref> quantifies semantic consistency based on knowledge graphs and further re-optimizes object detection to achieve better consistency. The incorporation of commonsense knowledge promotes the dialogue generation system <ref type="bibr" target="#b32">[33]</ref> to generate more accurate responses for both factoid-questions and knowledge grounded chats. By integrating knowledge, the model <ref type="bibr" target="#b34">[35]</ref> can obtain more explicit evidence in the reading comprehension process. <ref type="bibr" target="#b18">[19]</ref> solves the knowledgedriven problems in the plain text tasks leveraging domainspecific knowledge. In this paper, our goal is incorporating external commonsense knowledge into the visual commonsense reasoning task to answer complex questions even if the required information is not depicted in the image with cognitive reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MODEL DESCRIPTION</head><p>Given an input image, the VCR task is divided into two subtasks: (1) Q ? A: given a question (Q), select the correct answer (A) from candidate answers. (2) QA ? R: given a question (Q) and its correct answer (A), select the correct rationale (R) from candidate rationales. Both subtasks can be unified as choosing the correct response from candidate options given a query. For each query-response pair, the class score is calculated, and we choose the response with the highest score.</p><p>In this section, we present the overall framework of KVL-BERT and its detailed implementation, including the model architecture in Section III-A, the method of commonsense knowledge integration in Section III-B, the mechanism of sentence structure and semantic representation reservation in Section III-C, and the pre-trained visual-linguistic BERT model in Section III-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model architecture</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the whole model architecture of KVL-BERT consists of three modules:</p><p>Commonsense knowledge integration module is responsible to transform the original sentence into a commonsenseknowledge-enriched sentence. For an input sentence, this module retrieves relevant commonsense knowledge facts from ConceptNet and injects them into the original sentence. In <ref type="figure" target="#fig_1">Figure 2</ref>, the purple tokens "bride" and "get married" are the injected commonsense knowledge for token "church".</p><p>Sentence structure and semantic representation reservation module is responsible to adjust the effect between the injected commonsense knowledge and other components in the original input. In <ref type="figure" target="#fig_1">Figure 2</ref>, since the original absolute position indexes (marked in green) are changed due to the knowledge injection, we conduct relative position embedding (marked in blue) to keep the structural information of the original sentence. Then a visible matrix is constructed to limit the visible region of each token, which will be used to conduct mask-self-attention.</p><p>Pre-trained visual-Linguistic BERT module is responsible to align tokens in the input sentence with regions in the input image, and learn a joint representation of visual and linguistic contents. In this module, besides all the components of BERT, visual feature embedding is introduced to model the input image. All the embeddings are then passed to the multi-layer Transformer to learn a new joint representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Commonsense knowledge integration</head><p>We choose ConceptNet as the source of external commonsense knowledge, which is a knowledge graph that connects words and phrases of natural language with labeled and weighted edges. It can be seen as a large set of facts, and each fact f i is represented as a triple f i = (h, r, t), where h and t represent head and tail entities in the concept set V , r is a relation type from the pre-defined set R, e.g.,</p><formula xml:id="formula_0">([dog] h , [HasA] r , [tail] t ).</formula><p>Given an input sentence, we first retrieve the relevant commonsense knowledge facts via entity tokens contained in the input sentence. Each fact has a weight representing the credibility of it. The larger the weight is, the more credible the fact is. We sort the facts related to the input token by the weight value, because the facts with larger weight value are more trustworthy, i.e., they are more acceptable in the real world and more consistent with human cognition. Then we get the top k commonsense knowledge entities from the sorted list and insert them after their relevant token (k is a hyper parameter), while subsequent tokens in the sentence are moved backwards. In this way, the original sentence is transformed into a commonsense-knowledge-enriched sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sentence structure and semantic representation reservation</head><p>The input sentence becomes unreadable and its structure is deformed by the injection of commonsense knowledge. To tackle this issue, we propose to conduct relative position embedding. In addition, we set a visible matrix and conduct mask-self-attention simultaneously to reserve the semantic and visual representations of the original input.</p><p>1) Relative position embedding: For the self-attention mechanism in BERT, it does not take advantage of the position information of the word. In this case, even if two identical words appear in different positions, they will be encoded into a same vector when the model parameters are fixed. But in fact, these two same words appear in different positions may have different semantics, so the structural information of sentence will be utilized well by adding position embedding to the input of BERT. However, the position embedding is changed due to the injection of commonsense knowledge, which will deform the structure of the original sentence. To this end, we conduct relative position embedding for the commonsense-knowledgeenriched sentence. The position embedding of the original tokens is not changed, regardless of whether commonsense knowledge is injected, while the position embedding of the injected knowledge for a token increases from the position of the token. In this way, we can still use the structural information of the original sentence to calculate the selfattention score in the Transformer encoder.</p><p>In addition, as the visual position information is expressed by its coordinate and size, we will take it into consideration during conducting visual feature embedding. Here we assign the same position embedding for all [IMG] tokens.</p><p>2) Visible matrix: The injected commonsense knowledge will also change the representation of other components in the original input. Therefore, we set a visible matrix to weaken the effect between the injected commonsense knowledge and other unrelated components in the input sequence. For a certain token, the injected commonsense knowledge tokens are only related to it, but unrelated to other tokens contained in the original sentence, which are unrelated components. For example, in <ref type="figure" target="#fig_1">Figure 2</ref>, for the token "church" in the input text sequence, the injected commonsense knowledge token "bride" is only related to the token "church", but unrelated to other tokens contained in the original input sentence, such as "walking" and "stairs". We suppose that the injected knowledge only acts on its related entity token and doesn't influence other words or visual feature representation contained in the input sequence. Meanwhile, other words and visual feature representation shouldn't affect the representation of the external knowledge. For this reason, we set a visible matrix to limit the visible region of each token, i.e., we make the injected commonsense knowledge visible only to its related entity token, but not to other tokens in the original sentence or visual feature vectors. The visible matrix W is defined as</p><formula xml:id="formula_1">W ij = 0, w i is invisible to w j 1, w i is visible to w j<label>(1)</label></formula><p>where w i and w j are the i th and j th tokens in the commonsense-knowledge-enriched sentence, respectively.</p><p>3) Mask-self-attention: Although we conduct relative position embedding to reserve structural information of the original sentence, another problem appears simultaneously: different tokens in the commonsense-knowledge-enriched sentence may share the same relative position embedding. When calculating self-attention score, these two unrelated tokens may obtain a high score because of the same position embedding. To preserve the semantic and visual representations of the original input, and weaken the effect between the injected commonsense knowledge and other unrelated components, we conduct mask-self-attention mechanism via the visible matrix, which could limit the self-attention area effectively. Formally, the mask-self-attention is described by</p><formula xml:id="formula_2">Q t+1 , K t+1 , V t+1 = h t W q , h t W k , h t W v (2) S t+1 = sof tmax Q t+1 K t+1 + (W ? 1) * INF ? d k (3) h t+1 = S t+1 V t+1<label>(4)</label></formula><p>where h t and h t+1 denote the hidden state of the t th and (t+1) th mask-self-attention blocks, W q , W k , W v are trainable model parameters, and Q t+1 , K t+1 , V t+1 denote query, key and value respectively. W is the visible matrix we defined in Eq. 1. INF stands for an infinite number. d k is the scaling factor to counteract the effect of the dot products growing large in magnitude. S t+1 denotes the attention score between query and key. In this way, if w j is invisible to w i , S t+1 ij will approach 0 under the action of visible matrix, which means w j makes no contribution to the hidden state of w i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Pre-trained visual-linguistic BERT</head><p>To extend the powerful pre-trained BERT model to visualand-linguistic tasks, some researchers attempt to design crossmodal pre-training models, which can understand not only the semantic and visual contents, but the alignment and relationship between these two modals. In this paper, we adopt the pre-trained VL-BERT <ref type="bibr" target="#b15">[16]</ref> as the backbone and incorporate external commonsense knowledge into it.</p><p>In VL-BERT, two pre-training tasks are introduced. One is Masked Language Modeling with Visual Clues, which is similar to the Masked Language Modeling task utilized in BERT. The key difference is that visual clues are incorporated for capturing the dependencies among visual and linguistic contents. The model is trained to predict the masked words, based on the unmasked words and visual features. The other is Masked RoI Classification with Linguistic Clues, which is the dual task of the former. And the pre-training task is designed to predict the category label of the masked RoI from the other clues. Those pre-training tasks drive the network to not only model the dependencies in text and visual contents, but also to align the linguistic and visual contents.</p><p>Our KVL-BERT model takes token embedding, segment embedding, position embedding and visual feature embedding as the input into the pre-trained VL-BERT, these embeddings are then fed into a multi-layer Transformer to learn a crossmodal representation between visual regions and textual tokens. The details of the embeddings are as follows.</p><p>1) Token embedding: To encode the whole input text, first we merge the input query and one of the responses into a sentence separated by the special symbol <ref type="bibr">[SEP]</ref>. Each token in this sentence is either a word or an explicit reference to the bounding box. We treat each word as the non-visual element and each explicit reference to the bounding box as the visual element respectively. For the visual elements, a special [IMG] token is assigned for each one of them. Following the standard text preprocessing method of BERT, we tokenize each input text into WordPieces <ref type="bibr" target="#b36">[37]</ref>. The vocabulary is the same as BERT, which contains 30,522 tokens.</p><p>2) Segment embedding and position embedding: The input elements from different sources are separated with three types of segments. For the subtask of Q ? A, question, answer, and RoIs (regions-of-interest) from the input image are separated into three different segments. While for the subtask of QA ? R, question with its correct answer, rationale, and RoIs from the input image are separated into three different segments. For position embedding, we adopt relative position embedding introduced in Section III-C.</p><p>3) Visual feature embedding: The visual feature embedding is a sum of visual appearance feature embedding and visual position feature embedding. The visual appearance feature embedding is extracted by Faster R-CNN <ref type="bibr" target="#b37">[38]</ref>. For each visual element, its visual appearance feature is extracted on its reference bounding box. As for the non-visual element, its visual appearance feature is extracted on the whole input image. Additionally, to embed the position and size of a  D position vector is transformed into high-dimensional (under the same size of visual appearance feature embedding) visual position feature embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset and metrics</head><p>We conduct experiments on the VCR <ref type="bibr" target="#b9">[10]</ref> benchmark, a large-scale visual commonsense reasoning dataset containing over 212k (train set), 26k (validation set) and 25k (test set) questions on over 110k movie scenes. We follow this data partition in all of our experiments.</p><p>The models are evaluated with classification accuracy in three modes: Q ? A (given a question, select the correct answer from four candidate answers), QA ? R (given a question and its correct answer, select the correct rationale from four candidate rationales), and Q ? AR (given a question, select the correct answer first, then choose the correct rationale based on the answer). For the Q ? AR mode, a sample will be treated as correct if and only if the model predicts both correct answer and correct rationale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>Our model adopts pre-trained parameters from the VL-BERT <ref type="bibr" target="#b15">[16]</ref>, which are pre-trained jointly on Conceptual Captions <ref type="bibr" target="#b39">[40]</ref> as visual-linguistic corpus, and BooksCorpus <ref type="bibr" target="#b40">[41]</ref> and English Wikipedia as text-only corpus. The model is trained on the training set, and is evaluated on the validation and test sets. During training, we run our experiments on 4 NVIDIA Tesla V100 GPUs for 18 epochs, with the batch size of 256. The number of commonsense knowledge entities injected for each token is set to 2 (we will discuss it later). We use the SGD optimizer with base learning rate of 5e-3, momentum of 0.9, weight decay of 1e-4. Float16 operations are used to speed up the training process and reduce the usage of memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Quantitative evaluation</head><p>We train and evaluate the models developed from the original BERT BASE and BERT LARGE , where the subscripts "BASE" and "LARGE" are used to distinguish them. We compare our KVL-BERT with the VL-BERT <ref type="bibr" target="#b15">[16]</ref>. As shown in <ref type="table" target="#tab_1">Table I</ref>, our KVL-BERT BASE outperforms VL-BERT BASE on the validation set, and the KVL-BERT LARGE outperforms VL-BERT LARGE on the validation and test sets.</p><p>Compared to other task-specific models such as R2C <ref type="bibr" target="#b9">[10]</ref>, CCN <ref type="bibr" target="#b10">[11]</ref>, HGL <ref type="bibr" target="#b11">[12]</ref>, and existing pre-trained task-agnostic multimodal BERT models such as VisualBERT <ref type="bibr" target="#b13">[14]</ref>, ViL-BERT <ref type="bibr" target="#b14">[15]</ref>, Unicoder-VL <ref type="bibr" target="#b19">[20]</ref> and B2T2 <ref type="bibr" target="#b16">[17]</ref>, our KVL-BERT outperforms these single models (not ensemble ones) by a large margin.</p><p>In addition to the results listed in <ref type="table" target="#tab_1">Table I</ref>, some of the latest models have also achieved competitive results on the VCR task. UNITER LARGE <ref type="bibr" target="#b21">[22]</ref> outperforms our KVL-BERT LARGE because it conducts two-stage pre-training: first pre-trains their model on task-agnostic pre-training datasets, and then pre-trains on the downstream task-specific dataset. VILLA <ref type="bibr" target="#b29">[30]</ref> performs large-scale adversarial training (taskagnostic adversarial pre-training and task-specific adversarial pre-training) based on UNITER <ref type="bibr" target="#b21">[22]</ref>. ERNIE-ViL <ref type="bibr" target="#b28">[29]</ref> adds extra scene graph prediction tasks (object prediction, attribute prediction and relationship prediction) in the pre-training phase, where the scene graph is constructed by parsing the text sentence into object nodes, attribute nodes and relationship nodes. These three models outperform us due to the additional pre-training.</p><p>For the reason that pre-training is computationally expensive and time-consuming, we adopt the same comparison scheme as Unicoder-VL <ref type="bibr" target="#b19">[20]</ref>, comparing our KVL-BERT BASE with the UNITER's one-stage pre-training model developed from the original BERT BASE model. It is denoted as UNITER BASE * in <ref type="table" target="#tab_1">Table I</ref>, whose setting is similar to the our work. We directly use the results of UNITER BASE * published in the UNITER paper <ref type="bibr" target="#b21">[22]</ref>. As shown in <ref type="table" target="#tab_1">Table I</ref>, our KVL-BERT BASE outperforms UNITER BASE * on the subtasks Q ? A and Q ? AR, which strongly confirm the effectiveness of our commonsense knowledge incorporation method.</p><p>Compared to the baseline VL-BERT model which extends pre-trained BERT to the visual-linguistic tasks, our KVL-BERT model outperforms it due to the incorporation of the commonsense knowledge. And we expect that introducing our proposed mechanism of incorporating commonsense knowledge into other pre-trained multi-modal BERT models will also bring improvement. In our future work, we will adopt more pre-training tasks to further improve our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Case studies</head><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we show some examples to illustrate the effectiveness of our approach compared to the baseline model VL-BERT <ref type="bibr" target="#b15">[16]</ref>.</p><p>Example 1 and Example 2 show how our model picks the right answers and rationales when the questions are about "why". Based on the recognition-level perception such as detected objects and attributes, those reason-oriented questions can't be answered correctly. When the model is equipped with external commonsense knowledge, there would be enough clues supporting it to answer and reason the questions. In Example 1, when taking the question along with the first answer A1 as input, the related commonsense knowledge entity "gun" will be incorporated into the model through the token "policeman", so that the representation of the token "policeman" is enriched by the injected token "gun". Then the attention score between the token "policeman" in the answer and the bounding box where "gun" is detected in the question will be high. And when taking the question, the correct answer A1 and the rationale R2 as input, the related entity "policeman" will be incorporated into the model through the token "arrest", the representation of the token "arrest" is enriched by the external knowledge entity "policeman". Then the attention score between the token "arrest" in the rationale and the token "policeman" in the correct answer will be high. With the help of external commonsense knowledge, the model could answer and reason the question correctly. However, VL-BERT could not make the right choice as it is only equipped with the visual and text contexts, which are insufficient to answer and reason questions.</p><p>In Example 3, we show how our model answers the question about the function of the specific object. It is another kind of typical question that needs to be answered with the help of commonsense knowledge.</p><p>And there are also many examples similar to Example 4 that the objects or attributes in the input are ambiguous in the VCR dataset. In these situations, commonsense knowledge could provide extra semantic information to support answering and reasoning the questions.</p><p>In general, with the help of the external commonsense knowledge and our incorporation mechanism, the KVL-BERT could accurately choose the correct answer and rationale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation study</head><p>We perform ablation studies to assess the impact of relative position embedding and mask-self-attention mechanism on the VCR val set with the model developed from the origin BERT BASE .</p><p>As shown in <ref type="table" target="#tab_1">Table II</ref>, "KVL-BERT w/o relative position embedding" refers to conduct absolute position embedding, i.e., after inserting the external knowledge entities, the positions of all tokens in the overall transformed sentence are encoded in absolute sequence. "KVL-BERT w/o mask-selfattention" refers to remove the visible matrix from our model and just conduct self-attention mechanism. We can observed that without performing relative position embedding or maskself-attention mechanism, the performance of the KVL-BERT declines.</p><p>We infer that conducting absolute position embedding for the transformed sentence damages the structure information of the original sentence. And when visible matrix and maskself-attention mechanism are not employed, i.e., all the tokens in the transformed sentence are visible to each other, injected external knowledge entities would bring knowledge noise for other tokens in the original input sentence. Those ablation studies prove the effectiveness of the relative position embedding and mask-self-attention mechanism. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Variants and analysis</head><p>To find the most effective way to incorporate commonsense knowledge into the visual-and-linguistic BERT, we conduct the experiments with two variants of the KVL-BERT. We evaluate these three models on the validation set with the model developed from the original BERT BASE .</p><p>For Variant I, we attempt to extract commonsense knowledge embedding corresponding to each token with transE. Given an input sentence, the model first retrieves the corresponding commonsense knowledge subgraph from Concept-Net for each token. The knowledge subgraph consists of a set of triples. Then the model conducts transE on the knowledge subgraph to get its embedding. Finally, the commonsense knowledge embedding is fed to the pre-trained VL-BERT <ref type="bibr" target="#b15">[16]</ref> along with other embeddings. As shown in <ref type="table" target="#tab_1">Table III</ref>, the accuracy of Variant I is 2.3%, 3.1%, 4.0% lower than the KVL-BERT on the subtasks Q ? A, QA ? R and Q ? AR, respectively.</p><p>Variant II injects relevant entities extracted from Concept-Net into the input sentence in the same way as the KVL-BERT. Differently, we make the injected knowledge entity tokens share the same position embedding with their related token in the original sentence, and it lacks the mechanism of sentence structure and semantic representation reservation. <ref type="figure">Fig. 4</ref>. Experimental results for different numbers of knowledge entities injected for each token on KVL-BERT and Variant II. The scale of the left ordinate is used for the measurement of Q ? A and QA ? R, while the right ordinate is used for Q ? AR.</p><p>As shown in <ref type="figure">Figure 4</ref>, when the commonsense knowledge entities are injected, our KVL-BERT outperforms Variant II on all the subtasks in VCR, which verifies the effectiveness of sentence structure and semantic representation reservation mechanism. Note that these two models are identical when the number of knowledge entities injected for each token equals zero, i.e., there is no commonsense knowledge incorporated into the original sentence. The results listed in <ref type="table" target="#tab_1">Table III</ref> is the best performance of KVL-BERT and Variant II.</p><p>In addition, as shown in <ref type="figure">Figure 4</ref>, the accuracy rate of Variant II generally decreases as the number of knowledge entities injected for each token increases. On the contrary, this issue does not appear in our KVL-BERT model, which credits to sentence structure and semantic representation reservation mechanism. Note that the KVL-BERT achieves the best performance when the number of commonsense knowledge entities injected for each token equals 2. When it increases to 3, the classification accuracy decreases, we infer that some knowledge noise is incorporated in this situation.</p><p>V. CONCLUSION In this paper, we propose a novel KVL-BERT model to incorporate commonsense knowledge into the visual-andlinguistic BERT, which can improve the cognition-level visual understanding and reasoning abilities. Besides taking visual and linguistic contents as input, external commonsense knowledge extracted from ConceptNet is integrated into the multi-layer Transformer. In order to reserve the structural information and semantic representation of the original sentence, we propose conducting relative position embedding and mask-self-attention to weaken the effect between the injected commonsense knowledge and other unrelated components in the input sequence. In addition, to find the most effective way to integrate commonsense knowledge, we design and evaluate two variants of the KVL-BERT. When applying on the visual commonsense reasoning task, compared to other task-specific models and general task-agnostic pre-training models, our KVL-BERT outperforms them by a large margin. We will apply our KVL-BERT model to more tasks of visual sense analysis and interpretation for future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An illustrative example from the VCR benchmark (shown on the left). With the support of external commonsense knowledge (shown on the right), the question can be answered and reasoned more accurately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of KVL-BERT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Examples of Q ? A and QA ? R tasks from the VCR val set. The correct answer and rationale for each example is marked in bold. The answers picked by our KVL-BERT and baseline model VL-BERT are indicated in parenthesis. The tokens in red are the commonsense knowledge as the clue to answer and reason the question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>bounding box, each RoI is represented by a vector composed of normalized top-left and bottom-right coordinates as x LT RB , y RB ) denote the coordinate of the top-left and bottom-right corner, while H and W denote the height and width of the input image, respectively. Then, adopting the method in [39], the 4-</figDesc><table /><note>W , y LTH , x RBW , y RBH , where (x LT , y LT ) and (x</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I EXPERIMENTAL</head><label>I</label><figDesc>RESULTS OF OUR KVL-BERT MODEL COMPARED WITH OTHER SINGLE MODELS.</figDesc><table><row><cell></cell><cell cols="2">Q ? A</cell><cell cols="2">QA ? R</cell><cell cols="2">Q ? AR</cell></row><row><cell>Model</cell><cell>val</cell><cell>test</cell><cell>val</cell><cell>test</cell><cell>val</cell><cell>test</cell></row><row><cell>R2C [10]</cell><cell cols="6">63.8 65.1 67.2 67.3 43.1 44.0</cell></row><row><cell>CCN [11]</cell><cell cols="6">67.4 68.5 70.6 70.5 47.7 48.4</cell></row><row><cell>HGL [12]</cell><cell cols="6">69.4 70.1 70.6 70.8 49.1 49.8</cell></row><row><cell>VisualBERT [14]</cell><cell cols="6">70.8 71.6 73.2 73.2 52.2 52.4</cell></row><row><cell>ViLBERT [15]</cell><cell cols="6">72.4 73.3 74.5 74.6 54.0 54.8</cell></row><row><cell>Unicoder-VL [20]</cell><cell cols="6">72.6 73.4 74.5 74.4 54.5 54.9</cell></row><row><cell>B2T2 [17]</cell><cell cols="6">71.9 72.6 76.0 75.7 54.9 55.0</cell></row><row><cell>UNITER BASE * [22]</cell><cell>72.8</cell><cell>-</cell><cell>75.3</cell><cell>-</cell><cell>54.9</cell><cell>-</cell></row><row><cell>VL-BERT BASE [16]</cell><cell>73.8</cell><cell>-</cell><cell>74.4</cell><cell>-</cell><cell>55.2</cell><cell>-</cell></row><row><cell>KVL-BERT BASE (ours)</cell><cell>74.0</cell><cell>-</cell><cell>75.1</cell><cell>-</cell><cell>55.6</cell><cell>-</cell></row><row><cell>VL-BERT LARGE [16]</cell><cell cols="6">75.5 75.8 77.9 78.4 58.9 59.7</cell></row><row><cell>KVL-BERT LARGE (ours)</cell><cell cols="6">76.3 76.4 78.6 78.6 60.0 60.3</cell></row><row><cell></cell><cell cols="2">TABLE II</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">EXPERIMENTAL RESULTS OF ABLATION STUDIES.</cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell cols="3">Q ? A QA ? R</cell><cell>Q ? AR</cell></row><row><cell cols="3">KVL-BERT w/o relative position embedding</cell><cell>73.7</cell><cell>74.6</cell><cell></cell><cell>55.0</cell></row><row><cell cols="2">KVL-BERT w/o mask-self-attention</cell><cell></cell><cell>73.3</cell><cell>74.0</cell><cell></cell><cell>54.2</cell></row><row><cell>KVL-BERT</cell><cell></cell><cell></cell><cell>74.0</cell><cell>75.1</cell><cell></cell><cell>55.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III EXPERIMENTAL</head><label>III</label><figDesc>RESULTS OF KVL-BERT AND ITS TWO VARIANTS.</figDesc><table><row><cell>Model</cell><cell cols="2">Q ? A QA ? R</cell><cell>Q ? AR</cell></row><row><cell>Variant I</cell><cell>71.7</cell><cell>72.0</cell><cell>51.6</cell></row><row><cell>Variant II</cell><cell>73.1</cell><cell>74.6</cell><cell>54.5</cell></row><row><cell>KVL-BERT</cell><cell>74.0</cell><cell>75.1</cell><cell>55.6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards diverse and natural image descriptions via a conditional gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2970" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving image captioning with conditional generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8142" to="8150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast, diverse and accurate image captioning guided by part-of-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10695" to="10704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adversarial semantic alignment for improved image captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dognin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Melnyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10463" to="10471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4125" to="4134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning visual knowledge memory networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7736" to="7745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Murel: Multimodal relational reasoning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic fusion with intra-and inter-modality attention flow for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6639" to="6648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Explicit bias discovery in visual question answering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9562" to="9571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6720" to="6731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Connective cognition network for directional visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5670" to="5680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Heterogeneous graph learning for visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2765" to="2775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reitter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05054</idno>
		<title level="m">Fusion of detected objects in text for visual question answering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07606</idno>
		<title level="m">Enabling language representation with knowledge graph</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06066</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11740</idno>
		<title level="m">Uniter: Learning universal image-text representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Imagenet classification with deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for largescale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberta</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16934</idno>
		<title level="m">Ernievil: Knowledge enhanced vision-language representations through scene graph</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06195</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Are elephants bigger than butterflies? reasoning about sizes of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bagherinezhad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large scale semi-supervised object detection using visual and semantic knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dellandr?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2119" to="2128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge diffusion for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1489" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00610</idno>
		<title level="m">Dykgchat: Benchmarking dialogue generation grounding on dynamic knowledge graphs</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07858</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Explicit utilization of general knowledge in machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03449</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
