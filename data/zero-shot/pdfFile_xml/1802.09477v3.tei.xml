<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Addressing Function Approximation Error in Actor-Critic Methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Fujimoto</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herke</forename><surname>Van Hoof</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Meger</surname></persName>
						</author>
						<title level="a" type="main">Addressing Function Approximation Error in Actor-Critic Methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In reinforcement learning problems with discrete action spaces, the issue of value overestimation as a result of function approximation errors is well-studied. However, similar issues with actor-critic methods in continuous control domains have been largely left untouched. In this paper, we show overestimation bias and the accumulation of error in temporal difference methods are present in an actor-critic setting. Our proposed method addresses these issues, and greatly outperforms the current state of the art.</p><p>Overestimation bias is a property of Q-learning in which the maximization of a noisy value estimate induces a consistent overestimation <ref type="bibr" target="#b36">(Thrun &amp; Schwartz, 1993)</ref>. In a function approximation setting, this noise is unavoidable given the imprecision of the estimator. This inaccuracy is further exaggerated by the nature of temporal difference learning <ref type="bibr" target="#b34">(Sutton, 1988)</ref>, in which an estimate of the value function is updated using the estimate of a subsequent state. This Proceedings of the 35 th International Conference on Machine <ref type="bibr">Learning, Stockholm, Sweden, PMLR 80, 2018</ref><ref type="bibr">. Copyright 2018</ref> by the author(s). means using an imprecise estimate within each update will lead to an accumulation of error. Due to overestimation bias, this accumulated error can cause arbitrarily bad states to be estimated as high value, resulting in suboptimal policy updates and divergent behavior. This paper begins by establishing this overestimation property is also present for deterministic policy gradients <ref type="bibr" target="#b32">(Silver et al., 2014)</ref>, in the continuous control setting. Furthermore, we find the ubiquitous solution in the discrete action setting, Double <ref type="bibr">DQN (Van Hasselt et al., 2016)</ref>, to be ineffective in an actor-critic setting. During training, Double DQN estimates the value of the current policy with a separate target value function, allowing actions to be evaluated without maximization bias. Unfortunately, due to the slow-changing policy in an actor-critic setting, the current and target value estimates remain too similar to avoid maximization bias. This can be dealt with by adapting an older variant, Double Q-learning <ref type="bibr" target="#b39">(Van Hasselt, 2010)</ref>, to an actor-critic format by using a pair of independently trained critics. While this allows for a less biased value estimation, even an unbiased estimate with high variance can still lead to future overestimations in local regions of state space, which in turn can negatively affect the global policy. To address this concern, we propose a clipped Double Q-learning variant which leverages the notion that a value estimate suffering from overestimation bias can be used as an approximate upper-bound to the true value estimate. This favors underestimations, which do not tend to be propagated during learning, as actions with low value estimates are avoided by the policy.</p><p>Given the connection of noise to overestimation bias, this paper contains a number of components that address variance reduction. First, we show that target networks, a common approach in deep Q-learning methods, are critical for variance reduction by reducing the accumulation of errors. Second, to address the coupling of value and policy, we propose delaying policy updates until the value estimate has converged. Finally, we introduce a novel regularization strategy, where a SARSA-style update bootstraps similar action estimates to further reduce variance.</p><p>Our modifications are applied to the state of the art actorcritic method for continuous control, Deep Deterministic Policy Gradient algorithm (DDPG) <ref type="bibr" target="#b16">(Lillicrap et al., 2015)</ref>, to form the Twin Delayed Deep Deterministic policy gradient arXiv:1802.09477v3 [cs.AI] 22 Oct 2018 algorithm (TD3), an actor-critic algorithm which considers the interplay between function approximation error in both policy and value updates. We evaluate our algorithm on seven continuous control domains from OpenAI gym <ref type="bibr" target="#b5">(Brockman et al., 2016)</ref>, where we outperform the state of the art by a wide margin.</p><p>Given the recent concerns in reproducibility <ref type="bibr" target="#b11">(Henderson et al., 2017)</ref>, we run our experiments across a large number of seeds with fair evaluation metrics, perform ablation studies across each contribution, and open source both our code and learning curves (https://github.com/ sfujim/TD3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Function approximation error and its effect on bias and variance in reinforcement learning algorithms have been studied in prior works <ref type="bibr" target="#b25">(Pendrith et al., 1997;</ref><ref type="bibr" target="#b19">Mannor et al., 2007)</ref>. Our work focuses on two outcomes that occur as the result of estimation error, namely overestimation bias and a high variance build-up.</p><p>Several approaches exist to reduce the effects of overestimation bias due to function approximation and policy optimization in Q-learning. Double Q-learning uses two independent estimators to make unbiased value estimates <ref type="bibr" target="#b39">(Van Hasselt, 2010;</ref><ref type="bibr" target="#b39">Van Hasselt et al., 2016)</ref>. Other approaches have focused directly on reducing the variance <ref type="bibr" target="#b0">(Anschel et al., 2017)</ref>, minimizing over-fitting to early high variance estimates <ref type="bibr" target="#b8">(Fox et al., 2016)</ref>, or through corrective terms <ref type="bibr" target="#b15">(Lee et al., 2013)</ref>. Further, the variance of the value estimate has been considered directly for risk-aversion <ref type="bibr" target="#b18">Tsitsiklis, 2011) and</ref><ref type="bibr">exploration (O'Donoghue et al., 2017)</ref>, but without connection to overestimation bias.</p><p>The concern of variance due to the accumulation of error in temporal difference learning has been largely dealt with by either minimizing the size of errors at each time step or mixing off-policy and Monte-Carlo returns. Our work shows the importance of a standard technique, target networks, for the reduction of per-update error, and develops a regularization technique for the variance reduction by averaging over value estimates. Concurrently, <ref type="bibr" target="#b23">Nachum et al. (2018)</ref> showed smoothed value functions could be used to train stochastic policies with reduced variance and improved performance. Methods with multi-step returns offer a trade-off between accumulated estimation bias and variance induced by the policy and the environment. These methods have been shown to be an effective approach, through importance sampling <ref type="bibr" target="#b28">(Precup et al., 2001;</ref><ref type="bibr" target="#b22">Munos et al., 2016)</ref>, distributed methods <ref type="bibr" target="#b21">(Mnih et al., 2016;</ref><ref type="bibr" target="#b7">Espeholt et al., 2018)</ref>, and approximate bounds <ref type="bibr" target="#b10">(He et al., 2016)</ref>. However, rather than provide a direct solution to the accumulation of error, these methods circumvent the problem by considering a longer horizon. Another approach is a reduction in the discount factor <ref type="bibr" target="#b26">(Petrik &amp; Scherrer, 2009)</ref>, reducing the contribution of each error.</p><p>Our method builds on the Deterministic Policy Gradient algorithm (DPG) <ref type="bibr" target="#b32">(Silver et al., 2014)</ref>, an actor-critic method which uses a learned value estimate to train a deterministic policy. An extension of DPG to deep reinforcement learning, DDPG <ref type="bibr" target="#b16">(Lillicrap et al., 2015)</ref>, has shown to produce state of the art results with an efficient number of iterations. Orthogonal to our approach, recent improvements to DDPG include distributed methods <ref type="bibr" target="#b27">(Popov et al., 2017)</ref>, along with multi-step returns and prioritized experience replay <ref type="bibr" target="#b29">(Schaul et al., 2016;</ref><ref type="bibr" target="#b12">Horgan et al., 2018)</ref>, and distributional methods <ref type="bibr" target="#b2">(Bellemare et al., 2017;</ref><ref type="bibr" target="#b1">Barth-Maron et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>Reinforcement learning considers the paradigm of an agent interacting with its environment with the aim of learning reward-maximizing behavior. At each discrete time step t, with a given state s ? S, the agent selects actions a ? A with respect to its policy ? : S ? A, receiving a reward r and the new state of the environment s . The return is defined as the discounted sum of rewards</p><formula xml:id="formula_0">R t = T i=t ? i?t r(s i , a i ),</formula><p>where ? is a discount factor determining the priority of short-term rewards.</p><p>In reinforcement learning, the objective is to find the optimal policy ? ? , with parameters ?, which maximizes the expected return J(?) = E si?p?,ai?? [R 0 ]. For continuous control, parametrized policies ? ? can be updated by taking the gradient of the expected return ? ? J(?). In actor-critic methods, the policy, known as the actor, can be updated through the deterministic policy gradient algorithm <ref type="bibr" target="#b32">(Silver et al., 2014)</ref>:</p><formula xml:id="formula_1">? ? J(?) = E s?p? ? a Q ? (s, a)| a=?(s) ? ? ? ? (s) . (1)</formula><p>Q ? (s, a) = E si?p?,ai?? [R t |s, a], the expected return when performing action a in state s and following ? after, is known as the critic or the value function.</p><p>In Q-learning, the value function can be learned using temporal difference learning <ref type="bibr" target="#b34">(Sutton, 1988;</ref><ref type="bibr" target="#b41">Watkins, 1989)</ref>, an update rule based on the Bellman equation <ref type="bibr" target="#b3">(Bellman, 1957)</ref>. The Bellman equation is a fundamental relationship between the value of a state-action pair (s, a) and the value of the subsequent state-action pair (s , a ):</p><formula xml:id="formula_2">Q ? (s, a) = r + ?E s ,a [Q ? (s , a )] , a ? ?(s ). (2)</formula><p>For a large state space, the value can be estimated with a differentiable function approximator Q ? (s, a), with parameters ?. In deep Q-learning <ref type="bibr" target="#b20">(Mnih et al., 2015)</ref>, the network is updated by using temporal difference learning with a secondary frozen target network Q ? (s, a) to maintain a fixed objective y over multiple updates:</p><formula xml:id="formula_3">y = r + ?Q ? (s , a ), a ? ? ? (s ),<label>(3)</label></formula><p>where the actions are selected from a target actor network ? ? . The weights of a target network are either updated periodically to exactly match the weights of the current network, or by some proportion ? at each time step ? ? ? ? + (1 ? ? )? . This update can be applied in an off-policy fashion, sampling random mini-batches of transitions from an experience replay buffer <ref type="bibr" target="#b17">(Lin, 1992)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Overestimation Bias</head><p>In Q-learning with discrete actions, the value estimate is updated with a greedy target y = r + ? max a Q(s , a ), however, if the target is susceptible to error , then the maximum over the value along with its error will generally be greater than the true maximum, E [max a (Q(s , a ) + )] ? max a Q(s , a ) <ref type="bibr" target="#b36">(Thrun &amp; Schwartz, 1993)</ref>. As a result, even initially zero-mean error can cause value updates to result in a consistent overestimation bias, which is then propagated through the Bellman equation. This is problematic as errors induced by function approximation are unavoidable.</p><p>While in the discrete action setting overestimation bias is an obvious artifact from the analytical maximization, the presence and effects of overestimation bias is less clear in an actor-critic setting where the policy is updated via gradient descent. We begin by proving that the value estimate in deterministic policy gradients will be an overestimation under some basic assumptions in Section 4.1 and then propose a clipped variant of Double Q-learning in an actor-critic setting to reduce overestimation bias in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overestimation Bias in Actor-Critic</head><p>In actor-critic methods the policy is updated with respect to the value estimates of an approximate critic. In this section we assume the policy is updated using the deterministic policy gradient, and show that the update induces overestimation in the value estimate. Given current policy parameters ?, let ? approx define the parameters from the actor update induced by the maximization of the approximate critic Q ? (s, a) and ? true the parameters from the hypothetical actor update with respect to the true underlying value function Q ? (s, a) (which is not known during learning):</p><formula xml:id="formula_4">? approx = ? + ? Z 1 E s?p? ? ? ? ? (s)? a Q ? (s, a)| a=? ? (s) ? true = ? + ? Z 2 E s?p? ? ? ? ? (s)? a Q ? (s, a)| a=? ? (s) ,<label>(4)</label></formula><p>where we assume Z 1 and Z 2 are chosen to normalize the gradient, i.e., such that Z ?1 ||E[?]|| = 1. Without normalized gradients, overestimation bias is still guaranteed to occur with slightly stricter conditions. We examine this case further in the supplementary material. We denote ? approx and ? true as the policy with parameters ? approx and ? true respectively.</p><p>As the gradient direction is a local maximizer, there exists 1 sufficiently small such that if ? ? 1 then the approximate value of ? approx will be bounded below by the approximate value of ? true :</p><formula xml:id="formula_5">E [Q ? (s, ? approx (s))] ? E [Q ? (s, ? true (s))] .<label>(5)</label></formula><p>Conversely, there exists 2 sufficiently small such that if ? ? 2 then the true value of ? approx will be bounded above by the true value of ? true :</p><formula xml:id="formula_6">E [Q ? (s, ? true (s))] ? E [Q ? (s, ? approx (s))] .<label>(6)</label></formula><p>If in expectation the value estimate is at least as large as the true value with respect to ? true , E [Q ? (s, ? true (s))] ? E [Q ? (s, ? true (s))], then Equations <ref type="formula" target="#formula_5">(5)</ref> and <ref type="formula" target="#formula_6">(6)</ref> imply that if ? &lt; min( 1 , 2 ), then the value estimate will be overestimated:</p><formula xml:id="formula_7">E [Q ? (s, ? approx (s))] ? E [Q ? (s, ? approx (s))] .<label>(7)</label></formula><p>Although this overestimation may be minimal with each update, the presence of error raises two concerns. Firstly, the overestimation may develop into a more significant bias over many updates if left unchecked. Secondly, an inaccurate value estimate may lead to poor policy updates. This is particularly problematic because a feedback loop is created, in which suboptimal actions might be highly rated by the suboptimal critic, reinforcing the suboptimal action in the next policy update.</p><p>Does this theoretical overestimation occur in practice for state-of-the-art methods? We answer this question by plotting the value estimate of DDPG <ref type="bibr" target="#b16">(Lillicrap et al., 2015)</ref> over time while it learns on the OpenAI gym environments Hopper-v1 and Walker2d-v1 <ref type="bibr" target="#b5">(Brockman et al., 2016)</ref>. In <ref type="figure" target="#fig_0">Figure 1</ref>, we graph the average value estimate over 10000 states and compare it to an estimate of the true value. The true value is estimated using the average discounted return over 1000 episodes following the current policy, starting from states sampled from the replay buffer. A very clear overestimation bias occurs from the learning procedure, which contrasts with the novel method that we describe in the following section, Clipped Double Q-learning, which greatly reduces overestimation by the critic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Clipped Double Q-Learning for Actor-Critic</head><p>While several approaches to reducing overestimation bias have been proposed, we find them ineffective in an actorcritic setting. This section introduces a novel clipped variant of Double Q-learning (Van Hasselt, 2010), which can replace the critic in any actor-critic method.</p><p>In Double Q-learning, the greedy update is disentangled from the value function by maintaining two separate value estimates, each of which is used to update the other. If the value estimates are independent, they can be used to make unbiased estimates of the actions selected using the opposite value estimate. In Double DQN <ref type="bibr" target="#b39">(Van Hasselt et al., 2016)</ref>, the authors propose using the target network as one of the value estimates, and obtain a policy by greedy maximization of the current value network rather than the target network.</p><p>In an actor-critic setting, an analogous update uses the current policy rather than the target policy in the learning target:</p><formula xml:id="formula_8">y = r + ?Q ? (s , ? ? (s )).<label>(8)</label></formula><p>In practice however, we found that with the slow-changing policy in actor-critic, the current and target networks were too similar to make an independent estimation, and offered little improvement. Instead, the original Double Q-learning formulation can be used, with a pair of actors (? ?1 , ? ?2 ) and critics (Q ?1 , Q ?2 ), where ? ?1 is optimized with respect to Q ?1 and ? ?2 with respect to Q ?2 :</p><formula xml:id="formula_9">y 1 = r + ?Q ? 2 (s , ? ?1 (s )) y 2 = r + ?Q ? 1 (s , ? ?2 (s )).<label>(9)</label></formula><p>We measure the overestimation bias in <ref type="figure" target="#fig_1">Figure 2</ref>, which demonstrates that the actor-critic Double DQN suffers from a similar overestimation as DDPG (as shown in <ref type="figure" target="#fig_0">Figure 1</ref>). While Double Q-learning is more effective, it does not entirely eliminate the overestimation. We further show this reduction is not sufficient experimentally in Section 6.1.</p><p>As ? ?1 optimizes with respect to Q ?1 , using an independent estimate in the target update of Q ?1 would avoid the bias introduced by the policy update. However the critics are not entirely independent, due to the use of the opposite critic in the learning targets, as well as the same replay buffer. As a result, for some states s we will have Q ?2 (s, ? ?1 (s)) &gt; Q ?1 (s, ? ?1 (s)). This is problematic because Q ?1 (s, ? ?1 (s)) will generally overestimate the true value, and in certain areas of the state space the overestimation will be further exaggerated. To address this problem, we propose to simply upper-bound the less biased value estimate Q ?2 by the biased estimate Q ?1 . This results in taking the minimum between the two estimates, to give the target update of our Clipped Double Q-learning algorithm:</p><formula xml:id="formula_10">y 1 = r + ? min i=1,2 Q ? i (s , ? ?1 (s )).<label>(10)</label></formula><p>With Clipped Double Q-learning, the value target cannot introduce any additional overestimation over using the standard Q-learning target. While this update rule may induce an underestimation bias, this is far preferable to overestimation bias, as unlike overestimated actions, the value of underestimated actions will not be explicitly propagated through the policy update.</p><p>In implementation, computational costs can be reduced by using a single actor optimized with respect to Q ?1 . We then use the same target y 2 = y 1 for Q ?2 . If Q ?2 &gt; Q ?1 then the update is identical to the standard update and induces no additional bias. If Q ?2 &lt; Q ?1 , this suggests overestimation has occurred and the value is reduced similar to Double Qlearning. A proof of convergence in the finite MDP setting follows from this intuition. We provide formal details and justification in the supplementary material.</p><p>A secondary benefit is that by treating the function approximation error as a random variable we can see that the minimum operator should provide higher value to states with lower variance estimation error, as the expected minimum of a set of random variables decreases as the variance of the random variables increases. This effect means that the minimization in Equation (10) will lead to a preference for states with low-variance value estimates, leading to safer policy updates with stable learning targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Addressing Variance</head><p>While Section 4 deals with the contribution of variance to overestimation bias, we also argue that variance itself should be directly addressed. Besides the impact on overestimation bias, high variance estimates provide a noisy gradient for the policy update. This is known to reduce learning speed <ref type="bibr" target="#b35">(Sutton &amp; Barto, 1998)</ref> as well as hurt performance in practice.</p><p>In this section we emphasize the importance of minimizing error at each update, build the connection between target networks and estimation error and propose modifications to the learning procedure of actor-critic for variance reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Accumulating Error</head><p>Due to the temporal difference update, where an estimate of the value function is built from an estimate of a subsequent state, there is a build up of error. While it is reasonable to expect small error for an individual update, these estimation errors can accumulate, resulting in the potential for large overestimation bias and suboptimal policy updates. This is exacerbated in a function approximation setting where the Bellman equation is never exactly satisfied, and each update leaves some amount of residual TD-error ?(s, a):</p><formula xml:id="formula_11">Q ? (s, a) = r + ?E[Q ? (s , a )] ? ?(s, a).<label>(11)</label></formula><p>It can then be shown that rather than learning an estimate of the expected return, the value estimate approximates the expected return minus the expected discounted sum of future TD-errors:</p><formula xml:id="formula_12">Q ? (s t , a t ) = r t + ?E[Q ? (s t+1 , a t+1 )] ? ? t = r t + ?E [r t+1 + ?E [Q ? (s t+2 , a t+2 ) ? ? t+1 ]] ? ? t = E si?p?,ai?? T i=t ? i?t (r i ? ? i ) .<label>(12)</label></formula><p>If the value estimate is a function of future reward and estimation error, it follows that the variance of the estimate will be proportional to the variance of future reward and estimation error. Given a large discount factor ?, the variance can grow rapidly with each update if the error from each update is not tamed. Furthermore each gradient update only reduces error with respect to a small mini-batch which gives no guarantees about the size of errors in value estimates outside the mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Target Networks and Delayed Policy Updates</head><p>In this section we examine the relationship between target networks and function approximation error, and show the use of a stable target reduces the growth of error. This insight allows us to consider the interplay between high variance estimates and policy performance, when designing reinforcement learning algorithms.</p><p>Target networks are a well-known tool to achieve stability in deep reinforcement learning. As deep function approximators require multiple gradient updates to converge, target networks provide a stable objective in the learning  procedure, and allow a greater coverage of the training data. Without a fixed target, each update may leave residual error which will begin to accumulate. While the accumulation of error can be detrimental in itself, when paired with a policy maximizing over the value estimate, it can result in wildly divergent values.</p><p>To provide some intuition, we examine the learning behavior with and without target networks on both the critic and actor in <ref type="figure" target="#fig_3">Figure 3</ref>, where we graph the value, in a similar manner to <ref type="figure" target="#fig_0">Figure 1</ref>, in the Hopper-v1 environment. In (a) we compare the behavior with a fixed policy and in (b) we examine the value estimates with a policy that continues to learn, trained with the current value estimate. The target networks use a slow-moving update rate, parametrized by ? .</p><p>While updating the value estimate without target networks (? = 1) increases the volatility, all update rates result in similar convergent behaviors when considering a fixed policy. However, when the policy is trained with the current value estimate, the use of fast-updating target networks results in highly divergent behavior.</p><p>When do actor-critic methods fail to learn? These results suggest that the divergence that occurs without target networks is the result of policy updates with a high variance value estimate. <ref type="figure" target="#fig_3">Figure 3</ref>, as well as Section 4, suggest failure can occur due to the interplay between the actor and critic updates. Value estimates diverge through overestimation when the policy is poor, and the policy will become poor if the value estimate itself is inaccurate.</p><p>If target networks can be used to reduce the error over multiple updates, and policy updates on high-error states cause divergent behavior, then the policy network should be updated at a lower frequency than the value network, to first minimize error before introducing a policy update. We propose delaying policy updates until the value error is as small as possible. The modification is to only update the policy and target networks after a fixed number of updates d to the critic. To ensure the TD-error remains small, we update the target networks slowly ? ? ? ? + (1 ? ? )? .</p><p>By sufficiently delaying the policy updates we limit the likelihood of repeating updates with respect to an unchanged critic. The less frequent policy updates that do occur will use a value estimate with lower variance, and in principle, should result in higher quality policy updates. This creates a two-timescale algorithm, as often required for convergence in the linear setting <ref type="bibr" target="#b14">(Konda &amp; Tsitsiklis, 2003)</ref>. The effectiveness of this strategy is captured by our empirical results presented in Section 6.1, which show an improvement in performance while using fewer policy updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Target Policy Smoothing Regularization</head><p>A concern with deterministic policies is they can overfit to narrow peaks in the value estimate. When updating the critic, a learning target using a deterministic policy is highly susceptible to inaccuracies induced by function approximation error, increasing the variance of the target. This induced variance can be reduced through regularization. We introduce a regularization strategy for deep value learning, target policy smoothing, which mimics the learning update from SARSA <ref type="bibr" target="#b35">(Sutton &amp; Barto, 1998)</ref>. Our approach enforces the notion that similar actions should have similar value. While the function approximation does this implicitly, the relationship between similar actions can be forced explicitly by modifying the training procedure. We propose that fitting the value of a small area around the target action</p><formula xml:id="formula_13">y = r + E [Q ? (s , ? ? (s ) + )] ,<label>(13)</label></formula><p>would have the benefit of smoothing the value estimate by bootstrapping off of similar state-action value estimates. In practice, we can approximate this expectation over actions by adding a small amount of random noise to the target policy and averaging over mini-batches. This makes our modified target update:</p><formula xml:id="formula_14">y = r + ?Q ? (s , ? ? (s ) + ), ? clip(N (0, ?), ?c, c),<label>(14)</label></formula><p>where the added noise is clipped to keep the target close to the original action. The outcome is an algorithm reminiscent of Expected SARSA <ref type="bibr" target="#b40">(Van Seijen et al., 2009)</ref>, where the value estimate is instead learned off-policy and the noise added to the target policy is chosen independently of the exploration policy. The value estimate learned is with respect to a noisy policy defined by the parameter ?.</p><p>Intuitively, it is known that policies derived from SARSA value estimates tend to be safer, as they provide higher value to actions resistant to perturbations. Thus, this style of update can additionally lead to improvement in stochastic domains with failure cases. A similar idea was introduced concurrently by <ref type="bibr" target="#b23">Nachum et al. (2018)</ref>, smoothing over Q ? , rather than Q ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 TD3</head><p>Initialize critic networks Q ?1 , Q ?2 , and actor network ? ? with random parameters ? 1 , ? 2 , ? Initialize target networks ? 1 ? ? 1 , ? 2 ? ? 2 , ? ? ? Initialize replay buffer B for t = 1 to T do Select action with exploration noise a ? ? ? (s) + , ? N (0, ?) and observe reward r and new state s <ref type="figure">Store transition tuple (s, a, r, s )</ref>  <ref type="figure">N transitions (s, a, r, s )</ref> </p><formula xml:id="formula_15">in B Sample mini-batch of</formula><formula xml:id="formula_16">from B a ? ? ? (s ) + , ? clip(N (0,?), ?c, c) y ? r + ? min i=1,2 Q ? i (s ,?) Update critics ? i ? argmin ?i N ?1 (y?Q ?i (s, a)) 2 if t mod d then</formula><p>Update ? by the deterministic policy gradient: </p><formula xml:id="formula_17">? ? J(?) = N ?1 ? a Q ?1 (s, a)| a=? ? (s) ? ? ? ? (s) Update target networks: ? i ? ? ? i + (1 ? ? )? i ? ? ? ? + (1 ? ? )? end if end for (a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We present the Twin Delayed Deep Deterministic policy gradient algorithm (TD3), which builds on the Deep Deterministic Policy Gradient algorithm (DDPG) <ref type="bibr" target="#b16">(Lillicrap et al., 2015)</ref> by applying the modifications described in Sections 4.2, 5.2 and 5.3 to increase the stability and performance with consideration of function approximation error. TD3 maintains a pair of critics along with a single actor. For each time step, we update the pair of critics towards the minimum target value of actions selected by the target policy:</p><formula xml:id="formula_18">y = r + ? min i=1,2 Q ? i (s , ? ? (s ) + ),</formula><p>? clip <ref type="figure">(N (0, ?)</ref>, ?c, c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(15)</head><p>Every d iterations, the policy is updated with respect to Q ?1 following the deterministic policy gradient algorithm <ref type="bibr" target="#b32">(Silver et al., 2014)</ref>. TD3 is summarized in Algorithm 1.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Evaluation</head><p>To evaluate our algorithm, we measure its performance on the suite of MuJoCo continuous control tasks <ref type="bibr" target="#b37">(Todorov et al., 2012)</ref>, interfaced through OpenAI Gym <ref type="bibr" target="#b5">(Brockman et al., 2016)</ref>  <ref type="figure" target="#fig_4">(Figure 4</ref>). To allow for reproducible comparison, we use the original set of tasks from <ref type="bibr" target="#b5">Brockman et al. (2016)</ref> with no modifications to the environment or reward.</p><p>For our implementation of DDPG <ref type="bibr" target="#b16">(Lillicrap et al., 2015)</ref>, we use a two layer feedforward neural network of 400 and 300 hidden nodes respectively, with rectified linear units (ReLU) between each layer for both the actor and critic, and a final tanh unit following the output of the actor. Unlike the original DDPG, the critic receives both the state and action as input to the first layer. Both network parameters are updated using Adam <ref type="bibr" target="#b13">(Kingma &amp; Ba, 2014)</ref> with a learning rate of 10 ?3 . After each time step, the networks are trained with a mini-batch of a 100 transitions, sampled uniformly from a replay buffer containing the entire history of the agent.</p><p>The target policy smoothing is implemented by adding ? N (0, 0.2) to the actions chosen by the target actor network, clipped to (?0.5, 0.5), delayed policy updates consists of only updating the actor and target critic network every d iterations, with d = 2. While a larger d would result in a larger benefit with respect to accumulating errors, for fair comparison, the critics are only trained once per time step, and training the actor for too few iterations would cripple learning. Both target networks are updated with ? = 0.005.</p><p>To remove the dependency on the initial parameters of the policy we use a purely exploratory policy for the first 10000 time steps of stable length environments (HalfCheetah-v1 and Ant-v1) and the first 1000 time steps for the remaining environments. Afterwards, we use an off-policy exploration strategy, adding Gaussian noise N (0, 0.1) to each action. Unlike the original implementation of DDPG, we used uncorrelated noise for exploration as we found noise drawn from the Ornstein-Uhlenbeck <ref type="bibr" target="#b38">(Uhlenbeck &amp; Ornstein, 1930)</ref> process offered no performance benefits.</p><p>Each task is run for 1 million time steps with evaluations every 5000 time steps, where each evaluation reports the average reward over 10 episodes with no exploration noise. Our results are reported over 10 random seeds of the Gym simulator and the network initialization.</p><p>We compare our algorithm against DDPG <ref type="bibr" target="#b16">(Lillicrap et al., 2015)</ref> as well as the state of art policy gradient algorithms: PPO , ACKTR  and TRPO <ref type="bibr" target="#b30">(Schulman et al., 2015)</ref>, as implemented by OpenAI's baselines repository , and SAC <ref type="bibr" target="#b9">(Haarnoja et al., 2018)</ref>, as implemented by the author's GitHub 1 . Additionally, we compare our method with our re-tuned version of DDPG, which includes all architecture and hyper-parameter modifications to DDPG without any of our proposed adjustments. A full comparison between our re-tuned version and the baselines DDPG is provided in the supplementary material.</p><p>Our results are presented in <ref type="table" target="#tab_0">Table 1</ref> and learning curves in <ref type="figure" target="#fig_6">Figure 5</ref>. TD3 matches or outperforms all other algorithms in both final performance and learning speed across all tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablation Studies</head><p>We perform ablation studies to understand the contribution of each individual component: Clipped Double Q-learning (Section 4.2), delayed policy updates (Section 5.2) and target policy smoothing (Section 5.3). We present our results in <ref type="table" target="#tab_1">Table 2</ref> in which we compare the performance of removing each component from TD3 along with our modifications to the architecture and hyper-parameters. Additional learning curves can be found in the supplementary material.</p><p>The significance of each component varies task to task. While the addition of only a single component causes insignificant improvement in most cases, the addition of combinations performs at a much higher level. The full algorithm outperforms every other combination in most tasks. Although the actor is trained for only half the number of iterations, the inclusion of delayed policy update generally improves performance, while reducing training time.</p><p>We additionally compare the effectiveness of the actor-critic variants of Double Q-learning <ref type="bibr" target="#b39">(Van Hasselt, 2010)</ref> and <ref type="bibr">Double DQN (Van Hasselt et al., 2016)</ref>, denoted DQ-AC and DDQN-AC respectively, in <ref type="table" target="#tab_1">Table 2</ref>. For fairness in comparison, these methods also benefited from delayed policy updates, target policy smoothing and use our architecture and hyper-parameters. Both methods were shown to reduce overestimation bias less than Clipped Double Q-learning in Section 4. This is reflected empirically, as both methods result in insignificant improvements over TD3 -CDQ, with an exception in the Ant-v1 environment, which appears to benefit greatly from any overestimation reduction. As the inclusion of Clipped Double Q-learning into our full method outperforms both prior methods, this suggests that subduing the overestimations from the unbiased estimator is an effective measure to improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Overestimation has been identified as a key problem in value-based methods. In this paper, we establish overestimation bias is also problematic in actor-critic methods. We find the common solutions for reducing overestimation bias in deep Q-learning with discrete actions are ineffective in an actor-critic setting, and develop a novel variant of Double Q-learning which limits possible overestimation. Our results demonstrate that mitigating overestimation can greatly improve the performance of modern algorithms.</p><p>Due to the connection between noise and overestimation, we examine the accumulation of errors from temporal difference learning. Our work investigates the importance of a standard technique in deep reinforcement learning, target networks, and examines their role in limiting errors from imprecise function approximation and stochastic optimization. Finally, we introduce a SARSA-style regularization technique which modifies the temporal difference target to bootstrap off similar state-action pairs.</p><p>Taken together, these improvements define our proposed approach, the Twin Delayed Deep Deterministic policy gradient algorithm (TD3), which greatly improves both the learning speed and performance of DDPG in a number of challenging tasks in the continuous control setting. Our algorithm exceeds the performance of numerous state of the art algorithms. As our modifications are simple to implement, they can be easily added to any other actor-critic algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Implementation Details</head><p>For clarity in presentation, certain implementation details were omitted, which we describe here. For the most complete possible description of the algorithm, code can be found on our GitHub (https://github.com/sfujim/TD3).</p><p>Our implementation of both DDPG and TD3 follows a standard practice in deep Q-learning, in which the update differs for terminal transitions. For transitions where the episode terminates by reaching some failure state, and not due to the episode running until the max horizon, the value of Q(s, ?) is set to 0 in the target y: y = r if terminal s and t &lt; max horizon r + ?Q ? (s , ? ? (s )) else For target policy smoothing (Section 5.3), the added noise is clipped to the range of possible actions, to avoid error introduced by using values of impossible actions: y = r + ?Q ? (s , clip(? ? (s ) + , min action, max action)), ? clip <ref type="figure">(N (0, ?)</ref>, ?c, c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Soft Actor-Critic Implementation Details</head><p>For our implementation of Soft Actor-Critic <ref type="bibr" target="#b9">(Haarnoja et al., 2018)</ref> we use the code provided by the author (https: //github.com/haarnoja/sac), using the hyper-parameters described by the paper. We use a Gaussian mixture policy with 4 Gaussian distributions, except for the Reacher-v1 task, where we use a single Gaussian distribution due to numerical instability issues in the provided implementation. We use the environment-dependent reward scaling as described by the authors, multiplying the rewards by 3 for Walker2d-v1 and Ant-v1, and 1 for all remaining environments.</p><p>For fair comparison with our method, we train for only 1 iteration per time step, rather than the 4 iterations used by the results reported by the authors. This along with fewer total time steps should explain for the discrepancy in results on some of the environments. Additionally, we note this comparison is against a prior version of Soft Actor-Critic, while the most recent variant includes our Clipped Double Q-learning in the value update and produces competitive results to TD3 on most tasks. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Measuring overestimation bias in the value estimates of DDPG and our proposed method, Clipped Double Q-learning (CDQ), on MuJoCo environments over 1 million time steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Measuring overestimation bias in the value estimates of actor critic variants of Double DQN (DDQN-AC) and Double Qlearning (DQ-AC) on MuJoCo environments over 1 million time steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Average estimated value of a randomly selected state on Hopper-v1 without target networks, (? = 1), and with slowupdating target networks, (? = 0.1, 0.01), with a fixed and a learned policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Example MuJoCo environments (a) HalfCheetah-v1, (b) Hopper-v1, (c) Walker2d-v1, (d) Ant-v1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Learning curves for the OpenAI gym continuous control tasks. The shaded region represents half a standard deviation of the average evaluation over 10 trials. Curves are smoothed uniformly for visual clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FFigure 6 .Figure 7 .Figure 8 .</head><label>678</label><figDesc>Ablation over the varying modifications to our DDPG (AHE), comparing the subtraction of delayed policy updates (TD3 -DP), target policy smoothing (TD3 -TPS) and Clipped Double Q-learning (TD3 -CDQ). Ablation over the varying modifications to our DDPG (AHE), comparing the addition of delayed policy updates (AHE + DP), target policy smoothing (AHE + TPS) and Clipped Double Q-learning (AHE + CDQ). Comparison of TD3 and the Double Q-learning (DQ-AC) and Double DQN (DDQN-AC) actor-critic variants, which also leverage delayed policy updates and target policy smoothing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Max Average Return over 10 trials of 1 million time steps. Maximum value for each task is bolded. ? corresponds to a single standard deviation over trials.</figDesc><table><row><cell>Environment</cell><cell>TD3</cell><cell>DDPG</cell><cell>Our DDPG</cell><cell>PPO</cell><cell>TRPO</cell><cell>ACKTR</cell><cell>SAC</cell></row><row><cell>HalfCheetah</cell><cell cols="2">9636.95 ? 859.065 3305.60</cell><cell>8577.29</cell><cell>1795.43</cell><cell>-15.57</cell><cell cols="2">1450.46 2347.19</cell></row><row><cell>Hopper</cell><cell>3564.07 ? 114.74</cell><cell>2020.46</cell><cell>1860.02</cell><cell cols="4">2164.70 2471.30 2428.39 2996.66</cell></row><row><cell>Walker2d</cell><cell>4682.82 ? 539.64</cell><cell>1843.85</cell><cell>3098.11</cell><cell cols="4">3317.69 2321.47 1216.70 1283.67</cell></row><row><cell>Ant</cell><cell cols="2">4372.44 ? 1000.33 1005.30</cell><cell>888.77</cell><cell>1083.20</cell><cell>-75.85</cell><cell>1821.94</cell><cell>655.35</cell></row><row><cell>Reacher</cell><cell>-3.60 ? 0.56</cell><cell>-6.51</cell><cell>-4.01</cell><cell>-6.18</cell><cell>-111.43</cell><cell>-4.26</cell><cell>-4.44</cell></row><row><cell>InvPendulum</cell><cell>1000.00 ? 0.00</cell><cell>1000.00</cell><cell>1000.00</cell><cell>1000.00</cell><cell>985.40</cell><cell cols="2">1000.00 1000.00</cell></row><row><cell>InvDoublePendulum</cell><cell>9337.47 ? 14.96</cell><cell>9355.52</cell><cell>8369.95</cell><cell>8977.94</cell><cell>205.85</cell><cell cols="2">9081.92 8487.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Average return over the last 10 evaluations over 10 trials of 1 million time steps, comparing ablation over delayed policy updates (DP), target policy smoothing (TPS), Clipped Double Q-learning (CDQ) and our architecture, hyper-parameters and exploration (AHE). Maximum value for each task is bolded.</figDesc><table><row><cell>Method</cell><cell cols="3">HCheetah Hopper Walker2d</cell><cell>Ant</cell></row><row><cell>TD3</cell><cell>9532.99</cell><cell>3304.75</cell><cell>4565.24</cell><cell>4185.06</cell></row><row><cell>DDPG</cell><cell>3162.50</cell><cell>1731.94</cell><cell>1520.90</cell><cell>816.35</cell></row><row><cell>AHE</cell><cell>8401.02</cell><cell>1061.77</cell><cell>2362.13</cell><cell>564.07</cell></row><row><cell>AHE + DP</cell><cell>7588.64</cell><cell>1465.11</cell><cell>2459.53</cell><cell>896.13</cell></row><row><cell>AHE + TPS</cell><cell>9023.40</cell><cell>907.56</cell><cell>2961.36</cell><cell>872.17</cell></row><row><cell>AHE + CDQ</cell><cell>6470.20</cell><cell>1134.14</cell><cell>3979.21</cell><cell>3818.71</cell></row><row><cell>TD3 -DP</cell><cell>9590.65</cell><cell>2407.42</cell><cell>4695.50</cell><cell>3754.26</cell></row><row><cell>TD3 -TPS</cell><cell>8987.69</cell><cell>2392.59</cell><cell>4033.67</cell><cell>4155.24</cell></row><row><cell>TD3 -CDQ</cell><cell>9792.80</cell><cell>1837.32</cell><cell>2579.39</cell><cell>849.75</cell></row><row><cell>DQ-AC</cell><cell>9433.87</cell><cell>1773.71</cell><cell>3100.45</cell><cell>2445.97</cell></row><row><cell>DDQN-AC</cell><cell>10306.90</cell><cell>2155.75</cell><cell>3116.81</cell><cell>1092.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>A complete comparison of hyper-parameter choices between our DDPG and the OpenAI baselines implementation.</figDesc><table><row><cell>Hyper-parameter</cell><cell>Ours</cell><cell>DDPG</cell></row><row><cell>Critic Learning Rate</cell><cell>10 ?3</cell><cell>10 ?3</cell></row><row><cell>Critic Regularization</cell><cell>None</cell><cell>10 ?2 ? ||?|| 2</cell></row><row><cell>Actor Learning Rate</cell><cell>10 ?3</cell><cell>10 ?4</cell></row><row><cell>Actor Regularization</cell><cell>None</cell><cell>None</cell></row><row><cell>Optimizer</cell><cell>Adam</cell><cell>Adam</cell></row><row><cell>Target Update Rate (? )</cell><cell>5 ? 10 ?3</cell><cell>10 ?3</cell></row><row><cell>Batch Size</cell><cell>100</cell><cell>64</cell></row><row><cell>Iterations per time step</cell><cell>1</cell><cell>1</cell></row><row><cell>Discount Factor</cell><cell>0.99</cell><cell>0.99</cell></row><row><cell>Reward Scaling</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>Normalized Observations</cell><cell>False</cell><cell>True</cell></row><row><cell>Gradient Clipping</cell><cell>False</cell><cell>False</cell></row><row><cell>Exploration Policy</cell><cell cols="2">N (0, 0.1) OU, ? = 0.15, ? = 0, ? = 0.2</cell></row><row><cell cols="2">C. DDPG Network and Hyper-parameter Comparison</cell><cell></cell></row><row><cell>DDPG Critic Architecture</cell><cell></cell><cell></cell></row><row><cell>(state dim, 400)</cell><cell></cell><cell></cell></row><row><cell>ReLU</cell><cell></cell><cell></cell></row><row><cell>(action dim + 400, 300)</cell><cell></cell><cell></cell></row><row><cell>ReLU</cell><cell></cell><cell></cell></row><row><cell>(300, 1)</cell><cell></cell><cell></cell></row><row><cell>DDPG Actor Architecture</cell><cell></cell><cell></cell></row><row><cell>(state dim, 400)</cell><cell></cell><cell></cell></row><row><cell>ReLU</cell><cell></cell><cell></cell></row><row><cell>(400, 300)</cell><cell></cell><cell></cell></row><row><cell>ReLU</cell><cell></cell><cell></cell></row><row><cell>(300, 1)</cell><cell></cell><cell></cell></row><row><cell>tanh</cell><cell></cell><cell></cell></row><row><cell>Our Critic Architecture</cell><cell></cell><cell></cell></row><row><cell>(state dim + action dim, 400)</cell><cell></cell><cell></cell></row><row><cell>ReLU</cell><cell></cell><cell></cell></row><row><cell>(action dim + 400, 300)</cell><cell></cell><cell></cell></row><row><cell>RelU</cell><cell></cell><cell></cell></row><row><cell>(300, 1)</cell><cell></cell><cell></cell></row><row><cell>Our Actor Architecture</cell><cell></cell><cell></cell></row><row><cell>(state dim, 400)</cell><cell></cell><cell></cell></row><row><cell>ReLU</cell><cell></cell><cell></cell></row><row><cell>(400, 300)</cell><cell></cell><cell></cell></row><row><cell>RelU</cell><cell></cell><cell></cell></row><row><cell>(300, 1)</cell><cell></cell><cell></cell></row><row><cell>tanh</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">McGill University, Montreal, Canada 2 University of Amsterdam, Amsterdam, Netherlands. Correspondence to: Scott Fujimoto &lt;scott.fujimoto@mail.mcgill.ca&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See the supplementary material for hyper-parameters and a discussion on the discrepancy in the reported results of SAC.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>A. Proof of Convergence of Clipped Double Q-Learning</p><p>In a version of Clipped Double Q-learning for a finite MDP setting, we maintain two tabular value estimates Q A , Q B . At each time step we select actions a * = argmax a Q A (s, a) and then perform an update by setting target y: a * = argmax a Q A (s , a) y = r + ? min(Q A (s , a * ), Q B (s , a * )),</p><p>and update the value estimates with respect to the target and learning rate ? t (s, a):</p><p>In a finite MDP setting, Double Q-learning is often used to deal with noise induced by random rewards or state transitions, and so either Q A or Q B is updated randomly. However, in a function approximation setting, the interest may be more towards the approximation error and thus we can update both Q A and Q B at each iteration. The proof extends naturally to updating either randomly.</p><p>The proof borrows heavily from the proof of convergence of SARSA <ref type="bibr" target="#b33">(Singh et al., 2000)</ref> as well as Double Q-learning <ref type="bibr" target="#b39">(Van Hasselt, 2010)</ref>. The proof of lemma 1 can be found in <ref type="bibr" target="#b33">Singh et al. (2000)</ref>, building on a proposition from <ref type="bibr" target="#b4">Bertsekas (1995)</ref>.</p><p>where x t ? X and t = 0, 1, 2, .... Let P t be a sequence of increasing ?-fields such that ? 0 and ? 0 are P 0 -measurable and ? t , ? t and F t?1 are P t -measurable, t = 1, 2, .... Assume that the following hold:</p><p>Where || ? || denotes the maximum norm. Then ? t converges to 0 with probability 1.</p><p>Theorem 1. Given the following conditions:</p><p>1. Each state action pair is sampled an infinite number of times.</p><p>2. The MDP is finite.</p><p>3. ? ? [0, 1).</p><p>4. Q values are stored in a lookup table.</p><p>5. Both Q A and Q B receive an infinite number of updates.</p><p>6. The learning rates satisfy ? t (s, a) ? [0, 1], t ? t (s, a) = ?, t (? t (s, a)) 2 &lt; ? with probability 1 and ? t (s, a) = 0, ?(s, a) = (s t , a t ).</p><p>7. Var[r(s, a)] &lt; ?, ?s, a.</p><p>Then Clipped Double Q-learning will converge to the optimal value function Q * , as defined by the Bellman optimality equation, with probability 1.</p><p>Proof of Theorem 1. We apply Lemma 1 with P t = {Q A 0 , Q B 0 , s 0 , a 0 , ? 0 , r 1 , s 1 , ..., s t , a t },</p><p>First note that condition 1 and 4 of the lemma holds by the conditions 2 and 7 of the theorem respectively. Lemma condition 2 holds by the theorem condition 6 along with our selection of ? t = ? t .</p><p>Defining a * = argmax a Q A (s t+1 , a) we have</p><p>where we have defined F t (s t , a t ) as:</p><p>where</p><p>. As E F Q t |P t ? ?||? t || is a well-known result, then condition 3 of lemma 1 holds if it can be shown that c t converges to 0 with probability 1.</p><p>, where c t converges to 0 if ? BA converges to 0. The update of ? BA t at time t is the sum of updates of Q A and Q B :</p><p>Clearly ? BA t will converge to 0, which then shows we have satisfied condition 3 of lemma 1, implying that Q A (s t , a t ) converges to Q * t (s t , a t ). Similarly, we get convergence of Q B (s t , a t ) to the optimal vale function by choosing ? t = Q B t ? Q * and repeating the same arguments, thus proving theorem 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overestimation Bias in Deterministic Policy Gradients</head><p>If the gradients from the deterministic policy gradient update are unnormalized, this overestimation is still guaranteed to occur under a slightly stronger condition on the expectation of the value estimate. Assume the approximate value function is equal to the true value function, in expectation over the steady-state distribution, with respect to policy parameters between the original policy and in the direction of the true policy update:</p><p>Noting that ? true maximizes the rate of change of the true value ? ? true = Q ? (s, ? true (s)) ? Q ? (s, ? ? (s)), ? ? true ? ? ? approx . By the given condition 22 the maximal rate of change of the approximate value must be at least as great ? ? approx ? ? ? true . Given Q ? (s, ? ? ) = Q ? (s, ? ? ) this implies Q ? (s, ? approx (s)) ? Q ? (s, ? true (s)) ? Q ? (s, ? approx (s)), showing an overestimation of the value function.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Averaged-dqn: Variance reduction and stabilization for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Anschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Baram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shimkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="176" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muldal</surname></persName>
		</author>
		<title level="m">Heess, N., and Lillicrap, T. Distributional policy gradients. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Dynamic</forename><surname>Bellman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Programming</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic programming and optimal control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena scientific Belmont</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1995" />
			<publisher>MA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gym</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baselines</surname></persName>
		</author>
		<ptr target="https://github.com/openai/baselines" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01561</idno>
		<title level="m">Scalable distributed deep-rl with importance weighted actor-learner architectures</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Taming the noise in reinforcement learning via soft updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pakman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Thirty-Second Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01290</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to play in a day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01606</idno>
	</analytic>
	<monogr>
		<title level="m">Faster deep reinforcement learning by optimality tightening</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meger</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06560</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">D. Deep Reinforcement Learning that Matters. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<title level="m">Distributed prioritized experience replay. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On actor-critic algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1143" to="1166" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bias-corrected q-learning to control max-operator bias in q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Defourny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adaptive Dynamic Programming And Reinforcement Learning (ADPRL), 2013 IEEE Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="93" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-improving reactive agents based on reinforcement learning, planning and teaching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="293" to="321" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mean-variance optimization in markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bias and variance approximation in value function estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="308" to="322" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Safe and efficient off-policy reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stepleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1054" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Smoothed action value functions for learning gaussian policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02348</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>O&amp;apos;donoghue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05380</idno>
		<title level="m">The uncertainty bellman equation and exploration</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Estimator variance in reinforcement learning: Theoretical problems and practical solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Pendrith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Ryan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
		<respStmt>
			<orgName>University of New South Wales, School of Computer Science and Engineering</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Biasing approximate dynamic programming with a lower discount factor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Petrik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scherrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1265" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Data-efficient deep reinforcement learning for dexterous manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vecerik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lampe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03073</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Off-policy temporal-difference learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Prioritized experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deterministic policy gradient algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="387" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convergence results for single-step on-policy reinforcement-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesv?ri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="308" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to predict by the methods of temporal differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="44" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Issues in using function approximation for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1993 Connectionist Models</title>
		<meeting>the 1993 Connectionist Models<address><addrLine>Summer School Hillsdale, NJ. Lawrence Erlbaum</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the theory of the brownian motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Uhlenbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Ornstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">823</biblScope>
			<date type="published" when="1930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with double q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2094" to="2100" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A theoretical and empirical analysis of expected sarsa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Seijen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adaptive Dynamic Programming and Reinforcement Learning, 2009. ADPRL&apos;09. IEEE Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning from delayed rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>King&apos;s College, Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5285" to="5294" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
