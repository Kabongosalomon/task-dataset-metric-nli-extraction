<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Attention Network for Few-Shot Object Detection via Meta-Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwoo</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hanyang University</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Min</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hanyang University</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Attention Network for Few-Shot Object Detection via Meta-Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot object detection (FSOD) aims to classify and detect few images of novel categories. Existing meta-learning methods insufficiently exploit features between support and query images owing to structural limitations. We propose a hierarchical attention network with sequentially large receptive fields to fully exploit the query and support images. In addition, meta-learning does not distinguish the categories well because it determines whether the support and query images match. In other words, metric-based learning for classification is ineffective because it does not work directly. Thus, we propose a contrastive learning method called metacontrastive learning, which directly helps achieve the purpose of the meta-learning strategy. Finally, we establish a new state-of-the-art network, by realizing significant margins. Our method brings 2.3, 1.0, 1.3, 3.4 and 2.4% AP improvements for 1-30 shots object detection on COCO dataset. Our code is available at:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, object detection has undergone tremendous development, and excellent performance detectors have been proposed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref>. Object detection requires a large number of annotated images and considerable time, labor, and cost. In low-data scenarios, showing good performance is difficult with only a small amount of data owing to diversity. Therefore, few-shot object detection (FSOD) has attracted considerable attention because it does not require a large amount of data. However, despite extensive research in recent years, existing FSOD methods still have drawbacks.</p><p>(1) The existing structure interferes with the full exploitation of support and query images, and sufficient attention to the query image is not considered <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b28">28]</ref>. <ref type="bibr" target="#b1">(2)</ref> In the meta-learning strategy, metric-based learning, a method for constructing feature spaces well, is ineffective because it does not fit the principle of object detection based on metalearning. (3) Existing methods still exhibit extremely low performance compared with generic object detection methods. To address these problems, we propose a hierarchical attention network for FSOD via meta-contrastive learning. Our proposed method is a two-stage detector based on Faster R-CNN ResNet-101. This structure is composed of a hierarchical attention module (HAM) and meta-contrastive learning module (Meta-CLM).</p><p>The HAM efficiently uses the attention mechanism, which is often used in the existing FSOD method. Existing studies using the attention mechanism still show low performance because they do not fully exploit the support and query images or overcome the structural limitations of global attention. In a previous study <ref type="bibr" target="#b23">[23]</ref>, the transformer series did not show good performance in a low-data scenario because the earlier layer did not capture local information. Therefore, we propose hierarchical attention to sequentially extend the receptive field and overcome these structural limitations. In addition, the global attention of the query image is calculated to fully utilize the query image.</p><p>In the meta-CLM, contrastive learning is not well suited to the meta-learning method because its purpose is to make the embedding vectors between classes well distinguished on a feature space. Existing contrastive learning methods learn to pull and push from other classes, such as anchors, as shown in <ref type="figure" target="#fig_1">Figure 1</ref> (left). Meta-learning methods based on <ref type="bibr" target="#b35">[34]</ref> that aggregate features through multiplication between query and support images are rarely used in recent object detection studies owing to spatial misalignment. Therefore, concatenation is often used to aggregate features. Applying supervised contrastive learning in concatenation methods can create a similar embedding vector for each class by extracting class-specific features for each class of the query feature. However, this is only indirectly helpful because the same class creates a similar embedding vector in classification but is not good in terms of object detection because the purpose is to create similar embedding vectors regardless of the shape, size, or position of the object. Therefore, we propose meta-contrastive learning that generates correlation features and can consider the correlation between query and support images. The method is suitable for both classification and object detection because it extracts the correlation rather than the characteristics of each query and support image. In the proposed method, if the correlation feature consists of the same class of query and support images, it is called a matched sample; if the anchor is matched, it increases the similarity with each other. Otherwise, the similarity decreases.</p><p>We demonstrate that the proposed method efficiently solves the above problems. We achieved significant performance improvements regardless of the number of images on the COCO dataset <ref type="bibr" target="#b3">[4]</ref>. In particular, the proposed method  established a new state-of-the-art method with large margins. Our proposed method brings 2.3, 1.0, 1.3, 3.4 and 2.4% AP improvements for 1-30 shots object detection on COCO dataset. The main contributions of our approach are as follows.</p><p>? We propose a HAM to extend the receptive field from local to global. ? We propose a novel contrastive learning framework for the object detection based on meta-learning strategy. ? We establish a new state-of-the-art network, by realizing significant margins. ? Our proposed FSOD network can strongly detect novel categories without a fine-tuning stage in 1-5 shots object detection on COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Few-Shot Object Detection</head><p>FSOD can be divided into meta-learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">28]</ref> and transfer learning methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">12]</ref>. (1) Meta-learning solves the problem of matching a support image to the query image when the support and query images are given; this is a method that has been widely used in existing few-shot classification approaches. Early FSOD methods used a metalearning strategy. Kang et al. <ref type="bibr" target="#b4">[5]</ref> proposed a meta-learning method based on YOLO v2 that extracts the global features of the support image using a reweighting module. It then acquires the global features of the support image using the reweighting module and uses them as a coefficient to refine the query image. Inspired by the squeeze-and-excitation method, Hsieh et al. <ref type="bibr" target="#b8">[9]</ref> proposed co-excitation to make the query features attend to the support features, and vice versa. Yan et al. <ref type="bibr" target="#b9">[10]</ref> demonstrated a method to aggregate support and query features through channel-wise multiplication using a meta-learner. Fan et al. <ref type="bibr" target="#b28">[28]</ref> proposed a module that aggregates support and query images in an RPN to extract features that distinguish images from other categories. (2) Transfer learning methods refer to reusing the weights of a network trained with abundant D base . First, the model is pretrained using abundant data from base categories and subsequently, solves a few-shot learning problem using weights through the fine-tuning step with a few shots of novel category images. Qiao et al. <ref type="bibr" target="#b6">[7]</ref> stopped the gradient in the RPN because the RPN must generate many proposals regardless of the class, and the region of interest (RoI) head performs class-specific classification and regression. Cao et al. <ref type="bibr" target="#b7">[8]</ref> associated and discriminated the data from novel categories to the base categories to ensure inter-class separability and intra-class compactness. Sun et al. <ref type="bibr" target="#b12">[12]</ref> proposed a fine-tuning method that adds a contrastive head to the RoI head to perform contrastive learning between proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attention Mechanisms</head><p>FSOD, using an attention mechanism, is intended to highlight relevant features. In this manner, highlighting selfrelated features (global attention) or features related to another feature (cross attention) is possible. Therefore, FSOD methods are being studied to fully exploit support and query features using an attention mechanism. Hu et al. <ref type="bibr" target="#b16">[16]</ref> used cross-attention with the query feature to fully exploit the support features, and Chen et al. <ref type="bibr" target="#b21">[21]</ref> added a method to use  the attention of the support feature.</p><formula xml:id="formula_0">DW-Conv DW-D-Conv 1?1 Conv F ! F " DW-Conv DW-D-Conv 1?1 Conv F ! F " F ! ## F " ## F ! ## F "</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>First, we review the preliminaries of the FSOD setting. Then, we introduce our method, which addresses the proposed hierarchical attention and meta-contrastive learning for FSOD, as shown in <ref type="figure" target="#fig_3">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>For the FSOD dataset, {x,y} = D denotes that it contains image x and ground truth label y. They can be divided into two categories: D novel and D base . Note that C base and C novel are disjointed, i.e., C base ? C novel = ?. Each training data consists of an image x and corresponding label y, where y consists of a class label and bounding box. The fundamental purpose of FSOD is to evaluate the data of D novel from only a low shot of D novel using a model trained with abundant data D base . Similar to Kang et al. <ref type="bibr" target="#b4">[5]</ref>, we trained using up to 30 or fewer instances (1, 3, 5, 10, and 30) because only K box instances should be used for training or evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hierarchical Attention Module</head><formula xml:id="formula_1">F q = Conv 1?1 (DW-D-Conv(DW-Conv(F q ))<label>(1)</label></formula><formula xml:id="formula_2">F s = Conv 1?1 (DW-D-Conv(DW-Conv(F s )) (2) F q = (?((F q W Q q ) ? (F q W k q ) T ) ? F q ) (3) F s = 1 K K n ((?((F q W Q s ) ? (F s,n W K s ) T +?((F s,n W Q s ) ? (F s,n W K s ) T )) ? F s,n ) (4) F = Concat(F q + F s )<label>(5)</label></formula><p>Using the complementary relationship between convolutional neural network (CNN) and multi-head self attention (MSA) blocks, studies by Raghu et al. <ref type="bibr" target="#b23">[23]</ref> and Park et al. <ref type="bibr" target="#b24">[24]</ref> have shown that extracting local features at the early layer and global features at the end of the layer is beneficial. From the early layer to the end of the layer, we can see a relatively local part between the backbone and global attention and add the convolution-based attention of <ref type="bibr" target="#b25">[25]</ref> with three functions. When applying attention, we consider that the relationship with all pixels from the beginning, such as global attention, may not be able to extract local features. In the early layer, forcing the attention score to be calculated is important for the expansion of the receptive field sequentially. This forces the model to reason locally. Equations 1 and 2 correspond to the early layer of the HAM, which is an attention module with the local receptive field used in Guo et al. <ref type="bibr" target="#b25">[25]</ref> and relatively long-range dependence, consisting of DW-Conv, DW-D-Conv, Conv (1?1) , which are aimed at spatial local, spatial long-range, and channel convolution, respectively. Therefore, F q and F s are expressed as F q and F s respectively, using Equation 1 and Equation 2. Equation 3 calculates the global attention score through the FC layer and multiplies it by F q to obtain a global attention F q . Equation 4 shows the process of obtaining the sum of the cross attention between F q and F s and attention of F s itself. Equation 4 is used as the final feature by concatenating the F q and F s obtained in Equations 4 and 5. Therefore, we designed a hierarchical structure that gradually expands from a local feature to pass through a local attention module and, finally, through a cross and global attention module that can consider long-range dependence between all pixels. In addition, calculating the attention between the support and query images and their individual attentions is necessary. When properly considered, this shows a better performance than global attention without self-attention. Section 4.6 presents   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Meta-Contrastive Learning Module</head><formula xml:id="formula_3">L meta = i?I ?1 |M (i)| m?M (i) log( exp(z m ? z j /? ) a?A(i) (exp(z m ? z a /? ) )<label>(6)</label></formula><p>Contrastive learning enables the generation of embedding vectors that are well-differentiated between classes in the feature space. This type of learning strategy is a widely applied method for learning distinctive feature embeddings in a lower-dimensional space for each class. The meta-learning method in FSOD performs classification and object detection using a pair of query and support features. Therefore, <ref type="figure">Figure 3</ref>: Visualization on the attention map. The first row presents original novel category images. The second row uses only cross attention between the query and support images and the global attention of the support image. The third row uses additional global attention on the query image and hierarchical attention general contrastive learning methods generate similar embedding vectors for each class of query features or support images. It helps meta-learning to compare query and support images because it creates similar embedding vectors between the same classes. However, this adversely affects object detection in images with different characteristics, even those in the same class. Therefore, we propose a new contrastive learning, which uses the concept of correlation features to distinguish between matched support images and query images without classifying object categories suitable for meta-learning. The objective function aims to increase the similarity with the correlation features of the same class and decrease the similarity with the correlation features of different classes if the anchor is a correlation feature of the same class. In Equation 6, let i ? I ? 1. . . N be the index of a batch image, m ? M ? 1. . . M be the number of matched correlation features between the query proposal and support features, and j be the index of the other matched correlation feature called positive from the same image i. Here, ? is a scalar temperature parameter, z m is the anchor, and A(i) ? M\{z m } is a feature of positive and negative, excluding anchor. For example, when a correlation feature includes a support image of the same class as the query, and another includes a support image of a different class from the query, the two correlation features are trained to have a lower similarity. Because it learns to increase the similarity between correlation features composed only of the same class, regardless of any class, the correlation between features can be identified, and semantic and visual features can be effectively extracted. This enables the extraction of the positions of objects effectively in terms of object detection. Section 4.7 covers the method for generating the correlation feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning Strategy</head><p>The model training process can be divided into pre-training and fine-tuning steps. During pre-training, the entire model is trained using only abundant D base . In the fine-tuning stages, it is not used for extremely low-data scenarios; this prevents overfitting or a low generalization ability during fine-tuning with too few images. The fine-tuning method was used on a relatively large number of shots (10 and 30). In the fine-tuning stage, training is conducted with only 10 and 30 instances according to Kang et al. <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Function</head><formula xml:id="formula_4">L = L rpn cls + L rpnreg + L rcnn cls + L rcnnreg (7)</formula><p>The loss function consists of the classification and bounding box regression loss of the RPN, the classification and bounding box regression loss of the RCNN, and includes the loss used in meta-contrastive learning. The loss function is jointly optimized using Equation <ref type="bibr" target="#b6">7</ref>. The RPN and RCNN loss is the Faster R-CNN <ref type="bibr" target="#b0">[1]</ref> objective. ? of the L meta was experimentally set to 0.07.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Generic FSOD methods use their own datasets and data settings. We conduct the evaluation as a result of multiple runs to make fair comparisons for COCO dataset according to the setting of Kang et al. <ref type="bibr" target="#b4">[5]</ref>. MS-COCO: We use categories of the PASCAL VOC <ref type="bibr" target="#b36">[35]</ref> dataset as novel classes, and the remaining 60 categories as base classes. The validation set is used for testing according to Kang's split, and the remaining data in the train/val sets are used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setup</head><p>For a fair comparison, we used a Faster R-CNN framework with ResNet-101 and a feature pyramid network (FPN). We set the learning rate to start at 0.001 and increase by 0.1 times per 1000 steps. Similarly, in the fine-tuning stage, the learning rate decay coefficients were continuously trained from the pre-training stage. We used a stochastic gradient descent (SGD) to optimize the model using a momentum of 0.9 and weight decay of 0.0001. We trained the model using a batch size of 4, and both the pre-training and fine-tuning stages were conducted for 12 epochs on an Nvidia GeForce 3090 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Arts</head><p>We compared the performance of the proposed method with that of the latest FSOD methods. In <ref type="table" target="#tab_0">Tables 1, 2</ref>, and 3, the proposed method improves on the current state-of-the-art by a large margin. In particular, we exceeded SOTA by <ref type="bibr">13.4, 14.8, 15.1, 22.3, and 25</ref>.0 AP for K = 1, 3, 5, 10, and 30 shots on the COCO dataset, respectively. In general, comparing results fairly is difficult because metrics used in previous studies are different. Therefore, we report the results of multiple runs. We conducted ablation studies to verify the effectiveness of the proposed module. All results were averaged over multiple runs with randomly sampled support datasets according to <ref type="bibr">Kang et al. splits [5]</ref> on COCO. <ref type="table" target="#tab_3">Table 4</ref> shows the results of the ablation study to analyze whether each module improves in performance. Hierarchical attention can effectively utilize both query and support features. Among methods that obtain the attention score between the query and support features, the model by Hu et al. <ref type="bibr" target="#b16">[16]</ref> calculates the attention score between the query and support feature, and Chen et al. s model <ref type="bibr" target="#b21">[21]</ref> additionally calculates the attention score between the query and support features individually. However, calculating the global attention score of a query is also important. In addition, to compensate for the inability to focus on the local area, which is a disadvantage of global attention, we construct an attention module in a hierarchical structure that extends from local to global, thereby demonstrating significant performance improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study on Modules</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Hierarchical Attention Visualization</head><p>This is the result of visualizing the attention map of the existing and proposed methods for a qualitative comparison. <ref type="figure">Figure 3</ref> shows the results of evaluating the novel category data for the model trained with novel category data with the fine-tuning step. Compared with the existing method <ref type="bibr" target="#b21">[21]</ref>, when the attention map is visualized, the weight is more reliably applied to the area where the object is located. Additional experiments were conducted to prove that attention with a hierarchical structure is better than attention with a non-or inverse hierarchical structure. <ref type="table" target="#tab_4">Table 5</ref> shows the kernel sizes of early-late attention (the first, second, and third rows indicate kernel sizes of 14, 21, and 28, respectively). The results of using the attention of the non-and inverse hierarchical structures are shown in rows 4 and 5, respectively. As demonstrated by the results, the method of sequentially expanding the receptive fields from the layer at the early end to the layer at the rear end yields the best result on COCO. <ref type="figure" target="#fig_4">Figure 4</ref> shows two contrasting learning methods for query and support features in the meta-training strategy. The first method involves measuring the correlation feature between   the query and support features by concatenating two features that pass through the FC layer. The second method involves passing each feature through the same FC layer and concatenating it. In <ref type="table" target="#tab_5">Tables 6 and 7</ref>, the best result was obtained using the second method. This is probably because of features that have passed through an FC layer, sharing weights that can better extract semantically similar features. Therefore, the second method shows good results in a model fine-tuned on D novel of COCO, and the first method shows good results in a model only trained on a D base of COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Study on Receptive Fields</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Ablation Study on Contrastive Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this study, we propose a hierarchical attention network for FSOD via meta-contrastive learning. In the HAM, we apply a hierarchical attention mechanism that extends from attention that highlights a local area to attention that sequentially explores a global area. Owing to the characteristics of meta-training, the meta-CLM emphasizes the characteristics of the two correlation features to better extract important characteristics regardless of class. Our proposed method shows a state-of-the-art performance without a fine-tuning stage in low-data scenarios and significant performance improvements compared with existing methods for 10 and 30 shots using fine-tuning on COCO.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>between Query image and Support image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Conceptual visualization of supervised contrastive learning vs. meta-contrastive learning. (Left) contrasts the set of all samples from the same class as positives against the negatives from the remainder of the batch. (Right) correlation feature between query and support images is a basic unit. Meta-contrastive learning contrasts the set of correlation features from the proposals of the same image as positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Framework of the proposed architecture. Query and support images are processed by the hierarchical attention module (HAM), and are then efficiently exploited through global and cross attention. DW-Conv: depth-wise convolution; DW-D-Conv: depth-wise dilated convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Generation of correlation features by aggregating query and support features. F q and F s refer to the query and support feature, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance on the MS COCO dataset (1,3, and 5 shots). ? indicates re-implemented results. Ml: meta-learning method. Tl: transfer learning method. Ft.: fine-tuned on D novel . nAP: novel categories average precision (AP). Cat.: categorization by approach.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 shots</cell><cell></cell><cell></cell><cell>3 shots</cell><cell></cell><cell></cell><cell>5 shots</cell><cell></cell></row><row><cell cols="2">Cat. Model</cell><cell>Publication</cell><cell>Detector</cell><cell cols="2">Avg. Ft.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tl</cell><cell>TFA w/cos [31]</cell><cell>ICML 2020</cell><cell>Faster R-CNN R-101</cell><cell>X</cell><cell>O</cell><cell cols="3">1.9 3.8 1.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Tl</cell><cell>CoRPN [32]</cell><cell>arXiv 2020</cell><cell>Faster R-CNN R-101</cell><cell>X</cell><cell>O</cell><cell cols="3">4.1 7.2 4.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Ml Meta Faster-RCNN [33]</cell><cell>arXiv 2021</cell><cell>Faster R-CNN R-101</cell><cell>X</cell><cell>X</cell><cell cols="3">5.0 10.5 4.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Ml QA-FewDet [19]</cell><cell>ICCV 2021</cell><cell>Faster R-CNN R-101</cell><cell>O</cell><cell>O</cell><cell cols="3">5.1 10.5 4.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Tl</cell><cell>FADI [8]</cell><cell>NeurIPS 2021</cell><cell>Faster R-CNN R-101</cell><cell>X</cell><cell>O</cell><cell cols="3">5.7 10.4 6.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Ml Meta-DETR [20]</cell><cell>arXiv 2021</cell><cell>Deformable DETR R-101</cell><cell>X</cell><cell>O</cell><cell cols="3">7.5 12.5 7.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Tl</cell><cell>DeFRCN [7]</cell><cell>ICCV 2021</cell><cell>Faster R-CNN R-101</cell><cell>O</cell><cell>O</cell><cell>9.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Ml DAnA [21]</cell><cell>TMM 2021</cell><cell>Faster R-CNN R-50</cell><cell>X</cell><cell>X</cell><cell cols="9">11.9 25.6 10.4 14.0 28.9 12.3 14.4 30.4 13.0</cell></row><row><cell cols="2">Ml DAnA $ [21]</cell><cell>TMM 2021</cell><cell>Faster R-CNN R-50</cell><cell>X</cell><cell>X</cell><cell cols="9">9.4 20.4 8.1 12.2 25.4 10.9 12.5 26.2 10.9</cell></row><row><cell cols="2">Ml DAnA $ [21]</cell><cell>TMM 2021</cell><cell>Faster R-CNN R-101</cell><cell>X</cell><cell>X</cell><cell cols="9">11.1 21.8 10.4 13.8 26.8 12.9 13.8 26.8 13.0</cell></row><row><cell cols="2">Ml Ours (Meta-CLM 2)</cell><cell>-</cell><cell>Faster R-CNN R-101</cell><cell>X</cell><cell>X</cell><cell cols="9">12.9 25.0 12.1 14.4 28.0 13.3 14.5 27.9 13.3</cell></row><row><cell cols="2">Ml Ours (Meta-CLM 1)</cell><cell>-</cell><cell>Faster R-CNN R-101</cell><cell>X</cell><cell>X</cell><cell>13.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>4 26.7 12.0 14.8 29.0 13.1 15.1 30.1 13.5</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance on the MS COCO dataset (10 shots).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Average Precision</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Average Recall</cell></row><row><cell>Cat. Model</cell><cell cols="2">Publication Detector</cell><cell cols="2">Avg. Ft.</cell><cell cols="3">50 75 S</cell><cell cols="2">M L</cell><cell>1</cell><cell cols="3">10 100 S</cell><cell cols="2">M L</cell></row><row><cell>Ml MetaDet [11]</cell><cell cols="2">ICCV 2019 Faster R-CNN VGG-16</cell><cell>O</cell><cell>O</cell><cell cols="11">7.1 14.6 6.1 1.0 4.1 12.2 11.9 15.1 15.5 1.7 9.7 30.1</cell></row><row><cell>Ml Meta R-CNN [10]</cell><cell cols="2">ICCV 2019 Faster R-CNN R-101</cell><cell>X</cell><cell>O</cell><cell cols="11">8.7 19.1 6.6 2.3 7.7 14.0 12.6 17.8 17.9 7.8 15.6 27.2</cell></row><row><cell cols="3">Ml Meta-RetinaNet [13] BMVC 2020 RetinaNet ResNet18</cell><cell>X</cell><cell>O</cell><cell cols="11">9.7 19.9 7.7 2.8 8.5 15.2 13.4 18.9 19.0 8.3 17.1 32.8</cell></row><row><cell>Tl MPSR [14]</cell><cell cols="2">ECCV 2020 Faster R-CNN R-101</cell><cell>X</cell><cell>O</cell><cell cols="11">9.8 17.0 9.7 3.3 9.2 16.1 15.7 21.2 21.2 4.6 19.6 34.3</cell></row><row><cell>Ml FsDetView [15]</cell><cell cols="2">ECCV 2020 Faster R-CNN R-50</cell><cell>O</cell><cell>O</cell><cell cols="11">12.5 27.3 9.8 2.5 13.8 19.9 20.0 25.5 25.7 7.5 27.6 38.9</cell></row><row><cell>Ml DCNet [16]</cell><cell cols="2">CVPR 2021 Faster R-CNN R-101</cell><cell>O</cell><cell>O</cell><cell cols="11">12.8 23.4 11.2 4.3 13.8 21.0 18.1 26.7 25.6 7.9 24.5 36.7</cell></row><row><cell>Ml SQMG [17]</cell><cell cols="2">CVPR 2021 Faster R-CNN R-101</cell><cell>X</cell><cell>O</cell><cell cols="6">13.9 29.5 11.7 7.6 15.2 19.0 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ml CME [18]</cell><cell cols="2">CVPR 2021 MetaYOLO (YOLOv2)</cell><cell>X</cell><cell>O</cell><cell cols="11">15.1 24.6 16.4 4.6 16.6 26.0 16.3 22.6 22.8 6.6 24.7 39.7</cell></row><row><cell>Ml TIP [36]</cell><cell cols="2">CVPR 2021 Faster R-CNN R-101</cell><cell>O</cell><cell>O</cell><cell cols="11">16.3 33.2 14.1 5.4 17.5 25.8 23.6 30.2 30.5 12.7 32.3 43.8</cell></row><row><cell>Tl DeFRCN [7]</cell><cell cols="2">ICCV 2021 Faster R-CNN R-101</cell><cell>O</cell><cell>O</cell><cell>18.5 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ml DAnA [21]</cell><cell cols="2">TMM 2021 Faster R-CNN R-50</cell><cell>X</cell><cell>O</cell><cell cols="3">18.6 -17.2 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ml Meta-DETR [20]</cell><cell cols="2">arXiv 2021 Deformable DETR R-101</cell><cell>X</cell><cell>O</cell><cell cols="3">19.0 30.5 19.7 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ml Ours</cell><cell>-</cell><cell>Faster R-CNN R-101</cell><cell>O</cell><cell>O</cell><cell cols="11">21.8 39.8 21.2 7.3 19.4 33.1 20.9 33.4 34.0 14.0 32.1 50.8</cell></row><row><cell>Ml Ours</cell><cell>-</cell><cell>Faster R-CNN R-101</cell><cell>X</cell><cell>O</cell><cell>22.4 40.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>7 22.0 8.2 19.2 34.3 21.0 33.8 34.5 14.4 31.8 51.8</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance on the MS COCO dataset (30 shots).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Average Precision</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Average Recall</cell></row><row><cell>Cat. Model</cell><cell cols="2">Publication Detector</cell><cell cols="2">Avg. Ft.</cell><cell cols="3">50 75 S</cell><cell cols="2">M L</cell><cell>1</cell><cell cols="3">10 100 S</cell><cell>M L</cell></row><row><cell>Ml MetaDet [11]</cell><cell cols="2">ICCV 2019 Faster R-CNN VGG-16</cell><cell>O</cell><cell cols="11">O 11.3 21.7 8.1 1.1 6.2 17.3 14.5 18.9 19.2 1.8 11.1 34.4</cell></row><row><cell>Ml Meta R-CNN [10]</cell><cell cols="2">ICCV 2019 Faster R-CNN R-101</cell><cell>X</cell><cell cols="11">O 12.4 25.3 10.8 2.8 11.6 19.0 15.0 21.4 21.7 8.6 20.0 32.1</cell></row><row><cell cols="3">Ml Meta-RetinaNet [13] BMVC 2020 RetinaNet ResNet18</cell><cell>X</cell><cell cols="11">O 13.1 26.7 11.2 3.3 13.1 20.2 16.7 22.5 22.8 8.7 21.5 38.7</cell></row><row><cell>Tl MPSR [14]</cell><cell cols="2">ECCV 2020 Faster R-CNN R-101</cell><cell>X</cell><cell cols="11">O 14.1 25.4 14.2 4.0 12.9 23.0 17.7 24.2 24.3 5.5 21.0 39.3</cell></row><row><cell>Ml FsDetView [15]</cell><cell cols="2">ECCV 2020 Faster R-CNN R-50</cell><cell>O</cell><cell cols="11">O 14.7 30.6 12.2 3.2 15.2 23.8 22.0 28.2 28.4 8.3 30.3 42.1</cell></row><row><cell>Ml CME [18]</cell><cell cols="2">CVPR 2021 MetaYOLO (YOLOv2)</cell><cell>O</cell><cell cols="11">O 16.9 28.0 17.8 4.6 18.0 29.2 17.5 23.8 24.0 6.0 24.6 42.5</cell></row><row><cell>Ml TIP [36]</cell><cell cols="2">CVPR 2021 Faster R-CNN R-101</cell><cell>X</cell><cell cols="11">O 18.3 35.9 16.9 6.0 19.3 29.2 25.2 32.0 32.3 14.1 34.6 45.1</cell></row><row><cell>Ml DCNet [16]</cell><cell cols="2">CVPR 2021 Faster R-CNN R-101</cell><cell>X</cell><cell cols="11">O 18.6 32.6 17.5 6.9 16.5 27.4 22.8 27.6 28.6 8.4 25.6 43.4</cell></row><row><cell>Ml DAnA [21]</cell><cell cols="2">TMM 2021 Faster R-CNN R-101</cell><cell>O</cell><cell cols="4">O 21.6 -20.3 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ml Meta-DETR [20]</cell><cell cols="2">arXiv 2021 Deformable DETR R-101</cell><cell>O</cell><cell cols="4">O 22.2 35.0 22.8 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Tl DeFRCN [7]</cell><cell cols="2">ICCV 2021 Faster R-CNN R-101</cell><cell>X</cell><cell cols="2">O 22.6 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ml Ours</cell><cell>-</cell><cell>Faster R-CNN R-101</cell><cell>O</cell><cell cols="11">O 24.5 44.4 24.0 8.1 21.7 36.7 22.8 35.9 36.8 15.4 35.1 53.5</cell></row><row><cell>Ml Ours</cell><cell>-</cell><cell>Faster R-CNN R-101</cell><cell>X</cell><cell cols="11">O 25.0 45.2 24.6 7.9 22.4 37.7 22.9 36.1 37.0 15.3 34.7 53.7</cell></row><row><cell cols="3">more details regarding the experiments on the HAM.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :21.8 39.6 21.3 24.3 44.2 23.7</head><label>4</label><figDesc>Ablation study results on the effectiveness of each module. For fair comparison, we compare the model with baseline model trained by ResNet-101.</figDesc><table><row><cell></cell><cell>10shot</cell><cell>30shot</cell></row><row><cell>Model</cell><cell></cell></row><row><cell>Chen et al. [21] $ (R-101)</cell><cell cols="2">19.8 36.6 19.0 22.3 40.2 21.6</cell></row><row><cell>+Hierarchical Attention</cell><cell cols="2">21.4 38.4 21.4 23.1 41.2 23.1</cell></row><row><cell>+Meta Contrastive Learning</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of the mAP (mean average precision) of hierarchical, non-hierarchical, inverse hierarchical attention; 14, 21, and 28 indicate the kernel sizes.</figDesc><table><row><cell></cell><cell>Novel Categories Base Categories</cell></row><row><cell>Model</cell><cell>1shot 3shot 5shot 1shot 3shot 5shot</cell></row><row><cell>21-CSA (Hierarchical)</cell><cell>12.3 14.4 14.6 34.6 36.3 36.7</cell></row><row><cell>28-CSA (Hierarchical)</cell><cell>12.2 13.9 14.2 33.7 35.9 36.2</cell></row><row><cell>14-CSA (Hierarchical)</cell><cell>11.7 12.5 13.9 33.5 36.0 36.5</cell></row><row><cell>CSA only (Non-)</cell><cell>11.1 13.8 13.8 31.4 35.0 35.5</cell></row><row><cell>CSA-21 (Inverse)</cell><cell>11.8 14.1 14.8 32.4 35.9 36.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of the results of different correlated features on the MS COCO dataset (1, 3, and 5 shots). Not applied 12.3 23.4 11.6 14.4 27.0 13.5 14.6 27.9 13.Module 2 12.9 25.0 12.1 14.4 28.0 13.3 14.5 27.9 13.3</figDesc><table><row><cell>1shot</cell><cell>3shot</cell><cell>5shot</cell></row><row><cell>Model</cell><cell></cell><cell></cell></row></table><note>5 Module 1 13.4 26.7 12.0 14.8 29.0 13.1 15.1 30.1 13.5</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison of the results of different correlated features on the MS COCO dataset (10 and 30 shots).</figDesc><table><row><cell></cell><cell>10shot</cell><cell>30shot</cell></row><row><cell>Model</cell><cell></cell></row><row><cell>Not applied</cell><cell cols="2">21.4 38.4 21.4 23.1 41.2 23.1</cell></row><row><cell>Module 1</cell><cell cols="2">21.8 39.6 21.3 24.8 44.2 24.4</cell></row><row><cell>Module 2</cell><cell cols="2">22.4 40.7 22.0 25.6 45.9 25.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Few-shot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8420" to="8429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Few-shot object detection with fully cross-transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Defrcn: Decoupled faster r-cnn for few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8681" to="8690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Few-Shot Object Detection via Association and DIscrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One-Shot Object Detection with Co-Attention and Co-Excitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards general solver for instance-level low-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Meta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9576" to="9585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Meta-Learning to Detect Rare Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9924" to="9933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">FSCE: Few-Shot Object Detection via Contrastive Proposal Encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Meta-RetinaNet for Fewshot Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Quin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-Scale Positive Sample Refinement for Few-Shot Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="456" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Few-Shot Object Detection and Viewpoint Estimation for Objects in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="192" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dense Relation Distillation with Context-aware Aggregation for Few-Shot Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate Few-shot Object Detection with Support-Query Mutual Guidance and Hybrid Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Beyond Max-Margin: Class Margin Equilibrium for Few-shot Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Query adaptive few-shot object detection with heterogeneous graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3263" to="3272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11731</idno>
		<title level="m">Meta-DETR: Few-Shot Object Detection via Unified Image-Level Meta-Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dual-awareness Attention for Few-Shot Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-I</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia (TMM)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Do vision transformers see like convolutional neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">How do vision transformers work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations(ICLR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09741</idno>
		<title level="m">Visual attention network</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning(ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Few-shot object detection with attention-rpn and multi-relation detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mask R-Cnn</surname></persName>
		</author>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Frustratingly simple few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Cooperating RPN&apos;s improve few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10142</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Meta faster r-cnn: Towards accurate few-shot object detection with attentive feature alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Sixth AAAI Conference on Artificial Intelligence(AAAI)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The pascal visual object classes VOC challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Transformation Invariant Few-Shot Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3094" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
