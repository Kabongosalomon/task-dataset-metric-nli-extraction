<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fake It Till You Make It: Near-Distribution Novelty Detection by Score-Based Generative Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mirzaei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sharif University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Salehi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajjad</forename><surname>Shahabi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Sharif University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cees</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Institute for Research in Fundamental Sciences (IPM)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Hossein</forename><surname>Rohban</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">Sharif University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fake It Till You Make It: Near-Distribution Novelty Detection by Score-Based Generative Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We aim for image-based novelty detection. Despite considerable progress, existing models either fail or face a dramatic drop under the so-called "near-distribution" setting, where the differences between normal and anomalous samples are subtle. We first demonstrate existing methods experience up to 20% decrease in performance in the near-distribution setting. Next, we propose to exploit a scorebased generative model to produce synthetic near-distribution anomalous data. Our model is then fine-tuned to distinguish such data from the normal samples. We provide a quantitative as well as qualitative evaluation of this strategy, and compare the results with a variety of GAN-based models. Effectiveness of our method for both the near-distribution and standard novelty detection is assessed through extensive experiments on datasets in diverse applications such as medical images, object classification, and quality control. This reveals that our method considerably improves over existing models, and consistently decreases the gap between the near-distribution and standard novelty detection performance. Overall, our method improves the near-distribution novelty detection by 6% and passes the state-of-the-art by 1% to 5% across nine novelty detection benchmarks. The code repository is available at https://github.com/rohban-lab/FITYMI. DeepSVDD GT MHRot CSI PANDA Ours ND AUC Drop Near Novelty Detection Novelty Detection 0 10 20 30 40 50 60 70 90 80 100 ND Model ND Model</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In novelty detection (ND) <ref type="bibr" target="#b0">1</ref> , the objective is to identify test-time samples that are unlikely to come from the training distribution <ref type="bibr" target="#b0">[1]</ref>. Such samples are called anomalous, while the training set is referred to as normal. Recently, PANDA <ref type="bibr" target="#b1">[2]</ref> and CSI <ref type="bibr" target="#b2">[3]</ref> have considerably pushed state-of-the-art and achieved more than 90% AUROC on the CIFAR-10 dataset <ref type="bibr" target="#b3">[4]</ref> in the ND task, where one class is assumed to be normal and the rest are considered anomalous. However, these methods struggle to achieve a similar performance in situations where outliers are semantically close to the normal distribution. That is, they experience a performance drop when faced with such inputs. In this paper, our focus is on the near novelty detection (near-ND), which is a more challenging task and has been explored to a smaller extent. Near novelty detection has found several important practical applications in diverse areas such as medical imaging, and face liveness detection <ref type="bibr" target="#b4">[5]</ref>. <ref type="figure" target="#fig_1">Figure 1</ref>: Sensitivity of state-of-the-art anomaly detectors when differences between normal and anomalous samples are subtle. For each model, its performance in detecting far (Blue) and near (Orange) outliers has been reported. Assuming one class of CIFAR-10 is considered normal, the rest of the classes are considered far anomalies, and the semantically nearest class of CIFAR-100 to the normal class is the source of near anomalies (based on the bottom-1 metric, see sec. 2.) A considerable performance drop happens for the most recent methods, which means the learned normal boundaries are not tight enough and need to be adapted. For instance, while some might be able to distinguish a cat image from a car, they are still unable to detect tiger or lion images as anomalies.</p><p>Our first contribution is to provide a definition for the near-ND task and benchmark eight recent novelty detection methods in our near-ND setting. Our results reveal a significant performance drop of such approaches for the near-ND setting, despite their excellent results on the standard novelty detection tasks. <ref type="figure" target="#fig_1">Fig. 1</ref> compares the performance of PANDA <ref type="bibr" target="#b1">[2]</ref> and CSI <ref type="bibr" target="#b2">[3]</ref> in the near-ND and ND setups, which shows roughly a 20% AUROC drop. Furthermore, while MHRot <ref type="bibr" target="#b5">[6]</ref> performs relatively comparable to PANDA and CSI in ND, it is considerably worse in near-ND, highlighting the need for a near novelty detection benchmark.</p><p>A similar problem setup has recently been investigated in the out-of-distribution (OOD) detection domain, which is known as "near out-of-distribution" detection <ref type="bibr" target="#b6">[7]</ref>. Out-of-distribution detection and ND are closely related problems with the primary difference being that the normal data constitutes a single class in ND, and multiple classes in out-of-distribution detection. However, out-of-distribution detection models are susceptible to making false detections on anomalous samples that slightly deviate from the normal data. To cope with the challenge in the out-of-distribution detection domain, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b5">[6]</ref>, and <ref type="bibr" target="#b7">[8]</ref> employ outlier exposure techniques, i.e., exposing the model to the real outliers, available on the internet, during training. Alternatively, some approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> utilize GANs to generate outliers.</p><p>In spite of all these efforts, the issue of nearly anomalous samples has not been studied in the context of ND tasks (i.e., one-class classification). The challenge in the case of ND is that in most cases, the normal data constitutes less conceptual diversity compared with the out-of-distribution detection setup, making the uncertainty estimation challenging, especially for the nearly abnormal inputs. One has to note that some form of uncertainty estimation is required for ND and out-of-distribution detection. This makes near-ND an even more difficult task than near-OOD detection. To deal with this difficulty, we propose to utilize generative models to craft diverse anomalous data. Unfortunately most generative models suffer from (1) instability in the training phase, (2) poor performance on high-resolution images, and (3) low diversity of generated samples <ref type="bibr" target="#b10">[11]</ref>. These challenges have prevented their effective use in ND.</p><p>To address the mentioned challenges, we propose to use a "non-adversarial" anomaly data generation method. Hence, our second contribution is shedding light on the capabilities of the recently proposed SDE-based generative models <ref type="bibr" target="#b11">[12]</ref>, in the ND domain. By providing comprehensive experiments and visualizations, we show that a prematurely trained SDE-based model can generate diverse and non-noisy near-outliers, which considerably beat samples that are generated by GANs or obtained from the available datasets. Interestingly, the methods that employ outlier exposure, such as <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b7">[8]</ref>, show a large performance drop of roughly 15% when faced with our generated anomalous samples. This validates our assertion that the outlier exposure technique is incapable of detecting close outlier samples. Finally, our last contribution is to show that fine-tuning simple baseline ND methods with the generated samples to distinguish them from the normal data leads to a performance boost for both ND and near-ND. The nine benchmark datasets span a wide variety of applications and anomaly granularity. Our method achieves state-of-the-art results in the ND setting, and is especially effective for the defined near-ND setting, where we improve over existing work by a large margin up to 6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Near Novelty Detection Definition</head><p>The problem of Novelty Detection for one-class classification is defined using the following setup. Consider X and Y be the input and output spaces respectively and let P be a distribution over X ? Y. Suppose F to be a neural network trained on the samples drawn from P to output a feature representation, which is used to predict the novelty score of an input sample. Let D Normal to be the marginal distribution of P for X . ND aims to define a decision function G, demonstrating the following behavior for an arbitrary given test input x ? X :</p><formula xml:id="formula_0">G(x, F) = 0 if x ? D Normal, 1 otherwise.<label>(1)</label></formula><p>For ND |Y| is set to 1, while for out-of-distribution detection and open-set recognition is &gt; 1.</p><p>To provide a standard benchmark for the near-ND task there is a need to define a one-class distribution closeness score. Suppose a K-class training dataset is given from which a normal class C i is randomly sampled and used to train the model M. The closest abnormal distribution with respect to the selected normal class for M can be defined as C j = C i that minimizes the test time performance when it is considered as the abnormal distribution. We call the performance in the mentioned scenario the bottom-1 score of class C i with the backbone M. However, this score depends on the choice of M, making it specific for every problem setup and method. Therefore, inspired by the CLP criterion introduced in <ref type="bibr" target="#b12">[13]</ref> as a measurement of dataset distance, we could use CLP as an alternative closeness score for the novelty detection task.</p><p>Given a K-class training dataset, one category is randomly selected as the normal distribution. Then, a supervised classifier P is trained on the rest K ? 1 abnormal categories. Assume x to be a training sample of the selected normal class. The closeness score of each abnormal class i with respect to the normal class is obtained as follows:</p><formula xml:id="formula_1">Closeness score i = x?Normal Class P(? = i|x).<label>(2)</label></formula><p>The higher the closeness score of an abnormal category, the more similar it is to the normal class. The same situation also holds when abnormal categories are selected from another dataset. <ref type="figure" target="#fig_1">Fig. 1</ref> in the Appendix indicates a decent correlation between the bottom-1 score and closeness score, implying that our proposed criterion can be considered as a proxy for the ideal score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Near-Novelty Detection Method</head><p>We introduce a two-step training approach, which even can be employed to boost the performance of most of the existing SOTA models. Following the current trend in the field, we start with a pre-trained feature extractor as <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b1">2]</ref> have shown their effectiveness. We use a ViT <ref type="bibr" target="#b15">[16]</ref> backbone since <ref type="bibr" target="#b6">[7]</ref> has demonstrated its superiority on the near out-of-distribution detection. In the first step, a fake dataset is generated by a SDE-based model. We quantitatively and qualitatively show that generated Training Set Feature Bank</p><p>Before Fine-Tuning :</p><p>After Fine-Tuning : <ref type="figure">Figure 2</ref>: Overview of our framework for near distribution novelty detection. In the first step, the SDE generates semantically close outliers. As it is shown, they have very subtle yet semantic differences with the normal data distribution. After that, using a linear layer, a pre-trained feature extractor is fine-tuned to solve a binary classification between the normal and abnormal inputs. This modifies the normal boundaries toward including more distinctive features. Finally, normal embeddings are stored and used to compute the k-NN distance at the test time.</p><p>fake outliers are high-quality, diverse, and yet with semantic differences compared to normal inputs. In the second step, the pre-trained backbone is fine-tuned by the generated dataset and given normal training samples through solving a binary classification loss. Finally, all the normal training samples are passed to the fine-tuned feature extractor, and their embeddings are stored in a memory, which is further used to obtain the k-NN distance of each test input sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Phase</head><p>Step 1 : Near Anomaly Generation In order to generate near anomalous samples, the SDE-based model proposed in <ref type="bibr" target="#b11">[12]</ref> is adapted for the novelty detection task. In the SDE, the gradient of the logarithmic probability density of the data with respect to the input is called the score. A SDE model considers a continuum of distributions that develop over time in accordance with a diffusion process, rather than perturbing the data with a restricted set of noise distributions. The model is trained to learn how to gradually transform the noisy image back to its original form. The procedure that gradually transforms a data point into random noise is controlled by a pre-determined stochastic differential equation that does not contain any trainable parameters. Finally, inverting this process allows us to seamlessly transform random noise into the data that could be used for sample generation as follows:</p><formula xml:id="formula_2">x n?1 ? x n = [f (x, t) ? g 2 (t)? x log p t (x)]dt + g(t)d? x T ? N (0, 1),<label>(3)</label></formula><p>where f and g are called the drift and diffusion coefficients respectively. For more details see <ref type="bibr" target="#b11">[12]</ref>. The main benefit of using a score-based generative model is that in the backward process (Eq. 3), the score function denoises the input gradually that results in a relatively smooth decline in the FID across training epochs. This enables us to stably produce near anomalous samples based on the FID score of the generated outputs, which could be achieved by stopping the training process earlier than the stage where the model achieves its maximum performance. This is empirically assessed in <ref type="figure">Fig.  3</ref>. Note that a totally different training trajectory is obtained in GANs, where the FID oscillates and premature training does not necessarily produce high-quality anomalous samples. That is why methods like OpenGAN rely on a validation outlier dataset to determine where to stop.</p><p>Step 2 : Feature Fine-tuning and Collection Having generated a high-quality fake dataset, a lightweight projection head is added to the feature extractor, and the whole network is trained to solve a binary classification task between the given normal and abnormal inputs. This way, normal class boundaries are tightened and adjusted according to the abnormal inputs. After the training, all the normal embeddings are stored in the memory M that is used at the test time for assigning abnormality score to a given input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Testing Phase</head><p>At the test time, for each input x, its k nearest neighbours are found in the M, which are shown by</p><formula xml:id="formula_3">m 1 x , m 2 x ,..., m k x .</formula><p>Finally, the novelty score is found as follows:</p><formula xml:id="formula_4">Novelty Score(x) = k n=1 x ? m n x 2<label>(4)</label></formula><p>As the equation shows, the more the novelty score the more likely an input to be anomaly. We use k = 2 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Settings</head><p>Training Details. We use a ViT-B_16 as the feature extractor (pretrained on ImageNet 21k), learning rate = 4e-4, weight decay = 5e-5, batch size = 16, optimizer = SGD, a linear head with 2 output neurons. We freeze the first six layers of the network and the rest is fine-tuned till convergence for all the experiments. For the data generation phase, we use the SDE-based model explained in <ref type="bibr" target="#b11">[12]</ref> with exactly the same setup. The inputs are resized to 224, and the data generation stops on FID ? 40 and FID ? 200 for low and high-resolution datasets. All the results in our tables are either reported from the reference papers or run by us using their official repositories.</p><p>Datasets. Following previous works, we evaluate the methods on standard datasets CIFAR-10 <ref type="bibr" target="#b3">[4]</ref>, CIFAR-100 <ref type="bibr" target="#b3">[4]</ref> and extend the results on small and fine-grained datasets FGVC-Aircraft <ref type="bibr" target="#b16">[17]</ref>, Birds <ref type="bibr" target="#b17">[18]</ref>, Flowers <ref type="bibr" target="#b18">[19]</ref>, Stanford-Cars <ref type="bibr" target="#b19">[20]</ref>, MVTecAD <ref type="bibr" target="#b20">[21]</ref>, WBC <ref type="bibr" target="#b21">[22]</ref>, and Weather <ref type="bibr" target="#b22">[23]</ref>. For the full dataset descriptions and details see the Appendix. Following the standard ND protocol, multi-class datasets are converted into an anomaly detection task by setting a class as normal and all other classes as anomalies. This is performed for all classes, in practice turning a single dataset with C classes into C datasets <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b1">2]</ref>. For the Near-ND setup, each class of CIFAR-10 (C 10 i ) and its closest class in CIFAR-100 (C 100 i ) are selected. Then the model is trained on the training samples of (C 10 i ) and tested against the aggregation of (C 100 i ) and (C 10 i ) test sets. Finally, the average AUROC result across the dataset is reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Studies</head><p>Sensitivity to Backbone Replacements. <ref type="table" target="#tab_1">Table 1</ref> shows our method sensitivity to the backbone replacements. All the backbones are obtained and used as in <ref type="bibr" target="#b6">[7]</ref>. The results are provided for both the ND and near-ND evaluation setups. For each backbone, the performance with and without the feature fine-tuning phase is reported, showing a consistent improvement across different architectures. The performance boost is surprisingly large for some backbones such as ResNet-152, ViT-B_16, and R50+ViT-B_16 with roughly 16% , 25%, and 36% boosts in the near-ND setting. Moreover, ViT-B_16 and R50+ViT-B_16 perform roughly 8% and 16% better after the fine-tuning in the ND setup, showing the generality of our approach regardless of the setting or backbone. This mainly happens because of the diverse yet semantically close generated outliers that are used to modify the learned normal class boundaries. SDE vs. GAN. In this experiment, the one-vs-all setting is employed on the WBC dataset. <ref type="table" target="#tab_2">Table  2</ref> shows clear superiority of using a SDE-based model compared to the well-known StyleGAN2-ADA <ref type="bibr" target="#b24">[25]</ref> with 4% to 10% better performance based on the chosen normal class. Evidently, employing our fake samples generated by the SDE is almost always beneficial in spite of the ones generated Having artifacts, mode-collapse, and undiversified generated samples could be the reasons behind this observation, which are shown in Appendix <ref type="figure">(Fig. 2</ref>). This highlights that not all the generative models are beneficial for the ND and near-ND tasks, and they need to be carefully selected based on the constraints defined in such domains.  <ref type="table" target="#tab_3">Table 3</ref> shows the performance stability with respect to the number of nearest neighbours for both the ND and near-ND setups. Clearly, the method is barely sensitive to this parameter. DN2 <ref type="bibr" target="#b25">[26]</ref> has provided extensive experiments on the effectiveness of applying k-NN to the pre-trained features for the AD task, claiming k = 2 is the best choice. We also observe that the same trend happens in our experiments. Similarly, the method is robust against </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stopping Point</head><p>Steps 400 <ref type="figure">Figure 3</ref>: The process of abnormal data generation utilizing the SDE model is shown. On the left, the fake samples evolve during the generation phase. Thus, obtaining high-quality samples is feasible by early stopping. On the right, some generated samples of different datasets as well as their corresponding real anomalies are shown.</p><p>the stopping point with at most 2% variation for reasonable FID scores, making it practical for the real-world applications. CIFAR-10-FSDE Benchmark. To further show the quality of the proposed generated dataset, we evaluated the performance of some of the most outstanding methods when the generated dataset is considered as anomalous at the test time. We call the dataset CIFAR-10-FSDE. For the ND methods such as <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b2">3]</ref>, one class of CIFAR-10 is randomly sampled as normal and the test set is made using the corresponding test samples of the normal class and a random subset of the fake dataset with the same size as anomalies. For the OOD detection methods such as <ref type="bibr" target="#b5">[6]</ref> the entire CIFAR-10 is considered normal and a subset of the fake dataset equal to the entire CIFAR-10 test set forms anomalies.</p><p>As <ref type="table" target="#tab_4">Table 4</ref> represents, most of the methods significantly fail to detect our fake samples, which is particularly the case for PANDA <ref type="bibr" target="#b1">[2]</ref> with 40% performance drop. Surprisingly, OE and DeepSAD with roughly 15% and 30% performance decrease do not work decently despite being exposed to real outliers during the training process. It is worth noticing that CSI <ref type="bibr" target="#b2">[3]</ref> shows the least performance drop roughly 7%, revealing that SSL-based methods produce high quality features compared to either pre-trained or semi-supervised approaches in the ND task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparisons with state-of-the-art</head><p>In this section, we compare our method with the most outstanding SOTAs methods in the ND and NND setups. we have used the official implementations and produced the results for all the methods in the near-ND setup. To provide a fair comparison, we compare our method to all the recently proposed methods regardless of being published. For instance, Transformaly and MSAD are not published yet while obtaining the SOTA results. All the experiments are performed 10 times and the average is reported.</p><p>Comparison on Standard Datasets. We compare our approach with the top current self-supervised and pre-trained feature adaptation methods in the ND <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b5">6]</ref>. Results that were reported in the original papers were copied. When the results were not reported in the original papers, we ran the experiments (where possible). <ref type="table" target="#tab_5">Table 5</ref> shows our method surpasses the previous state-of-the-art on the standard benchmark while passing it by roughly 6% in the near-ND setup. Note that except CSI, all the other SOTA approaches employ a pre-trained architecture, particularly Transformaly <ref type="bibr" target="#b26">[27]</ref>, that utilizes a ViT backbone. However, due to the feature fine-tuning phase of our approach, the borders of the normal samples become well-structured, thus better representing the normal semantic features.</p><p>Small and Fine-grained Datasets. To further represent the effectiveness of our approach, it is evaluated on harder tasks such as fine-grained novelty detection and small datasets that do not contain enough normal diversity. As <ref type="table" target="#tab_5">Table 5</ref> shows, we achieve SOTA results on Birds, Flowers, Stanford-Cars, and MVTecAD while passing it by a large margin of 4.7% compared to Transformaly and 10% compared to PANDA on Airplane. We also considerably pass the SOTA by roughly 4% on both WBC and Weather datasets.</p><p>As <ref type="table" target="#tab_5">Table 5</ref> shows, our proposed method is effective for this setup and improves the SOTA by roughly 6% compared to Transformaly and 14% compared to PANDA. To further validate our method, we adapt OpenGAN to novelty detection and report its near novelty detection results. Surprisingly, even by employing fake abnormal samples generated by a GAN and being exposed to real outliers, it is not able to achieve a decent performance, which again supports the benefits of our proposed approach for generating near distribution abnormal samples.</p><p>Synthetic Novelty Detection. While the anomalous data generated by StyleGAN does not fit the fine-tuning phase due to the mentioned problems, they contain lots of artifact-full and distorted samples that a practical novelty detector is expected to detect. <ref type="table" target="#tab_6">Table 6</ref> compares our method with other SOTA methods. Due to learning more distinguishing features, we pass the SOTA by 12%, showing the applicability of our approach in detecting a wide range of anomalies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Outlier Exposure Based Approaches Utilizing the fake data for the novelty detection task has previously been considered. The general idea is to employ synthetic images, which may be generated by GANs, to augment the training set <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>. In the case of open-set recognition, <ref type="bibr" target="#b8">[9]</ref> proposed OpenGAN, which adverserially generates fake open-set images. The discriminator is then utilized at the test time for the novelty detection. The key point in OpenGAN is that a tiny additional dataset, containing both in-and outliers, is used as a validation set for the model selection. This, also known as outlier exposure, is necessary due to the GAN unstable training. Along the same line of work, <ref type="bibr" target="#b9">[10]</ref> considered generation of fake data in the one-class setting. Specifically, the fake anomalous data is generated through early stopping of a GAN that is trained on the normal samples. They subsequently train a binary classifier on the augmented dataset, to discriminate the normal and generated fake samples. This classifier is later utilized for anomaly detection at the inference time. Even though these models can work well on simple datasets, they cannot handle more complex datasets and cannot detect hard near-distribution anomalies.</p><p>It is shown in <ref type="bibr" target="#b6">[7]</ref> that by exploiting the pre-trained vision transformers that are fine-tuned on the indistribution dataset, one can boost near out-of-distribution detection results. They also demonstrated that additional outlier samples improve results even further. Finally, it has to be noted that similar relevant concepts to the near out-of-distribution detection has been explored in other fields e.g. adversarial robust training <ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref>.</p><p>Self-supervised learning Many studies have shown that Self-supervised methods can extract meaningful features that could be exploited in the anomaly detection tasks such as MHRot <ref type="bibr" target="#b5">[6]</ref>. GT <ref type="bibr" target="#b35">[36]</ref> uses geometric transformations including flip, translation, and rotation to learn normal features for anomaly detection. Alternatively, puzzle solving <ref type="bibr" target="#b36">[37]</ref>, and Cut-Paste <ref type="bibr" target="#b37">[38]</ref> are proposed in the context of anomaly detection. It has recently been shown that contrastive learning can also improve anomaly detection <ref type="bibr" target="#b2">[3]</ref>, which encourages the model to learn normal features by contrasting positive, and certain negative samples. The authors claim that negative samples should be made by applying certain transformations that tend to drastically change the data distribution (e.g. rotation).</p><p>Pre-trained Methods Several works used pre-trained networks as anomaly detectors. Intuitively, abnormal and normal data do not overlap in the feature space because many features model high-level and semantic image properties. So, in the pre-trained feature space, one may classify normal vs. anomaly. <ref type="bibr" target="#b25">[26]</ref> used the k-nearest neighbors distance between the test input and training set features as an anomaly score. <ref type="bibr" target="#b38">[39]</ref> trained a GMM on the normal sample features, which could then identify anomalous samples as low probability areas. PANDA <ref type="bibr" target="#b1">[2]</ref> attempts to project the pre-trained features of the normal distribution to another compact feature space employing the DSVDD <ref type="bibr" target="#b39">[40]</ref> objective function. In <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref>, the pre-trained model is a teacher and the student is trained to mimic the teacher's behavior solely on the normal samples. Then, the discrepancy between the teacher and student networks reveal anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Social Impacts</head><p>The goal of this study is to increase the safety of modern machine learning models. This will benefit a wide range of fields and activities in society. We believe that novelty detection is used in a variety of areas, including financial services (for example, fraud detection), manufacturing (for example, failure detection), and health care (e.g., unseen disease identification). We intend to offer machine learning researchers an easy-to-use tool that provides safety against a wide range of anomalous data in the real world applications through our study and the release of our code. While we do not expect our effort to have any negative implications, we hope to continue to build on our method in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we revealed a key weakness of existing SOTA novelty detection methods, where they fail to identify anomalies that are slightly different from the normal data. We proposed to take a non-adaptive generative modeling that allows controllable generation of anomalous data. It turns out that SDEs fit this specification much better compared to other SOTA generative models such as GANs.</p><p>The generated data levels up pre-trained distance-based novelty detection methods not only based on their AUROCs in the standard ND setting but also in the near-ND setting. The improvements that are made by our method are consistent across wide variety of datasets, and choices of the backbone model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A Near-ND</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Comparison between bottom-1 and CLP</head><p>In section 2, we proposed the closeness score (CLP) to find the closest abnormal class for each class of the normal dataset. In our experiments, we used the ViT-B_16 (pretrained on ImageNet-21K) as the backbone used for extracting the closest abnormal classes based on this criterion. The bottom-1 abnormal class is the class that has the lowest novelty detection performance during the testing phase. Note that the abnormal classes chosen based on the CLP criterion do not necessarily have the worst novelty detection performance; therefore, it is expected for the novelty detection methods to perform better on the CLP criterion. This section aims to investigate how well these two criteria match each other. In the <ref type="table" target="#tab_7">Table 7</ref>, for each novelty detection model and every class of the CIFAR-10 dataset, the bottom-1 class is shown. Furthermore, the last row indicates the closest abnormal class selected based on the CLP criterion. As shown in the <ref type="table" target="#tab_7">Table 7</ref>, the extracted classes based on the CLP criterion are the same or conceptually similar to the respected bottom-1 classes. <ref type="figure" target="#fig_1">Figure Figure 1</ref> also shows a decent correlation between the bottom-1 score and the CLP score, which means both these criteria could be used to extract near-distribution abnormal classes, and that the CLP could be used as a proxy to the bottom-1 criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUROC(%)</head><p>OC-   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Bottom-i Classes as the Abnormal Distribution</head><p>The Bottom-i means averaging the AUROCs for the i abnormal classes that have the lowest AUROCs, e.g. bottom-100 in the case of CIFAR-100 denotes averaging the AUROC results for all 100 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottom-i AUROC(%)</head><p>1 <ref type="figure">Figure 2</ref>: By increasing i, the AUROCs of the models improve due to the presence of more anomalies that are further from the boundary. As expected, in most cases, the gap between the performance of models has become smaller. Furthermore, in the case of i = 1, in which anomalies constitute the near distribution ones, the models' performances vary greatly. In this case, our proposed method achieves SOTA results by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Per-Class Results</head><p>In this section, we provide our model's performance for every class of the CIFAR-10 and CIFAR-100 datasets.   <ref type="figure">Figure 3</ref>: For each class of CIFAR-10, nearest class from CIFAR-100 has been provided, according to the closeness score. In each 3-row panel, the first row is assumed as normal, the second one is assumed the nearest abnormal class, and the third row indicates the corresponding generated fake images using the SDE model.  <ref type="figure">Figure 4</ref>: For each class of CIFAR-10, nearest class from CIFAR-100 has been provided, according to the closeness score. In each 3-row panel, the first row is assumed as normal, the second one is assumed the nearest abnormal class, and the third row indicates the corresponding generated fake images using the SDE model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B The SDE-based Generative Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 SDE vs. Other Generative Models</head><p>In this section, according to our main setting, we have replaced the SDE with other generative models, and sampled the images generated by these models before convergence. Because in our setup, only one class (the normal class) is available for the training of the generative model. As the sample size becomes limited in such a setting, model convergence and generation of high quality real samples become a challenge. Even under such circumstances, the SDE-based models converge. Furthermore, such models can eventually generate high quality real images. However, other generative models do not converge properly, and the images that are sampled using the early stopped model are often noisy and contain artifacts. As opposed to other generative methods, the proposed premature training of the SDE yields authentic, near-distribution, diverse, and artifact-free images. This makes it a better choice for the near-ND setup. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 CIFAR-10-FSDE</head><p>These images are randomly plotted for each of the classes in the proposed dataset, CIFAR-10-FSDE, generated through an early stopped SDE. As can be seen, the images are clearly different semantically from the normal class. Despite this semantic difference, most novelty detection models have a poor performance in the detection of these images as anomalous. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Airplane</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Overview of Generated Anomaly Images on Various Datasets</head><p>Each row is a class on which the SDE model is trained. The SDE model is trained on higher number of iterations in moving from the left to the right columns. The rightmost column contains a real images. <ref type="figure">Figure 8</ref>: Generated anomaly images on 102 category of the Flowers dataset. The images highlighted in green are normal samples.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Dataset Descriptions</head><p>The setting is reported from <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standard Datasets</head><p>We evaluate our method on a set of commonly used datasets: CIFAR-10 consists of RGB images of 10 object classes. CIFAR-100: we use the coarse-grained version that consists of 20 classes.</p><p>Small datasets To further extend our results, we compared the methods on a number of small datasets from different domains: 102 Category Flowers and Caltech-UCSD Birds 200. For each of these datasets, we evaluated the methods using only each of the first 20 classes as normal, and using the entire test set for the evaluation. For FGVC-Aircraft, due to the extreme similarity of the classes to each other, we randomly selected a subset of ten classes from the entire dataset, such that no two classes have the same Manufacturer. Following are the selected classes: [91,96,59,19,37,45 ,90,68,74,89].</p><p>MvTecAD: This dataset contains 15 different industrial products, with normal images of the proper products for training and 1 to 9 types of manufacturing errors as anomalies. The anomalies in MvTecAD are in-class, i.e. the anomalous images come from the same class of the normal images with subtle variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symmetric datasets</head><p>We evaluated our method on datasets that contain symmetries, such as images that have no preferred angle (microscopy, aerial images): WBC : we used the 4 big classes in "Dataset 1" of the microscopy images of white blood cells, with a 80%/20% train-test split.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The performance of novelty detection methods (AUROC %) in the near-ND and ND settings. In the near-ND setting, results are reported according to both criteria, i.e. CLP and bottom-1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Generated anomalous samples by different generative models on the FGVC-Aircraft dataset. Generated anomalous samples by different generative models on the WBC dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>An overview of images in the synthetic CIFAR10-FSDE dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Generated anomaly images on the StanfordCars dataset. The images highlighted in green are normal samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Generated anomaly images on the Caltech-UCSD Birds 200 dataset. The images highlighted in green are normal samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Generated anomaly images on the MVTecAD dataset. The images highlighted in green are normal samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Near Anomaly Generation Feature Fine-Tuning</head><label></label><figDesc></figDesc><table><row><cell>Training</cell><cell></cell><cell>Testing</cell></row><row><cell></cell><cell>Feature Collection</cell><cell>Test Samples</cell></row><row><cell>Normal Training Samples</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Fine-tuned Feature Extractor</cell></row><row><cell>Training Set</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Encoded Inputs:</cell></row><row><cell>Feature Extractor</cell><cell>Normal Training Samples</cell><cell></cell></row><row><cell></cell><cell></cell><cell>K-Nearest Neighbor Search</cell></row><row><cell>Generator Early Stopped SDE-based</cell><cell>Fine-tuned Feature Extractor</cell><cell>Training Set</cell></row><row><cell></cell><cell></cell><cell>Feature Bank</cell></row><row><cell></cell><cell>Extracted Features:</cell><cell></cell></row><row><cell>Generated Anomaly Data</cell><cell></cell><cell>Novelty Score</cell></row><row><cell>Binary Classifier</cell><cell>. . . .</cell><cell>KNN Distance:</cell></row><row><cell>BCE Loss</cell><cell></cell><cell></cell></row><row><cell>Normal / Anomaly</cell><cell>[Collecting]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The effectiveness of our approach over different backbones (in AUROC %) in the ND (far) and near-ND (near) settings. The results show consistent improvements regardless of the backbone or setting that is used. Pretrained on ImageNet 21K and finetuned on ImageNet 1K. by StyleGAN2-ADA. Interestingly, StyleGAN samples are harmful to the performance in class 2, significantly reducing the performance by 7%.</figDesc><table><row><cell>Setting</cell><cell>Dataset</cell><cell>Training</cell><cell></cell><cell></cell><cell>Models</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">ResNet-152 ViT-B_16 * ViT-B_32  ? R50+ViT-B_16 * ConvNeXt-B  ?</cell></row><row><cell>ND</cell><cell>CIFAR-10</cell><cell>Before After</cell><cell>92.5 95.3</cell><cell>91.0 99.1</cell><cell>95.3 98.3</cell><cell>82.2 98.8</cell><cell>97.1 97.8</cell></row><row><cell cols="2">near-ND CIFAR-10vs100</cell><cell>Before After</cell><cell>58.5 74.7</cell><cell>65.5 90.0</cell><cell>66.3 78.9</cell><cell>50.0 85.9</cell><cell>71.5 81.1</cell></row></table><note>* Pretrained on ImageNet 21K.?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of our model performance (in AUROC %) upon using different generative models for each class of the WBC dataset. The performance of the model base backbone is also included to measure the effectiveness of the training process. Score-based data generation considerably passes the GAN-based SOTA in the novelty detection task.</figDesc><table><row><cell cols="2">Dataset Generative Model</cell><cell></cell><cell></cell><cell>Labels</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>Mean</cell></row><row><cell></cell><cell>Base Backbone</cell><cell cols="4">88.8 96.8 70.2 76.6 83.1</cell></row><row><cell>WBC</cell><cell cols="5">StyleGAN2-ADA 90.5 89.7 82.4 78.6 85.3</cell></row><row><cell></cell><cell>SDE</cell><cell cols="4">94.8 96.5 87.8 88.9 91.2</cell></row></table><note>Sensitivity to k-NN and Stopping Point.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The sensitivity of the proposed method to the k-NN parameter and stopping point based on the FID score.</figDesc><table><row><cell>Setting</cell><cell>Dataset</cell><cell>KNN</cell><cell></cell><cell></cell><cell cols="2">Stopping Point</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">k=1 k=2 k=5 k=10 k=50 300&lt;FID 100&lt;FID&lt;200 30&lt;FID&lt;50 FID&lt;20</cell></row><row><cell>ND</cell><cell>CIFAR-10</cell><cell>99.0 99.1 98.9 98.9</cell><cell>98.7</cell><cell>96.8</cell><cell>97.7</cell><cell>99.1</cell><cell>92.6</cell></row><row><cell cols="3">near-ND CIFAR-10vs100 89.8 90.0 90.0 90.0</cell><cell>89.7</cell><cell>82.7</cell><cell>87.1</cell><cell>90.0</cell><cell>68.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell cols="2">OOD (Multi-Class)</cell><cell></cell><cell></cell><cell cols="2">ND (One-Class)</cell><cell></cell></row><row><cell></cell><cell cols="4">OpenGAN  ? OE  ? DeepSAD  ? GT</cell><cell cols="4">CSI PANDA * MSAD * Transformaly *</cell></row><row><cell>CIFAR-10-FSDE</cell><cell>88.4</cell><cell>78.2</cell><cell>53.4</cell><cell cols="2">60.5 87.4</cell><cell>57.7</cell><cell>64.1</cell><cell>75.0</cell></row></table><note>The performance of novelty detection methods (AUROC %) on our Cifar-10-FSDE dataset that consists of images generated using the proposed method on the CIFAR-10 dataset. Each model is trained according to its corresponding setting, except its test-time anomalies are replaced by our fake samples.* Pretrained on ImageNet.? Require extra Dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The performance of novelty detection methods (AUROC %) in the ND setting on various datasets. Our method achieves the SOTA or considerably pass it on almost all the datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(a) ND Setting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Setting</cell><cell>Datasets</cell><cell cols="3">From Scratch</cell><cell></cell><cell cols="3">Pre-trained</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>CSI</cell><cell cols="2">PANDA</cell><cell>MSAD</cell><cell></cell><cell>Transformaly</cell><cell cols="2">Ours</cell></row><row><cell></cell><cell></cell><cell cols="6">(ResNet-18) (ResNet-152) (ResNet-152)</cell><cell>(ViT-B_16)</cell><cell cols="2">(ViT-B_16)</cell></row><row><cell></cell><cell>Birds</cell><cell></cell><cell>52.4</cell><cell></cell><cell>95.3</cell><cell>96.7</cell><cell></cell><cell>97.8</cell><cell>98.5</cell><cell></cell></row><row><cell></cell><cell>Stanford-Cars</cell><cell></cell><cell>66.5</cell><cell></cell><cell>87.6</cell><cell>89.1</cell><cell></cell><cell>86.7</cell><cell>89.8</cell><cell></cell></row><row><cell></cell><cell>CIFAR-10</cell><cell></cell><cell>94.3</cell><cell></cell><cell>96.2</cell><cell>97.2</cell><cell></cell><cell>98.3</cell><cell>99.1</cell><cell></cell></row><row><cell></cell><cell>CIFAR-100</cell><cell></cell><cell>89.6</cell><cell></cell><cell>94.1</cell><cell>96.4</cell><cell></cell><cell>97.3</cell><cell>98.1</cell><cell></cell></row><row><cell>ND</cell><cell>Flowers</cell><cell></cell><cell>60.8</cell><cell></cell><cell>94.1</cell><cell>96.5</cell><cell></cell><cell>99.9</cell><cell>99.9</cell><cell></cell></row><row><cell></cell><cell>FGVC-Aircraft</cell><cell></cell><cell>64.6</cell><cell></cell><cell>77.7</cell><cell>79.8</cell><cell></cell><cell>84.0</cell><cell>88.7</cell><cell></cell></row><row><cell></cell><cell>MvTecAD</cell><cell></cell><cell>63.6</cell><cell></cell><cell>86.5</cell><cell>87.2</cell><cell></cell><cell>87.9</cell><cell>86.4</cell><cell></cell></row><row><cell></cell><cell>WBC</cell><cell></cell><cell>50.4</cell><cell></cell><cell>87.4</cell><cell>87.0</cell><cell></cell><cell>85.1</cell><cell>91.2</cell><cell></cell></row><row><cell></cell><cell>Weather</cell><cell></cell><cell>91.5</cell><cell></cell><cell>81.5</cell><cell>92.4</cell><cell></cell><cell>94.3</cell><cell>97.0</cell><cell></cell></row><row><cell>Setting</cell><cell>Dataset</cell><cell></cell><cell></cell><cell>From Scratch</cell><cell></cell><cell></cell><cell></cell><cell>Pre-trained</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="9">GT MHRot CSI one-class OpenGan DN2 PANDA MSAD Transformaly Ours</cell></row><row><cell>ND</cell><cell>CIFAR-10</cell><cell>86.0</cell><cell>90.1</cell><cell>94.3</cell><cell>83.6</cell><cell>92.5</cell><cell>96.2</cell><cell>97.2</cell><cell>98.3</cell><cell>99.1</cell></row><row><cell cols="3">near-ND CIFAR-10vs100 62.6</cell><cell>63.1</cell><cell>76.1</cell><cell>50.0</cell><cell>58.5</cell><cell>76.8</cell><cell>79.5</cell><cell>84.1</cell><cell>90.0</cell></row></table><note>(b) The performance of the SOTA novelty detection methods compared to ours in AUROC % in the near-ND and ND settings. In the near-ND setup, the normal class is considered a class of CIFAR-10, and the closest abnormal distribution in CIFAR-100 forms the abnormal distribution. OpenGan is adapted for the one-class setting. While Transformaly, PANDA, and CSI achieve very close results in the ND setup, they work significantly differently in the near-ND setting, highlighting the importance of having an near-ND benchmark.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>The performance of SOTA methods on the samples generated by StyleGAN2. The results reveal that regardless of employing an SSL loss with heavy augmentations or pre-trained features, fine-tuning the model with near outliers better forms the normal boundaries.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Methods</cell><cell></cell><cell></cell></row><row><cell cols="5">CSI PANDA MSAD Transformaly Ours</cell></row><row><cell>Cifar-10-FSGAN 83.1</cell><cell>66.1</cell><cell>69.4</cell><cell>82.2</cell><cell>95.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>For each model and every class of the CIFAR-10, the anomalous class that minimizes the anomaly detection performance among 100 classes of the CIFAR-100 dataset is reported. The last row is the CIFAR-100 class that achieves the highest CLP score. Note that the latter does not depend on the anomaly detection method.</figDesc><table><row><cell>Method</cell><cell>Airplane</cell><cell>Automobile</cell><cell>Bird</cell><cell>Cat</cell><cell>Deer</cell><cell>Dog</cell><cell>Frog</cell><cell>Horse</cell><cell>Ship</cell><cell>Truck</cell></row><row><cell>DeepSVDD</cell><cell>Plain</cell><cell>Plain</cell><cell>Plain</cell><cell>Cloud</cell><cell>Plain</cell><cell>Mountain</cell><cell>Plain</cell><cell>Plain</cell><cell>Plain</cell><cell>Plain</cell></row><row><cell>GT</cell><cell>Mountain</cell><cell>Pickup_Truck</cell><cell>Camel</cell><cell>Fox</cell><cell>Elephant</cell><cell>Bear</cell><cell>Crocodile</cell><cell>Elephant</cell><cell>Train</cell><cell>Bus</cell></row><row><cell>MHRot</cell><cell>Plain</cell><cell>Pickup_Truck</cell><cell>Camel</cell><cell>Fox</cell><cell>Cattle</cell><cell>Fox</cell><cell>Crocodile</cell><cell>Elephant</cell><cell>Sea</cell><cell>Pickup_Truck</cell></row><row><cell>one_class OpenGan</cell><cell>Plain</cell><cell>Plain</cell><cell>Plain</cell><cell>Wardrobe</cell><cell>Forest</cell><cell cols="3">Telephone Aquarium_Fish Oak_Tree</cell><cell>Whale</cell><cell>Plain</cell></row><row><cell>CSI</cell><cell cols="2">Pickup_Truck Pickup_Truck</cell><cell>Camel</cell><cell>Fox</cell><cell>Cattle</cell><cell>Cattle</cell><cell>Dinosaur</cell><cell>Cattle</cell><cell cols="2">Pickup_Truck Pickup_Truck</cell></row><row><cell>DN2</cell><cell>Plain</cell><cell cols="2">Pickup_Truck Willow_Tree</cell><cell>Fox</cell><cell>Forest</cell><cell>Fox</cell><cell>Forest</cell><cell>Cattle</cell><cell>Sea</cell><cell>Pickup_Truck</cell></row><row><cell>PANDA</cell><cell>Dolphin</cell><cell>Pickup_Truck</cell><cell>Kangaroo</cell><cell>Fox</cell><cell>Kangaroo</cell><cell>Raccoon</cell><cell>Crocodile</cell><cell>Cattle</cell><cell>Bridge</cell><cell>Pickup_Truck</cell></row><row><cell>MSAD</cell><cell>Cloud</cell><cell>Pickup_Truck</cell><cell>Kangaroo</cell><cell>Fox</cell><cell>Kangaroo</cell><cell>Fox</cell><cell>Beaver</cell><cell>Cattle</cell><cell>Sea</cell><cell>Pickup_Truck</cell></row><row><cell>Transformaly</cell><cell>Cloud</cell><cell>Pickup_Truck</cell><cell>Kangaroo</cell><cell>Rabbit</cell><cell>Kangaroo</cell><cell>Wolf</cell><cell>Lizard</cell><cell>Cattle</cell><cell>Sea</cell><cell>Pickup_Truck</cell></row><row><cell>Ours</cell><cell>Tank</cell><cell>Pickup_Truck</cell><cell>Camel</cell><cell>Wolf</cell><cell>Kangaroo</cell><cell>Wolf</cell><cell>Lizard</cell><cell>Camel</cell><cell>Streetcar</cell><cell>Pickup_Truck</cell></row><row><cell>CLP</cell><cell>Rocket</cell><cell>Pickup_Truck</cell><cell>Shrew</cell><cell>Leopard</cell><cell>Cattle</cell><cell>Rabbit</cell><cell>Lizard</cell><cell>Cattle</cell><cell>Bridge</cell><cell>Bus</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>The performance of novelty detection methods (AUROC %) in the near-ND setting. For each method and every normal class, both results have been reported according to the CLP and bottom-1 metrics.</figDesc><table><row><cell>Method</cell><cell>Metric</cell><cell cols="7">Airplane Automobile Bird Cat Deer Dog Frog Horse Ship Truck Mean</cell></row><row><cell>DeepSVDD</cell><cell>bottom-1 CLP</cell><cell>17.2 52.1</cell><cell>24.0 52.3</cell><cell>22.1 34.0 21.5 25.1 25.8 53.9 64.7 53.0 39.9 57.5</cell><cell>27.1 55.6</cell><cell>32.7 61.7</cell><cell>20.4 54.5</cell><cell>50.0 54.5</cell></row><row><cell>GT</cell><cell>bottom-1 CLP</cell><cell>39.2 85.6</cell><cell>61.8 61.8</cell><cell>61.0 54.2 60.1 65.1 71.0 81.4 58.7 61.6 80.2 78.8</cell><cell>70.4 76.1</cell><cell>80.5 80.6</cell><cell>62.9 62.9</cell><cell>62.6 72.8</cell></row><row><cell>MHRot</cell><cell>bottom-1 CLP</cell><cell>34.1 77.4</cell><cell>62.4 62.4</cell><cell>65.2 57.6 61.9 69.3 77.8 81.5 60.4 61.9 82.1 82.3</cell><cell>73.1 73.3</cell><cell>75.0 79.9</cell><cell>54.3 64.7</cell><cell>63.1 72.6</cell></row><row><cell>one_class OpenGan</cell><cell>bottom-1 CLP</cell><cell>11.4 66.3</cell><cell>18.1 92.2</cell><cell>10.0 15.3 19.2 29.8 16.9 94.9 64.7 51.7 63.7 93.3</cell><cell>20.9 66.5</cell><cell>40.4 85.7</cell><cell>6.2 85.0</cell><cell>50.0 76.4</cell></row><row><cell>CSI</cell><cell>bottom-1 CLP</cell><cell>73.4 94.2</cell><cell>68.4 68.4</cell><cell>81.3 65.0 71.8 78.1 88.3 93.7 79.9 71.9 90.6 91.5</cell><cell>85.9 87.1</cell><cell>91.1 91.0</cell><cell>57.5 74.6</cell><cell>76.1 84.3</cell></row><row><cell>DN2</cell><cell>bottom-1 CLP</cell><cell>57.5 87.5</cell><cell>61.8 61.8</cell><cell>43.8 54.0 51.5 66.2 59.4 61.7 66.5 84.4 83.5 74.3</cell><cell>74.6 74.6</cell><cell>54.9 77.0</cell><cell>61.0 69.8</cell><cell>58.5 74.1</cell></row><row><cell>PANDA</cell><cell>bottom-1 CLP</cell><cell>89.8 94.5</cell><cell>70.0 70.0</cell><cell>81.7 50.0 77.0 74.8 83.8 89.1 66.7 91.7 88.8 86.5</cell><cell>80.3 80.3</cell><cell>88.7 88.7</cell><cell>72.1 75.3</cell><cell>76.8 83.2</cell></row><row><cell>MSAD</cell><cell>bottom-1 CLP</cell><cell>90.4 94.0</cell><cell>72.0 72.0</cell><cell>87.5 52.9 81.1 80.5 85.3 89.8 75.3 87.3 94.5 90.2</cell><cell>83.0 83.0</cell><cell>83.8 88.6</cell><cell>78.3 79.0</cell><cell>79.5 85.4</cell></row><row><cell>Transformaly</cell><cell>bottom-1 CLP</cell><cell>84.1 92.1</cell><cell>69.5 69.5</cell><cell>90.3 78.4 82.9 91.8 87.3 91.0 81.1 92.1 95.6 87.3</cell><cell>95.1 95.1</cell><cell>86.0 92.2</cell><cell>75.5 76.7</cell><cell>84.1 87.3</cell></row><row><cell>Ours</cell><cell>bottom-1 CLP</cell><cell>95.6 96.3</cell><cell>83.5 83.5</cell><cell>93.4 86.9 92.3 84.5 94.8 99.2 89.4 97.0 95.8 94.8</cell><cell>97.8 98.0</cell><cell>93.7 93.8</cell><cell>77.0 79.9</cell><cell>90.0 92.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Performance of our method in AUROC (%) for each class of the CIFAR-10 dataset in the one-vs-all setting.</figDesc><table><row><cell>Method</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>Mean</cell></row><row><cell>Ours</cell><cell cols="11">99.2 99.4 99.2 98.1 99.5 98.1 99.8 99.5 99.2 98.8 99.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Performance of our method in AUROC (%) for each class of the coarse-grained CIFAR-100 dataset in the one-vs-all setting.In this section, we compare images of the CIFAR-10 dataset, images of respective classes selected from the CIFAR-100 dataset based on the CLP criterion, and the corresponding anomalies generated by our SDE model.</figDesc><table><row><cell>Method</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell>17</cell><cell>18</cell><cell>19</cell><cell>Mean</cell></row><row><cell>Ours</cell><cell cols="21">98.0 99.0 99.1 98.2 98.2 97.7 98.8 98.7 99.0 96.6 96.5 98.1 98.4 96.3 98.1 97.1 98.5 98.8 98.8 97.9 98.1</cell></row><row><cell cols="7">A.4 CIFAR-10 vs. CIFAR-100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CIFAR10</cell><cell>Airplane</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CIFAR100</cell><cell>Rocket</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Generated</cell><cell>Anomaly</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CIFAR10</cell><cell>Car</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR100 Pickup_Truck</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Generated</cell><cell>Anomaly</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CIFAR10</cell><cell>Bird</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CIFAR100</cell><cell>Shrew</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Generated</cell><cell>Anomaly</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CIFAR10</cell><cell>Cat</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CIFAR100</cell><cell>Leopard</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Generated</cell><cell>Anomaly</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CIFAR10</cell><cell>Deer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CIFAR100</cell><cell>Cattle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Generated</cell><cell>Anomaly</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In the literature novelty detection and anomaly detection are used interchangeably. We use the term novelty detection (ND) throughout this paper.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Hossein Rohban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14051</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Panda: Adapting pretrained features for anomaly detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liron</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2806" to="2814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Csi: Novelty detection via contrastive learning on distributionally shifted instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11839" to="11852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep anomaly detection for generalized face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>P?rez-Cabo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jim?nez-Cabello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artur</forename><surname>Costa-Pazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto J L?pez-Sastre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring the limits of out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep semi-supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>G?rnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02694</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Opengan: Open-set recognition via open data generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="813" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">G2d: generate to detect anomaly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masoud</forename><surname>Pourreza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahram</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Khaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samir</forename><surname>Bouindour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hichem</forename><surname>Snoussi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Amirreza Shaeiri, Mohammad Hossein Rohban, and Hamid R Rabiee. Arae: Adversarially robust training of autoencoders improves novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atrin</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbod</forename><surname>Pajoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Otoofi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="726" to="736" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<title level="m">Score-based generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Contrastive training for improved out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Winkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudy</forename><surname>Bunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit Guha</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Joseph R Ledsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Macwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.05566</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiresolution knowledge distillation for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niousha</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroosh</forename><surname>Baselizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Rohban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Rabiee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="14902" to="14912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Uninformed students: Studentteacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4183" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast and robust segmentation of white blood cell images by self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Micron</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="55" to="71" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-class weather dataset for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gbeminiyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mendeley Data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Mean-shifted contrastive loss for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03844</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12104" to="12114" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep nearest neighbor anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liron</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10445</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Transformaly-two (feature spaces) are better than one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Matan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Avidan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04185</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">History-based anomaly detector: An adversarial approach to anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierrick</forename><surname>Chatillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coloma</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SAI Intelligent Systems Conference</title>
		<meeting>SAI Intelligent Systems Conference</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="761" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Algan: Anomaly detection by generating pseudo anomalous data via latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hironori</forename><surname>Murase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.10281</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning placeholders for open-set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Jia</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-Chuan</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Open set learning with counterfactual images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weng-Keen</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="613" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Generative openmax for multi-class open set classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Demyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahil</forename><surname>Garnavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07418</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adversarial vulnerability of powerful near out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.07012</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A simple fix to mahalanobis distance for improving near-ood detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit Guha</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyas</forename><surname>Padhy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09022</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Close category generalization. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Yuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainaz</forename><surname>Eftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niousha</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohammad Hossein Rohban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hamid R Rabiee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12959</idno>
		<title level="m">Puzzle-ae: Novelty detection in images through solving puzzles</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cutpaste: Self-supervised learning for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9664" to="9674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Do we really need to learn representations from in-domain data for outlier detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Amit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.09270</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
