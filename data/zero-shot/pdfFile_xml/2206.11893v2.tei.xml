<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Parameterization and Initialization of Diagonal State Space Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
							<email>1albertgu@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University ? IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University ? IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Goel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University ? IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University ? IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On the Parameterization and Initialization of Diagonal State Space Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State space models (SSM) have recently been shown to be very effective as a deep learning layer as a promising alternative to sequence models such as RNNs, CNNs, or Transformers. The first version to show this potential was the S4 model, which is particularly effective on tasks involving long-range dependencies by using a prescribed state matrix called the HiPPO matrix. While this has an interpretable mathematical mechanism for modeling long dependencies, it introduces a custom representation and algorithm that can be difficult to implement. On the other hand, a recent variant of S4 called DSS showed that restricting the state matrix to be fully diagonal can still preserve the performance of the original model when using a specific initialization based on approximating S4's matrix. This work seeks to systematically understand how to parameterize and initialize such diagonal state space models. While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance. We explain why DSS works mathematically, by showing that the diagonal restriction of S4's matrix surprisingly recovers the same kernel in the limit of infinite state dimension. We also systematically describe various design choices in parameterizing and computing diagonal SSMs, and perform a controlled empirical study ablating the effects of these choices. Our final model S4D is a simple diagonal version of S4 whose kernel computation requires just 2 lines of code and performs comparably to S4 in almost all settings, with state-of-the-art results for image, audio, and medical time-series domains, and averaging 85% on the Long Range Arena benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A core class of models in modern deep learning are sequence models, which are parameterized mappings operating on arbitrary sequences of inputs. Recent approaches based on state space models (SSMs) have outperformed traditional deep sequence models such as recurrent neural networks (RNNs), convolutional neural networks (CNNs), and Transformers, in both computational efficiency and modeling ability. In particular, the S4 model displayed strong results on a range of sequence modeling tasks, especially on long sequences <ref type="bibr" target="#b8">[9]</ref>. Its ability to model long-range dependencies arises from being defined with a particular state matrix called the "HiPPO matrix" <ref type="bibr" target="#b5">[6]</ref>, which allows S4 to be viewed as a convolutional model that decomposes an input onto an orthogonal system of smooth basis functions <ref type="bibr" target="#b9">[10]</ref>.</p><p>However, beyond its theoretical interpretation, actually computing S4 as a deep learning model requires a sophisticated algorithm with many linear algebraic techniques that are difficult to understand and implement. These techniques were necessitated by parameterizing its state matrix as a diagonal plus low-rank (DPLR) matrix, which is necessary to capture HiPPO matrices. A natural question is whether simplifications of this parameterization and algorithm are possible. In particular, removing the low-rank term would result in a diagonal state space model (diagonal SSM) that is dramatically simpler to implement and understand.</p><p>Although it is known that almost all SSMs have an equivalent diagonal form-and therefore (complex) diagonal SSMs are fully expressive algebraically-they may not represent all SSMs numerically, and finding a good initialization is critical. Gu et al. <ref type="bibr" target="#b8">[9]</ref> showed that it is difficult to find a performant diagonal SSM, and that many alternative parameterizations of the state matrix -including by random diagonal matricesare much less effective empirically, which motivated the necessity of the more complicated HiPPO matrix. However, recently Gupta <ref type="bibr" target="#b10">[11]</ref> made the empirical observation that a variant of S4 using a particular diagonal matrix is nearly as effective as the original S4 method. This matrix is based on the original HiPPO matrix and is defined by simply chopping off the low-rank term in the DPLR representation.</p><p>The discovery of performant diagonal state matrices opens up new possibilities for simplifying deep state space models, and consolidating models such as S4 and DSS to understand and improve them. First, the strongest version of DSS computes the SSM with a complex-valued softmax that complicates the algorithm, and is actually less efficient than S4. Additionally, DSS and S4 differ in several auxiliary aspects of parameterizing SSMs that can conflate performance effects, making it more difficult to isolate the core effects of diagonal versus DPLR state matrices. Most importantly, DSS relies on initializing the state matrix to a particular approximation of S4's HiPPO matrix. While S4's matrix has a mathematical interpretation for addressing long-range dependencies, the efficacy of the diagonal approximation to it remains theoretically unexplained.</p><p>In this work, we seek to systematically understand how to train diagonal SSMs. We introduce the S4D method, a diagonal SSM which combines the best of S4's computation and parameterization and DSS's initialization, resulting in a method that is extremely simple, theoretically princpled, and empirically effective.</p><p>? First, we describe S4D, a simple method outlined by S4 for computing diagonal instead of DPLR matrices, which is based on Vandermonde matrix multiplication and is even simpler and more efficient than the DSS. Outside of the core state matrix, we categorize different representations of the other components of SSMs, introducing flexible design choices that capture both S4 and DSS and allow different SSM parameterizations to be systematically compared (Section 3). ? We provide a new mathematical analysis of DSS's initialization, showing that the diagonal approximation of the original HiPPO matrix surprisingly produces the same dynamics as S4 when the state size goes to infinity. We propose even simpler variants of diagonal SSMs using different initializations of the state matrix (Section 4). ? We perform a controlled study of these various design choices across many domains, tasks, and sequence lengths, and additionally compare diagonal (S4D) versus DPLR (S4) variants. Our best S4D methods are competitive with S4 on almost all settings, with near state-of-the-art results on image, audio, and medical time series benchmarks, and achieving 85% on the Long Range Arena benchmark (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Continuous State Spaces Models S4 investigated state space models (1) that are parameterized maps on signals u(t) ? y(t). These SSMs are linear time-invariant systems that can be represented either as a linear ODE (equation <ref type="formula">(1)</ref>) or convolution (equation <ref type="formula" target="#formula_0">(2)</ref>).</p><formula xml:id="formula_0">x (t) = Ax(t) + Bu(t) y(t) = Cx(t) (1) K(t) = Ce tA B y(t) = (K * u)(t)<label>(2)</label></formula><p>Here the parameters are the state matrix A ? C N ?N and other matrices B ? C N ?1 , C ? C 1?N . In the case of diagonal SSMs, A is diagonal and we will overload notation so that A n , B n , C n denotes the entries of the parameters.</p><p>An intuitive way to view the convolution kernel (2) is to interpret it as a linear combination (controlled by C) of basis kernels K n (t) (controlled by A, B)</p><formula xml:id="formula_1">K(t) = N ?1 n=0 C n K n (t) K n (t) := e n e tA B<label>(3)</label></formula><p>We denote this basis as K(t) = K A,B (t) = e tA B if necessary to disambiguate; note that it is a vector of N functions. In the case of diagonal SSMs, each function K n (t) is just e tAn B n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S4: Structured State Spaces</head><p>As a deep learning model, SSMs have many elegant properties with concrete empirical and computational benefits <ref type="bibr" target="#b7">[8]</ref>. For example, the convolutional form (2) can be converted into a temporal recurrence that is substantially faster for autoregressive applications <ref type="bibr" target="#b4">[5]</ref>.</p><p>However, making SSMs effective required overcoming two key challenges: choosing appropriate values for the matrices, and computing the kernel (2) efficiently.</p><p>First, Gu et al. <ref type="bibr" target="#b7">[8]</ref> showed that naive instantiations of the SSM do not perform well, and instead relied on a particular (real-valued) matrix A called the HiPPO-LegS matrix (4). <ref type="bibr" target="#b0">1</ref> These matrices were derived so that the basis kernels K n (t) have closed-form formulas L n (e ?t ), where L n (t) are normalized Legendre polynomials. Consequently, the SSM has a mathematical interpretation of decomposing the input signal u(t) onto a set of infinitely-long basis functions that are orthogonal respect to an exponentially-decaying measure, giving it long-range modeling abilities <ref type="bibr" target="#b9">[10]</ref>.</p><p>Second, S4 introduced a particular parameterization that decomposed this A matrix into the sum of a normal and rank-1 matrix (5), which can be unitarily conjugated into a (complex) diagonal plus rank-1 matrix. Leveraging this structured form, they then introduced a sophisticated algorithm for efficiently computing the convolution kernel (2) for state matrices that are diagonal plus low-rank (DPLR).</p><formula xml:id="formula_2">A nk = ? ? ? ? ? ? (2n + 1) 1 2 (2k + 1) 1 2 n &gt; k n + 1 n = k 0 n &lt; k Bn = (2n + 1) 1 2 Pn = (n + 1/2) 1 2 (HiPPO-LegS matrix used in S4)<label>(4)</label></formula><formula xml:id="formula_3">A (N ) nk = ? ? ? ? ? ? (n + 1 2 ) 1/2 (k + 1 2 ) 1/2 n &gt; k 1 2 n = k (n + 1 2 ) 1/2 (k + 1 2 ) 1/2 n &lt; k A = A (N ) ? P P , A (D) := eig(A (N ) ) (Normal / diagonal plus low-rank form)<label>(5)</label></formula><p>DSS: Diagonal State Spaces S4 was originally motivated by searching for a diagonal state matrix, which would be even more structured and result in very simple computation of the SSM. However, the HiPPO-LegS matrix cannot be stably transformed into diagonal form [9, Lemma 3.2], and they were unable to find any diagonal matrices that performed well, resulting in the DPLR formulation.</p><p>Gupta <ref type="bibr" target="#b10">[11]</ref> made the surprising empirical observation that simply removing the low-rank portion of the DPLR form of the HiPPO-LegS matrix results in a diagonal matrix that performs comparably to the original S4 method. More precisely, their initialization is the diagonal matrix A (D) , or the diagonalization of A (N ) in <ref type="bibr" target="#b4">(5)</ref>. They termed A (N ) the skew-HiPPO matrix, which we will also call the normal-HiPPO matrix. To be more specific and disambiguate these variants, we may also call A (N ) the HiPPO-LegS-N or HiPPO-N matrix and A (D) the HiPPO-LegS-D or HiPPO-D matrix.</p><p>In addition to this initialization, they proposed a method for computing a diagonal SSM kernel. Beyond these two core differences, several other aspects of their parameterization differ from S4's.</p><p>In Sections 3 and 4, we systematically study the components of DSS: we categorize different ways to parameterize and compute the diagonal state space, and explain the theoretical interpretion of this particular diagonal A matrix.</p><p>Because there are several different concrete matrices with different naming conventions, this table summarizes these special matrices and ways to refer to them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matrix Full Name Alternate Names</head><p>A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HiPPO-LegS HiPPO matrix, LegS matrix A (N )</head><p>HiPPO-LegS-N HiPPO-N, skew-HiPPO, normal-HiPPO</p><formula xml:id="formula_4">A (D)</formula><p>HiPPO-LegS-D HiPPO-D, diagonal-HiPPO</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Parameterizing Diagonal State Spaces</head><p>We describe various choices for the computation and parameterization of diagonal state spaces. Our categorization of these choices leads to simple variants of the core method. Both DSS and our proposed S4D can be described using a combination of these factors (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discretization</head><p>The true continuous-time SSM can be represented as a continuous convolution y(t) = (K * u)(t) = ? 0 Ce sA Bu(t ? s) ds. In discrete time, we view an input sequence u 0 , u 1 , . . . as uniformly-spaced samples from an underlying function u(t) and must approximate this integral. Standard methods for doing so that preserve the convolutional structure of the model exist. The first step is to discretize the parameters. Two simple choices that have been used in prior work include</p><formula xml:id="formula_5">(Bilinear) A = (I ? ?/2A) ?1 (I + ?/2A) (ZOH) A = exp(?A) B = (I ? ?/2A) ?1 ? ?B B = (?A) ?1 (exp(? ? A) ? I) ? ?B.</formula><p>With these methods, the discrete-time SSM output is just</p><formula xml:id="formula_6">y = u * K where K = (CB, CAB, . . . , CA L?1 B) .<label>(6)</label></formula><p>These integration rules have both been used in prior works (e.g. LMU and DSS use ZOH <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref> while S4 and its predecessors use bilinear <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>).</p><p>In Section 5, we show that there is little empirical difference between them. However, we note that there is a curious phenomenon where the bilinear transform actually perfectly smooths out the kernel used in DSS to match the S4 kernel (Section 4 <ref type="figure" target="#fig_3">Fig. 2d</ref>). We additionally note that numerical integration is a rich and well-studied topic and more stable methods of approximating the convolutional integral may exist. For example, it is well-known that simple rules like the Trapezoid rule <ref type="bibr" target="#b17">[18]</ref> can dramatically reduce numerical integration error when the function has bounded second derivative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolution Kernel</head><p>The main computational difficulty of the original S4 model is computing the convolution kernel K. This is extremely slow for general state matrices A, and S4 introduced a complicated algorithm for DPLR state matrices. When A is diagonal, the computation is nearly trivial. By <ref type="bibr" target="#b5">(6)</ref>,</p><formula xml:id="formula_7">K = N ?1 n=0 C n A n B n =? K = (B ? C) ? V L (A) where V L (A) n, = A n<label>(7)</label></formula><p>where ? is Hadamard product, ? is matrix multiplication, and V is known as a Vandermonde matrix.</p><p>Unpacking this a little more, we can write K as the following Vandermonde matrix-vector multiplication. We note that on modern parallelizable hardware such as GPUs, a simple fast algorithm is to compute <ref type="bibr" target="#b6">(7)</ref> with naive summation (using O(N L) operations), but without materializing the Vandermonde matrix (using O(N + L) space). Just as with S4, this may require implementing a custom kernel in some modern deep learning frameworks such as PyTorch to achieve the space savings.</p><formula xml:id="formula_8">K = B 0 C 0 . . . B N ?1 C N ?1 ? ? ? ? ? ? 1 A 0 A 2 0 . . . A L?1 0 1 A 1 A 2 1 . . . A L?1 1 . . . . . . . . . . . . . . . 1 A N ?1 A 2 N ?1 . . . A L?1 N ?1 ? ? ? ? ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parameterization</head><p>The next question is how to represent the parameters A, B, C.</p><p>Parameterization of A. Note that the kernel K(t) = Ce tA B blows up to ? as t ? ? if A has any eigenvalues with positive real part. Goel et al. <ref type="bibr" target="#b4">[5]</ref> found that this is a serious constraint that affects the stability of the model, especially when using the SSM as an autoregressive generative model. They propose to force the real part of A to be negative, also known as the left-half plane condition in classical controls, by parameterizing the real part inside an exponential function</p><formula xml:id="formula_9">A = ? exp(A Re ) + i ? A Im .</formula><p>We note that instead of exp, any activation function can be used as long as its range is bounded on one side, such as ReLU, softplus, etc. The original DSS does not constrain the real part of A, which is sufficient for simple tasks involving fixed-length sequences, but could become unstable in other settings.</p><p>Parameterization of B, C. Another choice in the parameterization is how to represent B and C. Note that the computation of the final discrete convolution kernel K depends only on the elementwise product B ? C (equation <ref type="formula" target="#formula_7">(7)</ref>). Therefore DSS chose to parameterize this product directly, which they call W , instead of B and C individually.</p><p>However, we observe that this is equivalent to keeping independent B and C, and simply freezing B = 1 while training C. Therefore, just as S4 has separate parameters A, B, and C and uses a fixed initialization for A and B, S4D also proposes separate A, B, and C and uses fixed initializations for A (discussed in Section 4) and B (set to 1). Then the difference between S4D and DSS is simply that DSS does not train B.</p><p>In our ablations, we show that training B gives a minor but consistent improvement in performance.</p><p>As described in <ref type="bibr" target="#b9">[10]</ref>, S4 initializes C randomly with standard deviation 1 (in contrast to standard deep learning initializations, which scale with the dimension e.g. N ? 1 2 ), which is variance-preserving for S4's (A, B) as a consequence of the HiPPO theory. Because it turns out that the diagonal approximation to HiPPO has similar theoretical properties, we retain this initialization in the diagonal case.</p><p>Conjugate Symmetry. Finally, we make note of a minor parameterization detail originally used in S4. Note that we ultimately care about sequence transformations over real numbers. For example, HiPPO defines real (A, B) matrices, and the base definition of S4 is a real SSM that is a map on sequences of real numbers.</p><p>In this case, note that the state x at any time is a vector R N , and similarly B and C would consist of N real parameters.</p><p>However, if using complex numbers, this effectively doubles the state dimension and the number of parameters in B, C. Furthermore, when using a complex SSM, the output of the SSM is not guaranteed to be real even if the input is real, and similarly the convolution kernel (7) will in general be complex.</p><p>To resolve this discrepency, note that when diagonalizing a real SSM into a complex SSM (see Proposition 2), the resulting parameters always occur in conjugate pairs. Therefore we can throw out half of the parameters.</p><p>In other words, to parameterize a real SSM of state size N , we can instead parameterize a complex SSM of state size N 2 , and implicitly add back the conjugate pairs of the parameters. This ensures that the total state size and parameter count is actually the equivalent of N real numbers, and also guarantees that the output of the kernel is real. The implementation of this is very simple; the sum in <ref type="bibr" target="#b6">(7)</ref> will implicitly include the conjugate pairs of A, B, C and therefore resolve to twice the real part of the original sum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">S4D: the Diagonal Version of S4</head><p>A key component of our exposition is disentangling the various choices possible in representing and computing state space models. With this categorization, different choices can be mixed and matched to define variants of the core method. <ref type="table" target="#tab_0">Table 1</ref> compares S4, DSS, and S4D, which have a core structure and kernel computation, but have various choices of other aspects of the parameterization. Comparison to S4 and DSS. We will define the base version of S4D to match the parameterization of S4 (i.e. bilinear discretization, (A) parameterized with exp, trainable B, and HiPPO-D initialization), but many other variants are possible. Note that unlike DSS, the output of S4D would be exactly the same as masking out the low-rank component of S4's DPLR representation. Thus comparing S4D vs. S4 is a comparison of diagonal vs. DPLR representations of A while controlling all other factors. In our empirical study in Section 5, we systematically ablate the effects of each of these components.</p><p>We elaborate more on the comparisons between S4, DSS, and S4D below.</p><p>Kernel computation. The original S4 work briefly considered the diagonal case as motivation [9, Section 3.1], and explicitly mentioned the connection to Vandermonde products and the computational complexity of diagonal SSMs. However, their focus was the more complex DPLR representation because it is difficult to find a performant diagonal state matrix. Compared to S4, we fleshed out details of the Vandermonde connection and its computational complexity, which matches that of S4.</p><p>On the other hand, DSS empirically found an effective diagonal state matrix, but introduced a more complicated method based on a complex softmax for computing it. Compared to S4D, this softmax essentially normalizes by the row-sums of the Vandermonde matrix, so we may sometimes refer to this distinction as "softmax normalization". This makes the kernel more complicated than necessary, and has a few concrete drawbacks. First, the row-normalization effectively makes the model dependent on a particular sequence length L, and special logic is required to handle different sequence lengths. Second, it does not expose the optimal computational complexity of the method, and the original version of DSS in fact uses O(N ) more memory in the kernel construction than S4(D). 2</p><p>Discretization. S4D disentangles the discretization method from the kernel computation (equation <ref type="formula" target="#formula_7">(7)</ref>), so that any discretization can be used, whereas previous methods required a specific discretization. For example, DSS requires the zero-order hold (ZOH) discretization because the exp term in the ZOH formula lends itself to be computed with a softmax. On the other hand, when A is not diagonal, ZOH involves a matrix exponential which can be slower to compute, so S4 uses the bilinear discretization which can be computed efficiently for DPLR matrices.</p><p>Eigenvalue constraint. All methods can enforce any constraint on the eigenvalues of A. While DSS found that letting them be unconstrained has slightly better performance, our experiments find that the difference is negligible and we recommend contraining negative real part of A as is standard practice in control systems. This ensures stability even in unbounded autoregressive settings.</p><p>The full model. The entire S4D method is very straightforward to implement, requiring just a few lines of code each for the parameterization and initialization, kernel computation, and full forward pass (Listing 1). This minimal model maps an input sequence of length L to an output of the same length; given multiple input channels, independent S4D layers are broadcast over them. Other details such as the initialization of ? and other components of the overall neural network architecture are the same as in S4 and DSS.</p><p>Finally, note that different combinations of parameterization choices can lead to slightly different implementations of the kernel. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the S4D kernel with ZOH discretization which can be simplified even further to just 2 lines of code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Initialization of Diagonal State Matrices</head><p>The critical question remains: which diagonal state matrices A are actually effective? We comment on the limitations of diagonal SSMs, and then provide three instantiations of S4D that perform well empirically.</p><p>Expressivity and Limitations of Diagonal SSMs. We first present a simplified view on the expressivity of diagonal SSMs mentioned by <ref type="bibr" target="#b10">[11]</ref>. First, it is well-known that almost all matrices diagonalize over the complex plane. Therefore it is critical to use complex-valued matrices in order to use diagonal SSMs.</p><p>Proposition 2. The set D ? C N ?N of diagonalizable matrices is dense in C N ?N , and has full measure (i.e.</p><p>its complement has measure 0).</p><p>It is also well known that the state space (A, B, C) is exactly equivalent to (i.e. expresses the same map u ? y) the state space (V ?1 AV , V ?1 B, CV ), known in the SSM literature as a state space transformation. Therefore Proposition 2 says that (almost) all SSMs are equivalent to a diagonal SSM. However, we emphasize that Proposition 2 is about expressivity which does not guarantee strong performance of a trained model after optimization. For example, Gu et al. <ref type="bibr" target="#b8">[9]</ref> and Gupta <ref type="bibr" target="#b10">[11]</ref> show that parameterizing A as a dense real matrix or diagonal complex matrix, which are both fully expressive classes, performs poorly if randomly initialized.</p><p>Second, Proposition 2 does not take into account numerical representations of data, which was the original reason S4 required a low-rank correction term instead of a pure diagonalization [9, Lemma 3.2]. In Section 5.2, we also show that two different initializations with the same spectrum (i.e., are equivalent to the same diagonal A) can have very different performance.</p><p>S4D-LegS. The HiPPO-LegS matrix has DPLR representation A (D) ? P P , and Gupta <ref type="bibr" target="#b10">[11]</ref> showed that simply approximating it with A (D) works quite well <ref type="bibr" target="#b4">(5)</ref>. Our first result is providing a clean mathematical interpretation of this method. Theorem 3 shows a surprising fact that does not hold in general for DPLR matrices (Appendix A.1), and arises out of the special structure of this particular matrix.</p><p>Theorem 3. Let A = A (N ) ? P P and B be the HiPPO-LegS matrices, and K A,B (t) be its basis. As the state size N ? ?, the SSM basis K A (N ) ,B/2 (t) limits to K A,B (t) <ref type="figure" target="#fig_3">(Fig. 2)</ref>.</p><p>Note that A (N ) is then unitarily equivalent to A (D) , which preserves the stability and timescale <ref type="bibr" target="#b9">[10]</ref> of the system.</p><p>We define S4D-LegS to be the S4D method for this choice of diagonal A = A (D) . Theorem 3 explains the empirical results in <ref type="bibr" target="#b10">[11]</ref> whereby this system performed quite close to S4, but was usually slightly worse. This is because DSS is a variant of S4D-LegS, which by Theorem 3 is a noisy approximation to S4-LegS. <ref type="figure" target="#fig_3">Fig. 2</ref> illustrates this result, and also shows a curious phenomenon involving different discretization rules that is open for future work.</p><p>S4D-Inv. To further simplify S4D-LegS, we analyze the structure of A (D) = diag A in more detail. The real part is easy to understand, which follows from the analysis in <ref type="bibr" target="#b8">[9]</ref>:</p><formula xml:id="formula_10">Proposition 4. (A) = ? 1 2 1</formula><p>Let the imaginary part be sorted, i.e. (A) n is the n-th largest (positive) imaginary component. We empirically deduced the following conjecture for the asymptotics of the imaginary part.</p><formula xml:id="formula_11">Conjecture 5. As N ? ?, (A) 0 ? 1 ? N 2 + c where c ? 0.</formula><p>5236 is a constant. For a fixed N , the other eigenvalues satisfy an inverse scaling in n: (A) n = ?(n ?1 ). <ref type="figure" target="#fig_4">Fig. 3</ref> empirically supports this conjecture. Based on Conjecture 5, we propose the initialization S4D-Inv to use the following inverse-law diagonal matrix which closely approximates S4D-LegS.</p><formula xml:id="formula_12">(S4D-Inv) A n = ? 1 2 + i N ? N 2n + 1 ? 1 (8) (S4D-Lin) A n = ? 1 2 + i?n<label>(9)</label></formula><p>S4D-Lin. While S4D-Inv can be seen as an approximation to the original S4-LegS, we propose an even simpler scaling law for the imaginary parts that can be seen as an approximation of S4-FouT ( <ref type="bibr" target="#b9">[10]</ref>), where the imaginary parts are simply the Fourier series frequencies (i.e. matches the diagonal part of the DPLR form of S4-FouT). <ref type="figure" target="#fig_0">Fig. 1 (Right)</ref> illustrates the S4D-Lin basis e tA B, which are simply damped Fourier basis functions.</p><p>General Diagonal SSM Basis Functions. The empirical study in Section 5 performs many ablations of different diagonal initializations, showing that many natural variants of the proposed methods do not perform as well. The overall guiding principles for the diagonal state matrix A are twofold, which can be seen from the closed form of the basis functions K n (t) = e tAn B n (Eq. <ref type="formula" target="#formula_1">(3)</ref>).</p><p>First, the real part of A n controls the decay rate of the function. A n = ? 1 2 is a good default that bounds the basis functions by the envelope e ? t 2 , giving a constant timescale ( <ref type="figure" target="#fig_0">Fig. 1 (Right)</ref>). Second, the imaginary part of A n controls the oscillating frequencies of the basis function. Critically, these should be spread out, which explains why random initializations of A do not perform well. S4D-Inv and S4D-Lin use simple asymptotics for these imaginary components that provide interpretable bases. We believe that alternative initializations that have different mathematical interpretations may exist, which is an interesting question for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our experimental study shows that S4D has strong performance in a wide variety of domains and tasks, including the well-studied Long Range Arena (LRA) benchmark where the best S4D variant is competitive with S4 on all tasks and significantly outperforms all non-SSM baselines.</p><p>We begin with controlled ablations of the various representations of diagonal state space models.</p><p>? In Section 5.1, we compare the different methods of parameterizing and computing a diagonal state space model (Section 3).</p><p>? In Section 5.2, we compare our proposed initializations of the critical A matrix and perform several ablations showing that simple variants can substantially degrade performance, underscoring the importance of choosing A carefully (Section 4).</p><p>? In Section 5.3, we compare our proposed S4D methods against the original S4 method (and the variants proposed in <ref type="bibr" target="#b9">[10]</ref>).</p><p>Methodology and Datasets. In order to study the effects of different S4 and S4D variants in a controlled setting, we propose the following protocol. We focus on three datasets covering a varied range of data modalities (image pixels, biosignal time series, audio waveforms), sequence lengths (1K, 4K, 16K), and tasks (classification and regression with bidirectional and causal models).</p><p>? Sequential CIFAR (sCIFAR). CIFAR-10 images are flattened into a sequence of length 1024, and a bidirectional sequence model is used to perform 10-way classification. ? BIDMC Vital Signs. EKG and PPG signals of length 4000 are used to predict respiratory rate (RR), heart rate (HR), and blood oxygen saturation (SpO2). We focus on SpO2 in this study.</p><p>? Speech Commands (SC). 3 A 1-second raw audio waveform comprising 16000 samples is used for 35-way spoken word classification. We use an autoregressive (AR) model to vary the setting; this causal setting more closely imitates autoregressive speech generation, where SSMs have shown recent promise <ref type="bibr" target="#b4">[5]</ref>.</p><p>We fix a simple architecture and training protocol that works generically. The architecture has 4 layers and hidden dimension H = 128, resulting in ? 100K parameters. All results are averaged over multiple seeds (full protocol and results including std. reported in Appendix B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Parameterization, Computation, Discretization</head><p>Given the same diagonal SSM matrices A, B, there are many variants of how to parameterize the matrices and compute the SSM kernel described in Section 3. We ablate the different choices described in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Results are in <ref type="table" target="#tab_2">Table 2</ref>, and show that: <ref type="figure" target="#fig_4">Figure 3</ref>: (S4D eigenvalues.) All S4D methods have eigenvalues ? 1 2 + ?ni. S4D-LegS theoretically approximates dynamics of the original (non-diagonal) S4 (Blue), and has eigenvalues following an inverse law ?n ? n ?1 (Orange). The precise law is important: other scaling laws with the same range, including an inverse law with different constant (Purple) and a quadratic law (Red), perform empirically worse (Section 5.2). A very different linear law based on Fourier frequencies also performs well (Green).   3, we fix the S4D parameterization and algorithm described in Section 3. Note that this computes exactly the same kernel as the original S4 algorithm when the low-rank portion is set to 0, allowing controlled comparisons of the critical state matrix A for the remainder of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">S4D Initialization Ablations</head><p>The original S4 model proposed a specific formula for the A matrix, and the first diagonal version <ref type="bibr" target="#b10">[11]</ref> used a specific matrix based on it. Our new proposed variants S4D-Inv and S4D-Lin also define precise formulas for the initialization of the A matrix <ref type="bibr" target="#b7">(8)</ref>. This raises the question of whether the initialization of the A still needs to be so precise, despite the large simplifications from the original version. We perform several natural ablations on these initializations, showing that even simple variations of the precise formula can degrade performance.</p><p>Imaginary part scaling factor. The scaling rules for the imaginary parts of S4D-Inv and S4D-Lin are simple polynomial laws, but how is the constant factor chosen and how important is it? These constants are based on approximations to HiPPO methods (e.g. Conjecture 5). Note that the range of imaginary components for S4D-Inv and S4D-Lin are quite different <ref type="figure" target="#fig_4">(Fig. 3)</ref>; the largest imaginary part is N 2 ? for S4D-Inv and ?N for S4D-Lin.</p><p>We consider scaling all imaginary parts by a constant factor of 0.01 or 100.0 to investigate whether the constant matters. Note that this preserves the overall shape of the basis functions ( <ref type="figure" target="#fig_0">Fig. 1, dashed</ref>  simply changes the frequencies, and it is not obvious that this should degrade performance. However, both changes substantially reduce the performance of S4D in all settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>lines) and</head><p>Randomly initialized imaginary part. Next, we consider choosing the imaginary parts randomly. For S4D-Inv, we keep the real parts equal to ? 1 2 and set each imaginary component to</p><formula xml:id="formula_13">A n = ? 1 2 + i N ? N 2u + 1 ? 1 u ? N ? U[0, 1]<label>(10)</label></formula><p>Note that when u is equally spaced in [0, 1] instead of uniformly random, this exactly recovers S4D-Inv <ref type="formula">(8)</ref>, so this is a sensible random approximation to it.</p><p>Similarly, we consider a variant of S4D-Lin</p><formula xml:id="formula_14">A n = ? 1 2 + i?uN u ? N ? U[0, 1]<label>(11)</label></formula><p>that is equal to equation (9) when u is equally spaced instead of random.</p><p>Table 3a (Random Imag) shows that this small change causes minor degradation in performance. We additionally note that the randomly initialized imaginary ablation can be interpreted as follows. <ref type="figure" target="#fig_4">Fig. 3</ref> shows the asymptotics of the imaginary parts of SSM matrices, where the imaginary parts of the eigenvalues correspond to y-values corresponding to uniformly spaced nodes on the x-axis. This ablation then replaces the uniform spacing on the x-axis with uniformly random x values.</p><p>Randomly initialized real part. We considering initializing the real part of each eigenvalue as ?U[0, 1] instead of fixing them to ? 1 2 . <ref type="table" target="#tab_3">Table 3a</ref>(Left, Random Real ) shows that this also causes minor but consistent degradation in performance on the ablation datasets. Finally, we also consider randomizing both real and imaginary parts, which degrades performance even further. Ablation: Other S4D matrices. Other simple variants of initializations show that it is not just the range of the eigenvalues but the actual distribution that is important <ref type="figure" target="#fig_4">(Fig. 3)</ref>. Both S4D-Inv2 and S4D-Quad have real part ? 1 2 and imaginary part satisfying the same maximum value as Conjecture 5. The S4D-Inv2 initialization uses the same formula as S4D-Inv, but replaces a 2n + 1 in the denominator with n + 1. The S4D-Quad initialization uses a polynomial law with power 2 instead of ?1 (S4D-Inv) or 1 (S4D-Lin).</p><formula xml:id="formula_15">(S4D-Inv2) A n = ? 1 2 + i N ? N n + 1 ? 1 (12) (S4D-Quad) A n = 1 ? (1 + 2n) 2<label>(13)</label></formula><p>We include two additional methods here that are not based on the proposed S4D-Inv or S4D-Lin methods. First, S4D-Rand uses a randomly initialized diagonal A, and validates that it performs poorly, in line with earlier findings <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>. Second, S4D-Real uses a particular real initialization with A n = ?(n + 1). This is the exact same spectrum as the original S4(-LegS) method, which validates that it is not just the diagonalization that matters, highlighting the limitations of Proposition 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Full Comparisons of S4D and S4 Methods</head><p>Trainable A, B matrices. <ref type="table" target="#tab_3">Table 3b</ref> shows the performance of all S4D and S4 variants <ref type="bibr" target="#b9">[10]</ref> on the ablations datasets. We observe several interesting phenomena:</p><p>(i) Freezing the matrices performs comparably to training them on sCIFAR and BIDMC, but is substantially worse on SC. We hypothesize that this results from ? being poorly initialized for SC, so that at initialization models do not have context over the entire sequence, and training A and B helps adjust for this. As further evidence, the finite window methods S4-LegT and S4-FouT (defined in <ref type="bibr" target="#b9">[10]</ref>) have the most limited context and suffer the most when A is frozen.</p><p>(ii) The full DPLR versions are often slightly better than the diagonal version throughout the entire training curve. We report the validation accuracy after 1 epoch of training on sCIFAR and SC to illustrate this phenomenon. Note that this is not a consequence of having more parameters (Appendix B).</p><p>Large models on ablation datasets. Finally, we relax the strict requirements on model size and regularization for the ablation datasets, and show the performance of S4 and S4D variants on the test sets with a larger model (architecture and training details in Appendix B) when the model size and regularization is simply increased ( <ref type="table" target="#tab_4">Table 4</ref>). We note that results for each dataset are better than the original S4 model, which was already state-of-the-art on these datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>Long Range Arena. We use the same hyperparameter setting for the state-of-the-art S4 model in <ref type="bibr" target="#b9">[10]</ref> on the Long Range Arena benchmark for testing long dependencies in sequence models. S4D variants are highly competitive on all datasets except Path-X, and outperform the S4 variants on several of them. On Path-X using this hyperparameter setting with bidirectional models, only S4D-Inv, our simpler approximation to the  original S4-LegS model, achieves above random chance, and has an average of 85% on the full LRA suite, more than 30 points better than the original Transformer <ref type="bibr" target="#b23">[24]</ref>.</p><p>Final parameterization ablations on Path-X. Finally, we return to the parameterization choices presented in Section 3 and ablated in Section 5.1, and ablate them once more on the difficult Path-X dataset.</p><p>We use small models of between 150K and 200K parameters (differing only depending on whether B is trained). We fix the S4D-LegS initialization (i.e., the diagonal HiPPO initialization <ref type="formula" target="#formula_3">(5)</ref>).</p><p>We start from the base S4D parameterization based on S4: bilinear discretization, exp (A), trainable B, and no softmax ( <ref type="table" target="#tab_0">Table 1)</ref>. We ablate each of these choices one at a time for the discretization, constraint on (A), trainability of B, and normalization. We also consider the combination that defines DSS: ZOH discretization, identity (A), frozen B, softmax normalization. <ref type="table" target="#tab_6">Table 6</ref> shows that the default S4 parameterization choices are a strong baseline. As in Section 5.1, we find that most of the other choices do not make much difference:</p><p>(i) letting (A) be unconstrained has little benefit, and can theoretically cause instabilities, so we do not recommend it, (ii) the bilinear vs. ZOH discretizations make no difference, (iii) training B helps slightly, for a minor increase in parameter count and no change in speed.</p><p>Finally, on this task -unlike the easier ablation datasets in Section 5.1 -the softmax normalization of DSS actually hurts performance, and we do not recommend it in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>State space models based on S4 are a promising family of models for modeling many types of sequential data, with particular strengths for continuous signals and long-range interactions. These models are a large departure from conventional sequence models such as RNNs, CNNs, and Transformers, with many new ideas and moving parts. This work provides a more in-depth exposition for all aspects of working with S4-style models, from their core structures and kernel computation algorithms, to miscellaneous choices in their parameterizations, to new theory and methods for their initialization. We systematically analyzed and ablated each of these components, and provide recommendations for building a state space model that is as simple as possible, while as theoretically principled and empirically effective as S4. We believe that S4D can be a strong generic sequence model for a variety of domains, that opens new directions for state space models theoretically, and is much more practical to understand and implement for practitioners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Method Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Proofs</head><p>We prove Theorem 3, and then show why this it is a surprising result that is not true in general to low-rank perturbations of SSMs.</p><p>We start with the interpretation of the S4-LegS matrix shown in <ref type="bibr" target="#b9">[10]</ref>, which corresponds to <ref type="figure" target="#fig_0">Fig. 1 (Left)</ref>.</p><p>Theorem 6. Let A, B, P be the matrices defined in equation <ref type="bibr" target="#b3">(4)</ref>. The SSM kernels K n (t) = e n e tA B have the closed form formula</p><formula xml:id="formula_16">K n (t) = L n (e ?t )e ?t</formula><p>where L n are the Legendre polynomials shifted and scaled to be orthonormal on the interval [0, 1].</p><p>Lemma A.1. The functions L n (e ?t ) are a complete orthonormal basis with respect to the measure ?(t) = e ?t .</p><p>Proof. The polynomials are defined to be orthonormal on [0, 1], i.e. Completeness follows from the fact that polynomials are complete.</p><p>Proof of Theorem 3. We start with the standard interpretation of SSMs as convolutional systems. The SSM x (t) = Ax(t) + Bu(t) is equivalent to the convolution</p><formula xml:id="formula_17">x n (t) = (u * K n )(t) = t ?? u(s)K n (t ? s) ds = ? 0 u(t ? s)K n (s) ds</formula><p>for the SSM kernels (equation <ref type="formula" target="#formula_1">(3)</ref>).</p><p>Defining u (t) (s) = u(t ? s), we can write this as</p><formula xml:id="formula_18">x n (t) = u (t) , K n ?</formula><p>where ?(s) = e ?s and p(s), q(s) ? = ? 0 p(s)q(s)?(s) ds is the inner product in the Hilbert space of L2 functions with respect to measure ?.</p><p>By Theorem 6, the K n are a complete orthonormal basis in this Hilbert space. There x n (t) represents a decomposition of the function u (t) with respect to this basis, and can be recovered as a linear combination of these projections</p><formula xml:id="formula_19">u (t) = ? n=0</formula><p>x n (t)K n .</p><p>Pointwise over the inner times s,</p><formula xml:id="formula_20">u (t) (s) = ? n=0</formula><p>x n (t)K n (s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This implies that</head><formula xml:id="formula_21">u(t) = u (t) (0) = ? n=0 x n (t)K n (0) = ? n=0 x n (t)L n (0) = ? n=0</formula><p>x n (t)(2n + 1)</p><formula xml:id="formula_22">1 2 = B x(t)</formula><p>Intuitively, due to the function reconstruction interpretation of HiPPO <ref type="bibr" target="#b9">[10]</ref>, we can approximate u(t) using knowledge in the current state x(t). There in the limit N ? ?, the original SSM is equivalent to</p><formula xml:id="formula_23">x (t) = Ax(t) + Bu(t) = Ax(t) + 1 2 Bu(t) + 1 2 Bu(t) = Ax(t) + 1 2 BB x(t) + 1 2 Bu(t) = Ax(t) + P P x(t) + 1 2 Bu(t) = A N x(t) + 1 2 Bu(t)</formula><p>General low-rank perturbations. Finally, we remark that this phenomenon where removing the low-rank correction to a DPLR matrix approximates the original dynamics, is unique to this HiPPO-LegS matrix. We note that if instead of P P , a random rank-1 correction is added to the HiPPO-LegS matrix in Theorem 3, the resulting SSM kernels look completely different and in fact diverge rapidly as the magnitude of P increases ( <ref type="figure" target="#fig_4">Fig. 4)</ref>.  Similarly, <ref type="figure" target="#fig_4">Fig. 5a</ref> shows a new S4 variant called S4-FouT that is also DPLR <ref type="bibr" target="#b9">[10]</ref>, but removing the low-rank component dramatically changes the SSM kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiment Details</head><p>Ablation datasets training protocol. The architecture has 4 layers and hidden dimension H = 128, resulting in around 100K trainable parameters. The A and B parameters were tied across the H SSM copies;   All results are averaged over 2 or 3 seeds.</p><p>All models use learning rate 0.004, 0.01 weight decay, and no other regularization or data augmentation. For the classification tasks (sCIFAR and SC). we use a cosine scheduler with 1 epoch warmup and decaying to 0. For the regression task (BIDMC), we use a multistep scheduler following <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Reported results are all best validation accuracy, except for the large models in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>Full results for parameterization ablations. <ref type="table" target="#tab_7">Table 7</ref> and <ref type="table" target="#tab_8">Table 8</ref> contain the raw results for <ref type="table" target="#tab_2">Table 2</ref> including standard deviations.</p><p>Full results for large models on ablations datasets. <ref type="table" target="#tab_0">Tables 9 to 11</ref> show full results comparing our proposed methods against the best models from the literature; citations indicate numbers from prior work.</p><p>Note that earlier works on the Speech Commands dataset typically use pre-processing such as MFCC features, or a 10-class subset of the full 35-class dataset <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref>. As we are not aware of a collection of strong baselines for raw waveform classification using the full dataset, we trained several baselines from scratch for <ref type="table" target="#tab_0">Table 11</ref>. The InceptionNet, ResNet-18, and XResNet-50 models are 1D adaptations from Nonaka and Seita <ref type="bibr" target="#b15">[16]</ref> of popular CNN architectures for vision. The ConvNet architecture is a generic convolutional neural network that we tuned for strong performance, comprising:   <ref type="bibr" target="#b19">[20]</ref> 80.82 TrellisNet <ref type="bibr" target="#b1">[2]</ref> 73.42 LSTM <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref> 63.01 r-LSTM <ref type="bibr" target="#b24">[25]</ref> 72.2 UR-GRU <ref type="bibr" target="#b6">[7]</ref> 74.4 HiPPO-RNN <ref type="bibr" target="#b5">[6]</ref> 61.1 LipschitzRNN [4] 64.2 ? Four stages, each composed of three identical residual blocks.</p><p>? The first stage has model dimension (i.e. channels, in CNN nomenclature) H = 64. Each stage doubles the dimension of the previous stage (with a position-wise linear layer) and ends in an average pooling layer of width 4. Thus, the first stage operates on inputs of length 16384, dimension 64 (the input is zero-padded from 16000 to 16384) and the last on length 256, dimension 512.</p><p>? Each residual block has a (pre-norm) BatchNorm layer followed by a convolution layer and GeLU activation.</p><p>? Convolution layers have a kernel size of 25.</p><p>Long Range Arena. Our Long Range Arena experiments follow the same setup as the original S4 paper with some differences in model architecture and hyperparameters. The main global differences are as follows:</p><p>Bidirectional The original S4 layer is unidirectional or causal, which is an unnecessary constraint for the classification tasks appearing in LRA. Goel et al. <ref type="bibr" target="#b4">[5]</ref> propose a bidirectional version of S4 that simply concatenates two S4 convolution kernels back-to-back. We use this for all tasks.</p><p>GLU feedforward S4 consists of H independent 1-dimensional SSMs, each of which are processed by an independent S4 SSM mapping (A, B, C, D). These outputs are then mixed with a position-wise linear layer, i.e. W y for a learned matrix W ? R H?H . Instead of this linear mapping, we use a GLU activation (W 1 y) ? ?(W 2 y) for W 1 , W 2 ? R H?H <ref type="bibr" target="#b2">[3]</ref>. These have been empirically found to improve linear layers of DNNs in general <ref type="bibr" target="#b21">[22]</ref>. Cosine scheduler Instead of the plateau scheduler used in <ref type="bibr" target="#b8">[9]</ref>, we use a cosine annealing learning rate scheduler for all tasks.</p><p>Regularization Almost all tasks used no dropout and 0.05 weight decay.</p><p>Architecture Almost all tasks used an architecture with 6 layers, H = 256 hidden units, BatchNorm, pre-norm placement of the normalization layer.</p><p>Exceptions to the above rules are described below. Full hyperparameters are in <ref type="table" target="#tab_0">Table 12</ref>.</p><p>sCIFAR / LRA Image. This dataset is grayscale sequential CIFAR-10, and the settings for this task were taken from S4's hyperparameters on the normal sequential CIFAR-10 task. In particular, this used LayerNorm <ref type="bibr" target="#b0">[1]</ref> instead of BatchNorm <ref type="bibr" target="#b12">[13]</ref>, a larger number of hidden features H, post-norm instead of pre-norm, and minor dropout. We note that the choice of normalization and increased H do not make a significant difference on final performance, still attaining classification accuracy in the high 80's. Dropout does seem to make a difference.</p><p>BIDMC. We used a larger state size of N = 256, since we hypothesized that picking up higher frequency features on this dataset would help. We also used a step scheduler that decayed the LR by 0.5 every 100 epochs, following prior work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>ListOps. We hypothesized that this task benefits from deeper models, because of the explicit hierarchical nature of the task, so the architecture used here had 8 layers and H = 128 hidden features. However, results are very close with much smaller models. We also found that post-norm generalized better than pre-norm, but results are again close (less than 1% difference).</p><p>PathX. As described in <ref type="bibr" target="#b9">[10]</ref>, the initialization range for PathX is decreased from (? min , ? max ) = (0.001, 0.1) to (? min , ? max ) = (0.0001, 0.01). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>S4D is a diagonal SSM which inherits the strengths of S4 while being much simpler. (Left) The diagonal structure allows it to be viewed as a collection of 1-dimensional SSMs. (Right) As a convolutional model, S4D has a simple interpretable convolution kernel which can be implemented in two lines of code. Colors denote independent 1-D SSMs; purple denotes trainable parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Proposition 1 .</head><label>1</label><figDesc>Time and Space ComplexityThe naive way to compute<ref type="bibr" target="#b6">(7)</ref> is by materializing the Vandermonde matrix V L (A) and performing a matrix multiplication, which requires O(N L) time and space.However, Vandermonde matrices are well-studied and theoretically the multiplication can be computed in O(N + L) operations and O(N + L) space. In fact, Vandermonde matrices are closely related to Cauchy matrices, which are the computational core of S4's DPLR algorithm, and have identical complexity<ref type="bibr" target="#b16">[17]</ref>. The time and space complexity of computing the kernel of diagonal SSMs is equal to that of computing DPLR SSMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Listing 1 Full</head><label>1</label><figDesc>Numpy example of the parameterization and computation of a 1-dimensional S4D-Lin model def parameters(N, dt_min=1e-3, dt_max=1e-1): # Initialization log_dt = np.random.rand() * (np.log(dt_max)-np.log(dt_min)) + np.log(dt_min) # Geometrically uniform timescale A = -0.5 + 1j * np.pi * np.arange(N//2) # S4D-Lin initialization B = np.ones(N//2) + 0j C = np.random.randn(N//2) + 1j * np.random.randn(N) # Variance preserving initialization return log_dt, np.log(-A.real), A.imag, B, C def kernel(L, log_dt, log_A_real, A_imag, B, C): # Discretization (e.g. bilinear transform) dt, A = np.exp(log_dt), -np.exp(log_A_real) + 1j * A_imag dA, dB = (1+dt*A/2) / (1-dt*A/2), dt*B / (1-dt*A/2) # Computation (Vandermonde matrix multiplication -can be optimized) # Return twice the real part -same as adding conjugate pairs return 2 * ((B*C) @ (dA[:, None] ** np.arange(L))).real def forward(u, parameters): L = u.shape[-1] K = kernel(L, *parameters) # Convolve y = u * K using FFT K_f, u_f = np.fft.fft(K, n=2*L), np.fft.fft(u, n=2*L) return np.fft.ifft(K_f*u_f, n=2*L)[..., :L]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>(Visualization of Theorem 3). (a) The particular (A, B) matrix chosen in S4 results in smooth basis functions e tA B with a closed form formula in terms of Legendre polynomials. By the HiPPO theory, convolving against these functions has a mathematical interpretation as orthogonalizing against an exponentially-decaying measure. (b, c) By special properties of this state matrix, removing the low-rank term of its NPLR representation produces the same basis functions as N ? ?, explaining the empirical effectiveness of DSS. (c) Curiously, the bilinear transform instead of ZOH smooths out the kernel to exactly match S4-LegS as N grows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( i )</head><label>i</label><figDesc>Computing the model with a softmax instead of Vandermonde product does not make much difference (ii) Training B is consistently slightly better (iii) Different discretizations (Section 3.1) do not make a noticeable difference (iv) Unrestricting the real part of A (Section 3.3) may be slightly better These ablations show that for a fixed initialization (A, B), different aspects of parameterizing SSMs make little difference overall. This justifies the parameterization and algorithm S4D uses (Section 3.4), which preserves the choices of the original S4 model and is simpler than DSS. For the remaining of the experiments in Section 5.2 and Section 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 0LL 0 L</head><label>10</label><figDesc>n (t)L m (t) dt = ? n,m .By the change of variables t = e ?s with dt ds = ?e ?s , n (e ?s )L m (e ?s )e ?s ds = ? n,m = ? n (e ?s )L m (e ?s )e ?s ds which shows the orthonormality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 : 1 2</head><label>41</label><figDesc>Basis kernels for (A + P P , B) for HiPPO-LegS (A, B) and random i.i.d. Gaussian P with varying std ?, illustrating that the SSM basis is very sensitive to low-rank perturbations. Note that the normal-HiPPO matrix A (N ) = A + P P for P with entries of magnitude N which is far larger, highlighting how unexpected the theoretical result Theorem 3 is.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Diagonal approximation to S4-FouT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>(a) S4-FouT is a version of S4 that produces truncated Fourier basis functions choosing a particular (A, B). This captures sliding Fourier transforms as a state space model. (b) Removing the low-rank term from the FouT matrix does not approximate S4-FouT. This diagonal state matrix has real part 0 that produces infinite oscillations and does not perform well empirically.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>(Parameterization choices for Structured SSMs.) Aside from the core structure of A and the computation of its convolution kernel, SSMs have several design choices which are consolidated in S4D.</figDesc><table><row><cell>S4</cell><cell>DPLR</cell><cell>Cauchy</cell><cell>Bilinear</cell><cell>exp</cell><cell>Yes</cell><cell>HiPPO</cell></row><row><cell>DSS</cell><cell>diagonal</cell><cell>softmax</cell><cell>ZOH</cell><cell>id (none)</cell><cell>No</cell><cell>HiPPO-D</cell></row><row><cell>S4D</cell><cell>diagonal</cell><cell>Vandermonde</cell><cell>either</cell><cell>exp / ReLU</cell><cell>optional</cell><cell>various</cell></row></table><note>Method Structure Kernel Computation Discretization Constraint (A) Trainable B Initialization of A</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Ablations of different parameterizations of diagonal SSMs using S4D-Inv. (Left) trainability and computation; (Right) discretization and parameterization.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>(Initialization and Trainability ablations)</figDesc><table><row><cell></cell><cell cols="2">SC (uni)</cell><cell></cell><cell cols="2">BIDMC (SpO2)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>l (best)</cell><cell>Val (first)</cell><cell cols="2">Val (best)</cell><cell cols="2">Val (best)</cell><cell></cell><cell></cell><cell cols="2">sCIFAR</cell><cell cols="2">SC (AR)</cell><cell>BIDMC</cell></row><row><cell>86.19 86.30</cell><cell cols="2">Ablation S4D-Lin 33.87 Scale 0.01 8.77</cell><cell cols="2">sCIFAR 85.12 85.33 -7.27 57.35</cell><cell>SC (AR) 90.66 0.1049 -1.92 0.1106</cell><cell>BIDMC 0.128 +0.040</cell><cell>Frozen (A, B) S4-LegS</cell><cell>Acc (first) 53.63</cell><cell>Acc (best) 86.19</cell><cell>Acc (first) 33.87</cell><cell>Acc (best) 85.33</cell><cell>RMSE (best) 0.1049</cell></row><row><cell>86.05</cell><cell cols="2">Scale 100 9.27</cell><cell>69.57</cell><cell>-7.91</cell><cell>-4.04 0.1072</cell><cell>+0.077</cell><cell>S4-LegT</cell><cell>54.76</cell><cell>86.30</cell><cell>8.77</cell><cell>57.35</cell><cell>0.1106</cell></row><row><cell>86.53</cell><cell cols="2">Random Imag 34.06</cell><cell>83.37</cell><cell>-0.42</cell><cell>-3.08 0.0887</cell><cell>-0.001</cell><cell>S4-FouT</cell><cell>55.28</cell><cell>86.05</cell><cell>9.27</cell><cell>69.57</cell><cell>0.1072</cell></row><row><cell>84.81</cell><cell cols="2">Random Real 22.76</cell><cell>77.18</cell><cell>-0.73</cell><cell>-0.87 0.0960</cell><cell>+0.011</cell><cell>S4-LegS+FouT</cell><cell>54.38</cell><cell>86.53</cell><cell>34.06</cell><cell>83.37</cell><cell>0.0887</cell></row><row><cell>84.40</cell><cell cols="2">Random Both 18.49</cell><cell>76.53</cell><cell>-1.28</cell><cell>-5.88 0.0995</cell><cell>+0.007</cell><cell>S4D-LegS</cell><cell>50.87</cell><cell>84.81</cell><cell>22.76</cell><cell>77.18</cell><cell>0.0960</cell></row><row><cell>84.96</cell><cell>19.09</cell><cell></cell><cell>75.58</cell><cell></cell><cell>0.0935</cell><cell></cell><cell>S4D-Inv</cell><cell>53.19</cell><cell>84.40</cell><cell>18.49</cell><cell>76.53</cell><cell>0.0995</cell></row><row><cell></cell><cell>S4D-Inv</cell><cell></cell><cell cols="2">84.79</cell><cell>90.27</cell><cell>0.114</cell><cell>S4D-Lin</cell><cell>51.75</cell><cell>84.96</cell><cell>19.09</cell><cell>75.58</cell><cell>0.0935</cell></row><row><cell>86.29</cell><cell cols="2">Scale 0.01 Scale 100 62.19</cell><cell>90.68</cell><cell>-5.03 -7.77</cell><cell>-0.08 -52.31 0.1033</cell><cell>+0.028 +0.034</cell><cell>Trainable (A, B)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>86.12</cell><cell cols="2">Random Imag 55.86</cell><cell>90.42</cell><cell>-0.29</cell><cell>-0.52 0.1146</cell><cell>+0.010</cell><cell>S4-LegS</cell><cell>54.23</cell><cell>86.29</cell><cell>62.19</cell><cell>90.68</cell><cell>0.1033</cell></row><row><cell>85.93</cell><cell cols="2">Random Real 60.56</cell><cell>90.83</cell><cell>0.12</cell><cell>-2.18 0.1136</cell><cell>+0.032</cell><cell>S4-LegT</cell><cell>55.16</cell><cell>86.12</cell><cell>55.86</cell><cell>90.42</cell><cell>0.1146</cell></row><row><cell>86.18</cell><cell cols="2">Random Both 61.76</cell><cell>91.01</cell><cell>-1.55</cell><cell>-0.55 0.0970</cell><cell>+0.024</cell><cell>S4-FouT</cell><cell>55.89</cell><cell>85.93</cell><cell>60.56</cell><cell>90.83</cell><cell>0.1136</cell></row><row><cell>85.64</cell><cell cols="2">S4D-Inv2 47.54</cell><cell>88.47</cell><cell>-2.62</cell><cell>-39.84 0.1148</cell><cell>+0.005</cell><cell>S4-LegS+FouT</cell><cell>55.00</cell><cell>86.18</cell><cell>61.76</cell><cell>91.01</cell><cell>0.0970</cell></row><row><cell>84.59</cell><cell cols="2">S4D-Quad 45.73</cell><cell>89.69</cell><cell>-1.83</cell><cell>-0.62 0.1132</cell><cell>+0.024</cell><cell>S4D-LegS</cell><cell>50.41</cell><cell>85.64</cell><cell>47.54</cell><cell>88.47</cell><cell>0.1148</cell></row><row><cell>85.75</cell><cell cols="2">S4D-Random S4D-Real 47.68</cell><cell>89.56</cell><cell>-6.32 -5.45</cell><cell>-1.95 -10.17 0.1032</cell><cell>+0.034 +0.066</cell><cell>S4D-Inv S4D-Lin</cell><cell>53.42 52.23</cell><cell>84.59 85.75</cell><cell>45.73 47.68</cell><cell>89.69 89.56</cell><cell>0.1132 0.1032</cell></row><row><cell></cell><cell cols="6">(a) Ablations of the initialization of the diagonal</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">A matrix in S4D. Very simple changes that largely</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">preserve the structure of the diagonal eigenvalues</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">all degrade performance.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(b) Results for all S4 and S4D methods on the ablation datasets, when the A and B matrices are either frozen (Top) or trained (Bottom). Diagonal state matrices are highly competitive with full DPLR versions, achieving strong results on all datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>(Ablation datasets: Full results with larger models.) For Speech Commands, we show both an autoregressive model as in the ablations, and an unconstrained bidirectional model.</figDesc><table><row><cell>Model</cell><cell>sCIFAR</cell><cell></cell><cell>SC</cell><cell></cell><cell>BIDMC</cell><cell></cell></row><row><cell></cell><cell>Test</cell><cell>AR</cell><cell>Bi.</cell><cell>HR</cell><cell>RR</cell><cell>SpO2</cell></row><row><cell>S4-LegS</cell><cell cols="3">91.80 (0.43) 93.60 (0.13) 96.08 (0.15)</cell><cell cols="2">0.332 (0.013) 0.247 (0.062)</cell><cell>0.090 (0.006)</cell></row><row><cell>S4-FouT</cell><cell>91.22 (0.25)</cell><cell>91.78 (0.10)</cell><cell>95.27 (0.20)</cell><cell>0.339 (0.020)</cell><cell>0.301 (0.030)</cell><cell>0.068 (0.003)</cell></row><row><cell cols="2">S4D-LegS 89.92 (1.69)</cell><cell>93.57 (0.09)</cell><cell>95.83 (0.14)</cell><cell>0.367 (0.001)</cell><cell>0.248 (0.036)</cell><cell>0.102 (0.001)</cell></row><row><cell>S4D-Inv</cell><cell>90.69 (0.06)</cell><cell>93.40 (0.67)</cell><cell>96.18 (0.27)</cell><cell>0.373 (0.024)</cell><cell>0.254 (0.022)</cell><cell>0.110 (0.001)</cell></row><row><cell>S4D-Lin</cell><cell>90.42 (0.03)</cell><cell>93.37 (0.05)</cell><cell cols="2">96.25 (0.03) 0.379 (0.006)</cell><cell cols="2">0.226 (0.008) 0.114 (0.003)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>(Long Range Arena) Accuracy on full suite of LRA tasks. Hyperparameters in Appendix B.</figDesc><table><row><cell>Model</cell><cell>ListOps</cell><cell>Text</cell><cell>Retrieval</cell><cell>Image</cell><cell cols="3">Pathfinder Path-X Avg</cell></row><row><cell>S4-LegS</cell><cell>59.60 (0.07)</cell><cell>86.82 (0.13)</cell><cell>90.90 (0.15)</cell><cell>88.65 (0.23)</cell><cell>94.20 (0.25)</cell><cell>96.35</cell><cell>86.09</cell></row><row><cell>S4-FouT</cell><cell>57.88 (1.90)</cell><cell>86.34 (0.31)</cell><cell>89.66 (0.88)</cell><cell cols="2">89.07 (0.19) 94.46 (0.24)</cell><cell></cell><cell>77.90</cell></row><row><cell>S4D-LegS</cell><cell>60.47 (0.34)</cell><cell>86.18 (0.43)</cell><cell>89.46 (0.14)</cell><cell>88.19 (0.26)</cell><cell>93.06 (1.24)</cell><cell>91.95</cell><cell>84.89</cell></row><row><cell>S4D-Inv</cell><cell>60.18 (0.35)</cell><cell cols="3">87.34 (0.20) 91.09 (0.01) 87.83 (0.37)</cell><cell>93.78 (0.25)</cell><cell>92.80</cell><cell>85.50</cell></row><row><cell>S4D-Lin</cell><cell cols="2">60.52 (0.51) 86.97 (0.23)</cell><cell>90.96 (0.09)</cell><cell>87.93 (0.34)</cell><cell>93.96 (0.60)</cell><cell></cell><cell>78.39</cell></row><row><cell cols="2">S4 (original) 58.35</cell><cell>76.02</cell><cell>87.09</cell><cell>87.26</cell><cell>86.05</cell><cell>88.10</cell><cell>80.48</cell></row><row><cell cols="2">Transformer 36.37</cell><cell>64.27</cell><cell>57.46</cell><cell>42.44</cell><cell>71.40</cell><cell></cell><cell>53.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>(S4D Path-X Ablations.) Ablating parameterization choices for models with less than 200K parameters.</figDesc><table><row><cell>S4D</cell><cell cols="2">Identity (?) ReLU (?) ZOH disc.</cell><cell>Frozen B</cell><cell>ZOH + softmax DSS</cell></row><row><cell cols="2">92.12 (0.34) 92.32 (0.16)</cell><cell cols="3">92.29 (0.20) 92.09 (0.08) 91.66 (0.62) 90.92 (0.34)</cell><cell>89.72 (0.33)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Full results for Table 2 (Left) including standard deviations. models have only H ? {num. layers} more parameters than S4D models, arising from the P tensor in the DPLR representation A = ? ? P P . This choice was made because it generally does not affect performance much, while reducing parameter count and ensuring that S4 vs. S4D models have very similar numbers of parameters.</figDesc><table><row><cell cols="2">Trainable B Method</cell><cell>sCIFAR</cell><cell>SC (AR)</cell><cell>BIDMC (SpO2)</cell></row><row><cell>No</cell><cell>Softmax</cell><cell cols="3">85.04 (0.22) 89.80 (0.21) 0.1299 (0.0048)</cell></row><row><cell>No</cell><cell cols="4">Vandermonde 84.78 (0.16) 89.62 (0.03) 0.1355 (0.0039)</cell></row><row><cell>Yes</cell><cell>Softmax</cell><cell cols="3">85.37 (0.43) 90.06 (0.11) 0.1170 (0.0039)</cell></row><row><cell>Yes</cell><cell cols="4">Vandermonde 85.37 (0.43) 90.34 (0.18) 0.1274 (0.0020)</cell></row><row><cell>therefore the S4</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Full results for Table 2 (Right) including standard deviations.</figDesc><table><row><cell cols="3">Discretization Real part sCIFAR</cell><cell>SC (AR)</cell><cell>BIDMC (SpO2)</cell></row><row><cell>Bilinear</cell><cell>Exp</cell><cell cols="2">85.20 (0.18) 89.52 (0.01) 0.1193 (0.0069)</cell></row><row><cell>Bilinear</cell><cell>-</cell><cell cols="2">85.35 (0.27) 90.58 (0.37) 0.1102 (0.0075)</cell></row><row><cell>Bilinear</cell><cell>ReLU</cell><cell cols="2">85.06 (0.06) 90.22 (0.25) 0.1172 (0.0063)</cell></row><row><cell>ZOH</cell><cell>Exp</cell><cell cols="2">85.02 (0.24) 89.93 (0.07) 0.1303 (0.0014)</cell></row><row><cell>ZOH</cell><cell>-</cell><cell cols="2">85.15 (0.13) 90.19 (0.58) 0.1289 (0.0035)</cell></row><row><cell>ZOH</cell><cell>ReLU</cell><cell cols="2">84.98 (0.72) 90.03 (0.13) 0.1232 (0.0065)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="2">: (Sequential CIFAR image</cell></row><row><cell cols="2">classification. Test accuracy (Std. dev.)</cell></row><row><cell>Model</cell><cell>sCIFAR</cell></row><row><cell>S4-LegS</cell><cell>91.80 (0.43)</cell></row><row><cell>S4-FouT</cell><cell>91.22 (0.25)</cell></row><row><cell>S4-(LegS/FouT)</cell><cell>91.58 (0.17)</cell></row><row><cell>S4D-LegS</cell><cell>89.92 (1.69)</cell></row><row><cell>S4D-Inv</cell><cell>90.69 (0.06)</cell></row><row><cell>S4D-Lin</cell><cell>90.42 (0.03)</cell></row><row><cell>Transformer [25]</cell><cell>62.2</cell></row><row><cell>FlexConv</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>(BIDMC Vital signs prediction.) RMSE for predicting respiratory rate (RR), heart rate (HR), and blood oxygen (SpO2).</figDesc><table><row><cell>Model</cell><cell>HR</cell><cell>RR</cell><cell>SpO2</cell></row><row><cell>S4-LegS</cell><cell cols="2">0.332 (0.013) 0.247 (0.062)</cell><cell>0.090 (0.006)</cell></row><row><cell>S4-FouT</cell><cell>0.339 (0.020)</cell><cell>0.301 (0.030)</cell><cell>0.068 (0.003)</cell></row><row><cell>S4-(LegS/FouT)</cell><cell>0.344 (0.032)</cell><cell cols="2">0.163 (0.008) 0.080 (0.007)</cell></row><row><cell>S4D-LegS</cell><cell>0.367 (0.001)</cell><cell>0.248 (0.036)</cell><cell>0.102 (0.001)</cell></row><row><cell>S4D-Inv</cell><cell>0.373 (0.024)</cell><cell>0.254 (0.022)</cell><cell>0.110 (0.001)</cell></row><row><cell>S4D-Lin</cell><cell>0.379 (0.006)</cell><cell>0.226 (0.008)</cell><cell>0.114 (0.003)</cell></row><row><cell>UnICORNN [21]</cell><cell>1.39</cell><cell>1.06</cell><cell>0.869</cell></row><row><cell>coRNN [21]</cell><cell>1.81</cell><cell>1.45</cell><cell>-</cell></row><row><cell>CKConv</cell><cell>2.05</cell><cell>1.214</cell><cell>1.051</cell></row><row><cell>NRDE [15]</cell><cell>2.97</cell><cell>1.49</cell><cell>1.29</cell></row><row><cell>LSTM</cell><cell>10.7</cell><cell>2.28</cell><cell>-</cell></row><row><cell>Transformer</cell><cell>12.2</cell><cell>2.61</cell><cell>3.02</cell></row><row><cell>XGBoost [23]</cell><cell>4.72</cell><cell>1.67</cell><cell>1.52</cell></row><row><cell cols="2">Random Forest [23] 5.69</cell><cell>1.85</cell><cell>1.74</cell></row><row><cell>Ridge Regress. [23]</cell><cell>17.3</cell><cell>3.86</cell><cell>4.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>(Speech Commands classification.) Test accuracy on 35-way keyword spotting. Training examples are 1-second audio waveforms sampled at 16000Hz, or a 1-D sequence of length 16000. Last column indicates 0-shot testing at 8000Hz where examples are constructed by naive decimation.</figDesc><table><row><cell>Model</cell><cell cols="2">Parameters 16000Hz</cell><cell>8000Hz</cell></row><row><cell>S4-LegS</cell><cell>307K</cell><cell>96.08 (0.15)</cell><cell>91.32 (0.17)</cell></row><row><cell>S4-FouT</cell><cell>307K</cell><cell>95.27 (0.20)</cell><cell>91.59 (0.23)</cell></row><row><cell cols="2">S4-(LegS/FouT) 307K</cell><cell>95.32 (0.10)</cell><cell>90.72 (0.68)</cell></row><row><cell>S4D-LegS</cell><cell>306K</cell><cell>95.83 (0.14)</cell><cell>91.08 (0.16)</cell></row><row><cell>S4D-Inv</cell><cell>306K</cell><cell>96.18 (0.27)</cell><cell>91.80 (0.24)</cell></row><row><cell>S4D-Lin</cell><cell>306K</cell><cell cols="2">96.25 (0.03) 91.58 (0.33)</cell></row><row><cell>InceptionNet</cell><cell>481K</cell><cell>61.24 (0.69)</cell><cell>05.18 (0.07)</cell></row><row><cell>ResNet-18</cell><cell>216K</cell><cell>77.86 (0.24)</cell><cell>08.74 (0.57)</cell></row><row><cell>XResNet-50</cell><cell>904K</cell><cell>83.01 (0.48)</cell><cell>07.72 (0.39)</cell></row><row><cell>ConvNet</cell><cell>26.2M</cell><cell>95.51 (0.18)</cell><cell>07.26 (0.79)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>The values of the best hyperparameters found for all datasets; full models on ablation datasets (Top) and LRA (Bottom). LR is learning rate and WD is weight decay. BN and LN refer to Batch Normalization and Layer Normalization.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">HiPPO also specifies formulas for B, but the state matrix A is more important. There are many other HiPPO instantiations besides LegS, but HiPPO-LegS is the main one that S4 uses and the term "HiPPO matrix" without the suffix refers to this one.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">An early version of DSS claimed that it did not require a custom kernel while S4 does, but this is because of its extra memory usage. The PyTorch implementation of S4 has an optional custom CUDA kernel primarily to save this factor of N in space.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We note that a line of prior work including S4<ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref> all used a smaller 10-class subset of SC, so our results on the full dataset are not directly comparable.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, Brown Institute for Media Innovation, Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, Fannie and John Hertz Foundation, National Science Foundation Graduate Research Fellowship Program, Texas Instruments, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Trellis networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lipschitz recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>N Benjamin Erichson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Azencot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Queiruga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Hodgkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">It&apos;s raw! audio generation with state-space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09729</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hippo: Recurrent memory with optimal polynomial projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving the gating mechanism of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">Le</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">How to train your hippo: State space models with generalized basis projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Timalsina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.12037</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Diagonal state spaces are as effective as structured state spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.14343</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural controlled differential equations for irregular time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08926</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural rough differential equations for long time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristopher</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">In-depth benchmarking of deep neural network architectures for ecg diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Nonaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Seita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="414" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Structured matrices and polynomials: unified superfast algorithms</title>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A first course in numerical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Ralston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Rabinowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Courier Corporation</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Ckconv: Continuous kernel convolution for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>David W Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Kuzina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoogendoorn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02611</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flexconv: Continuous kernel convolutions with differentiable kernel sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert-Jan</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruintjes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Hoogendoorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unicornn: A recurrent model for learning very long time dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05202</idno>
		<title level="m">Glu variants improve transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Time series extrinsic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Wei Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bergmeir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-021-00745-9</idno>
		<ptr target="https://doi.org/10.1007/s10618-021-00745-9" />
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long range arena : A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=qVyeW-grC2k" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning longer-term dependencies in RNNs with auxiliary losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Legendre memory units: Continuous-time representation in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Kaji?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15544" to="15553" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
