<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Instance Segmentation by Learning Annotation Consistent Instances</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Arun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVIT</orgName>
								<orgName type="institution" key="instit2">KCIS</orgName>
								<orgName type="institution" key="instit3">IIIT Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVIT</orgName>
								<orgName type="institution" key="instit2">KCIS</orgName>
								<orgName type="institution" key="instit3">IIIT Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pawan Kumar</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">OVAL</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised Instance Segmentation by Learning Annotation Consistent Instances</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent approaches for weakly supervised instance segmentations depend on two components: (i) a pseudo label generation model which provides instances that are consistent with a given annotation; and (ii) an instance segmentation model, which is trained in a supervised manner using the pseudo labels as ground-truth. Unlike previous approaches, we explicitly model the uncertainty in the pseudo label generation process using a conditional distribution. The samples drawn from our conditional distribution provide accurate pseudo labels due to the use of semantic class aware unary terms, boundary aware pairwise smoothness terms, and annotation aware higher order terms. Furthermore, we represent the instance segmentation model as an annotation agnostic prediction distribution. In contrast to previous methods, our representation allows us to define a joint probabilistic learning objective that minimizes the dissimilarity between the two distributions. Our approach achieves state of the art results on the PASCAL VOC 2012 data set, outperforming the best baseline by 4.2% mAP r 0.5 and 4.8% mAP r 0.75 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The instance segmentation task is to jointly estimate the class labels and segmentation masks of the individual objects in an image. Significant progress on instance segmentation has been made based on the convolutional neural networks (CNN) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref>. However, the traditional approach of learning CNN-based models requires a large number of training images with instancelevel pixel-wise annotations. Due to the high cost of collecting these supervised labels, researchers have looked at training these instance segmentation models using weak annotations, ranging from bounding boxes <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref> to image-level labels <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Many of the recent approaches for weakly supervised instance segmentation can be thought of as consisting of two components. First, a pseudo label generation model, which provides instance segmentations that are consistent with the weak annotations. Second, an instance segmentation model which is trained by treating the pseudo labels as ground-truth, and provides the desired output at test time.</p><p>Seen from the above viewpoint, the design of a weakly supervised instance segmentation approach boils down to three questions. First, how do we represent the instance segmentation model? Second, how do we represent the pseudo label generation model? And third, how do we learn the parameters of the two models using weakly supervised data? The answer to the first question is relatively clear: we should use a model that performs well when trained in a supervised manner, for example, Mask R-CNN <ref type="bibr" target="#b16">[17]</ref>. However, we argue that the existing approaches fail to provide a satisfactory answer to the latter two questions.</p><p>Specifically, the current approaches do not take into account the inherent uncertainty in the pseudo label generation process <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>. Consider, for instance, a training image that has been annotated to indicate the presence of a person. There can be several instance segmentations that are consistent with this annotation, and thus, one should not rely on a single pseudo label to train the instance segmentation model. Furthermore, none of the existing approaches provide a coherent learning objective for the two models. Often they suggest a simple two-step learning approach, that is, generate one set of pseudo labels followed by a one time training of the instance segmentation model <ref type="bibr" target="#b0">[1]</ref>. While some works consider an iterative training procedure <ref type="bibr" target="#b22">[23]</ref>, the lack of a learning objective makes it difficult to analyse and adapt them in varying settings.</p><p>In this work, we address the deficiencies of prior work by (i) proposing suitable representations for the two aforementioned components; and (ii) estimating their parameters using a principled learning objective. In more detail, we explicitly model the uncertainty in pseudo labels via a conditional distribution. The conditional distribution consists of three terms: (i) a semantic class aware unary term to predict the score of each segmentation proposal; (ii) a boundary aware pairwise term that encourages the segmentation proposal to completely cover the object; and (iii) an annotation consistent higher order term that enforces a global constraint on all segmentation proposals (for example, in the case of image-level labels, there exists at least one corresponding segmentation proposal for each class, or in the case of bounding boxes, there exists a segmentation proposal with sufficient overlap to each bounding box). All three terms combined enable the samples drawn from the conditional distribution to provide accurate annotation consistent instance segmentations. Furthermore, we represent the instance segmentation model as an annotation agnostic prediction distribution. This choice of representation allows us to define a joint probabilistic learning objective that minimizes the dissimilarity between the two distributions. The dissimilarity is measured using a task-specific loss function, thereby encouraging the models to produce high quality instance segmentations.</p><p>We test the efficacy of our approach on the Pascal VOC 2012 data set. We achieve 50.9% mAP r 0.5 , 28.5% mAP r 0.75 for image-level annotations and 32.1% mAP r 0.75 for bounding box annotations, resulting in an improvement of over 4% and 10% respectively over the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Due to the taxing task of acquiring the expensive per-pixel annotations, many weakly supervised methods have emerged that can leverage cheaper labels. For the task of semantic segmentation various types of weak annotations, such as image-level <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>, point <ref type="bibr" target="#b5">[6]</ref>, scribbles <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b38">39]</ref>, and bounding boxes <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31]</ref>, have been utilized. However, for the instance segmentation, only image-level <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> and bounding box <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref> supervision have been explored. Our setup considers both the image-level and the bounding box annotations as weak supervision. For the bounding box annotations, Hsu et al. <ref type="bibr" target="#b17">[18]</ref> employs a bounding box tightness constraint and train their method by employing a multiple instance learning (MIL) based objective but they do not model the annotation consistency constraint for computational efficiency.</p><p>Most of the initial works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref> on weakly supervised instance segmentation using image-level supervision were based on the class activation maps (CAM) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>. In their work, Zhou et al. <ref type="bibr" target="#b41">[42]</ref> identify the heatmap as well as its peaks to represent the location of different objects. Although these methods are good at finding the spatial location of each object instance, they focus only on the most discriminative regions of the object and therefore, do not cover the entire object. Ge et al <ref type="bibr" target="#b12">[13]</ref> uses the CAM output as the initial segmentation seed and refines it in a multi-task setting, which they train progressively. We use the output of <ref type="bibr" target="#b41">[42]</ref> as the initial segmentation seed of our conditional distribution but the boundary aware pairwise term in our conditional distribution encourages pseudo labels to cover the entire object.</p><p>Most recent works on weakly supervised learning adopt a two-step process -generate pseudo labels and train a supervised model treating these pseudo labels as ground truth. Such an approach provides state-of-the-art results for various weakly supervised tasks like object detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>, semantic segmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>, and instance segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>. Ahn et al. <ref type="bibr" target="#b0">[1]</ref> synthesizes pseudo labels by learning the displacement fields and pairwise pixel affinities. These pseudo labels are then used to train a fully supervised Mask R-CNN <ref type="bibr" target="#b16">[17]</ref>, which is used at the test time. Laradji et al. <ref type="bibr" target="#b22">[23]</ref> iteratively samples the pseudo segmentation label from MCG segmentation proposal set <ref type="bibr" target="#b2">[3]</ref> and train a supervised Mask R-CNN <ref type="bibr" target="#b16">[17]</ref>. This is similar in spirit to our approach of using the two distributions. However, they neither have a unified learning objective for the two distribution nor do they model the uncertainty in their pseudo label generation model. Regardless, the improvement in the results reported by these two methods advocates the importance of modeling two separate distributions. In our method, we explicitly model the two distributions and define a unified learning objective that minimizes the dissimilarity between them.</p><p>Our framework has been inspired by the work of Kumar et al. <ref type="bibr" target="#b21">[22]</ref> who were the first to show the necessity of modeling uncertainty by employing two separate distributions in a latent variable model. This framework has been adopted for weakly supervised training of CNNs for learning human poses and object detection tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. While their framework provides an elegant formulation for weakly supervised learning, its various components need to be carefully constructed for each task. Our work can be viewed as designing conditional and prediction distributions, as well as the corresponding inference algorithms, which are suited to instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>We denote an input image as x ? R (H?W ?3) , where H and W are the height and the width of the image respectively. For each image, a set of segmentation proposals R = {r 1 , . . . , r P } are extracted from a class-agnostic object proposal algorithm. In this work, we use Multiscale Combinatorial Grouping (MCG) <ref type="bibr" target="#b2">[3]</ref> to obtain the object proposals. For the sake of simplicity, we only consider imagelevel annotations in our description. However, our framework can be easily extended to other annotations such as bounding boxes. Indeed, we will use bounding box annotations in our experiments. Given an image and the segmentation proposals, our goal is to classify each of the segmentation proposals to one of the C + 1 categories from the set {0, 1, . . . , C}. Here category 0 is the background and categories {1, . . . , C} are object classes.</p><p>We denote the image-level annotations by a = {0, 1} C , where a (j) = 1 if image x contains the j?th object. Furthermore, we denote the unknown instancelevel (segmentation proposal) label as y = {0, . . . , C} P , where y (i) = j if the i?th segmentation proposal is of the j?th category. A weakly supervised data set W = {(x n , a n ) | n = 1, . . . , N } contains N pairs of images x n and their corresponding image-level annotations a n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conditional Distribution</head><p>Given the weakly supervised data set W, we wish to generate pseudo instancelevel labels y such that they are annotation consistent. Specifically, given the segmentation proposals R for an image x, there must exists at least one segmentation proposal for each image-level annotation a (j) = 1. Since the annotations are image-level, there is inherent uncertainty in the figure-ground separation of the objects. We model this uncertainty by defining a distribution Pr c (y | x, a; ? c ) over the pseudo labels conditioned on the image-level weak annotations. Here, ? c are the parameters of the distribution. We call this a conditional distribution.</p><p>The conditional distribution itself is not explicitly represented. Instead, we use a neural network with parameters ? c which generates samples that can be used as pseudo labels. For the generated samples to be accurate, we wish that they have the following three properties: (i) they should have high fidelity with the scores assigned by the neural network for each region proposal belonging to each class; (ii) they should cover as large a portion of an object instance as possible; and (iii) they should be consistent with the annotation.</p><p>Modeling: In order for the conditional distribution to be annotation consistent, the instance-level labels y need to be compatible with the image-level annotation a. This constraint cannot be trivially decomposed over each segmentation proposal. As a result, it would be prohibitively expensive to model the conditional distribution directly as one would be required to compute its partition function. Taking inspiration from Arun et al. <ref type="bibr" target="#b4">[5]</ref>, we instead draw representative samples <ref type="figure">Fig. 1</ref>. The conditional network: a modified U-Net architecture is used to model the conditional network. For a single input image and three different noise samples {z 1 , z 2 , z 3 } (represented as red, green, and blue matrix) and a pool of segmentation proposals, three different instances are predicted for the given weak annotation (aeroplane in this example). Here the noise sample is concatenated as an extra channel to the final layer of the U-Net. The segmentation proposals are multiplied element-wise with the global feature to obtain the proposal specific feature. A global average pooling is applied to get class specific score. Finally, an inference algorithm generates the predicted samples.</p><p>from the conditional distribution using the Discrete Disco Nets <ref type="bibr" target="#b6">[7]</ref>. We will now describe how we model the conditional distribution through a Discrete Disco Nets, which we will now call a conditional network.</p><p>Consider the modified fully convolutional U-Net <ref type="bibr" target="#b33">[34]</ref> architecture shown in <ref type="figure">figure 1</ref> for the conditional distribution. The parameters of the conditional distribution ? c are modeled by the weights of the conditional network. Similar to <ref type="bibr" target="#b20">[21]</ref>, noise sampled from a uniform distribution is added after the U-Net block (depicted by the colored filter). Each forward pass through the network takes the image x and noise sample z k as input and produces a score function F k u,yu (? c ) for each segmentation proposal u and the corresponding putative label y u . We generate K different score functions using K different noise samples. These score functions are then used to sample the segmentation region proposals y k c such that they are annotation consistent. This enables us to efficiently generate the samples from the underlying distribution.</p><p>Inference: Given the input pair (x, z k ) the conditional network outputs K score functions for each of the segmentation proposal F k u,yu (? c ). We redefine these score functions to obtain a final score function such that it is then used to sample the segmentation region proposals y k c . The final score function has the following three properties.</p><p>1. The score of the sampled segmentation region proposal should be consistent with the score function. This semantic class aware unary term ensures that the final score captures the class specific features of each segmentation proposal. Formally, G k u,yu (y c ) = F k u,yu (? c ).</p><p>2. The unary term alone is biased towards segmentation proposals that are highly discriminative. This results in selecting a segmentation proposal which does not cover the object in its entirety. We argue that all the neighboring segmentation proposals must have the same score discounted by the edge weights between them. We call this condition boundary aware pairwise term.</p><p>In order to make the score function G k u,yu (y c ) pairwise term aware, we employ a simple but efficient iterative algorithm. The algorithm proceeds by iteratively updating the scores G k u,yu (y c ) by adding the contribution of their neighbors discounted by the edge weights between them until convergence. In practice, we fix the number of iteration to 3. Note that, it is possible to backpropagate through the iterative algorithm by simply unrolling its iterations, similar to a recurrent neural networks (RNN). Formally,</p><formula xml:id="formula_0">(1) G k,n u,yu (y c ) = G k,n?1 u,yu (y c ) + 1 H k,n?1 u,v (y c ) + ? exp (?I u,v ).</formula><p>Here, n denotes the iteration step for the iterative algorithm and ? is a small positive constant added for numerical stability. In our experiments, we set ? = 0.1. The term H k,n?1 u,v (y c ) is the difference between the scores of the neighboring segmentation proposal. It helps encourage same label for the neighboring segmentation proposals that are not separated by the edge pixels. It is given as,</p><formula xml:id="formula_1">(2) H k,n?1 u,v (y c ) = u,v?Nu G k,n?1 u,yu (y c ) ? G k,n?1 v,yu (y c ) 2 .</formula><p>The term I u,v is the sum of the edge pixel values between the two neighboring segmentation regions. Note that the pairwise term is a decay function weighted by the edge pixel values. This ensures a high contribution to the pairwise term is only from the pair of segmentation proposals that does not share an edge. 3. In order to ensure that at there must exist at least one segmentation proposal for every image-level annotation,a higher order penalty is added to the score. We call this annotation consistent higher order term. Formally,</p><formula xml:id="formula_2">(3) S k (y c ) = P u=1 G k,n u,yu (y c ) + Q k (y c ).</formula><p>Here,</p><formula xml:id="formula_3">(4) Q k (y c ) = ? ? ? ? ? 0 if ?j ? {1, . . . , C} s.t. a (j) = 1, ?i ? R s.t. y (i) = j, ?? otherwise.</formula><p>Given the scoring function in equation <ref type="formula">(3)</ref>, we compute the k?th sample of the conditional network as,</p><formula xml:id="formula_4">(5) y k c = arg max y?Y S k (y c ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Inference Algorithm for the Conditional Net</head><p>Input : Region masks: R, Image-level labels: a Output: Predicted instance level instances:</p><formula xml:id="formula_5">y k c /* Iterative Algorithm */ 1 G k u,yu (yc) = F k u,yu (?c) 2 repeat 3 for v ? Nu do 4 H k,n?1 u,v (yc) = u,v?Nu G k,n?1 u,yu (yc) ? G k,n?1 v,yv (yc) 2 . 5 G k,n u,yu (yc) = G k,n?1 u,yu (yc) + 1 H k,n?1 u,v (yc)+? exp (?Iu,v) 6 until G k,n u,yu (yc) has coverged /* Greedily select highest scoring non-overlapping proposal */ 7 Y ? ? 8 for j ? {1, . . . , C} ? a (j) = 1 do 9 Yj ? ? 10 Rj ? sort(G k,n u,yu (yc)) 11 for i ? 1, . . . , P do 12 Yj ? ri 13 Rj ? Rj ? ri 14 for l ? Rj ? r i ?r l r l &gt; t do 15 Rj ? Rj ? r l 16 Y ? Yj 17 return y k c = Y</formula><p>Observe that in equation <ref type="formula">(5)</ref>, the arg max is computed over the entire output space Y. A na?ve brute force algorithm is therefore not feasible. We design an efficient greedy algorithm that selects the highest scoring non-overlapping proposal. The inference algorithm is described in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prediction Distribution</head><p>The task of the supervised instance segmentation model is to predict the instancemask given an image. We employ Mask R-CNN <ref type="bibr" target="#b17">[18]</ref> for this task. As predictions for each of the regions in the Mask R-CNN is computed independently, we can view the output of the Mask R-CNN as the following fully factorized distribution,</p><formula xml:id="formula_6">(6) Pr p (y | x; ? p ) = R i=1 Pr(y i | r i , x i ; ? p ).</formula><p>Here, R are the set of bounding box regions proposed by the region proposal network and r i are its corresponding region features. The term y i is the corresponding prediction for each of the bounding box proposals. We call the above distribution a prediction distribution and the Mask R-CNN a prediction network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning Objective</head><p>Given the weakly supervised data set W, our goal is to learn the parameters of the prediction and the conditional distribution, ? p and ? c respectively. We observe that the task of both the prediction and the conditional distribution is to predict the instance segmentation mask. Moreover, the conditional distribution utilizes the extra information in the form of image-level annotations. Therefore, it is expected to produce better instance segmentation masks. Leveraging the task similarity between the two distribution, we would like to bring the two distribution close to each other. Inspired by the work of <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22]</ref>, we design a joint learning objective that can minimize the dissimilarity coefficient <ref type="bibr" target="#b32">[33]</ref> between the prediction and the conditional distribution. In what follows, we briefly describe the dissimilarity coefficient before applying it to our setting.</p><p>Dissimilarity Coefficient: The dissimilarity coefficient between any two distributions Pr 1 (?) and Pr 2 (?) is determined by measuring their diversities. Given a task-specific loss function ?(?, ?), the diversity coefficient between the two distribution Pr 1 (?) and Pr 2 (?) is defined as the expected loss between two samples drawn randomly from the two distributions respectively. Formally,</p><formula xml:id="formula_7">(7) DIV ? (Pr 1 , Pr 2 ) = E y1?Pr1(?) E y2?Pr2(?) [?(y 1 , y 2 )] .</formula><p>If the model brings the two distributions close to each other, we could expect the diversity DIV ? (Pr 1 , Pr 2 ) to be small. Using this definition, the dissimilarity coefficient is defined as the following Jensen difference,</p><formula xml:id="formula_8">(8) DISC ? (Pr 1 , Pr 2 ) = DIV ? (Pr 1 , Pr 2 ) ? ?DIV ? (Pr 2 , Pr 2 ) ? (1 ? ?)DIV ? (Pr 1 , Pr 1 ),</formula><p>where, ? = [0, 1]. In our experiments, we use ? = 0.5, which results in dissimilarity coefficient being symmetric for the two distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task-Specific Loss Function:</head><p>The dissimilarity coefficient objective requires a task-specific loss function. To this end, we use the multi-task loss defined by Mask R-CNN <ref type="bibr" target="#b16">[17]</ref> as,</p><formula xml:id="formula_9">(9) ?(y 1 , y 2 ) = ? cls (y 1 , y 2 ) + ? box (y 1 , y 2 ) + ? mask (y 1 , y 2 ).</formula><p>Here, ? cls is the classification loss defined by the log loss, ? box is the bounding box regression loss defined as the smooth-L1 loss, and ? mask is the segmentation loss for the mask defined by pixel-wise cross entropy, as proposed by <ref type="bibr" target="#b16">[17]</ref>.</p><p>Note that the conditional network outputs the segmentation region y, where there are no bounding box coordinates predicted. Therefore, for the conditional network, only ? cls and ? mask is active as the gradients for ? box is 0. For the prediction network, all three components of the loss functions are active. We construct a tight bounding box around the pseudo segmentation label, which acts as a pseudo bounding box label for Mask R-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning Objective for Instance Segmentation:</head><p>We now specify the learning objective for instance segmentation using the dissimilarity coefficient and the task-specific loss function defined above as,</p><formula xml:id="formula_10">(10) ? * p , ? * c = arg min ?p,?c DISC ? (Pr p (? p ), Pr c (? c )) .</formula><p>As discussed in Section 3.2, modeling the conditional distribution directly is difficult. Therefore, the corresponding diversity terms are computed by stochastic estimators from K samples y k c of the conditional network. Thus, each diversity term is written as 3 ,</p><formula xml:id="formula_11">(11a) DIV ? (Pr p , Pr c ) = 1 K K k=1 y (i) p Pr p (y (i) p ; ? p )?(y (i) p , y k c ), (11b) DIV ? (Pr c , Pr c ) = 1 K(K ? 1) K k,k =1 k =k ?(y k c , y k c ), (11c) DIV ? (Pr p , Pr p ) = y (i) p y (i) p Pr p (y (i) p ; ? p ) Pr p (y (i) p ; ? p )?(y (i) p , y (i) p )</formula><p>Here, DIV ? (Pr p , Pr c ) measures the cross diversity between the prediction and the conditional distribution, which is the expected loss between the samples of the two distribution. Since Pr p is a fully factorized distribution, the expectation of its output can be trivially computed. As Pr c is not explicitly modeled, we draw K different samples to compute its required expectation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Optimization</head><p>As the parameters of the two distribution, ? p and ? c are modeled by a neural network, it is ideally suited to be minimized by stochastic gradient descent. We employ a block coordinate descent strategy to optimize the two sets of parameters. The algorithm proceeds by iteratively fixing the prediction network and training the conditional network, followed by learning the prediction network for a fixed conditional network. The iterative learning strategy results in a fully supervised training of each network by using the output of the other network as the pseudo label. This allows us to readily use the algorithms developed in Mask R-CNN <ref type="bibr" target="#b16">[17]</ref> and Discrete Disco Nets <ref type="bibr" target="#b6">[7]</ref>. Note that, as the conditional network obtains samples over the arg max operator in equation <ref type="formula">(5)</ref>, the objective (10) for the conditional network is non-differentiable. However, the scoring function S k (y c ) in equation <ref type="formula">(3)</ref> itself is differentiable. This allows us to use the direct loss minimization strategy <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36]</ref> developed for computing estimated gradients over the arg max operator <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref>. We provide the details of the algorithm in appendix B. <ref type="figure">Fig. 2</ref>. Examples of the predictions from the conditional and prediction networks for three different cases of varying difficulty. Columns 1 through 3 are different samples from the conditional network. For each case, its first row shows the output of the two networks after the first iteration and its second row represents the output of the two networks after the fourth (final) iteration. Each instance of an object is represented by different mask color. Best viewed in color. <ref type="figure">Figure 2</ref> provides the visualization of the output of the two networks for the first and the final iterations of the training process. The first three columns are the three output samples of the conditional distribution. Note that in our experiments, we output 10 samples corresponding to 10 different noise samples. The fourth column shows the output of the prediction distribution. The output for the prediction network is selected by employing a non-maximal suppression (NMS) with its score threshold kept at 0.7, as is the default setting in <ref type="bibr" target="#b16">[17]</ref>. The first row represents the output of the two networks after the first iteration and the second row shows their output after the fourth (final) iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Visualization of the learning process</head><p>The first case demonstrates an easy example where two cats are present in the image. Initially, the conditional distribution samples the segmentation proposals which do not cover the entire body of the cat but still manages to capture the boundaries reasonably well. However, due to the variations in these samples, the prediction distribution learns to better predict the extent of the cat pixels. This, in turn, encourages the conditional network to generate a better set of samples. Indeed, by the fourth iteration, we see an improvement in the quality of samples by the conditional network and they now cover the entire body of the cat, thereby improving the performance. As a result, we can see that finally the prediction network successfully learns to segment the two cats in the image.</p><p>The second case presents a challenging scenario where a person is riding a horse. In this case, the person is occluding the front and the rear parts of the horse. Initially, we see that the conditional network only provides samples for the most discriminative region of the horse -its face. The samples generated for the person class, though not accurate, covers the entire person. We observe that over the subsequent iterations, we get an accurate output for the person class. The output for the horse class also expands to cover its front part completely. However, since its front and the rear parts are completely separated, the final segmentation could not cover the rear part of the horse.</p><p>The third case presents another challenging scenario where there are multiple people present. Four people standing in front and two are standing at the back. Here, we observe that initially, the conditional network fails to distinguish between the two people standing in the front-left of the image and fails to detect persons standing at the back. The samples for the third and the fourth persons standing in front-center and front-right respectively are also not accurate. Over the iterations, the conditional network improves its predictions for the four people standing in front and also sometimes detect the people standing at the back. As a result, prediction network finally detects five of the six people in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data set and Evaluation Metric</head><p>Data Set: We evaluate our proposed method on Pascal VOC 2012 segmentation benchmark <ref type="bibr" target="#b11">[12]</ref>. The data set consists of 20 foreground classes. Following previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>, we use the augmented Pascal VOC 2012 data set <ref type="bibr" target="#b13">[14]</ref>, which contains 10, 582 training images.</p><p>From the augmented Pascal VOC 2012 data set, we construct two different weakly supervised data sets. The first data set is where we retain only the image-level annotations. For the second data set, we retain the bounding box information along with the image-level label. In both the data sets, the pixel-level labels are discarded.</p><p>Evaluation Metric: We adopt the standard evaluation metric for instance segmentation, mean average precision (mAP) <ref type="bibr" target="#b14">[15]</ref>. Following the same evaluation protocol from other competing approaches, we report mAP with four intersection over union (IoU) thresholds, denoted by mAP r k where k denotes the different values of IoU and k = {0.25, 0.50, 0.70, 0.75}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Initialization</head><p>We now discuss various strategies to initialize our conditional network for different levels of weakly supervised annotations.</p><p>Image Level Annotations: Following the previous works on weakly supervised instance segmentation from image-level annotations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43]</ref>, we use the Class Activation Maps (CAMs) to generate the segmentation seeds for each image in the training set. Specifically, like <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43]</ref>, we rely on the Peak Response Maps (PRM) <ref type="bibr" target="#b41">[42]</ref> to generate segmentation seeds that identify the salient parts of the objects. We utilize these seeds as pseudo segmentation labels to initially train our conditional network. We also filter the MCG <ref type="bibr" target="#b2">[3]</ref> segmentation proposal such that each selected proposal has at least a single pixel overlap with the PRM segmentation seeds. This helps us reduce the number of segmentation proposals needed thereby reducing the memory requirement. Once the initial training for the conditional network is over, we proceed with the iterative optimization strategy, described in section 5.</p><p>Bounding Box Annotations For the weakly supervised data set where bounding box annotations are present, we filter the MCG <ref type="bibr" target="#b2">[3]</ref> segmentation proposals such that only those who have a high overlap with the ground-truth bounding boxes are retained. The PRM <ref type="bibr" target="#b41">[42]</ref> segmentation seeds are also pruned such that they are contained within each of the bounding box annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison with other methods</head><p>We compare our proposed method with other state-of-the-art weakly supervised instance segmentation methods. The mean average precision (mAP) over different IoU thresholds are shown in table 1. Compared with the other methods, our proposed framework achieves state-of-the-art performance for both image-level and the bounding box labels. We also study the effect of using a different conditional network architecture based on ResNet-50 and ResNet-101. This is shown in the table as 'Ours (ResNet-50)' and 'Ours (ResNet 101)' respectively. Our main result employs a U-Net based architecture for the conditional network and is presented by 'Ours' in the table. The implementation details and the details of the alternative architecture are presented in appendix C. The encoder-decoder architecture of the U-Net allows us to learn better features. As a result, we observe that our method which adopts U-Net architecture for the conditional network consistently outperforms the one which adopts a ResNet based architecture. In table 1, observe that our approach performs particularly well for the <ref type="table">Table 1</ref>. Evaluation of instance segmentation results from different methods with varying level of supervision on Pascal VOC 2012 val set. The terms F, B, and I denotes a fully supervised approach, methods that uses the bounding box labels, and methods that uses the image-level labels respectively. Our prediction network results when using a ResNet based conditional network is presented as 'Ours (ResNet-*) and the results of the prediction network using a U-Net based conditional network is presented as 'Ours'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Supervision higher IoU thresholds (mAP r 0.70 and mAP r 0.75 ) for both the image-level and the bounding-box labels. This demonstrates that our model can predict the instance segments most accurately by respecting the object boundaries. The per-class quantitative and qualitative results for our method is presented in appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Ablation Experiments</head><p>Effect of the unary, the pairwise and the higher order terms We study the effect of the conditional distributions unary, pairwise and the higher order terms have on the final output in table 2. We use the terms U, U+P, and U+P+H to denote the settings where only the unary term is present, both the unary and the pairwise terms are present and all three terms are present in the conditional distribution. We see that unary term alone performs poorly across the different IoU thresholds. We argue that this is because of the bias of the unary term for segmenting only the most discriminative regions. The pairwise term helps allay this problem and we observe a significant improvement in the results. This is specially noticeable for higher IoU thresholds that require more accurate seg- mentation. The higher order term helps in improving the accuracy by ensuring that correct samples are generated by the conditional distribution.</p><p>Effect of the probabilistic learning objective To understand the effect of explicitly modeling the two distributions (Pr p and Pr c ), we compare our approach with their corresponding pointwise network. In order to sample a single output from our conditional network, we remove the self-diversity coefficient term and feed a zero noise vector (denoted by P W c ). For a pointwise prediction network, we remove its self-diversity coefficient. The prediction network still outputs the probability of each proposal belonging to a class. However, by removing the self-diversity coefficient term, we encourage it to output a peakier distribution (denoted by P W p ). <ref type="table" target="#tab_1">Table 3</ref> shows that both the diversity coefficient term is important for maximum accuracy. We also note that modeling uncertainty over the pseudo label generation model by including the self-diversity in the conditional network is relatively more important. The self-diversity coefficient in the conditional network enforces it to sample a diverse set of outputs which helps in dealing with the difficult cases and in avoiding overfitting during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We present a novel framework for weakly supervised instance segmentation. Our framework efficiently models the complex non-factorizable, annotation consistent and boundary aware conditional distribution that allows us to generate accurate pseudo segmentation labels. Furthermore, our framework provides a joint probabilistic learning objective for training the prediction and the conditional distributions and can be easily extendable to different weakly supervised labels such as image-level and bounding box annotations. Extensive experiments on the benchmark Pascal VOC 2012 data set has shown that our probabilistic framework successfully transfers the information present in the image-level annotations for the task of instance segmentation achieving state-of-the-art result for both image-level and bounding box annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>This work is partly supported by DST through the IMPRINT program. Aditya Arun is supported by Visvesvaraya Ph.D. fellowship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Learning Objective</head><p>In this section we provide detailed derivation of the objective presented in the section 4.2.</p><p>Given the loss function ? (9) which is tuned for the task of instance segmentation, we compute the diversity terms <ref type="bibr" target="#b6">(7)</ref>. Recall that the diversity for any two distributions is the expected loss of the samples drawn from the two distributions. For the prediction distribution Pr p and the conditional distribution Pr c , we derive the diversity between them and their self diversities as follows.</p><p>Diversity between prediction network and conditional network: Following <ref type="formula">(7)</ref>, the diversity between prediction and conditional distribution can be written as,</p><formula xml:id="formula_12">(12) DIV ? (Pr p , Pr c ) = E yp?Prp(y|x;?p) [E yc?Prc(y|x,a;?c) [?(y p , y c )]].</formula><p>We then write the expectation with respect to the conditional distribution (the inner distribution) as expectation over the random variables z with distribution Pr(z) using Law of the Unconscious Statistician (LOTUS). The expectation over the random variable z with distribution Pr(z) is approximated by taking K samples from Pr(z),</p><formula xml:id="formula_13">(13) DIV ? (Pr p , Pr c ) = E yp?Prp(y|x;?p) 1 K K k=1 ?(y p , y k c ) .</formula><p>We finally compute the expectation with respect to the prediction distribution as,</p><formula xml:id="formula_14">(14) DIV ? (Pr p , Pr c ) = 1 K K k=1 y (i) p</formula><p>Pr p (y (i) p ; ? p )?(y (i) p , y k c ).</p><p>Self diversity for conditional network: As above, using <ref type="bibr" target="#b6">(7)</ref>, we write the self diversity coefficient of the conditional distribution as We now write the two expectations with respect to the conditional distribution as the expectation over the random variables z and z respectively. In order to approximate the expectation over the random variables z and z , we use K samples from the distribution Pr(z) as,</p><formula xml:id="formula_15">(16) DIV ? (Pr c , Pr c ) = 1 K K k=1 1 K ? 1 K k =1, k =k ?(y k c , y k c ).</formula><p>On re-arranging the above equation, we get</p><formula xml:id="formula_16">(17) DIV ? (Pr c , Pr c ) = 1 K(K ? 1) K k,k =1 k =k ?(y k c , y k c ).</formula><p>Self diversity for prediction network: Similar to the above two cases, using <ref type="formula">(7)</ref>, we can write the self diversity of the prediction network as</p><formula xml:id="formula_17">(18) DIV ? (Pr p , Pr p ) = E yp?Prp(y|x;?p) [E y p ?Prp(y|x;?p) [?(y p , y p )]].</formula><p>Note that the prediction distribution is a fully factorized distribution, and we can compute its exact expectation. Therefore, we compute the two expectations with respect to the inner prediction distribution as,</p><formula xml:id="formula_18">(19) DIV ? (Pr p , Pr p ) = E yp?Prp(y|x;?p) y (i) p Pr p (y (i) p ; ? p )?(y p , y (i) p ) ;</formula><p>and the expectation with respect to the outer prediction distribution as,</p><formula xml:id="formula_19">(20) DIV ? (Pr p , Pr p ) = y (i) p y (i) p Pr p (y (i) p ; ? p ) Pr p (y (i) p ; ? p )?(y (i) p , y (i) p ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Optimization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Optimization over Prediction Distribution</head><p>As parameters ? c of the conditional distribution are constant, the learning objective of the prediction distribution is written as, This results in a fully supervised training of the Mask R-CNN network <ref type="bibr" target="#b16">[17]</ref>. Note that the only difference between training of a standard Mask R-CNN architecture and our prediction network is the use of the dissimilarity objective function <ref type="bibr" target="#b20">(21)</ref> above, instead of simply minimizing the multi-task loss of the Mask R-CNN. The prediction network takes as the input an image and the K predictions sampled from the conditional network. Treating these outputs of the conditional network as the pseudo ground truth label, we compute the gradient of our dissimilarity coefficient based loss function. As the objective (21) above is differentiable with respect to parameters ? p , we update the network by employing stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Optimization over Conditional Distribution</head><p>Similar to the prediction network, the conditional network is optimized by treating the parameters of the prediction network ? p as constant. Its learning obective is given as, A non-differentiable training procedure: The conditional network is modeled using a Discrete Disco Nets which employs a sampling step from the scoring function S k (y c ). This sampling step makes the objective function non-differentiable with respect to the parameters ? c , even though the scoring function S k (y c ) itself is differentiable. However, as the prediction network is fixed, the above objective function reduces to the one used in Bouchacourt et al. <ref type="bibr" target="#b6">[7]</ref> for fully supervised training. Therefore, similar to Bouchacourt et al. <ref type="bibr" target="#b6">[7]</ref> we solve this problem by estimating the gradients of our objective function with the help of temperature parameter as,</p><formula xml:id="formula_20">? ?c DISC ? (Pr p (? p ), Pr c (? c )) = ? lim ?0 1 (DIV ? (Pr p , Pr c ) ? ?DIV ? (Pr c , Pr c ))<label>(23)</label></formula><p>where, </p><formula xml:id="formula_21">(24) DIV ? (Pr p , Pr c ) = E yp?Prp(?p) E z k ?Pr(z) [? ?c S k (? a ) ? ? ?c S k (? c )] , (25) DIV ? (Pr c , Pr c ) = E z k ?Pr(z) E z k ?Pr(z) [? ?c S k (? b ) ? ? ?c S k (? c )] ,</formula><p>In our experiments, we fix the temperature parameter as, = +1.</p><p>Intuition for the gradient computation: We now present an intuitive explanation of the computation of gradient, as given in equation <ref type="bibr" target="#b22">(23)</ref>. For an input x and two noise samples z k , z k , the conditional network outputs two scores S k (y c ) and S k (y c ), with the corresponding maximum scoring outputs? c and? c . The model parameters ? c are updated via gradient descent in the negative direction of ? ?c DISC ? (Pr p (? p ), Pr c (? c )).</p><p>-The term DIV ? (Pr p , Pr c ) updates the model parameters towards the maximum scoring prediction? c of the score S k (y c ) while moving away from? a , where? a is the sample corresponding to the maximum loss augmented score S k (y c ) ? ?(y p ,? c ) with respect to the fixed prediction distribution samples y p . This encourages the model to move away from the prediction providing high loss with respect to the pseudo ground truth labels. </p><formula xml:id="formula_23">DIV ? (Prp, Prc) = 1 K K k=1 ? ?c S k (?a) ? ? ?c S k (?c) , DIV ? (Prc, Prc) = 2 K(K ? 1) K k,k =1 k =k ? ?c S k (? b ) ? ? ?c S k (? c ) .</formula><p>Update model parameters by descending to the approximated gradients: Training algorithm for conditional network: Pseudo-code for training the conditional network for a single sample from weakly supervised data is presented in algorithm 2 below. In algorithm 2, statements 1 to 3 describe the sampling process and computing the loss augmented prediction. We first sample K different predictions? k c corresponding to each noise vector z k in statement 2. For the sampled prediction? k c we compute the maximum loss augmented score S k (y c ) ? ?(y p ,? c ). This is then used to find the loss augmented prediction y a given in statement 3.</p><p>In order to compute the gradients of the self diversity of conditional distribution, we need to find the maximum loss augmented prediction? b . Here, the loss is computed between a pair of K different predictions of the conditional network that we have already obtained. This is shown by statements 4 to 7 in algorithm 2.</p><p>For the purpose of optimizing the conditional network using gradient descent, we need to find the gradients for the objective function of the conditional network defined in equation <ref type="bibr" target="#b21">(22)</ref> above. The computation of the unbiased approximate gradients for the individual terms in the objective function is shown in statement 8. We finally optimize the conditional network by the employing gradient descent step and updating the model parameters by descending to the approximated gradients as shown in statement 9 of algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C Experiments</head><p>In this section we present the implementation details of the prediction and the conditional network. Next, we present details of our ResNet based architecture and the detailed class-specific results on the Pascal VOC 2012 data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Implementation Details</head><p>We use the standard Mask R-CNN as the prediction network and adapt the U-Net architecture for the conditional network, as shown figure 1. For a fair comparison, the prediction network, we use ImageNet pre-trained ResNet-50 architecture for experiments with image-level annotation and a pretrained ResNet-101 architecture for the bounding box annotations.</p><p>Similar to <ref type="bibr" target="#b20">[21]</ref>, the U-Net architecture is modified by adding a noise sample as an extra channel after the deconvolutional layers as shown in <ref type="figure">figure 1</ref>. A 1 ? 1 convolution is applied to bring the number of channels back to the original dimensions (32 channels). The segmentation region proposal masks taken from MCG <ref type="bibr" target="#b2">[3]</ref> is then multiplied element-wise with the features from all the channels. This allows us to extract features only from the segmentation proposal. A 1 ? 1 convolution is applied again to make the number of channels equal to the number of classes. This is followed by a global average pooling layer which gives us, for each of the segmentation proposals, a vector of dimensions equal to the number of classes. This vector for each of the segmentation proposal is passed to the inference algorithm which in turn provides the output segmentation masks corresponding to the image-level annotations. For all our experiments we choose K=10 for the conditional network and use the Adam optimizer. For all the other hyper-parameters we use the same configuration as described in <ref type="bibr" target="#b20">[21]</ref>. For the prediction network, we use default hyper-parameters described in <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 ResNet based architecture for the conditional network</head><p>In section 6.4 of the main paper, we study the effect of an alternative architecture for the conditional network. In what follows, we provide the details of this ResNet based conditional network. For the experiments where we use only the imagelevel annotations, a ResNet-50 module is employed and where we use the boundingbox level annotations, a ResNet-101 module is used. A noise filter is appended as an extra channel followed by a 1 ? 1 convolutional filter, which brings the number of channels back to the original dimensions. The segmentation proposal masks are then multiplied element-wise to obtain segmentation proposal specific features. Next, a 1 ? 1 convolutional is applied to make the number of channels equal to the number of classes. Finally, a global average pooling is applied to obtain a vector whose dimensions is equal to the number of classes in the data set. This vector is then passed through the inference algorithm to obtain the final predicted samples. As mentioned in section 6.4 of the paper, the results obtained using this ResNet based conditional network architecture are called as Ours (ResNet-50) and Ours (ResNet-101).</p><p>Note that, the U-Net based conditional network provides a higher resolution image features as compared to its ResNet based counterparts. These are then used to obtain the individual features of the segmentation mask proposals. The higher resolution features thus provide richer per-mask features. These are especially useful for smaller objects and cluttered environment where context resolution is important. The superior results of our method when using a U-Net based conditional network empirically verify this claim. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Class specific results on VOC 2012 data set</head><p>We present the per-class result for our method on the Pascal VOC 2012 data set in table 4. The first two rows correspond to the result where our method was trained only using the image-level annotations. The last two rows correspond to the results where our methods were trained using the bounding box annotations.The qualitative results for each class is presented in <ref type="figure" target="#fig_6">figure 4</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 15 )</head><label>15</label><figDesc>DIV ? (Pr c , Pr c ) = E yc?Prc(y|x,a;?c) [E y c ?Prc(y|x,a;?c) [?(y c , y c )]].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>?p DIV ? (Pr p , Pr c ) ? (1 ? ?)DIV ? (Pr p , Pr p ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>?c DIV ? (Pr p , Pr c ) ? ?DIV ? (Pr c , Pr c ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>max y?Y S k (y c ) y c = arg max y?Y S k (y c ) y a = arg max y?Y S k (y c ) ? ?(y p ,? c ) y b = arg max y?Y S k (y c ) ? ?(? c ,? c )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>-</head><label></label><figDesc>c ? ?? ?c DISC?(Prp(?p), Prc(?c)) The term ?DIV ? (Pr c , Pr c ) updates the model towards? b and away from th? y c . Note the two negative signs giving the update in the positive direction. Here? b is the sample corresponding to the maximum loss augmented score S k (y c ) ? ?(? c ,? c ) with respect to the other prediction? c , encouraging diversity between? c and? c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>ResNet based conditional network The architecture for the ResNet based conditional network is shown in figure 3. The image is first passed through the ResNet module to obtain lowresolution high-level features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative results of our proposed approach on VOC 2012 validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Evaluation of the instance segmentation results for the various ablative settings of the conditional distribution on Pascal VOC 2012 data set</figDesc><table><row><cell></cell><cell></cell><cell cols="5">Backbone mAP r 0.25 mAP r 0.50 mAP r 0.70 mAP r 0.75</cell></row><row><cell>Mask R-CNN [17]</cell><cell>F</cell><cell>R-101</cell><cell>76.7</cell><cell>67.9</cell><cell>52.5</cell><cell>44.9</cell></row><row><cell>PRN [42]</cell><cell>I</cell><cell>R-50</cell><cell>44.3</cell><cell>26.8</cell><cell>-</cell><cell>9.0</cell></row><row><cell>IAM [43]</cell><cell>I</cell><cell>R-50</cell><cell>45.9</cell><cell>28.8</cell><cell>-</cell><cell>11.9</cell></row><row><cell>OCIS [10]</cell><cell>I</cell><cell>R-50</cell><cell>48.5</cell><cell>30.2</cell><cell>-</cell><cell>14.4</cell></row><row><cell>Label-PEnet [13]</cell><cell>I</cell><cell>R-50</cell><cell>49.1</cell><cell>30.2</cell><cell>-</cell><cell>12.9</cell></row><row><cell>WISE [23]</cell><cell>I</cell><cell>R-50</cell><cell>49.2</cell><cell>41.7</cell><cell>-</cell><cell>23.7</cell></row><row><cell>IRN [1]</cell><cell>I</cell><cell>R-50</cell><cell>-</cell><cell>46.7</cell><cell>-</cell><cell>23.5</cell></row><row><cell>Ours (ResNet-50)</cell><cell>I</cell><cell>R-50</cell><cell>59.1</cell><cell>49.7</cell><cell>29.2</cell><cell>27.1</cell></row><row><cell>Ours</cell><cell>I</cell><cell>R-50</cell><cell>59.7</cell><cell>50.9</cell><cell>30.2</cell><cell>28.5</cell></row><row><cell>SDI [20]</cell><cell>B</cell><cell>R-101</cell><cell>-</cell><cell>44.8</cell><cell>-</cell><cell>46.7</cell></row><row><cell>BBTP [18]</cell><cell>B</cell><cell>R-101</cell><cell>75.0</cell><cell>58.9</cell><cell>30.4</cell><cell>21.6</cell></row><row><cell>Ours (ResNet-101)</cell><cell>B</cell><cell>R-101</cell><cell>73.1</cell><cell>57.7</cell><cell>33.5</cell><cell>31.2</cell></row><row><cell>Ours</cell><cell>B</cell><cell>R-101</cell><cell>73.8</cell><cell>58.2</cell><cell>34.3</cell><cell>32.1</cell></row><row><cell cols="2">mAP r 0.25</cell><cell cols="2">mAP r 0.50</cell><cell cols="2">mAP r 0.75</cell><cell></cell></row><row><cell cols="6">U U+P U+P+H U U+P U+P+H U U+P U+P+H</cell><cell></cell></row><row><cell>57.9 59.1</cell><cell cols="2">59.7 47.6 49.9</cell><cell cols="2">50.9 23.1 26.9</cell><cell>28.5</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Evaluation of the instance segmentation results for the various ablative settings of the loss function's diversity coefficient terms on Pascal VOC 2012 data set</figDesc><table><row><cell>Method mAP r k</cell><cell>Prp, Prc (proposed)</cell><cell cols="3">P Wp, Prc Prp, P Wc P Wp, P Wc</cell></row><row><cell>mAP r 0.25</cell><cell>59.7</cell><cell>59.5</cell><cell>57.3</cell><cell>57.2</cell></row><row><cell>mAP r 0.50</cell><cell>50.9</cell><cell>50.3</cell><cell>46.9</cell><cell>46.6</cell></row><row><cell>mAP r 0.75</cell><cell>28.5</cell><cell>27.7</cell><cell>23.4</cell><cell>23.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Algorithm 2 :</head><label>2</label><figDesc>Conditional network training algorithmInput : Training input (x, a) ? W, and prediction network outputs yp Output:? 1 c , . . . ,? K c , sample K predictions from the model 1 for k = 1 . . . K do</figDesc><table><row><cell>2</cell><cell cols="3">Sample noise vector z k , generate output? k c :</cell></row><row><cell></cell><cell></cell><cell cols="2">y k c = arg max</cell><cell>S k (yc)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>y?Y</cell></row><row><cell>3</cell><cell cols="3">Find loss augmented prediction? k a w.r.t. output from prediction network</cell></row><row><cell></cell><cell>yp:?</cell><cell></cell></row><row><cell></cell><cell cols="2">k a = arg max</cell><cell>S k (yc) ? ?(yp,? k c )</cell></row><row><cell></cell><cell></cell><cell>y?Y</cell></row><row><cell>7</cell><cell cols="3">Find loss augmented prediction? k b w.r.t. other conditional network</cell></row><row><cell></cell><cell>outputs? k c :?</cell><cell></cell></row><row><cell></cell><cell>k,k b</cell><cell cols="2">y?Y = arg max</cell><cell>S k (yc) ? ?(? k c ,? c )</cell></row></table><note>4 Compute loss augmented predictions:5 for k = 1, . . . , K do6 for k = 1, . . . , K, k = k do8 Compute unbiased approximate gradients for DIV ? (Prc, Prc) and DIV? (Prc, Prc) as:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Per class result for mAP r 0.5 metric on Pascal VOC 2012 data set for methods that are trained on using image-level supervision I and bounding box annotations B Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike pson plant sheep sofa train tv mAP Ours (ResNet-50) I 74.2 52.6 68.6 44.1 25.0 63.4 35.9 72.6 18.2 47.1 24.6 63.5 53.7 67.3 40.9 29.4 42.8 39.6 69.5 61.2 49.7 Ours I 75.5 53.6 69.9 45.3 26.7 64.3 37.4 73.7 19.3 48.7 25.3 64.6 55.0 68.3 42.1 30.8 44.2 40.5 70.6 62.2 50.9 Ours (ResNet-101) B 77.9 62.6 73.8 49.0 35.9 72.6 45.8 78.4 29.7 55.7 31.9 70.6 61.3 73.6 49.2 39.9 50.8 47.9 76.5 69.6 57.7 Ours B 79.1 63.9 75.1 49.3 36.5 73.1 46.4 78.8 30.1 56.4 32.1 71.3 61.6 74.8 49.5 40.2 51.1 48.3 77.2 69.9 58.2</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Details in Appendix A</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Weakly supervised learning of instance segmentation with inter-pixel relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning human poses from actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dissimilarity coefficient based weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Whats the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Task-Oriented Learning of Structured Probability Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouchacourt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>University of Oxford</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<title level="m">DISCO Nets: Dissimilarity coefficients networks</title>
		<imprint>
			<publisher>NIPS</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Masklab: Instance segmentation by refining object detection with semantic and direction features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Object counting and instance segmentation with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Label-PEnet: Sequential label propagation and enhancement networks for weakly supervised instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Direct loss minimization for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>Mask R-CNN</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Weakly supervised instance segmentation using the bounding box tightness prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A probabilistic u-net for segmentation of ambiguous images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Ledsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Modeling latent variable uncertainty for lossbased learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Where are the masks: Instance segmentation with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Direct optimization through argmax for discrete variational auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lorberbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hazan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semi-convolutional operators for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Exploiting saliency for object segmentation from image level labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Is object localization for free?-weaklysupervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Weakly-and semi-supervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Diversity and dissimilarity coefficients: a unified approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theoretical population biology</title>
		<imprint>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Gradcam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Training deep neural networks via direct loss minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Weakly supervised region proposal network and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning random-walk label propagation for weaklysupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Weakly supervised instance segmentation using class peak response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning instance activation maps for weakly supervised instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
