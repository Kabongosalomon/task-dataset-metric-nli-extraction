<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hate speech detection using static BERT embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Rajput</surname></persName>
							<email>gauravrajputoths@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Information Technology</orgName>
								<address>
									<postCode>211015</postCode>
									<settlement>Allahabad</settlement>
									<region>Uttar Pradesh</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narinder</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Information Technology</orgName>
								<address>
									<postCode>211015</postCode>
									<settlement>Allahabad</settlement>
									<region>Uttar Pradesh</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">Kumar</forename><surname>Sonbhadra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Information Technology</orgName>
								<address>
									<postCode>211015</postCode>
									<settlement>Allahabad</settlement>
									<region>Uttar Pradesh</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonali</forename><surname>Agarwal</surname></persName>
							<email>sonali@iiita.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Information Technology</orgName>
								<address>
									<postCode>211015</postCode>
									<settlement>Allahabad</settlement>
									<region>Uttar Pradesh</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hate speech detection using static BERT embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Hate speech detection ? BERT embeddings ? Word embed- dings ? BERT</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With increasing popularity of social media platforms hate speech is emerging as a major concern, where it expresses abusive speech that targets specific group characteristics, such as gender, religion or ethnicity to spread violence. Earlier people use to verbally deliver hate speeches but now with the expansion of technology, some people are deliberately using social media platforms to spread hate by posting, sharing, commenting, etc. Whether it is Christchurch mosque shootings or hate crimes against Asians in west, it has been observed that the convicts are very much influenced from hate text present online. Even though AI systems are in place to flag such text but one of the key challenges is to reduce the false positive rate (marking non hate as hate), so that these systems can detect hate speech without undermining the freedom of expression. In this paper, we use ETHOS hate speech detection dataset and analyze the performance of hate speech detection classifier by replacing or integrating the word embeddings (fastText (FT), GloVe (GV) or FT + GV) with static BERT embeddings (BE). With the extensive experimental trails it is observed that the neural network performed better with static BE compared to using FT, GV or FT + GV as word embeddings. In comparison to fine-tuned BERT, one metric that significantly improved is specificity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With growing access to internet and many people joining the social media platforms, people tend to post online as per their desire and tag it as freedom of speech. It is one of the major problems on social media that tend to degrade the overall user's experience. Facebook defines hate speech as a direct attack against people on the basis of protected characteristics: race, ethnicity, national origin, disability, religious affiliation, caste, sexual orientation, sex, gender identity and serious disease <ref type="bibr">[3]</ref>, while for Twitter hateful conduct includes language that dehumanizes others on the basis of religion or caste <ref type="bibr" target="#b1">[6]</ref>. In March 2020, Twitter all authors contributed equally.</p><p>expanded the rule to include languages that dehumanizes on the basis of age, disability, or disease. Furthermore, the hateful conduct policy was expanded to also include race, ethnicity, or national origin <ref type="bibr" target="#b1">[6]</ref>. Following this context, hate speech can be defined as an abusive speech that targets specific group characteristics, such as gender, religion, or ethnicity.</p><p>Considering the massive amount of text what people post on social media, it is impossible to manually flag them as hate speech and remove them. Hence, it is required to have automated ways using artificial intelligence (AI) to flag and remove such content in real-time. While such automated AI systems are in place on social media platform, but one of the key challenges is the separation of hate speech from other instances of offensive language and other being higher false positive (marking non-hate as hate) rates of such system. Higher false positive rate means system will tag more non-hate content as hate content which can undermine the right to speak freely.</p><p>Hate speech can be detected using state-of-the-art machine learning classifiers such as logistic regression, SVM, decision trees, random forests, etc. However, deep neural networks (DNNs) such as convolutional neural networks (CNNs), long short-term memory networks (LSTMs) <ref type="bibr" target="#b9">[15]</ref>, bidirectional long short-term memory networks (BiLSTMs) <ref type="bibr" target="#b20">[26]</ref>, etc. have outperformed the former mentioned classifiers for hate speech detection <ref type="bibr" target="#b13">[19]</ref>. Former classifiers do not require any word embedding [7] to work with while the latter ones i.e DNNs requires word embeddings such as GloVe (GV) <ref type="bibr" target="#b14">[20]</ref>, fastText (FT) <ref type="bibr" target="#b10">[16]</ref>, Word2Vec <ref type="bibr" target="#b12">[18]</ref>, etc. Following this context, the present research work focuses on the scope of improvement of the existing state-of-the-art deep learning based classifiers by using static BERT <ref type="bibr" target="#b7">[13]</ref> embeddings (BE) with CNNs, BiLSTMs, LSTMs and gated recurrent unit (GRU) <ref type="bibr" target="#b5">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">BERT</head><p>Bidirectional encoder representations from transformers (BERT) was developed by Devlin et al. <ref type="bibr" target="#b7">[13]</ref> in 2018. BERT is a transformer-based ML technique pretrained from unlabeled data that is taken from Wikipedia (language: English) and BookCorpus. Transformer <ref type="bibr" target="#b22">[28]</ref> model has two main parts: encoder and decoder. BERT is created by stacking the encoders. Two major strategies that BERT uses for training are masked language modelling (MLM) and next sentence prediction (NSP). The MLM strategy and fine-tuning of BERT is pictorially depicted in <ref type="figure" target="#fig_0">Fig 1.</ref> In MLM technique 15% of the words in a sentence are selected randomly and masked. Based on the context of the other words (which are not masked) the model tries to predict the masked word.</p><p>In NSP technique model is given pairs of sentences as input. The model learns to predict if the second sentence in a selected pair is the subsequent sentence in original document. During the training phase half of the inputs are a pair in which second sentence is subsequent sentence to the first in the original document while the rest half of the input pairs has a randomly selected sentence as second sentence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Attention in neural networks</head><p>While processing a sentence in natural language for any NLP task, all words are not of equal importance, hence it is necessary to put more attention to the important words of the sentence. The importance of words depends on the context and is learned through training data. Bahdanau et al. <ref type="bibr" target="#b3">[9]</ref> proposed the attention mechanism for improving machine translation that uses seq-to-seq model. It is done by aligning the decoder with the relevant input sentences and implementing attention. Steps for applying attention mechanism are as follows:</p><p>1 Produce the encoder hidden states. 2 Calculate alignment scores. 3 Soft-max the alignment scores. 4 Calculate the context vector. 5 Decode the output. 6 At each time step, based on the current state of decoder and input received by decoder, an element of decoder's output sequence is generated by decoder. Besides that decoder also updates its own state for next time step. Steps 2-5 repeats itself for each time step of the decoder until the output length exceeds a specified maximum length or end of sentence token is produced.</p><p>The rest of the paper is organised as follows: literature review in Section 2 which focuses on the related work and recent developments in this field, Section 3 describes the proposed methodology. Section 4 covers the exhaustive experimental trials followed by improved results in section 5, and lastly in Section 6 the concluding remarks are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>The advancements in deep learning technology have widen spectrum of its application tasks involving classification, segmentation, object detection, etc., across various domains such as healthcare, image processing, natural language processing, etc. <ref type="bibr" target="#b4">[10,</ref><ref type="bibr" target="#b15">[21]</ref><ref type="bibr" target="#b16">[22]</ref><ref type="bibr" target="#b17">[23]</ref><ref type="bibr" target="#b24">30]</ref>. With hate speech detection being one of the major problems in the evergrowing social media platforms, it has drawn keen interest of the research community to develop AI assisted applications. Following this, Badjatiya et al. <ref type="bibr" target="#b2">[8]</ref> proposed a deep learning approach to perform hate speech detection in tweets. The approach was validated on the dataset [29] that consists of 16,000 tweets, of which 1972 are marked as racist, 3383 as sexist and the remaining ones as neither. The authors utilized convolutional neural networks, long short-term memory networks and FastText. The word embeddings are initialized with either random embeddings or GV <ref type="bibr" target="#b14">[20]</ref> embeddings. The authors achieved promising results with "LSTM + Random Embedding + GBDT" model. In this model, the tweet embeddings were initialized to random vectors, LSTM was trained using back-propagation, and then learned embeddings were used to train a GBDT classifier.</p><p>Rizos et al. <ref type="bibr" target="#b19">[25]</ref> experimented by using short-text data augmentation technique in deep learning for hate speech classification. For short-text data augmentation they used substitution based augmentation (ThreshAug), word position augmentation (PosAug) and neural generative augmentation (GenAug).</p><p>For performing experiments they used the dataset <ref type="bibr" target="#b6">[12]</ref> which consists of around 24k samples, of which 5.77% samples are marked as hate, 77.43% samples are marked as offensive and 16.80% samples as neither. The authors experimented with multiple DNNs such as CNN, LSTM and GRU. In addition, fastText, GloVe and Word2Vec were used as word embeddings. They achieved their best results by using GloVe + CNN + LSTM + BestAug, where BestAug is combination of PosAug and ThreshAug. Faris et al. <ref type="bibr" target="#b8">[14]</ref> proposed a deep learning approach to detect hate speech in Arabic language context. They created their dataset by scraping tweets from twitter using an application programming interface (API) [1] and performed standard dataset cleaning methods. The obtained dataset have 3696 samples of which 843 samples are labelled as hate and 791 samples as normal while rest of the samples were labelled as neutral. Word2Vec and AraVec <ref type="bibr" target="#b21">[27]</ref> were used for feature representation and embedding dimension was kept to 100. The authors achieved promising results using combination of CNN and LSTM with AraVec.</p><p>Ranasinghe et al. <ref type="bibr" target="#b18">[24]</ref> in hate speech and offensive content identification in Indo-European languages (HASOC) shared task 2019 experimented with multiple DNNs such as pooled GRU, stacked LSTM + attention, LSTM + GRU + attention, GRU + capsule using fastText as word embedding on the dataset having posts written in 3 languages: German, English and code-mixed Hindi. Furthermore, they also experimented with fine-tuned BERT <ref type="bibr" target="#b7">[13]</ref> which outperformed every above mentioned DNN for all 3 languages. In another work, Mollas et al. <ref type="bibr" target="#b13">[19]</ref> proposed ETHOS dataset to develop AI based hate speech detection framework that have used FT <ref type="bibr" target="#b10">[16]</ref>, GV <ref type="bibr" target="#b14">[20]</ref> and FT + GV as word embeddings with CNNs, BiLSTMs and LSTMs. In contrast to other datasets which are based on tweets scraped from Twitter, this new dataset is based on YouTube and Reddit comments. A binary version of ETHOS dataset has 433 sentences containing hate text and 565 sentences containing non-hate text. Besides, transfer learning was used to fine-tune BERT model on the proposed dataset that outperformed the above mentioned deep neural networks. The results of the aforementioned experiments are shown in <ref type="table" target="#tab_0">Table 1</ref>, where bold values represent the highest value of metrics among all models <ref type="bibr" target="#b13">[19]</ref>. Ever since the researchers started using BERT <ref type="bibr" target="#b7">[13]</ref> for natural language processing tasks, it has been observed that a fine-tuned BERT usually outperforms other state-of-the-art deep neural networks in same natural language processing task. The same has been observed in the results of experiment carried out by Mollas et al <ref type="bibr" target="#b13">[19]</ref>. Motivated from this, the experiments carried out in this paper aims to analyse the performance of fine-tuned BERT with other deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed methodology</head><p>Following the state-of-the-art deep learning classification models, in the proposed approach the impact of BERT based embeddings is analyzed. The hate speech detection framework is designed by combining DNNs (CNN, LSTM, BiLSTM and GRU) with static BERT embeddings to better extract the contextual information. Initially, the static BERT embedding matrix is generated from large corpus of dataset, representing embedding for each word and later, this matrix is processed using DNN classifiers to identify the presence of hate. The schematic representation of the proposed model is shown in <ref type="figure" target="#fig_1">Fig 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Static BERT embedding matrix</head><p>The embedding matrix contains the word embeddings for each word in dataset. Each row in the embedding matrix contains word embedding for a unique word and they are passed to the DNNs (by converting natural language sentences to vectors) that accepts input in fixed dimensions, therefore the word embeddings have to be static. Since BERT <ref type="bibr" target="#b7">[13]</ref> gives contexualised embedding of each word according to the usage of the word in sentence, thereby same word will have different embeddings depending on the usage context unlike in other static word embeddings where each word has unique static embedding irrespective of the context in which it is used.</p><p>Initially, the raw BERT embeddings are generated using bert-embedding library [2] to provide contextualized word embedding. An embedding dictionary (key-value pair) is developed where key is the unique word and value is an array containing contextualized embeddings of that unique word. Since same word can be used in different context in different sentences, hence it will have different word embeddings depending on the context. Every contextualized embedding for a word are stored in the dictionary [5] by pushing the embedding into the vector corresponding to the unique word. Furthermore, the static BE of a word is obtained by taking mean of the vector containing the contexualized BERT embeddings of that word. For example, a word 'W 'occurs 4 times in the dataset, then there will be 4 contexualized embeddings of 'W ', let it be E 1 , E 2 , E 3 , E 4 . These 4 embeddings each of dimension (768,) are stored in the array corresponding to the key 'W 'in the dictionary. Later, mean of E 1 , E 2 , E 3 , E 4 is computed that represents the static BERT <ref type="bibr" target="#b7">[13]</ref> embedding of 'W '. For words which are not in vocabulary, BERT <ref type="bibr" target="#b7">[13]</ref> splits them into subwords and generate their embeddings, then take the average of embeddings of subwords to generate the embedding of the word which was not in vocabulary. Finally, by using keras Tokenizer <ref type="bibr" target="#b11">[17]</ref> and static BERT embeddings we create the embedding matrix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Choice of dataset</head><p>Even though there are multiple datasets that are publicly available for hate speech detection but we chose to use binary version of ETHOS dataset <ref type="bibr" target="#b13">[19]</ref> because it is the most recent dataset and the two classes (hate and non-hate) present in it are almost balanced as compared to other datasets. For example, Davidson dataset having around 24k samples (Hate speech: 5.77%, Offensive: 77.43% and 16.80% as Neither) <ref type="bibr" target="#b6">[12]</ref> is highly imbalanced. ETHOS dataset address all such issues of available datasets.</p><p>The Shannon entropy can be used as a measure of balance for datasets. On a dataset of n instances, if we have k classes of size c i we can compute entropy as follows:</p><formula xml:id="formula_0">H = ? k i = 1 c i n log c i n (1)</formula><p>It is equal to zero if there is a single class. In other words, it tends to 0 when the dataset is very unbalanced and log(k) when all the classes are balanced and of the same size n/k. Therefore, we use the following measure of balance (shown in Eq. 2) for a dataset <ref type="bibr" target="#b0">[4]</ref>:</p><formula xml:id="formula_1">Balance = H log k = ? k i=1 ci n log ci n log k (2)</formula><p>Binary version of ETHOS dataset has 433 samples containing hate text and 565 samples containing non-hate text. For binary version of ETHOS dataset, Balance = 0.986 which is nearly equal to 1, indicating balance between classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neural network architectures and testing environment</head><p>The proposed approach is trained and validated on the binary version of ETHOS dataset. For the purpose of comparison, the neural network architectures are kept exactly same as described by the Mollas et al. <ref type="bibr" target="#b13">[19]</ref>. From the number of units in a neural network to the arrangement of layers in a neural network everything is kept same so as to create the same training and testing environment but change the word embeddings to static BERT embeddings.</p><p>To establish robust results, stratified k-fold validation technique with value of k = 10 is utilized. Furthermore, the training phase is assisted with callbacks such as early stopping (stop the training if performance doesn't improve) to avoid the overfitting problem and model-checkpointing (saving the best model).</p><p>Finally, the trained model is evaluated using standard classification performance metrics i.e. accuracy, precision, recall (sensitivity), F1-score and specificity.</p><formula xml:id="formula_2">Accuracy = T P + T N T P + T N + F P + F N (3) P recision = T P T P + F P (4) Recall = T P T P + F N (5) F 1 ? score = 2 ? P recision ? Recall P recision + Recall<label>(6)</label></formula><p>Specif icity = T N T N + F P</p><p>Where, TP : True Positive, TN : True Negative, FP : False Positive, FN : False Negative 5 Results and discussion  attention, the average (avg) increase in F1-score is 3.56%, accuracy is 3.39%, precision is 3.40%, recall is 3.55% and sensitivity is 2.37%. Hence, it is evident that static BERT embeddings tend to provide better feature representation as compared to fastText, GloVe or fastText + GloVe.</p><p>Furthermore, BiLSTM using static BERT embeddings (BiLSTM + static BE) performs better in all metrics as compared to other DNNs under consideration. In the results of experiments done by Mollas et al. <ref type="bibr" target="#b13">[19]</ref>, fine-tuned BERT outperformed other models in every metric with specificity as 74.31%, which increases to 83.03% using static BERT embeddings (BiLSTM + static BE), thereby attaining a significant increase of 8.72%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this article, the impact of performance in deep learning based hate speech detection using static BERT embeddings is analysed. With exhaustive experimental trials on various deep neural networks it is observed that using neural networks with static BERT embeddings can significantly increase the performance of the hate speech detection models especially in terms of specificity, indicating that model is excellent at correctly identifying non hate speeches. Therefore, it flags lesser non hate speech as hate speech, thereby protecting the freedom of speech. With such promising improvements in the results, the same concept of integrating static BERT embedddings with state-of-the-art models can further be extended to other natural language processing based applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Pre-training and fine-tuning of BERT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Block diagram of proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance of BERT (fine-tuned on binary ETHOS dataset) with various neural networks using FT, GV or FT + GV as word embedding<ref type="bibr" target="#b13">[19]</ref> </figDesc><table><row><cell>Model</cell><cell cols="5">F1 Score Accuracy Precision Recall Specificity</cell></row><row><cell cols="2">CNN + Attention + FT + GV 74.41</cell><cell>75.15</cell><cell cols="3">74.92 74.35 80.35</cell></row><row><cell>CNN + LSTM + GV</cell><cell>72.13</cell><cell>72.94</cell><cell>73.47</cell><cell>72.4</cell><cell>76.65</cell></row><row><cell>LSTM + FT + GV</cell><cell>72.85</cell><cell>73.43</cell><cell cols="2">73.37 72.97</cell><cell>76.44</cell></row><row><cell>BiLSTM + FT + GV</cell><cell>76.85</cell><cell>77.45</cell><cell cols="2">77.99 77.10</cell><cell>79.66</cell></row><row><cell>BiLSTM + Attention + FT</cell><cell>76.80</cell><cell>77.34</cell><cell cols="2">77.76 77.00</cell><cell>79.63</cell></row><row><cell>BERT</cell><cell>78.83</cell><cell>76.64</cell><cell cols="3">79.17 78.43 74.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>represents the results of experiment carried out by Mollas et al. [19] on binary version of ETHOS dataset, hence the DNNs uses only FT, GV or FT + GV as word embeddings. It is evident from the Table 1 that BERT (fine-tuned on binary ETHOS dataset) outperformed other models in all metrics except accuracy and specificity, its specificity stands at 74.31% which indicates high false positive hate speech classification.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparative analysis of the performance of various DNNs with and without static BERT embeddings (BE). Bold model names represent static BERT embedding variants of the models Bold values represent the highest value of any metric among all models TheTable 2 presents the obtained results on various DNNs, where bold model names represent BERT variant of a DNN model and bold quantities represent the highest values. It is observed that a deep neural network with static BERT embeddings outperforms the same deep neural network which is using word embedding as fastText, GloVe or fastText + GloVe in all metrics. For DNNs like CNN using attention, LSTM, CNN + LSTM, BiLSTM and BiLSTM using</figDesc><table><row><cell>Model</cell><cell cols="5">F1-Score Accuracy Precision Recall Specificity</cell></row><row><cell>CNN + Attention + FT + GV</cell><cell>74.41</cell><cell>75.15</cell><cell cols="2">74.92 74.35</cell><cell>80.35</cell></row><row><cell>CNN + Attention + static BE</cell><cell>77.52</cell><cell>77.96</cell><cell cols="2">77.89 77.69</cell><cell>79.62</cell></row><row><cell>CNN + LSTM + GV</cell><cell>72.13</cell><cell>72.94</cell><cell>73.47</cell><cell>72.4</cell><cell>76.65</cell></row><row><cell>CNN + LSTM + static BE</cell><cell>76.04</cell><cell>76.66</cell><cell cols="2">77.20 76.18</cell><cell>79.43</cell></row><row><cell>LSTM + FT + GV</cell><cell>72.85</cell><cell>73.43</cell><cell cols="2">73.37 72.97</cell><cell>76.44</cell></row><row><cell>LSTM + static BE</cell><cell>79.08</cell><cell>79.36</cell><cell cols="2">79.38 79.37</cell><cell>79.49</cell></row><row><cell>BiLSTM + FT + GV</cell><cell>76.85</cell><cell>77.45</cell><cell cols="2">77.99 77.10</cell><cell>79.66</cell></row><row><cell>BiLSTM + static BE</cell><cell>79.71</cell><cell>80.15</cell><cell cols="3">80.37 79.76 83.03</cell></row><row><cell>BiLSTM + Attention + FT</cell><cell>76.80</cell><cell>77.34</cell><cell cols="2">77.76 77.00</cell><cell>79.63</cell></row><row><cell cols="2">BiLSTM + Attention+static BE 78.52</cell><cell>79.16</cell><cell cols="2">79.67 78.58</cell><cell>83.00</cell></row><row><cell>GRU + static BE</cell><cell>77.91</cell><cell>78.36</cell><cell cols="2">78.59 78.18</cell><cell>79.47</cell></row><row><cell>BERT</cell><cell>78.83</cell><cell>76.64</cell><cell cols="2">79.17 78.43</cell><cell>74.31</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A general measure of data-set imbalance</title>
		<ptr target="https://www.programiz.com/python-programming/dictionary" />
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
	<note>Online accessed 10 June, 2021] 5. Python dictionary</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Updating our rules against hateful conduct</title>
		<ptr target="https://blog.twitter.com/en_us/topics/company/2019/hatefulconductupdate.html" />
		<imprint>
			<date type="published" when="2021-06-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning for hate speech detection in tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Badjatiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on World Wide Web companion</title>
		<meeting>the 26th international conference on World Wide Web companion</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="759" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bert based sentiment analysis: A software engineering perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Punn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sonbhadra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02581</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<title level="m">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automated hate speech detection and the problem of offensive language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warmsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Macy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International AAAI Conference on Web and Social Media</title>
		<meeting>the International AAAI Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hate speech detection using word embedding and deep learning in the arabic language context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Faris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Aljarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPRAM. pp</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="453" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03651</idno>
		<title level="m">Fasttext. zip: Compressing text classification models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keras-Team</surname></persName>
		</author>
		<ptr target="https://github.com/keras-team/keras" />
		<title level="m">keras-team/keras</title>
		<imprint>
			<date type="published" when="2021-06-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.4546</idno>
		<title level="m">Distributed representations of words and phrases and their compositionality</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ethos: an online hate speech detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mollas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chrysopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08328</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inception u-net architecture for semantic segmentation to identify nuclei in microscopy cell images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Punn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-modality encoded fusion with 3d inception u-net and decoder model for brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Punn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications pp</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automated diagnosis of covid-19 with limited posteroanterior chest x-ray images using fine-tuned deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Punn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2689" to="2702" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Brums at hasoc 2019: Deep learning models for multilingual hate speech and offensive language identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zampieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hettiarachchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FIRE (Working Notes)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Augment to prevent: short-text data augmentation in deep learning for hate-speech classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="991" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks. Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Paliwal</surname></persName>
		</author>
		<idno type="DOI">10.1109/78.650093</idno>
		<ptr target="https://doi.org/10.1109/78.650093" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Aravec: A set of arabic word embedding models for use in arabic nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Soliman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Eissa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>El-Beltagy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="256" to="265" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hateful symbols or hateful people? predictive features for hate speech detection on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Waseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL student research workshop</title>
		<meeting>the NAACL student research workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="88" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An empirical study of common challenges in developing deep learning applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 30th International Symposium on Software Reliability Engineering (ISSRE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="104" to="115" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
