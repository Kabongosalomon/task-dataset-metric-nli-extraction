<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transfer beyond the Field of View: Dense Panoramic Semantic Segmentation via Unsupervised Domain Adaptation Segmentation Network Segmentation Network Domain Adaptation Training Training Adapting Before Adaptation After Adaptation Input Panoramic Image Pinhole domain Panoramic domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
							<email>jiaming.zhang@kit.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Authors are with Institute for Anthropomatics and Robotics</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxiang</forename><surname>Ma</surname></persName>
							<email>chaox-iang.ma.1024@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Authors are with Institute for Anthropomatics and Robotics</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
							<email>kailun.yang@kit.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Authors are with Institute for Anthropomatics and Robotics</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Roitberg</surname></persName>
							<email>alina.roitberg@kit.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Authors are with Institute for Anthropomatics and Robotics</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyu</forename><surname>Peng</surname></persName>
							<email>kunyu.peng@kit.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Authors are with Institute for Anthropomatics and Robotics</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
							<email>rainer.stiefelhagen@kit.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Authors are with Institute for Anthropomatics and Robotics</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Hangzhou SurImage Company Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transfer beyond the Field of View: Dense Panoramic Semantic Segmentation via Unsupervised Domain Adaptation Segmentation Network Segmentation Network Domain Adaptation Training Training Adapting Before Adaptation After Adaptation Input Panoramic Image Pinhole domain Panoramic domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 (Corresponding author:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Semantic segmentation</term>
					<term>domain adaptation</term>
					<term>panoramic segmentation</term>
					<term>scene parsing</term>
					<term>intelligent vehicles</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a) Pinhole?Panoramic Domain Adaptation (b) Narrow and 360? FoVs</p><p>Fig. 1: (a) An overview of the formalized task of domain adaptation for panoramic semantic segmentation. The source domain (green background) contains pinhole images with semantic annotations, while the target domain (blue background) contains panoramic images without annotations. (b) FoV comparison between pinhole forward-view and 360 ? panoramic surround-view imaging of self-driving scenes.</p><p>Abstract-Autonomous vehicles clearly benefit from the expanded Field of View (FoV) of 360 ? sensors, but modern semantic segmentation approaches rely heavily on annotated training data which is rarely available for panoramic images. We look at this problem from the perspective of domain adaptation and bring panoramic semantic segmentation to a setting, where labelled training data originates from a different distribution of conventional pinhole camera images. To achieve this, we formalize the task of unsupervised domain adaptation for panoramic semantic segmentation and collect DENSEPASS -a novel densely annotated dataset for panoramic segmentation under cross-domain conditions, specifically built to study the PINHOLE?PANORAMIC domain shift and accompanied with pinhole camera training examples obtained from Cityscapes. DENSEPASS covers both, labelled-and unlabelled 360 ? images, with the labelled data comprising 19 classes which explicitly fit the categories available in the source (i.e. pinhole) domain. Since data-driven models are especially susceptible to changes in data distribution, we introduce P2PDA -a generic framework for PINHOLE?PANORAMIC semantic segmentation which addresses the challenge of domain divergence with different variants of attention-augmented domain adaptation modules, enabling the transfer in output-, feature-, and feature confidence spaces. P2PDA intertwines uncertainty-aware adaptation using confidence values regulated on-the-fly through attention heads with discrepant predictions. Our framework facilitates context exchange when learning domain correspondences and dramatically ).</p><p>Code and dataset will be made publicly available at: https://github.com/chma1024/DensePASS improves the adaptation performance of accuracy-and efficiencyfocused models. Comprehensive experiments verify that our framework clearly surpasses unsupervised domain adaptationand specialized panoramic segmentation approaches as well as state-of-the-art semantic segmentation methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>: (a) An overview of the formalized task of domain adaptation for panoramic semantic segmentation. The source domain (green background) contains pinhole images with semantic annotations, while the target domain (blue background) contains panoramic images without annotations. (b) FoV comparison between pinhole forward-view and 360 ? panoramic surround-view imaging of self-driving scenes.</p><p>Abstract-Autonomous vehicles clearly benefit from the expanded Field of View (FoV) of 360 ? sensors, but modern semantic segmentation approaches rely heavily on annotated training data which is rarely available for panoramic images. We look at this problem from the perspective of domain adaptation and bring panoramic semantic segmentation to a setting, where labelled training data originates from a different distribution of conventional pinhole camera images. To achieve this, we formalize the task of unsupervised domain adaptation for panoramic semantic segmentation and collect DENSEPASS -a novel densely annotated dataset for panoramic segmentation under cross-domain conditions, specifically built to study the PINHOLE?PANORAMIC domain shift and accompanied with pinhole camera training examples obtained from Cityscapes. DENSEPASS covers both, labelled-and unlabelled 360 ? images, with the labelled data comprising 19 classes which explicitly fit the categories available in the source (i.e. pinhole) domain. Since data-driven models are especially susceptible to changes in data distribution, we introduce P2PDA -a generic framework for PINHOLE?PANORAMIC semantic segmentation which addresses the challenge of domain divergence with different variants of attention-augmented domain adaptation modules, enabling the transfer in output-, feature-, and feature confidence spaces. P2PDA intertwines uncertainty-aware adaptation using confidence values regulated on-the-fly through attention heads with discrepant predictions. Our framework facilitates context exchange when learning domain correspondences and dramatically This work was supported in part through the AccessibleMaps project by the Federal Ministry of Labor and Social Affairs (BMAS) under the Grant No. 01KM151112, in part by the University of Excellence through the "KIT Future Fields" project, and in part by Hangzhou SurImage Company Ltd. (Corresponding author: Kailun Yang.) <ref type="bibr" target="#b0">1</ref> Authors are with Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany (e-mail: jiaming.zhang@kit.edu, chaoxiang.ma.1024@gmail.com, kailun.yang@kit.edu, alina.roitberg@kit.edu, kunyu.peng@kit.edu, rainer.stiefelhagen@kit.edu).</p><p>Code and dataset will be made publicly available at: https://github.com/chma1024/DensePASS improves the adaptation performance of accuracy-and efficiencyfocused models. Comprehensive experiments verify that our framework clearly surpasses unsupervised domain adaptationand specialized panoramic segmentation approaches as well as state-of-the-art semantic segmentation methods.</p><p>Index Terms-Semantic segmentation, domain adaptation, panoramic segmentation, scene parsing, intelligent vehicles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S EMANTIC segmentation assigns a category label to every image pixel <ref type="bibr" target="#b0">[1]</ref> and is vital for perception of autonomous vehicles as it enables to locate key entities of a driving scene, such as road, sidewalk or person <ref type="bibr" target="#b1">[2]</ref>. Although the semantic segmentation accuracy has increased at a rapid pace thanks to the resilience of Convolutional Neural Networks (CNNs), most of the previously published frameworks are developed under the assumption that the driving scene images are captured with a pinhole camera <ref type="bibr" target="#b2">[3]</ref>. However, the comparably narrow Field of View (FoV) largely limits the perception capacity. Mounting multiple sensors can mitigate this issue, but requires additional data fusion mechanisms in return <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Recently, leveraging a single panoramic camera, which offers a unified 360 ? perception of the driving environment, as depicted in <ref type="figure">Fig. 1(b)</ref>, started to gain attention as a novel alternative way for expanding the FoV <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>Unfortunately, the scarcity of pixel-wise annotation of panoramic images still hinders the progress of the semantic segmentation research for such data. At the same time, recent progress in the area of Domain Adaptation (DA) has lead to highly effective techniques, giving a new perspective to complement the insufficient coverage of training data in driving scenarios, e.g., at the nighttime <ref type="bibr" target="#b7">[8]</ref> or in accident scenes <ref type="bibr" target="#b8">[9]</ref>. arXiv:2110.11062v1 [cs.CV] 21 Oct 2021</p><p>In this paper, we look at the problem of label-scarce panoramic segmentation through the lens of domain adaptation and target knowledge transfer from the significantly larger datasets available in the domain of standard images. To this intent, we formalize the task of unsupervised domain adaptation for panoramic segmentation in a novel Dense PAnoramic Semantic Segmentation (DENSEPASS) benchmark, where images in the label-scarce panoramic target domain are handled by adapting from data of the label-rich pinhole source domain ( <ref type="figure">Fig. 1(a)</ref> provides an overview of the formalized task).</p><p>To foster research of panoramic semantic segmentation under cross-domain conditions, we introduce DENSEPASS -a new dataset covering 360 ? images captured from all around the globe to facilitate diversity. To provide credible quantitative evaluation, our benchmark comprises (1) an unlabelled panoramic training set used for optimization of the domain adaptation model and (2) a panoramic test set manually labelled with 19 classes defined in accordance to Cityscapes <ref type="bibr" target="#b2">[3]</ref>, a dataset with pinhole images which we use as the label-rich training data from the source domain.</p><p>Unfortunately, a straightforward transfer of models trained on pinhole images to panoramic data often results in a significant drop in accuracy, as the panoramic layout of images passed through the equirectangular projection deviates from the standard pinhole camera data. For example, as shown in <ref type="figure">Fig. 1(b)</ref>, panoramic images have longer horizontal distribution or geometric distortion on both sides of the viewing direction, resulting in a considerable domain shift.</p><p>To meet the challenge of label-scarce panoramic segmentation by learning from label-rich pinhole image datasets, we implement P2PDA -a generic framework for the Pinhole to Panoramic Domain Adaptation. We examine different domain adaptation mechanisms: (i) Segmentation Domain Adaptation Module (SDAM), (ii) Attentional Domain Adaptation Module (ADAM), (iii) Regional Context Domain Adaptation Module (RCDAM), and (iv) Feature Confidence Domain Adaptation Module (FCDAM).</p><p>Specifically, the proposed SDAM module allows greater flexibility than the previous DA methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> used in the output-space as it can be plugged in at different feature levels. Another challenge is learning robust representations, which are not only discriminative for various categories with similar appearances, but also connect regions of the same category at diverse locations across the 360 ? . To address this, we leverage the progress of attention-based models <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> and propose the ADAM module for capturing long-range dependencies and positional relations. Besides, the RCDAM module addresses the horizontal distribution of panoramic images and obtains region-level context in order to effectively resist the geometric distortion caused by the equirectangular projection. Lastly, the FCDAM module enforces the backbone model to generate features with higher confidence in the respective domain. These modules can be flexibly activated through our P2PDA framework individually or jointly.</p><p>According to our observations, despite the different FoVs, the predictions of the two domains in the output-space still have similar contextual distributions. For instance, sky is still distributed in the upper part of the image, vegetation in the middle part, the road-related mostly in the lower part, etc. Therefore, we advocate multi-level alignment by deploying our building blocks for domain adaptation in different spaces. Through the complementary nature of the DA modules and their combination through P2PDA, we achieve simultaneous adaptation in the output-, feature-, and feature confidence spaces. Furthermore, P2PDA intertwines an uncertainty-aware adaptation phase using confident online panoramic pseudolabels, where the uncertainty estimation is regulated by attention heads with discrepant predictions.</p><p>Extensive evaluation of the PINHOLE?PANORAMIC transfer demonstrates the effectiveness of our framework, significantly boosting the domain adaptation performance of both accuracy-and efficiency-oriented models <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Finally, the proposed P2PDA strategy outperforms recent panoramic segmentation and domain adaptation frameworks, with our P2PDA-driven DANet surpassing &gt; 20 state-of-theart CNN-and transformer-based segmentation models.</p><p>This work is the extension of our conference paper <ref type="bibr" target="#b15">[16]</ref>, extended with several domain adaptation modules, a detailed description of the DensePASS dataset, and an extended set of experiments and analysis. In summary, our main contributions are summarized as follows:</p><p>? We create and publicly release DENSEPASS -a new benchmark for panoramic semantic segmentation collected from locations all around the world and densely annotated with 19 classes in accordance to the pinhole camera dataset Cityscapes to enable proper PINHOLE?PANORAMIC evaluation. ? We propose a generic P2PDA framework and investigate various DA modules both in a separate and joint manner, validating their effectiveness with various networks designed for self-driving scene segmentation. ? We advocate attentional domain adaptation by integrating attention-augmented adversarial-and attention-regulated self-learning adaptation, verifying that uncertainty-aware distillation with adapted knowledge can further boost the DA performance significantly. ? With the DANet <ref type="bibr" target="#b12">[13]</ref> baseline, our P2PDA framework achieves +13.5% and +16.2% mIoU gains by adapting from Cityscapes <ref type="bibr" target="#b2">[3]</ref> and further adding WildDash <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semantic Segmentation and Self-attention Modules</head><p>Semantic segmentation has progressed almost exponentially due to the architectural advances of Fully Convolutional Networks <ref type="bibr" target="#b0">[1]</ref> and the increasing amount of available reallife training data <ref type="bibr" target="#b17">[18]</ref>. Models such as PSPNet <ref type="bibr" target="#b18">[19]</ref> and DeepLabV3+ <ref type="bibr" target="#b19">[20]</ref> attain great accuracy improvements on conventional benchmarks by harvesting multi-scale feature representations using atrous convolution or pyramid pooling. In <ref type="bibr" target="#b20">[21]</ref>, a dense prediction branch, Semantic-FPN, is attached on top of the feature pyramid, individually for semantic segmentation. While these composite architectures achieve finegrained precise segmentation, efficient and compact networks like ERFNet <ref type="bibr" target="#b14">[15]</ref>, SwiftNet <ref type="bibr" target="#b21">[22]</ref>, and Fast-SCNN <ref type="bibr" target="#b22">[23]</ref> aim to perform both fast and accurate segmentation. For selfdriving scene parsing, most of existing benchmark datasets are captured by pinhole cameras, for example Cityscapes <ref type="bibr" target="#b2">[3]</ref>, Mapillary Vistas <ref type="bibr" target="#b23">[24]</ref>, ApolloScape <ref type="bibr" target="#b24">[25]</ref>, BDD <ref type="bibr" target="#b25">[26]</ref>, IDD <ref type="bibr" target="#b26">[27]</ref>, KITTI <ref type="bibr" target="#b27">[28]</ref>, and WildDash <ref type="bibr" target="#b16">[17]</ref>. Despite large receptive fields, most segmentation algorithms are driven by data availability and are therefore designed for standard narrow-FoV images.</p><p>Another line of work leverages self-attention modules <ref type="bibr" target="#b28">[29]</ref>, which automatically weighs input positions (i.e., temporal <ref type="bibr" target="#b28">[29]</ref> or spatial <ref type="bibr" target="#b11">[12]</ref>), gains increasing attention in the field. Such mechanisms are broadly used for capturing long-range contextual dependencies, which are crucial for dense-pixel prediction tasks <ref type="bibr" target="#b29">[30]</ref>. The success of attention mechanisms in visual recognition <ref type="bibr" target="#b11">[12]</ref>, leads to various explorations in semantic segmentation works focused on both, accuracy-oriented networks <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> and efficiency-oriented networks <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. For instance, Fast Attention Network (FANet) <ref type="bibr" target="#b13">[14]</ref> uses a fast attention mechanism by replacing the Softmax normalization with cosine similarity, whereas Dual Attention Network (DANet) <ref type="bibr" target="#b12">[13]</ref> appends position-and channel attention modules to model semantic associations between any two pixels or channels. Other prominent techniques include criss-cross attention <ref type="bibr" target="#b34">[35]</ref> and disentangled non-local attention blocks <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. In particular, ECANet <ref type="bibr" target="#b32">[33]</ref> concurrently highlights horizontally-driven dependencies and collects omni-range contextual information for large-FoV semantic segmentation.</p><p>We leverage such attention principles to mitigate the domain shift by highlighting regional context and present a crossdomain segmentation framework with attentional domain adaptation modules. We experiment with both, accuracy-and efficiency-focused networks <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> as the segmentation architecture, and demonstrate the consistent effectiveness of our adaptation modules for bringing standard semantic segmentation models to panoramic imagery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Semantic Segmentation for 360 ? Panoramic Images</head><p>Segmentation of panoramic data, which is often captured through distortion-pronounced fisheye lenses <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> or multiple surround-view cameras <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, is challenging as it entails a set of hard tasks like distortion elimination, camera synchronization and calibration, as well as data fusion, resulting in higher latency and complexity. Yang et al. introduce the PASS <ref type="bibr" target="#b6">[7]</ref> and the DS-PASS <ref type="bibr" target="#b43">[44]</ref> frameworks which naturally mitigate the effect of distortions by using a single-shot panoramic annular lens system, but come with an expensive memory-and computation cost, as it requires separating the panorama into multiple partitions for predictions, each resembling a narrow-FoV pinhole image. This is significantly improved by the OOSS framework <ref type="bibr" target="#b44">[45]</ref> through multi-source omni-supervised learning. The latest advancements include frameworks focusing on dimension-wise positional priors <ref type="bibr" target="#b45">[46]</ref>, omni-range contextual dependencies <ref type="bibr" target="#b32">[33]</ref>, or leveraging contrastive pixel-propagation pre-training <ref type="bibr" target="#b46">[47]</ref>.</p><p>All previous frameworks <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b46">[47]</ref> are developed under the assumption that the labelled training data are implicitly or partially available in the target domain of panoramic segmentation. Since panoramic datasets are comparably small in size, we argue, that panoramic segmentation might strongly benefit from the significantly larger datasets available in the domain of pinhole camera image segmentation. While synthetic omnidirectional datasets exist <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, they lack diversity and realism present in the largescale pinhole image collections with rich ontologies <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b16">[17]</ref>. In this work, we look at panoramic segmentation from a domain adaptation perspective and introduce the DENSEPASS dataset covering images with 19 annotated categories available in both, standard-and panoramic domains. We introduce a framework for unsupervised domain adaptation for panoramic semantic segmentation, where we combine prominent segmentation approaches with attention-based domain adaptation modules involving attention-augmented adversarial learning and attention-regulated self-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Unsupervised Domain Adaptive Semantic Segmentation</head><p>To tackle model generalization to new scenes, domain adaptation became an increasingly popular topic, offering new ways to complement the insufficient coverage of training data in driving scenarios, e.g., at the nighttime <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, or in accident scenes <ref type="bibr" target="#b8">[9]</ref>. As neural networks are especially vulnerable to changes in data distribution, various domain adaptation frameworks have been proposed to address this challenge with the most common practices being self-training <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref> and adversarial learning <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>. Self-training based methods are grounded in iterative improvement, e.g., through pseudo-labels. PyCDA <ref type="bibr" target="#b56">[57]</ref> views self-training from the perspective of curriculum adaptation by introducing a pyramid curriculum consisting of various properties about the target domain. CRST <ref type="bibr" target="#b57">[58]</ref> is formulated as a regularized self-training framework, which takes the confident predictions in target domains as soft pseudo-labels for segmentation network retraining. Zheng and Yang <ref type="bibr" target="#b58">[59]</ref> leverage uncertainty estimation to enable automatic thresholding of noisy pseudo-labels for unsupervised segmentation adaptation. The recent ProDA <ref type="bibr" target="#b59">[60]</ref> resorts to representative prototypes, namely class-wise cluster centroids, to gradually rectify soft pseudo-labels and produce a compact target feature structure.</p><p>The second prominent group of approaches is driven by Generative Adversarial Networks (GANs) <ref type="bibr" target="#b63">[64]</ref>, which learn domain translations, e.g., via image-to-image conversions <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b61">[62]</ref>, feature alignment <ref type="bibr" target="#b60">[61]</ref>, or semantic layout matching <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref>. To utilize the significant amount of source-target similarities in the resulting segmentation masks, Tsai et al. <ref type="bibr" target="#b9">[10]</ref> align the domains in output-space via adversarial learning (AdaptSegNet). Chang et al. <ref type="bibr" target="#b10">[11]</ref> construct the DISE framework which extracts domain-invariant structure and domain-specific texture information to reduce sourcetarget discrepancies. More recent works further prioritize category-level alignment (CLAN) <ref type="bibr" target="#b62">[63]</ref>, minimize adversarial entropy <ref type="bibr" target="#b66">[67]</ref>, or perform affinity-space domain adaptation <ref type="bibr" target="#b67">[68]</ref>.</p><p>Domain adaptation also increasingly leverages attention mechanisms by, e.g., attending feature maps to highlight transferable areas <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>, minimizing the source-target distribution divergence of the attention maps <ref type="bibr" target="#b70">[71]</ref>, or using multiple cross-domain attention modules for obtaining The SDAM module is applied on the high-level feature-space or output-space and the FCDAM on the feature confidence space, while ADAM is performed on the dual attended feature-space and RCDAM on the output-space after the region attention module. Each of these four modules includes an individual discriminator and an adversarial loss. In the second stage of pseudo-label self-supervised learning, the uncertainty map is calculated based on the C1 and C2 predictions and used for element-wise multiplication with pseudo-labels as an online selection of pseudo-labels.</p><p>context dependencies from both local and global perspectives <ref type="bibr" target="#b71">[72]</ref>. mDALU <ref type="bibr" target="#b72">[73]</ref> performs partially-supervised learning and attention-guided fusion to tackle a specific problem of multi-source domain adaptation and label unification.</p><p>In this work, we specifically focus on domain transfer for panoramic semantic segmentation, which differs from the standard pinhole images in several important aspects, such as discontinuous boundaries and distorted objects. To exploit long-range correlations between pixels and semantic regions, we extend AdaptSegNet <ref type="bibr" target="#b9">[10]</ref> with attention-augmented modules in multiple levels and a regional context exchange, simultaneously adapting in the output-, feature-, and feature confidence spaces. This enables direct information exchange across the whole FoV, mitigating the influence of discrepancy in positional priors and local distortions, which is vital for the PINHOLE?PANORAMIC transfer. Moreover, the proposed framework intertwines an uncertainty-aware adaptation phase using confident online panoramic labels regulated by attention modules, which further reduces the domain gap.</p><p>III. P2PDA: PROPOSED FRAMEWORK In this work, we introduce a generic framework for 360 ? perception of self-driving scenes by learning to adapt semantic segmentation networks from a label-rich source domain of standard pinhole camera images to the unlabelled target domain of panoramic data. Conceptually, our framework covers an encoder-decoder based semantic segmentation network and four different building blocks for domain alignment: Segmentation domain adaptation module (SDAM), Attentional domain adaptation module (ADAM), Regional context domain adaptation module (RCDAM), and Feature confidence domain adaptation module (FCDAM), which we place at two different network stages: after and before the decoder of the segmentation network. In the following, we give an overview of the proposed framework (Sec. III-A) and present the integrated domain adaptation modules in detail (Sec. III-B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Framework Overview</head><p>Attention-augmented adversarial adaptation. Our framework builds on the AdaptSegNet architecture <ref type="bibr" target="#b9">[10]</ref> extending it with multiple variants of region-or attention-augmented DA modules plugged in at different network depths with an overview provided in <ref type="figure" target="#fig_0">Fig. 2</ref>. The main components of our framework are a weight-shared segmentation network G with attention modules and multiple DA modules equipped with the corresponding discriminators D. For unsupervised domain adaptation methods, only the source domain dataset</p><formula xml:id="formula_0">D s = {(x s , y s )|x s ? R Hs?Ws?3 , y s ? R Hs?Ws?1 } and the unlabelled target domain dataset D t = {(x t )|x t ? R Ht?Wt?3 }</formula><p>are given, where x s and x t denote the input images from source and target domains, and y s are the ground truth labels in source domain. We note that (H s , W s ) and (H t , W t ) are the height and width of the source and target images, respectively.</p><p>For ease of understanding, we only list the formulas for a single classifier on G and single D. For multiple G or D, they will be combined through specific hyper-parameters. First, the source domain images x s are fed into the segmentation network G (also referred to as the generator) to create prediction results? s = G(x s ) and the source ground-truth labels y s are used to compute the segmentation loss L seg as follows:</p><formula xml:id="formula_1">L seg (G) = E [ (G(x s ), y s )] ,<label>(1)</label></formula><p>where E[?] is the statistical expectation and (?, ?) is the standard cross entropy loss. Next, the discriminator D is trained with the binary objective to distinguish between the source and target domains of the input, so that the discriminator loss is formulated as:</p><formula xml:id="formula_2">L d (D) = E [ (D(G(x s )), 0)] + E [ (D(G(x t )), 1)] , (2)</formula><p>where the (?, ?) is binary cross entropy, with 0 and 1 being the two-class labels (panoramic and pinhole).</p><p>Then, to enforce the generator G to align the distribution of? t closer to? s , the prediction results? t = G(x t ) for the target domain is directly used to estimate the adversarial loss, which is updated alongside with L seg and is formulated as:</p><formula xml:id="formula_3">L adv (G) = E [ (D(G(x t )), 0)] .<label>(3)</label></formula><p>The adversarial loss is high if the discriminator prediction is correct, so the adversarial loss facilitates generation of segmentation masks in the target domain which successfully "fool" the discriminator. In other words, the discriminators are trained to distinguish between the source and target domains with L d (D), while the segmentation network G is trained to 1) correctly segment the images from the source domain with L seg , and 2) "fool" the discriminator by making the target domain data indistinguishable from the source domain data. The join loss from Eqn. (1) and Eqn. <ref type="formula" target="#formula_3">(3)</ref> used to train the generator G becomes:</p><formula xml:id="formula_4">L(G) = ? seg L seg (G) + ? adv L adv (G),<label>(4)</label></formula><p>where ? adv and ? seg are weights used to balance the domain adaptation and semantic segmentation losses. To perform endto-end training for multiple classifiers of G and multiple D, our final loss function is denoted as:</p><formula xml:id="formula_5">L(G, D) = i ? i seg L i seg (G) + i ? i adv L i adv (G) + i ? i d L i d (D).<label>(5)</label></formula><p>Attention-regulated self-learning adaptation. Our network can readily generate highly qualified segmentation masks on panoramic images, after the main stage of multi-level alignment through the domain adaptation modules described in Sec. III-B. Our next goal is to advance the training procedure by leveraging the inherent knowledge present in the pixel-wise predictions obtained after the first training stage as panoramic pseudo-labels. To achieve this, P2PDA intertwines an uncertainty-aware domain adaptation stage by using panoramic pseudo-labels with high confidences on-thefly, therefore improving the prediction in iterative fashion. In this stage, the source images are replaced by the selfsupervised panoramic images, i.e., the predictions are used to refine the model itself. The key idea of this training stage is to employ multiple classifiers with attention heads naturally encouraged to produce discrepant predictions in order to assess the uncertainty of the pseudo-labels. First, we estimate the uncertainty map by using the variance operation on predictions produced with two different classifiers with disparate attention modules as in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Then, we apply element-wise multiplication of the pseudo-labels with the resulting uncertainty map and, finally, we threshold the resulting value to obtain the certain pseudo-labels. An overview of our uncertainty-driven self-training is illustrated in the bottom part of <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>In the next section, we describe our building blocks for domain alignment and adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Domain Adaptation Modules</head><p>Segmentation domain adaptation module. Our initial domain adaptation module SDAM is derived from AdaptSegNet and attempts to match the source and target segmentation output (the module is illustrated in <ref type="figure">Fig. 3</ref>). After a segmentation network forward pass with images from both domains (x s and x t ), feature maps of the both representations are used as input to the discriminator D which learns to distinguish the domain with L d (D), while the segmentation network G learns to segment the pinhole images with L seg (G) and align the domains with L adv (G). SDAM learns a PINHOLE?PANORAMIC domain adaptation model at multiple levels jointly within our P2PDA framework, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Attentional domain adaptation module. Next, we design ADAM, an attentional domain adaptation module, aimed at detecting and magnifying the significant amount of pinholepanoramic correspondences at both, local and global levels (overview in <ref type="figure">Fig. 4</ref>). ADAM differs from SDAM as it leverages the attention mechanism to learn an optimal weighting scheme for the discriminator input. As in the Dual Attention Module (DAM) <ref type="bibr" target="#b12">[13]</ref> shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the feature map extracted by the backbone model is denoted as F ? R h?w?c , where the h, w, and c are the height, width and channel of the feature map. After this representation is reshaped as F ? R (h?w)?c , the position-wise attended feature is calculated as: dual attended feature is concatenated with the the reshaped S ? R h?w?c and the reshaped R ? R h?w?c . By doing this, ADAM enables direct context information exchange among all pixels, mitigating the influence of discrepancy in positional priors and local distortions. Relevant portions of the feature maps of both, x s and x t inputs are enhanced through the attention and the re-weighted source and target representations are both used to optimize the corresponding discriminator D. Regional domain adaptation module. Next, we focus on region relationship of the panoramic images. Inspired by RANet, we design the RCDAM module based on the Regional Attention Module (RAM) <ref type="bibr" target="#b30">[31]</ref> to configure the information flow between different regions and within the same region, as illustrated in <ref type="figure">Fig. 5</ref>. RCDAM follows a hierarchical adversarial learning scheme with two-stage discriminators, where the first stage is identical to the previously described SDAM. The second stage is conducted by the RAM module, which includes two blocks: a Region Construction Block (RCB) and a Region Interaction Block (RIB) first introduced in RANet. The inputs to this stage are the feature maps of F s and F t after a segmentation network forward pass. <ref type="figure">Fig. 6</ref> gives a detailed overview of the RCB and RIB building blocks.</p><formula xml:id="formula_6">S = ?(F T , F ?F T ), S ? R c?(h?w) ,</formula><p>In contrast to <ref type="bibr" target="#b30">[31]</ref>, we define three classifiers for the given feature map. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the B1 classifier outputs the binary boundary map, whereas the C1 and C2 classifiers are for the semantic maps according to the number of classes. To perform the hierarchical learning and to lead the discrepancy between two semantic classifiers C1 and C2, we utilize the semantic classifier from the original RCB block. Then, RCB receives the binary boundary map and semantic maps as in <ref type="bibr" target="#b30">[31]</ref> to link each pixel (the red dot in <ref type="figure">Fig. 6</ref>) to different semantic regions, so as to conduct a region decision map with the representative pixels (the larger dots). Based on the region decision map, RIB can select representative pixels (the larger dots in color) for each region. According to the given feature map, RIB will back-project the prototypical information to each pixel (the smaller dots) in the same region to form the global contextual representations which are afterwards propagated to all pixels in the image. Finally, regional context is sufficiently exchanged and the regional context-driven feature maps are built to generate final prediction outputs, which will be used to perform the same operations as in the first stage.</p><p>It is worth noting that the second stage helps improving the segmentation result in the first stage, which can make the outcome more compact and helps the PINHOLE?PANORAMIC adaptation by exchanging regional context. Consequently, the given feature map belongs to the first stage of our framework (see <ref type="figure">Fig. 5</ref>) and this domain adaptation module does not impact the original segmentation architecture. It can therefore be flexibly used in models with or without attention layers.</p><p>Feature confidence domain adaptation module. Compared to the aforementioned domain adaptation model, our next module FCDAM mainly operates in the feature confidence space. After the model undergoes the alignment operation in feature-and output-space, FCDAM is used to further improve the confidence of domain-specific features given by the backbone architecture. Different from <ref type="bibr" target="#b66">[67]</ref>, the entropy map E ? [0, 1] h?w is calculated by the given feature map. Thus, the loss of entropy map is formulated as: L ent (F ) = ? h,w (?(F (h,w) )log(?(F (h,w) ))), where ? is the Sigmoid function applied at each pixel of feature map F ? R h?w . During training G with the feature map F s = G(x s ) and F t = G(x t ) from source-and target domain, FCDAM can improve the feature confidence by minimizing the loss of the feature entropy map. Differing from previous DA modules, L adv (G) for FCDAM in Eqn. (5) is replaced by L ent (G(F )) = ? s ent L ent (G(F s )) + ? t ent L ent (G(F t )), where both ? ent are same as ? adv . Note that, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, this FCDAM module eases the process of adding feature confidence learning to the original backbone without modification to the architecture of the whole domain adaptation framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We conduct extensive experiments with different variants of our P2PDA framework in order to validate our the idea of panoramic segmentation through domain adaptation from the pinhole camera images. First, we introduce DENSEP-ASS -a novel dataset for dense panoramic segmentation of driving scenes annotated in accordance to the pinhole camera benchmark Cityscapes (Sec. IV-A). Then, we quantitatively evaluate how well our P2PDA framework can handle the PINHOLE?PANORAMIC transfer and conduct extensive ablation studies for different versions of DA modules and two segmentation networks: the speed-oriented FANet <ref type="bibr" target="#b13">[14]</ref> (Sec. IV-B) and accuracy-oriented DANet <ref type="bibr" target="#b12">[13]</ref> (Sec. IV-C). We further benchmark our framework against &gt; 20 state-ofthe-art semantic segmentation models (Sec. IV-D), compare our approach with competitive panoramic segmentation and domain adaptation frameworks (Sec. IV-E), and examine the impact of expanding the training set with more examples (Sec. IV-F). Finally, we showcase multiple qualitative results (Sec. IV-G). We adopt Mean Intersection over Union (mIoU) as our primary evaluation metric.  <ref type="figure">Fig. 7</ref>: Distributions of DensePASS, Cityscapes <ref type="bibr" target="#b2">[3]</ref>, and WildDash <ref type="bibr" target="#b16">[17]</ref> in terms of class-wise pixel counts per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Experimental Settings</head><p>Source dataset (pinhole). We leverage Cityscapes <ref type="bibr" target="#b2">[3]</ref> as our label-rich source dataset providing a high amount of annotated pinhole camera images. Cityscapes comprises 2979 training-and 500 validation images (1024?2048 resolution) captured in 50 different European cities and annotated with 19 categories. In most experiments, we use the 2979 training samples as our source training data. To investigate the potential of P2PDA, in some cases, we also leverage the WildDash <ref type="bibr" target="#b16">[17]</ref> dataset which includes 4256 pinhole images (1080?1920 resolution) for improving the generalization.</p><p>DensePASS target dataset (panoramic). Since no established segmentation benchmarks address the PINHOLE?PANORAMIC recognition and previous panoramic test-beds cover a very limited number of classes <ref type="bibr" target="#b6">[7]</ref>[33], we collect DensePASS -a novel densely annotated dataset for panoramic segmentation of driving scenes. It is created with the PINHOLE?PANORAMIC transfer in mind, so that the test data is annotated with 19 categories present in the pinhole camera dataset Cityscapes and other prominent semantic segmentation benchmarks <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b25">[26]</ref>. To facilitate the unsupervised domain adaptation task, DensePASS covers both, labelled data (100 panoramic images used for testing) and unlabelled training data (2000 panoramic images used for the domain transfer optimization). A FoV of 70 ? ?360 ? is covered in the captured panoramic images with a 400?2048 resolution. The data is collected using Google Street View and includes images from different continents (25 different cities for testing and 40 for training).</p><p>In <ref type="figure">Fig. 7</ref>, we compare label distributions of DensePASS with Cityscapes <ref type="bibr" target="#b2">[3]</ref> and WildDash <ref type="bibr" target="#b16">[17]</ref> datasets in terms of pixel counts by averaging the number of images used in our domain adaptation study. Our histogram analysis indicates, that DensePASS and the mentioned pinhole camera datasets follow a relatively close distribution of categories. This observation indicates, that distribution alignment not only in the featurespace but also in the semantic output-space might be beneficial and is therefore integrated in our framework.</p><p>Training settings. We use stochastic gradient descent (initial learning rate of 1e ? 5, momentum of 0.9, decay of 5e ? 4) for optimization of the segmentation network G and the Adam optimizer <ref type="bibr" target="#b73">[74]</ref> for discriminators D (initial learning rate set to 4e ? 6). For both optimizers, the learning rate is decreased polynomially through multiplication with (1 ? iter max_iter ) 0.9 after each iteration, where max_iter is set to 200000 with a batch-size of 2. The semantic loss (i.e. the cross-entropy loss) in Eqn. <ref type="formula" target="#formula_1">(1)</ref> is updated with class weights, which are calculated in advance from the source annotated dataset, following ERFNet <ref type="bibr" target="#b14">[15]</ref>. The loss balancing weights regarding to Eqn. <ref type="bibr" target="#b4">(5)</ref>, ? adv and ? seg are set to 0.001 and 1.0 for SDAM/FCDAM, and 0.0002 and 0.1 for ADAM. In the RCDAM module, ? seg is set to 1.5 for the prediction result before RCB. During training, pinhole data is resized to 720?1280 while panoramic images remain at the 400?2048 resolution. In the FCDAM training stage and the pseudo-label self-supervised learning stage, the learning rate is decreased as 1e ? 8 for the generators and 4e ? 9 for the discriminators. Horizontal flipping and random translation between [?2, 2] pixels are performed for data augmentation. Different from the attended feature map setting of DANet, which forwards the attended feature map with a downsampling rate of 16 as the input of two classifiers (B1 and C1), FANet has multi-level attended feature maps. After our experiment, the feature maps with downsampling rates of 16 and 4 are concatenated and upsampled as input to the B1 classifier. At the same time, feature maps with downsampling rates of 16 and 8 are concatenated and upsampled as input to the C1 classifier in the FANet setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Studies for Segmentation Network with FANet</head><p>We first consider FANet <ref type="bibr" target="#b13">[14]</ref>, a lightweight speed-oriented network, as the segmentation model and investigate different combinations of our four domain adaptation modules. As shown in <ref type="table" target="#tab_0">Table I</ref>, before adaptation, FANet yields a mIoU of 26.90% indicating large room for improvement in crossdomain generalization. Our framework improves the result to 32.17% by using the SDAM module in both featureand output-space (+5.27% gain). Integrating the attentional ADAM module also leads to a considerable boost (32.67% in mIoU, a +5.77% gain over the source-only baseline). A combination of our four modules yields the recognition result of 33.52% in mIoU. Furthermore, the pseudo-label selfsupervised learning boosts our S+A+R and S+A+F+R adaptation results to 34.26% and 35.67% in mIoU, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Studies for Segmentation Network with DANet</head><p>Our main architecture for the in-depth experiments is the accuracy-oriented segmentation network DANet <ref type="bibr" target="#b12">[13]</ref> (results provided in <ref type="table" target="#tab_0">Table II</ref>). The native source-trained DANet achieves a mIoU of only 28.50%, highlighting  <ref type="bibr" target="#b13">[14]</ref> as the segmentation network and set different domain adaptation modules on our P2PDA framework to test on DensePASS with the size of input 2048?400. S, A, R, F represent the SDAM, ADAM, RCDAM and FCDAM respectively. Feature-and output-space are named as FS and OS for short. SSL represents the self-supervised learning with pseudo-labels. The first line is the Cityscapes source-only result without adaptation.  the sensitivity of modern segmentation networks to the PINHOLE?PANORAMIC domain shift. The performance is strongly improved (+10.01% boost) through SDAM modules placed in feature-and output-space, achieving 38.51% in mIoU. Similarly, using the ADAM module yields a result of 39.16% (a +10.66% improvement over the source-only baseline). Combining both the SDAM and ADAM modules again slightly improves the performance (39.28% in mIoU).</p><p>We further explore the use of the RCDAM module in output-space, yielding 39.46% mIoU (+10.96% boost over the baseline). The performance of 39.76% (+11.26% boost with respect to the original segmentation network) is achieved by combining three modules: SDAM, ADAM and RCDAM. Integrating FCDAM leads 40.52% in mIoU (a +12.02% increase). Overall, our experiments showcase that direct crossdomain semantic segmentation is a hard task and P2PDA framework with attentional and regional domain adaptation modules clearly helps to close the domain gap. Furthermore, applying pseudo-label self-supervised learning based on prediction uncertainties improves the results to 41.99% in mIoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Benchmarking and Comparison with the State-of-the-Art</head><p>Until now, we compared different framework configurations with each other and the native segmentation network. Next, we aim to quantify pinhole-panoramic domain gap and extend our evaluation with over 20 off-the-shelf segmentation models trained on Cityscapes and evaluated on both, Cityscapes (no domain shift) and the panoramic DensePASS images. <ref type="bibr" target="#b0">1</ref> Table III summarizes our results. It is evident, that modern CNNbased segmentation models trained on pinhole camera images struggle with generalization to panoramic data, with performance degrading by ? 50% as we move from the standard PINHOLE?PINHOLE setting to the PINHOLE?PANORAMIC evaluation. The recent transformer-based method SETR <ref type="bibr" target="#b75">[76]</ref> with a powerful computation-intensive backbone is more robust, yet also suffering from &gt; 40% accuracy drops.</p><p>We also verify our P2PDA domain adaptation strategy with three different segmentation models (bottom part of <ref type="table" target="#tab_0">Table III)</ref>. Our P2PDA strategy with regional and attentional context exchange improves the PINHOLE?PANORAMIC outcome by a large margin (mIoU gains of +17.4%, +8.8% and +13.5% for ERFNet, FANet and DANet, respectively). Our experiments provide encouraging evidence that P2PDA can be successfully deployed for cross-domain 360 ? understanding. We now consider the inference speed and test the forward pass time with the batch-size of 1 to stimulate real driving applications. We report the mean Frames Per Second (FPS) running through the 100 panoramic images at the resolution of 400?2048 on a GTX 1080Ti GPU processor. It turns out that ERFNet, FANet and DANet reach 32.3, 67.7 and 17.2 FPS, respectively. We see clear speed-accuracy trade-offs, with FANet being the fastest and DANet being the most accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison to Panoramic Semantic Segmentation, Unsupervised Domain Adaptation, and Multi-Supervision Methods</head><p>Before delving into more comparisons, we note that the category definitions of ApolloScape <ref type="bibr" target="#b24">[25]</ref>, IDD <ref type="bibr" target="#b26">[27]</ref>, and Mapillary Vistas <ref type="bibr" target="#b23">[24]</ref> datasets are different from other datasets, in which the identical 19 categories following Cityscapes <ref type="bibr" target="#b2">[3]</ref> can be obtained by class mapping. Models trained on Apol-loScape <ref type="bibr" target="#b24">[25]</ref> perform segmentation with 16 overlapping categories, where the terrain, sky, and train classes are discarded. The train class in IDD <ref type="bibr" target="#b26">[27]</ref> and Mapillary Vistas <ref type="bibr" target="#b23">[24]</ref> is excluded, thus other 18 classes are remained.</p><p>Next, we compare our approach with previous segmentation frameworks specifically developed for panoramic images, including PASS <ref type="bibr" target="#b6">[7]</ref> and ECANet <ref type="bibr" target="#b32">[33]</ref>. PASS elevates the accuracy of ERFNet by fusing the semantically-meaningful features of panorama segments (4 segments as suggested by <ref type="bibr" target="#b6">[7]</ref>). ECANet <ref type="bibr" target="#b32">[33]</ref> highlights horizontal dependencies and relies on omni-supervised learning using heavy training data, thereby reaching a high performance. Our P2PDAdriven DANet trained with Cityscapes and WildDash sources (detailed in Sec. IV-F) outperforms these works, <ref type="table" target="#tab_0">achieving   0?4   5?9   0?1   35?1   80?2   25?2   70?3   15?1   0  20  30  40  50  60   mIoU/mAcc   0?4   5?9   0?1   35?1   80?2   25?2   70?3   15?2   0  40  60  80  100   Road   0?4   5?9   0?1   35?1   80?2   25?2   70?3   15?5   10  15  20  25  30  35   Sidewalk   0?4   5?9   0?1   35?1   80?2   25?2   70?3   15?2   0  40  60  80  100   Vegetation   0?4   5?9   0?1   35?1   80?2   25?2   70?3   15?1   0  20  30  40  50  60</ref>  44.66% by using pinhole data annotations only and successfully transferring beyond the FoV. Performing another run of the self-supervised learning stage elevates the mIoU to 48.52%, leading to the best segmentation result. We now compare P2PDA with two state-of-the-art approaches for unsupervised domain adaptation: one method based on adversarial learning (CLAN <ref type="bibr" target="#b62">[63]</ref>) and one built on self-training (CRST <ref type="bibr" target="#b57">[58]</ref>), both adapting from Cityscapes to DensePASS. Our proposed framework clearly stands out in front of other domain adaptation pipelines, improving the performance by ? 10%, showcasing the effectiveness of the attention-based design which intertwines attention-augmented adversarial learning and attention-regulated self-training for an effective knowledge transfer.</p><p>To broaden our comparison, we also consider several multisupervision methods which benefit from multi-source data for a better generalization. Seamless-Scene-Segmentation <ref type="bibr" target="#b76">[77]</ref> uses instance segmentation labels for auxiliary supervision, whereas USSS <ref type="bibr" target="#b77">[78]</ref> performs multi-source semi-supervised learning. The outputs of these models are mapped to the 19 classes in DensePASS to be comparable with other models. ISSAFE <ref type="bibr" target="#b8">[9]</ref> merges multiple training datasets including Cityscapes, KITTI-360 <ref type="bibr" target="#b27">[28]</ref>, and BDD <ref type="bibr" target="#b25">[26]</ref> for safety-critical accident scene segmentation. Our experiments indicate that all these multi-source frameworks are sub-optimal in contrast to P2PDA which consistently leads to the best recognition rates. At the same time, P2PDA is trained on far less trainig data as the above approaches leverage larger databases, such as BDD/IDD and Mapillay, for training <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Especially for the classes building, truck, train, and bicycle, our framework is a front-runner by a large margin, as seen in <ref type="table" target="#tab_0">Table IV.</ref> To grasp the key prediction differences before and after the domain adaptation with P2PDA, we compare the Pixel Accuracy (Acc) and IoU in different directions of the panoramic image in <ref type="figure" target="#fig_4">Fig. 8</ref>, where the blue-tinted regions indicate the section visible to a forward-facing narrow-FoV pinhole camera. We partition the 360 ? into 8 directions and compute the classwise accuracy of navigation-critical categories separately for each direction. Our model leads to a considerable performance increase in all directions and for all the classes. While the same panoramic view can be achieved from multiple cameras surrounding a vehicle, our system enables reliable deployment  <ref type="bibr" target="#b6">[7]</ref> and ECANet <ref type="bibr" target="#b32">[33]</ref>), unsupervised domain adaptation (CLAN <ref type="bibr" target="#b62">[63]</ref> and CRST <ref type="bibr" target="#b57">[58]</ref>), and multi-supervision methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b77">[78]</ref>. * denotes performing two runs of the self-supervised learning.  using a single camera together with good performances in certain safety-critical directions. In particular, the recognition quality of sidewalk, person, and motorcycle is improved by an especially large margin through the domain adaptation paradigm. For the critical road and car segmentation relevant to autonomous driving, we have reached pixel accuracy at the level of 90% around the 360 ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Complementing the Cityscapes Source with WildDash</head><p>Since most of the pinhole images are captured in the front view <ref type="bibr" target="#b2">[3]</ref>, the features differ from the ones of 360 ? panoramic images, which leads to the feature insufficiency of the side and rear views. With the self-supervised learning phase, from high-confidence pseudo-labels, our model can learn more panoramic-oriented features including the side and rear views. While the attentional-and regional adaptation modules already help address this issue by propagating features across the entire FoV and reduce the domain gap, to further complement the image feature from these perspectives, we consider exploiting a more diverse dataset in the P2PDA framework.</p><p>Thereby, our next area of investigation is the impact of expanding the source data with a more complex dataset, since DensePASS contains highly composite scenes due to larger FoV, while Cityscapes is large but relatively simple. To achieve this, we leverage the WildDash dataset <ref type="bibr" target="#b16">[17]</ref> with 4256 pinhole images, pixel-level annotations and more unstructured surroundings. For the training, we aggregate Cityscapes and WildDash sources without any complex joint training methodologies. As shown in the last rows of <ref type="table" target="#tab_0">Table II</ref>, we obtain better mIoU with the expanded training set, achieving 42.87% and 44.66% with different P2PDA variants. Interestingly, the IoUs of road, sidewalk, fence, terrain, and car are significantly improved, which we link to the strong positional priors of these categories in structured urban scenes, while DensePASS and WildDash environment is more chaotic and unconstrained. Moreover, direct comparison of the different P2PDA variants further verifies the effectiveness of the attention-augmented and uncertainty-aware adaptation strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Qualitative Analysis</head><p>In our final study, we showcase multiple examples of representative qualitative results in <ref type="figure" target="#fig_3">Fig. 9, Fig. 10, and Fig. 11</ref>. <ref type="figure" target="#fig_5">Fig. 9</ref>   <ref type="figure" target="#fig_3">Fig. 10</ref>: Visualization of attention maps from DANet before and after domain adaptation. For each input panorama, we select two points and show their corresponding position attention maps. Zoom in for a better view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Before adaptation Ours Uncertainty map Ground truth <ref type="figure">Fig. 11</ref>: Qualitative examples of semantic segmentation on panoramic images. From left to right columns are the input image, DANet predictions before adaptation, our predictions, the uncertainty map (brighter areas indicate higher uncertainty), and the ground truth.</p><p>adaptation (e.g., car and truck in all examples). There is also more clarity as it comes to detailed segmentation of small objects, such as traffic light and traffic sign in the first and the second row, as well as pole in all rows. These qualitative examples consistently confirm the conclusions of our quantitative evaluation, highlighting the benefits of the proposed P2PDA strategy for 360 ? self-driving scene understanding through attention-augmented adaptation from pinhole camera data.</p><p>We now deepen the analysis and study the influence of our framework on the attention maps. As shown in <ref type="figure" target="#fig_3">Fig. 10</ref>, for each input panorama, we select two points and visualize their corresponding position attention maps generated by DANet. Prior to adaptation, the results are very chaotic and it is hard to find correct meaningful correlations. However, after adaptation, the network has learned to capture semantic associations with clear similarity and long-range dependencies that can stretch across the 360 ? . For example, for the point on the car in the third row, the attention map allows to focus and highlights other pixels of car. In the fourth row, the position attention successfully distinguishes road and sidewalk, which are crucial for self-driving applications. In the last row, for the point marked on terrain, the corresponding attention map spotlights the other area of terrain lying at a long distance. In a nutshell, these results further demonstrate that the attention-augmented P2PDA enables to seamlessly adapt to and effectively exploit semantic correlations in the panoramic imagery.</p><p>We further demonstrate multiple predictions of our adapted DANet in <ref type="figure">Fig. 11</ref>, showing a clear performance decline of the source-only DANet when applied on panoramic images. In some of the top examples, the baseline often confuses the segmentation of some foreground categories, such as cars. Even in the last three lines, it cannot distinguish other categories from the building category in complex scenarios which is clearly better with the adapted DANet version. Despite lacking sharp boundaries, the adapted model is superior at distinguishing the categories, which is particularly important for autonomous vehicles. Strategies to augment the details include leveraging disentangled attention to handle detailed dependencies <ref type="bibr" target="#b32">[33]</ref> or directly using detail-sensitive networks <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b43">[44]</ref> for adaptation. We want to mention, that while the uncertainty maps are mainly used to select high-confident pseudo-labels for the selfsupervised learning, they could also be utilized as an attention cue for the assistance system during driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Semantic scene understanding is vital for self-driving cars but requires models which can deal with changes in data distribution. Panoramic image segmentation is able to provide an entire 360 ? surrounding perception. While appealing, it is challenging due to the discrepancy to common pinhole image segmentation in terms of the field of view and semantic distribution. In this work, we introduced the new task of cross-domain semantic segmentation for panoramic driving scenes, which extends the standard panoramic segmentation with the premise of the training data originating from a different domain (e.g. pinhole camera images).</p><p>First, we formulate the problem of unsupervised domain adaptation for panoramic segmentation and introduce the novel DensePASS dataset which we use to study the PINHOLE?PANORAMIC transfer. To meet the challenge of domain divergence, we developed a generic P2PDA framework enhancing conventional segmentation algorithms with different domain adaptation modules. While our experiments demonstrate that cross-domain panoramic segmentation task is difficult for modern algorithms, our proposed domain-agnostic framework with attention-based and uncertainty-aware adaptation modules consistently improves the results. Our dataset will be publicly released upon publication and we believe that DensePASS has strong potential to motivate the needed development of generalizable semantic segmentation models.</p><p>In the future, we aim to explore efficient transformer architectures and investigate their adaptation and distillation for semantic segmentation. We intend to incorporate dense 360 ? top-view LiDAR data panoramic segmentation to establish a more complete surrounding perception system and perform cross-view multi-dimension semantic mapping. Further, we seek to expand the applicability of omnidirectional sensing and are particularly interested in aerial image segmentation by lifting panoramic scene parsing in drone videos.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Diagram of the P2PDA framework. The shared backbone is an encoder-decoder segmentation network (i.e. DANet).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>An overview of the SDAM module. An overview of the ADAM module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>where ? is the Softmax function. Similarly, the channel-wise attended feature is denoted as R = ?(F , F T ?F ), R ? R (h?w)?c . Then, the final An overview of the RCDAM module. Diagram of Region Construction and Interaction Blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>r o a d s i d e w a l k b u i l d i n g w a l l f e n c e p o l e t r a f f i c l i g h t t r a f f i c s i g n v e g e t a t i o n t e r r a i n s k y p e r s o n r i d e r c a r t r u c k b u s t r a i n m o t o r c y c l e b i c y c l e 10</head><label>10</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Class-wise Pixel Accuracy (Acc) and IoU comparison in different directions of the panoramic image, before and after adaptation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :</head><label>9</label><figDesc>Qualitative examples of semantic segmentation on panoramic images. The columns from left to right are the original input panoramic image, ERFNet predictions before adaptation, our predictions, and the ground truth. Zoom in for a better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Per-class results on DensePASS. We use FANet</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>62.98 10.64 72.41 7.80 20.74 11.77 6.85 3.75 68.11 21.56 87.00 23.73 5.33 49.61 10.65 0.54 16.76 24.15 6.62 FANet S S 32.17 62.16 16.85 78.78 13.67 24.07 19.72 11.42 9.68 71.42 18.22 85.72 32.66 11.75 54.34 17.61 0.00 41.52 29.30 12.30 FANet A S 32.67 62.28 16.86 79.99 17.64 23.96 19.78 12.33 9.58 72.01 19.29 85.91 32.85 11.03 55.75 15.38 0.38 43.53 29.19 12.95 FANet S+A S 33.05 61.74 17.70 80.07 16.38 24.64 19.61 12.04 9.79 72.27 17.94 86.31 33.17 11.47 55.18 15.61 0.04 52.55 28.68 12.82 FANet S+A R 33.02 62.58 19.25 80.07 15.68 24.87 19.27 11.54 9.01 71.95 19.65 86.89 32.18 12.03 55.12 17.37 0.21 44.98 29.93 14.87 FANet S+A+F R 33.52 57.16 25.66 78.43 16.02 26.88 12.76 2.30 7.34 68.73 26.92 87.45 36.51 1.20 62.83 20.16 0.00 68.46 17.86 20.19 FANet-SSL S+A R 34.26 57.92 24.22 78.84 14.94 25.42 13.39 4.82 7.14 69.47 25.77 87.92 36.12 4.27 62.83 22.90 0.00 78.73 16.15 20.02 FANet-SSL S+A+F R 35.67 58.08 28.75 78.19 16.47 26.86 13.78 4.76 7.62 69.01 34.58 87.51 36.12 0.90 64.06 27.50 0.00 84.99 18.13 20.35</figDesc><table><row><cell>Methods</cell><cell>FS</cell><cell>OS</cell><cell>Mean IoU</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic light</cell><cell>traffic sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorcycle</cell><cell>bicycle</cell></row><row><cell>FANet</cell><cell>-</cell><cell>-</cell><cell>26.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Per-class results on DensePASS. We use DANet<ref type="bibr" target="#b12">[13]</ref> as the segmentation network and set different domain adaptation modules on our P2PDA framework to test on DensePASS with the size of input 2048?400. S, A, R, F represent the SDAM, ADAM, RCDAM and FCDAM respectively. Feature-and output-space are named as FS and OS for short. SSL represents the self-supervised learning with pseudo-labels. The first line is the Cityscapes source-only result without adaptation. * denotes further adding source images from WildDash to complement Cityscapes.<ref type="bibr" target="#b69">70</ref>.68 8.30 75.80 9.49 21.64 15.91 5.85 9.26 71.08 31.50 85.13 6.55 1.68 55.48 24.91 30.22 0.52 0.53 17.00 DANet S S 38.51 61.78 21.11 74.59 22.59 29.93 14.79 15.00 10.17 66.94 19.03 82.57 31.03 21.24 53.26 54.67 37.77 39.40 43.84 31.95 DANet A S 39.16 61.34 20.71 76.52 20.53 30.03 14.19 15.69 10.09 68.60 18.84 82.08 33.16 21.75 57.68 53.88 40.33 41.47 46.11 31.00 DANet S+A S 39.28 62.43 21.89 76.22 21.42 30.54 14.85 14.10 9.76 69.07 19.94 82.84 34.56 19.30 56.51 53.04 42.51 39.47 45.71 32.09 DANet S R 39.46 62.75 23.17 76.65 23.90 30.82 14.84 18.44 10.09 69.10 17.60 82.78 33.51 21.53 55.97 51.78 41.77 36.90 46.11 32.12 DANet S+A R 39.76 63.11 24.63 76.17 25.03 30.56 13.68 15.68 10.53 67.31 22.41 80.15 32.95 21.11 54.39 53.51 43.64 42.20 46.71 31.66 DANet S+A+F R 40.52 62.90 25.58 76.62 24.45 30.37 14.45 16.75 9.96 67.87 19.70 82.04 34.18 22.95 56.99 54.27 44.15 47.75 46.98 31.86 DANet-SSL S+A R 41.39 67.24 27.98 77.18 25.11 25.80 15.33 10.59 6.58 69.24 33.89 80.96 32.18 5.29 69.86 59.70 36.20 65.99 47.47 29.87 DANet-SSL S+A+F R 41.99 70.21 30.24 78.44 26.72 28.44 14.02 11.67 5.79 68.54 38.20 85.97 28.14 0.00 70.36 60.49 38.90 77.80 39.85 24.02 DANet* S R 41.35 68.38 37.26 75.51 26.28 31.81 15.62 8.99 10.33 66.22 31.74 80.68 33.69 16.81 64.81 47.67 28.05 61.81 44.92 34.98 DANet* S+A R 42.47 67.47 30.16 75.27 30.26 37.50 16.19 9.35 9.78 63.14 30.44 77.07 34.82 15.24 64.33 53.70 43.33 71.57 46.80 30.47 DANet* S+A+F R 42.87 66.92 29.97 77.34 30.87 37.85 15.04 11.12 9.60 62.80 31.03 78.08 36.27 18.01 63.66 54.83 42.86 74.22 45.96 28.13 DANet-SSL* S+A R 44.27 70.63 35.30 78.52 25.27 33.51 14.43 13.80 7.31 63.52 34.94 84.31 34.54 19.08 70.05 49.14 48.80 75.11 47.53 35.36 DANet-SSL* S+A+F R 44.66 75.85 34.21 82.58 28.75 35.58 18.51 12.65 12.49 71.33 37.51 89.80 38.68 15.99 76.59 62.81 12.25 61.56 48.18 33.26</figDesc><table><row><cell>Methods</cell><cell>FS</cell><cell>OS</cell><cell>Mean IoU</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic light</cell><cell>traffic sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorcycle</cell><cell>bicycle</cell></row><row><cell>DANet</cell><cell>-</cell><cell>-</cell><cell>28.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Performance of CNN-and transformer-based semantic segmentation models on Cityscapes and DensePASS.</figDesc><table><row><cell>Network</cell><cell>Backbone</cell><cell>Cityscapes</cell><cell cols="2">DensePASS mIoU Gap</cell></row><row><cell>SwiftNet [22]</cell><cell>ResNet-18</cell><cell>75.4</cell><cell>25.7</cell><cell>-49.7</cell></row><row><cell>DeepLabV3+ [20]</cell><cell>ResNet-18</cell><cell>76.8</cell><cell>25.6</cell><cell>-51.2</cell></row><row><cell>OCRNet [32]</cell><cell>HRNetV2p-W18s</cell><cell>77.1</cell><cell>25.9</cell><cell>-51.2</cell></row><row><cell>Fast-SCNN [23]</cell><cell>Fast-SCNN</cell><cell>69.1</cell><cell>24.6</cell><cell>-44.5</cell></row><row><cell>DeepLabV3+ [20]</cell><cell>ResNet-50</cell><cell>80.1</cell><cell>29.0</cell><cell>-51.1</cell></row><row><cell>PSPNet [19]</cell><cell>ResNet-50</cell><cell>78.6</cell><cell>29.5</cell><cell>-49.1</cell></row><row><cell>DNL [36]</cell><cell>ResNet-50</cell><cell>79.3</cell><cell>28.7</cell><cell>-50.6</cell></row><row><cell>Semantic-FPN [21]</cell><cell>ResNet-50</cell><cell>74.5</cell><cell>29.9</cell><cell>-44.6</cell></row><row><cell>OCRNet [32]</cell><cell>HRNetV2p-W18</cell><cell>78.6</cell><cell>30.8</cell><cell>-47.8</cell></row><row><cell>DeepLabV3+ [20]</cell><cell>ResNet-101</cell><cell>80.9</cell><cell>32.5</cell><cell>-48.4</cell></row><row><cell>PSPNet [19]</cell><cell>ResNet-101</cell><cell>79.8</cell><cell>30.4</cell><cell>-49.4</cell></row><row><cell>DANet [13]</cell><cell>ResNet-101</cell><cell>80.4</cell><cell>28.5</cell><cell>-51.9</cell></row><row><cell>DNL [36]</cell><cell>ResNet-101</cell><cell>80.4</cell><cell>32.1</cell><cell>-48.3</cell></row><row><cell>Semantic-FPN [21]</cell><cell>ResNet-101</cell><cell>75.8</cell><cell>28.8</cell><cell>-47.0</cell></row><row><cell>ResNeSt [75]</cell><cell>ResNeSt-101</cell><cell>79.6</cell><cell>28.8</cell><cell>-50.8</cell></row><row><cell>OCRNet [32]</cell><cell>HRNetV2p-W48</cell><cell>80.7</cell><cell>32.8</cell><cell>-47.9</cell></row><row><cell>SETR-MLA [76]</cell><cell>Transformer-Large</cell><cell>77.2</cell><cell>35.6</cell><cell>-41.6</cell></row><row><cell>SETR-PUP [76]</cell><cell>Transformer-Large</cell><cell>79.3</cell><cell>35.7</cell><cell>-43.6</cell></row><row><cell>ERFNet [15]</cell><cell>ERFNet</cell><cell>72.1</cell><cell>16.7</cell><cell>-55.4</cell></row><row><cell cols="2">ERFNet [15] (Ours) ERFNet</cell><cell>72.1</cell><cell>34.1</cell><cell>-38.0</cell></row><row><cell>FANet [14]</cell><cell>ResNet-34</cell><cell>71.3</cell><cell>26.9</cell><cell>-44.4</cell></row><row><cell>FANet [14] (Ours)</cell><cell>ResNet-34</cell><cell>71.3</cell><cell>35.7</cell><cell>-35.6</cell></row><row><cell>DANet [13]</cell><cell>ResNet-50</cell><cell>79.3</cell><cell>28.5</cell><cell>-50.8</cell></row><row><cell>DANet [13] (Ours)</cell><cell>ResNet-50</cell><cell>79.3</cell><cell>42.0</cell><cell>-37.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Per-class results on DensePASS. Comparison with state-of-the-art panoramic semantic segmentation (PASS</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>63.59 18.22 47.01 9.45 12.79 17.00 8.12 6.41 34.24 10.15 18.43 4.96 2.31 46.03 3.19 0.59 0.00 8.30 5.55 PASS [7] (ERFNet) 23.66 67.84 28.75 59.69 19.96 29.41 8.26 4.54 8.07 64.96 13.75 33.50 12.87 3.17 48.26 2.17 0.82 0.29 23.76 19.46 ECANet (Omni-supervised) [33] 43.02 81.60 19.46 81.00 32.02 39.47 25.54 3.85 17.38 79.01 39.75 94.60 46.39 12.98 81.96 49.25 28.29 0.00 55.36 29.47 CLAN (Adversarial training) [63] 31.46 65.39 21.14 69.10 17.29 25.49 11.17 3.14 7.61 71.03 28.19 55.55 18.86 2.76 71.60 26.42 17.99 59.53 9.44 15.91 CRST-LRENT (Self-training) [58] 31.67 68.18 15.72 76.78 14.06 26.11 9.90 0.82 2.66 69.36 21.95 80.06 9.71 1.25 65.12 38.76 27.22 48.85 7.10 18.08 Seamless (Mapillary) [77] 34.14 59.26 24.48 77.35 12.82 30.91 12.63 15.89 17.73 75.61 33.30 87.30 19.69 4.59 63.94 25.81 57.16 0.00 11.59 19.04 USSS (IDD) [78] 26.98 68.85 5.41 67.39 15.10 21.79 13.18 0.12 7.73 70.27 8.84 85.53 22.05 1.71 58.69 16.41 12.01 0.00 23.58 13.90 68.31 38.59 81.48 15.65 23.91 20.74 5.95 0.00 70.64 25.09 90.93 32.66 0.00 66.91 42.30 5.97 0.07 6.85 12.66 Ours (Cityscapes) 41.99 70.21 30.24 78.44 26.72 28.44 14.02 11.67 5.79 68.54 38.20 85.97 28.14 0.00 70.36 60.49 38.90 77.80 39.85 24.02 Ours (Cityscapes+WildDash) 44.66 75.85 34.21 82.58 28.75 35.58 18.51 12.65 12.49 71.33 37.51 89.80 38.68 15.99 76.59 62.81 12.25 61.56 48.18 33.26 Ours* (Cityscapes+WildDash) 48.52 76.87 35.70 85.16 33.93 38.86 18.18 10.52 13.71 73.98 41.89 92.08 42.38 8.26 78.62 60.12 42.17 81.21 53.82 34.49</figDesc><table><row><cell>Methods</cell><cell>Mean IoU</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic light</cell><cell>traffic sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorcycle</cell><cell>bicycle</cell></row><row><cell cols="9">ERFNet [15] 16.65 SwiftNet (ApolloScape) 14.08 61.21 34.93 57.92 7.85 23.37 13.33 9.04</cell><cell cols="3">6.44 50.39 0.00</cell><cell cols="2">0.00 0.44</cell><cell>0.00</cell><cell>0.09</cell><cell>0.36</cell><cell cols="2">1.83 0.00</cell><cell cols="2">0.04 0.24</cell></row><row><cell>SwiftNet (Cityscapes) [22]</cell><cell cols="8">25.67 50.73 32.76 70.24 12.63 24.02 18.79 7.18</cell><cell cols="8">4.01 64.93 23.70 84.29 14.91 0.97 43.46 8.92</cell><cell cols="4">0.04 4.45 12.77 8.77</cell></row><row><cell>SwiftNet (KITTI-360)</cell><cell cols="8">25.00 69.03 27.71 68.07 15.70 16.26 15.29 0.00</cell><cell cols="8">4.43 64.71 31.01 84.86 23.02 0.00 45.08 9.72</cell><cell cols="2">0.00 0.00</cell><cell cols="2">0.00 0.00</cell></row><row><cell>SwiftNet (BDD)</cell><cell cols="17">24.69 4.26 25.11 74.16 15.53 22.74 11.70 0.00 10.58 70.86 26.55 92.26 25.12 0.00 58.78 31.35 0.00</cell><cell cols="3">0.00 0.00 0.00</cell></row><row><cell cols="2">SwiftNet (Merge3) [9] 32.04 Input</cell><cell></cell><cell></cell><cell cols="3">Before adaptation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ground truth</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>displays the segmentation results of ERFNet. The segmentation boundaries of regions such as sky, building, and vegetation are clearly improved through the P2PDA strategy in every case, while sidewalk segmentation clearly benefits from domain adaptation in the second row example. At the same time, some misclassified categories are corrected after</figDesc><table><row><cell>#1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>#2</cell><cell></cell><cell></cell></row><row><cell>#1</cell><cell>#2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>#2</cell><cell>#1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>#2</cell><cell>#1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>#2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>#1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Input</cell><cell>Before adaptation #1</cell><cell>Ours #1</cell><cell>Before adaptation #2</cell><cell>Ours #2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For a fair comparison, model weights are provided by the same framework MMSegmentation: https://github.com/open-mmlab/mmsegmentation</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">MASS: Multi-attentional semantic segmentation of LiDAR data for dense top-view understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00346</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Restricted deformable convolution-based road scene semantic segmentation using surround view cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4350" to="4362" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Camera-LIDAR integration: Probabilistic sensor fusion for semantic mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Berrio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nebot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Can we PASS beyond the field of view? Panoramic annular semantic segmentation for real-world surrounding perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="446" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PASS: Panoramic annular semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4171" to="4185" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bridging the day and night domain gap for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1312" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ISSAFE: Improving semantic segmentation in accidents by fusing event-based data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">All about structure: Adapting structural information across domains for boosting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1900" to="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3141" to="3149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time semantic segmentation with fast attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="270" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient residual factorized ConvNet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DenseP-ASS: Dense panoramic semantic segmentation via unsupervised domain adaptation with attention-augmented context exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roitberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 24th International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">WildDash -Creating hazard-aware benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murschitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steininger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Dom?nguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="407" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1341" to="1360" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="833" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6392" to="6401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">In defense of pre-trained imagenet architectures for real-time semantic segmentation of roaddriving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast-SCNN: Fast semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P K</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">289</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5000" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The ApolloScape open dataset for autonomous driving and its application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2702" to="2719" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BDD100K: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2633" to="2642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">IDD: A dataset for exploring problems of autonomous navigation in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1743" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic instance annotation of street scenes by 3D to 2D label transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3688" to="3697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Trans4Trans: Efficient transformer for transparent object segmentation to help visually impaired people navigate in the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Constantinescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1760" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">RANet: Region attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="173" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Capturing omni-range context for omnidirectional segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rei?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1376" to="1386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">AttaNet: Attention-augmented network for fast and accurate scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2567" to="2575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">CCNet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Disentangled non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="191" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Asymmetric nonlocal neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">CNN based semantic segmentation for urban traffic scenes using fisheye camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="231" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Universal semantic segmentation for fisheye urban driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiang</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="648" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Adaptable deformable convolutions for semantic segmentation of fisheye images in autonomous driving systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Playout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lecue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cheriet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10191</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cross-view semantic segmentation for sensing surroundings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4867" to="4873" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">OmniDet: Surround view cameras based multitask visual perception network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ravikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2830" to="2837" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">FIERY: Future instance prediction in bird&apos;s-eye view from surround monocular cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">DS-PASS: Detail-sensitive panoramic annular semantic segmentation through SwaftNet for surrounding sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Omnisupervised omnidirectional semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Is context-aware CNN ready for the surroundings? Panoramic semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1866" to="1881" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Panoramic panoptic segmentation: Towards complete surrounding understanding via unsupervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Intelligent Vehicles Symposium (IV), 2021</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Orientation-aware semantic segmentation on icosahedron spheres</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3532" to="3540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic segmentation of panoramic images using a synthetic dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Machine Learning in Defense Applications</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The OmniScape dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Sekkat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dupuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vasseur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1603" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Semantics-aware multi-modal domain translation: From LiDAR point clouds to panoramic color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cortinhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kurnaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aksoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13974</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Semantic segmentation of outdoor panoramic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bastanlar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal, Image and Video Processing</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">See clearer at night: Towards robust nighttime semantic segmentation through day-night image conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Machine Learning in Defense Applications</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Nighttime road scene parsing by unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Rainy night scene understanding with near scene semantic adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Di</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1594" to="1602" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2039" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Constructing self-motivated pyramid curriculums for cross-domain semantic segmentation: A nonadversarial approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6757" to="6766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5981" to="5990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="1106" to="1120" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">FCNs in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2502" to="2511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Content-consistent matching for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="440" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Contextual-relation consistent domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="705" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">ADVENT: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2512" to="2521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Affinity space adaptation for semantic segmentation across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2549" to="2561" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Transferable attention for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5345" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">SS-SFDA : Selfsupervised source-free domain adaptation for road segmentation in hazardous environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kothandaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08939</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Source-free domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1215" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Context-aware domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="514" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">mDALU: Multisource domain adaptation and label unification with partial datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8876" to="8885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">ResNeSt: Split-attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-tosequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Seamless scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Colovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8269" to="8278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Universal semisupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5258" to="5269" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
