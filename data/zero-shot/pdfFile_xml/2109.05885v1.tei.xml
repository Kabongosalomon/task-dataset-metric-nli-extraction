<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph-Based 3D Multi-Person Pose Estimation Using Multi-View Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Size</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">SenseTime Research and Tetras.AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">SenseTime Research and Tetras.AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">SenseTime Research and Tetras.AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Bai</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
							<email>qianchen@sensetime.com</email>
							<affiliation key="aff2">
								<orgName type="institution">SenseTime Research and Tetras.AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
							<email>dongeliu@ustc.edu.cnwanli.ouyang@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph-Based 3D Multi-Person Pose Estimation Using Multi-View Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies the task of estimating the 3D human poses of multiple persons from multiple calibrated camera views. Following the top-down paradigm, we decompose the task into two stages, i.e. person localization and pose estimation. Both stages are processed in coarse-to-fine manners. And we propose three task-specific graph neural networks for effective message passing. For 3D person localization, we first use Multi-view Matching Graph Module (MMG) to learn the cross-view association and recover coarse human proposals. The Center Refinement Graph Module (CRG) further refines the results via flexible pointbased prediction. For 3D pose estimation, the Pose Regression Graph Module (PRG) learns both the multi-view geometry and structural relations between human joints. Our approach achieves state-of-the-art performance on CMU Panoptic and Shelf datasets with significantly lower computation complexity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of estimating 3D human poses of multiple persons from multiple views is a long-standing problem. It has attracted increasing attention for its wide range of applications, e.g. sports broadcasting <ref type="bibr" target="#b5">[6]</ref> and retail analysis <ref type="bibr" target="#b34">[35]</ref>.</p><p>Recent research on 3D multi-person pose estimation using multi-view images generally follows two streams: 2Dto-3D lifting-based approaches and direct 3D estimation approaches. As shown in Figrue 1(a), 2D-to-3D lifting approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> first estimate 2D joints in each view through monocular pose estimator, then associate 2D poses across views, and finally lift the matched 2D single-view poses to 3D via triangulation <ref type="bibr" target="#b1">[2]</ref> or Pictorial Structure Models (PSM) <ref type="bibr" target="#b10">[11]</ref>. Such approaches are generally efficient and are the de-facto standard when seeking real-time performance <ref type="bibr" target="#b30">[31]</ref>. However, the 3D reconstruction accuracy is * Corresponding author. <ref type="figure">Figure 1</ref>. Overview of mainstream multi-view 3D pose estimation frameworks. (a) 2D-to-3D lifting-based approaches (b) Direct 3D pose estimation approaches. (c) Our approach applies graph-based matching algorithm to detect human centers, and applies a graphbased pose refinement model to effectively utilize both geometric cues and human structural prior to achieve better performance. limited by the 2D pose estimation, which is not robust to occlusion. As shown in <ref type="figure">Figure 1</ref>(b), direct 3D approaches <ref type="bibr" target="#b34">[35]</ref> construct the discretized 3D volumetric representations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> by gathering multi-view features and directly operate in the 3D space. Such approaches avoid making incorrect decisions in 2D camera views. However, their computation cost increases cubically with the size of the space. They also suffer the quantization errors caused by space discretization <ref type="bibr" target="#b34">[35]</ref>.</p><p>As shown in <ref type="figure">Figure 1</ref>(c), we combine the virtues of both approaches by adopting 2D-to-3D lifting for efficient 3D human center detection in the first stage, and direct 3D estimation approach for accurate single-person 3D pose estimation in the second stage. To strike a balance between accuracy and efficiency, both stages are processed in coarseto-fine manners with task-specific graph neural networks.</p><p>For coarse-level 3D human center detection in the first stage, we generate coarse human center predictions via multi-view matching. Previous methods perform associa-tion across views by multi-view geometric constraints <ref type="bibr" target="#b17">[18]</ref> and appearance similarity <ref type="bibr" target="#b10">[11]</ref>. However, their matching criteria are hand-crafted and not learnable, which may suffer from tedious hyper-parameter tuning and inaccurate matching results. To solve this problem, we propose the Multi-view Matching Graph Module (MMG) to learn from data to match people across views by considering both the visual and geometric cues. It also captures the relationship among multiple views to make more reliable predictions.</p><p>For fine-level 3D human center detection in the first stage, we propose a graph-based point predictor, i.e. Center Refinement Graph Module (CRG), to refine the coarse human center locations. Previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref> mostly discretize the space into voxels and operate on a regular grid. CRG instead adopts implicit field representations <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> and directly operates on the continuous 3D space to predict whether a point is a human center or not. It gives us the flexibility to balance between accuracy and speed, by sampling with arbitrary step sizes. Additionally, we propose to use graph models to learn to fuse multi-view features, which are not well-exploited in literature.</p><p>For coarse-level single-person pose estimation, we simply use an off-the-shelf pose estimator to generate initial 3D poses based on the detected human proposals. For fine-level single-person pose estimation, we propose the Pose Regression Graph Module (PRG) to refine the initial 3D poses, by exploiting both the spatial relations between body joints and the geometric relations across multiple views.</p><p>The three graph modules can alleviate the aforementioned weakness caused by inaccurate 2D detection or space discretization and improve the pose estimation accuracy.</p><p>Our main contributions can be summarized as follows:</p><p>? To the best of our knowledge, this is the first attempt of using task-specific graph neural networks for multiview 3D pose estimation. We propose a novel coarseto-fine framework that significantly outperforms the previous approaches both in accuracy and efficiency.</p><p>? We propose Multi-view Matching Graph Module (MMG) to significantly improve the performance of multi-view human association via learnable matching.</p><p>? We propose Center Refinement Graph Module (CRG) for point-based human center refinement, which effectively aggregates multi-view features via graph neural networks, and adaptively samples points to achieve more efficient and accurate localization.</p><p>? We propose a powerful graph-based model, termed Pose Regression Graph (PRG) for 3D human pose refinement. It accounts for both the human body structural information and the multi-view geometry to generate more accurate 3D human poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Single-view 3D pose estimation</head><p>For single-person 3D pose estimation from a monocular camera, we briefly classify the existing works into three categories: (1) from 2D poses to 3D poses <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b40">41]</ref> (2) jointly learning 2D and 3D poses <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, and (3) directly regressing 3D poses <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b42">43]</ref> from images. They have shown remarkable results in reconstructing 3D poses, which motivates more research efforts on the more challenging multi-person tasks. Multi-person 3D pose estimation from a single RGB image generally follows two streams: top-down and bottom-up. Top-down approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39]</ref> first use a human detector to produce human locations and then apply single-person pose estimation for each detected person. Bottom-up approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40]</ref> directly localize keypoints of all people and perform keypoint-to-person association.</p><p>Single-view 3D pose estimation has achieved significant progress in recent years. However, inferring 3D poses from a single view is an ill-posed problem. And its reconstruction accuracy is not comparable with that of the multi-view approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-view 3D pose estimation</head><p>We mainly focus on the multi-person 3D pose estimation from multiple views. Existing approaches can be mainly categorized into 2D-to-3D pose lifting approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44]</ref> and direct 3D pose estimation approaches <ref type="bibr" target="#b34">[35]</ref>.</p><p>2D-to-3D lifting approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref> first estimate 2D joints of the same person in each view through monocular pose estimator, then lift the matched 2D singleview poses to 3D locations. Belagiannis et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> first extends 2D PSM to 3D Pictorial Structure Model (3DPS) to encode body joint locations and pairwise relations in between. Other works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref> first solve multi-person 2d pose detection and associate poses in multiple camera views. The 3D poses are recovered using triangulation <ref type="bibr" target="#b5">[6]</ref> or singleperson 3D PSM <ref type="bibr" target="#b10">[11]</ref>. Concurrently Lin et al. <ref type="bibr" target="#b21">[22]</ref> propose to use 1D convolution to jointly address the cross-view fusion and 3D pose reconstruction based on plane sweep stereo. However, such approaches heavily rely on 2D detection results, and the gross errors in 2D may largely degrade 3D reconstruction. In comparison, our approach makes predictions in a coarse-to-fine manner. It models the interaction between multiple camera views using graph neural networks, which are much more efficient and accurate.</p><p>Direct 3D pose estimation approaches <ref type="bibr" target="#b34">[35]</ref> discretize the 3D space with volumetric representation and gather features from all camera views via multi-view geometry. Tu et al. proposes to solve multi-person multi-view 3D pose estimation following the top-down paradigm. Specifically, it first discretizes 3D space with voxels and intensively oper-ates on 3D space via 3DCNN to give human proposals. For each human proposal, another 3DCNN is applied to recover 3D human poses. Such approaches reliably recover 3D poses but are computationally demanding. In comparison, our approach introduces MMG to significantly reduce the searching space using the multi-view geometric cues. Combined with point-based predictor CRG, we achieve higher accuracy with less computation complexity.</p><p>Aggregating features from arbitrary views is important but not well-exploited in literature. Traditional methods aggregate multi-view features by concatenation or averagepooling <ref type="bibr" target="#b34">[35]</ref>. Feature concatenation can hardly generalize to different camera settings by design. Average-pooling is permutation invariant but ignores the relations between views. In this paper, we propose a novel graph neural network model to learn to combine geometric knowledge with the corresponding 2D visual features from different views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Graph Neural Networks</head><p>Graph Convolutional Networks (GCN) generalizes convolutional neural networks to handle graphic data. GCNs have shown effectiveness in message passing, and global relations modeling in various tasks, e.g. action recognition <ref type="bibr" target="#b37">[38]</ref> and tracking <ref type="bibr" target="#b13">[14]</ref>. Recent GCNs can be categorized into spectral approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20]</ref> and spatial approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b35">36]</ref>. In this paper, we use spatial approaches for better efficiency and generalizability.</p><p>Recently, GCN have shown effectiveness in modeling human body structure for single-view 2D human pose estimation. Zhang et al. <ref type="bibr" target="#b41">[42]</ref> proposes to use PGNN to learn the structured representation of keypoints for 2D singleperson pose estimation. Qiu et al. <ref type="bibr" target="#b29">[30]</ref> proposes OPEC-Net to handle occlusions for 2D top-down pose estimation. Jin et al. <ref type="bibr" target="#b15">[16]</ref> proposes the hierarchical graph grouping module to learn to associate joints for 2D bottom-up pose estimation. There are also works for single-view single-person 3D pose estimation. Zhao et al. <ref type="bibr" target="#b44">[45]</ref> proposes SemGCN to capture both local and global semantic relationships between joints. Zou et al. <ref type="bibr" target="#b45">[46]</ref> proposes to capture the long-range dependencies via high-order graph convolution.</p><p>We propose to use graph-based models to learn to aggregate features from multiple camera views via multi-view geometry, which was not investigated in existing GCN works. In Pose Refinement Graph Module (PRG), both the body structure priori and the geometric correspondence of multiple views are encoded for more robust and accurate human pose estimation. Moreover, we propose EdgeConv-E, a variant of EdgeConv <ref type="bibr" target="#b35">[36]</ref>, to explicitly incorporate geometric correspondence as the edge attributes in GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Implicit Field Representations</head><p>Most 3D multi-view pose estimators <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref> use 3D volumetric representations, where 3D space is dis-cretized into regular grids. However, constructing a 3D volume suffers from the cubic scaling problem. This limits the resolution of the volumetric representations, leading to large quantization errors. Using finer grids can improve the performance, but it incurs prohibitive memory costs and computation complexity.</p><p>Recently, implicit neural representation or implicit field <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> have become popular. Such approaches learn 3D reconstruction in continuous function space. Kirillov et al. proposes PointRend <ref type="bibr" target="#b20">[21]</ref> to select a set of points at which to make predictions for instance segmentation. Inspired by PointRend <ref type="bibr" target="#b20">[21]</ref>, we propose Center Refinement Graph (CRG), a point-based predictor, to operate on continuous 3D space in a coarse-to-fine manner. We are able to achieve higher accuracy with significantly lower computation complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>We directly use the same pre-trained 2D bottom-up pose estimator from Tu et al. <ref type="bibr" target="#b34">[35]</ref> to localize 2D human centers in each camera view and to provide feature maps for our task-specific GCNs.</p><p>To predict the 3D human centers from 2D locations, we propose Multi-view Matching Graph Module (MMG) to match the centers from different camera views corresponding to the same person. Then we obtain coarse 3D human center locations from the matching results via simple triangulation <ref type="bibr" target="#b1">[2]</ref>. The coarse center candidates are further refined by the Center Refinement Graph Module (CRG).</p><p>After 3D human centers are predicted, we follow Tu et al. <ref type="bibr" target="#b34">[35]</ref> to generate 3D bounding boxes with the fixed orientation and size, and apply the 3D pose estimator <ref type="bibr" target="#b34">[35]</ref> to generate initial 3D poses. To improve the pose estimation accuracy, the predicted initial 3D poses are further refined by our proposed Pose Regression Graph Module (PRG).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-view Matching Graph Module (MMG)</head><p>Given the 2D human centers generated by the 2D pose estimator, the proposed Multi-view Matching Graph Module (MMG) aims to match them across different camera views, and lift the 2D human centers to coarse 3D human centers via triangulation <ref type="bibr" target="#b1">[2]</ref>. We construct a multi-view matching graph, where a vertex represents a human center candidate in a view and an edge represents the connectivity between a pair of human centers in two camera views. The edge connectivity is a binary value in {0, 1} representing whether the two corresponding vertices belong to the same person or not. Therefore, the problem of multi-view matching is formulated as the edge connectivity prediction problem. Our MMG applies a graph-based model to solve this problem. The graph model consists of two layers of EdgeConv-E (see Sec. 3.2.1) followed by two fully-connected layers. It takes both the vertex features and edge features as input, extracts representative features via message passing, and learns to predict the edge connectivity scores.</p><p>The vertex feature encodes the 2D visual cues which are obtained from the feature maps of the 2D backbone networks. Specifically, the vertex feature vector R 512 is extracted at each human center location. The edge feature encodes the pair-wise geometric correspondences of two 2D human centers from two distinct views via epipolar geometry <ref type="bibr" target="#b1">[2]</ref>. Specifically, we first compute the symmetric epipolar distance <ref type="bibr" target="#b1">[2]</ref> d between the two centers. Then the correspondence score s corr can be calculated by s corr = e ?m?d , where m is a constant and is empirically set to 10.0 in our implementation. In this way, We explicitly use the geometric correspondence score s corr as the edge feature in MMG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Incorporating edge attributes with EdgeConv-E</head><p>EdgeConv <ref type="bibr" target="#b35">[36]</ref> is a popular graph convolution prediction to capture local structure and learn the embeddings for the edges. Mathematically, EdgeConv can be represented as:</p><formula xml:id="formula_0">x v . = max v ?N (v) h ? (Concat(x v , x v ? x v )) ,<label>(1)</label></formula><p>where x v and x v represent the node features at v and v . 'Concat' denotes the feature concatenation operation. N (v) is the neighbor vertices of v. h ? is a neural network, i.e. a multi-layer perceptron (MLP).</p><p>In standard EdgeConv (Eq.1), the feature aggregation procedure only takes into account the node features x v and the relative relation of two neighboring nodes (x v ? x v ). It does not explicitly utilize edge attributes for message passing. Based on EdgeConv <ref type="bibr" target="#b35">[36]</ref>, we propose EdgeConv-E to explicitly incorporate edge attributes e (v,v ) into the aggregation procedure. The propagation rule of EdgeConv-E is illustrated in Eq.2. x</p><formula xml:id="formula_1">v . = max v ?N (v) h ? Concat(x v , x v ? x v , e (v,v )) .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Training</head><p>We first construct a multi-view graph, where the vertices are generated using the 2D human centers, and the edges connect each pair of 2D human centers in distinct camera views. The target edge connectivity is assigned "1"s for edges connecting the same persons, and "0"s otherwise.</p><p>To avoid overfitting, we augment by adding uniform noises ranging from 0 to 25 pixels to the ground-truth 2D human center coordinates. Binary cross-entropy loss between the predicted and the target edge connectivity is used for training. We adopt Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with a learning rate of 10 ?4 to train the model for 2 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Center Refinement Graph Module (CRG)</head><p>Center Refinement Graph Module (CRG) is built on top of MMG to refine the 3D human center detection results. CRG adaptively samples query points in the 3D search space, and predicts the possibility of the query point being a human center. It replaces the commonly used volumetric representations with the implicit field representations, which enables querying at any real-value point for more flexible search and accurate localization in the 3D space.</p><p>Search space. Instead of operating on the whole 3D space, we propose to restrict the search space based on the matching results from MMG. For each pair of matched 2D human centers, we recover a coarse 3D human center proposal via triangulation <ref type="bibr" target="#b1">[2]</ref>. We generate a 3D ball surrounding each 3D human center proposal within a radius of r 0 = 300mm. The search space (denoted as ? 0 ) is thus the union of these 3D balls.</p><p>Feature extraction. Each query 3D point is first projected to all 2D camera views to get its corresponding 2D locations. Then the point-wise feature representations of the corresponding 2D point locations are obtained from the 2D feature maps. Features for a real-value 2D location are obtained via bilinear interpolation, using the surrounding four nearest neighbors located on the regular grid.</p><p>We first introduce a baseline model, which concatenates the point-wise features from different views and processes with a learnable multi-layer perceptron (MLP). For each candidate point, the MLP outputs a confidence score of being a human center. We refer to this approach as MLP-Baseline. Although intuitive, we argue that this approach is limited for two reasons: (1) it assigns the same weights to all views, and cannot handle occlusion in some viewpoints.</p><p>(2) it cannot generalize to other camera settings (different number of cameras) by design.</p><p>To alleviate these limitations, we propose to use graph neural networks for efficient message passing across views. Our Center Refinement Graph Module (CRG) learns to fuse information from multiple views and verify the proposals from the previous stage. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, for each 3D query point, we construct a multi-view graph. The vertices represent the 2D projections in each camera view. The vertex features include (1) visual features R 512 extracted in the image plane (2) normalized 3D coordinates R 3 of the query point. (3) 2D center confidence score from the 2D backbone. The edges densely connect these 2D projections to each other, enabling cross-view feature aggregation.</p><p>Our CRG uses three layers of EdgeConv for cross-view feature message passing, followed by a max-pooling layer for feature fusion and one fully-connected (FC) layer to predict the center confidence score. We use the standard Edge-Conv instead of EdgeConv-E, because CRG does not have explicit edge features for aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Point Selection</head><p>Inference. Given a search region from MMG, we iteratively search for the human centers in a coarse-to-fine man-ner. CRG starts with the search space ? 0 described in Sec. 3.3. In the iteration t, it uniformly samples query points in the search space, with the step size ? t . The graph model processes the sampled queries and predicts their possibility of being a human center. The point with the highest confidence score is selected as the refined human center x t . We update the search space for the next iteration, ? t+1 , as the 3D ball subspace surrounding the human center x t with a radius of r t+1 = r t ? ?. We shrink the sampling step size by i.e. ? t+1 = ? ? ? t . The iteration continues until the step size reaches the desired precision ( ).</p><p>Complexity analysis. In Tu et al. <ref type="bibr" target="#b34">[35]</ref>, the search space of human center proposals, as well as the time complexity, is O(L?W ?H), where L, W , and H are the size of the 3D space. Applying our proposed MMG and CRG, the size of the search space is significantly reduced to O(N ), where N is the number of people. Here we omit the size of the search region of an instance, which is a constant. It is noticeable that the complexity is independent of the size of the space, making it applicable to large space applications, e.g. the football field. In the experiments, we set the initial step size ? 0 = 200mm, the shrinking factor ? = 0.6 and ? = 0.25, the desired precision = 50mm. On CMU Panoptic <ref type="bibr" target="#b16">[17]</ref> dataset, we record an average of 1,830 queries per frame compared with 128,000 queries taken by Tu et al. <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Training</head><p>The model learns to predict the confidence score for each query point. We develop an effective sampling strategy for selecting training samples to train CRG. Two types of samples are considered for training: positive samples that are located around the ground-truth human centers and negative ones that are far away from human locations. We take positive samples around ground-truth human centers following the Gaussian distributions with the standard deviation ? pos = 400mm. For negative ones, we take samples uniformly in the entire 3D space. Empirically, the ratio of the number of positive and negative samples is 4 : 1.</p><p>For a sample located at X, the target confidence score is calculated by</p><formula xml:id="formula_2">s * conf = max j=1:N exp ? X ? X * j 2 2 2? 2 ,<label>(3)</label></formula><p>where N is the number of human instances and X * j is the 3D coordinate of the center point of person j. And ? is the standard deviation of the Gaussian distribution, which is set as ? = 200mm. The training loss of CRG is the 2 loss between the predicted and the target confidence score. We adopt Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with a learning rate of 10 ?4 . It takes 4 epochs to reach the best performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pose Regression Graph Module (PRG)</head><p>Existing 3D pose regression models produce reliable results on joints that are visible in most views, but will generate inaccurate localization results for occluded joints. Human beings can easily recognize occluded poses, mainly because of their prior knowledge of bio-mechanical body structure constraints and multi-view geometry. The knowledge helps remove ambiguity in localization caused by selfocclusion or inter-occlusion. In light of this, we design the Pose Regression Graph Module (PRG) to learn to refine joint locations considering both the multi-view geometry and structural relations between human joints.</p><p>An overview of the 3D pose estimation stage is illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>. We applied PRG to each individual to further improve the accuracy. The PRG module takes an initial 3D pose as the input. In our implementation, we simply use the pose regressor of <ref type="bibr" target="#b34">[35]</ref> to generate the initial 3D pose. The initial 3D pose is projected to all camera views to obtain multiple 2D poses. We construct a multi-view pose graph based on the projected 2D poses in different camera views. The graph predicts the offsets of each keypoint in 3D space, which are added to the initial 3D pose for refinement.</p><p>For the multi-view pose graph, the vertices represent the 2D keypoints in a certain camera view. We concatenate the following features the initialize all the nodes in the graph: (1) visual features R 512 obtained from the feature maps of the 2D backbone networks at the projected 2D location. (2) one-hot representation of the joint type R K (3) normalized initial 3D coordinates R 3 .</p><p>The multi-view pose graph consists of two types of edges: (1) single-view edges that connect two keypoints of different types in the canonical skeleton structure in a certain camera view. (2) cross-view edges that connect two keypoints of the same type in different views. We use the one-hot feature vector R 2 to distinguish these two types of edges. The one-hot edge features are passed to the EdgeConv-E defined by Eq. 2.</p><p>Our graph model of PRG first uses two consecutive EdgeConv-E layers for message passing between neighbor-ing body joints and multiple camera views. Then a maxpooling layer is applied to aggregate the cross-view features and coarsen the graph. The max pooled features are updated by the following three EdgeConv-E layers via effective information flow between the body joints. Finally, the extracted features are passed to one MLP with two fullyconnected (FC) layers to regress a refinement vector for each joint.</p><p>Training. The target offset is the difference between the ground-truth 3D pose and the initial 3D pose. We use 1 regression loss between the predicted offset and the target offset to train PRG. Note that the loss gradients of PRG can be back-propagated to the 2D backbone network, which will further improve its feature representation ability. We train PRG using the Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with a learning rate of 5 ? 10 ?5 . We train it for 4 epochs to obtain the best model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>CMU Panoptic <ref type="bibr" target="#b16">[17]</ref>: The CMU Panoptic dataset is currently the largest real-world dataset for multi-person 3D pose estimation. It is captured in a studio laboratory, with multiple people doing social activities. In total, it contains 65 sequences (5.5 hours) and 1.5 million of 3D skeletons with 30+ HD camera views. We follow <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37]</ref> to split the dataset into training and testing subsets. However, we lack the '160906band3' training subset due to broken images. Mean Average Precision (mAP) and mean Average Recall (mAR) are popular metrics for comprehensive evaluation. We calculate mAP and mAR by taking the mean of AP and AR over all the Mean Per Joint Position Error (MPJPE) thresholds (from 25mm to 150mm with a step size of 25mm). We report mAP and mAR along with MPJPE for evaluating the performance of both 3D human center detection and 3D human pose estimation.</p><p>Shelf <ref type="bibr" target="#b2">[3]</ref>: The Shelf dataset consists of four people disassembling a shelf captured by five cameras. It is challenging due to the complex environment and heavy occlusion. We follow <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b34">35]</ref> to prepare the training and testing datasets. Following <ref type="bibr" target="#b34">[35]</ref>, we use the same 2D pose estimator trained on the COCO dataset. We follow <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref> to use the percentage of correctly estimated parts (PCP3D) to evaluate the estimated 3D poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons to the state-of-the-arts</head><p>In this section, we compare with the state-of-the-art approaches on CMU Panoptic <ref type="bibr" target="#b16">[17]</ref> and Shelf <ref type="bibr" target="#b2">[3]</ref> datasets.</p><p>On CMU Panoptic dataset, we follow <ref type="bibr" target="#b34">[35]</ref> to experiment with the five camera setups. To make fair comparisons, we use the same HD camera views (id: 3, 6, 12, 13, 23). As the AP 75 , AP 125 and mAR are not reported in the original paper of Tu et al. <ref type="bibr" target="#b34">[35]</ref>, we reproduce the re- <ref type="table">Table 1</ref>. Comparisons to the state-of-the-art approaches on CMU Panoptic dataset <ref type="bibr" target="#b16">[17]</ref>. The symbol ? means that the higher score the better, while ? means that the lower the better. '*' indicates the mean value of four APK metrics reported in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b21">22]</ref>. ' ?' indicates that better 2D pose estimator <ref type="bibr" target="#b33">[34]</ref>   <ref type="table">Table 2</ref>. In the experiments, we follow the evaluation protocol of Tu et al. <ref type="bibr" target="#b34">[35]</ref>. We show that our approach achieves the state-of-the-art performance. <ref type="table">Table 2</ref>. Quantitative comparisons to the state-of-the-art approaches on Shelf <ref type="bibr" target="#b2">[3]</ref> datasets. The metric is the percentage of correctly estimated parts (PCP3D). ' ?' means method with temporal information.</p><p>Shelf Actor1 Actor2 Actor3 Average Belagiannis et al. <ref type="bibr" target="#b2">[3]</ref> 66. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>In this section, we conduct ablative experiments to analyze each component in our proposed framework in detail.</p><p>Effect of MMG. In <ref type="table" target="#tab_2">Table 3</ref>, we evaluate the performance of Multi-view Matching Graph Module (MMG) on 3D human center detection and 3D human pose estimation. All results use the same 2D detections and 3D human centers are recovered using multi-view triangulation <ref type="bibr" target="#b1">[2]</ref>. Traditional methods perform association across views using epipolar constraints <ref type="bibr" target="#b17">[18]</ref>. However, they do not generate 1 https://github.com/microsoft/voxelpose-pytorch reliable matching results in occluded scenes. MMG learns from data to match people across views. We observe significant improvement in the matching performance (75.91 mAP vs 61.65 mAP). We also notice that replacing MMG with the ground-truth matching results does not notably improve the human center detection results (78.70 mAP vs 75.91 mAP). This implies that the human association results generated by MMG are already very accurate.</p><p>Effect of CRG. The Center Refinement Graph Module (CRG) aims at refining the coarse human center predictions. To show the effectiveness of the graph reasoning for human center prediction, we compare CRG with the MLP-Baseline introduced in Sec. 3.3 on CMU Panoptic dataset. For fair comparisons, we make both models share the same input features, and have roughly the same number of parameters. As shown in <ref type="table" target="#tab_2">Table 3</ref>, CRG outperforms the MLP-Baseline in terms of both human detection accuracy (82.10 mAP vs 81.38 mAP) and 3D human pose estimation accuracy (98.10 mAP vs 97.82 mAP). This indicates the importance of learning the multi-view relationship via graphbased message passing. Effect of PRG. To analyze the effect of the Pose Regression Graph (PRG), we conduct experiments on CMU Panoptic dataset with multiple initial 3D pose regressors of different accuracy. These models are obtained by varying the granularity of the voxels, i.e. 32 3 , 48 3 , and 64 3 . We report the accuracy of the poses before and after the PRG refinement in <ref type="table" target="#tab_3">Table 4</ref>. Our PRG is a general pose refiner, which can be applied to various pose estimators to consistently improve the 3D pose estimation accuracy. Note that the 3D pose estimator of (c), is from Tu et al. <ref type="bibr" target="#b34">[35]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Study</head><p>We qualitatively compare our results with those of Tu et al. <ref type="bibr" target="#b34">[35]</ref> in <ref type="figure" target="#fig_3">Figure 5</ref>. In this example, the body of the woman (blue) is only clearly captured by one camera (view #2), while it is either truncated or occluded in other views. Tu et al. <ref type="bibr" target="#b34">[35]</ref> simply averages features from all the views with the same weights. This will make the features unreliable, leading to false negatives (FN). In comparison, our approach learns the multi-view feature fusion via GCN. We obtain more comprehensive features which allows us to make more robust estimation. Our approach also gets fewer false positives (FP) and predicts human poses with higher precision. Please see the supplementary for more examples.  <ref type="table" target="#tab_4">Table 5</ref> reports the memory and runtime on the sequences with 5 camera views on CMU Panoptic dataset. The results are tested on a desktop with one Titan X GPU. Tu et al. <ref type="bibr" target="#b34">[35]</ref> proposes CPN to localize people, and PRN to regress 3D poses. Both of them use volumetric representations, which suffer from large amount of memory. In comparison, the memory cost of our proposed graph neural networks is negligible. Our presented modules are also very efficient. On average, our unoptimized implementation takes only 2.4ms for multi-view matching (MMG) and 5.6ms for finer multi-person human center prediction (CRG). Compared with the CPN in <ref type="bibr" target="#b34">[35]</ref>, CRG requires tens of fewer sampling queries (1.8K vs 128K) due to smaller searching space. And the time cost of PRG is 6.8ms for each person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Memory and Runtime Analysis</head><p>When using the PRN as the initial pose estimator, our method facilitates the use of fewer bins of the voxel representation. Comparing #1 and #4 in <ref type="table" target="#tab_5">Table 6</ref>, our method using 32 3 bins has about 1/4 computational cost and higher accuracy (1.84mm improvement) than Tu et al. <ref type="bibr" target="#b34">[35]</ref>. Reducing the bins leads to smaller error increase for ours (0.11mm comparing #2 and #4), but large error increase for Tu et al. <ref type="bibr" target="#b34">[35]</ref> (1.51mm comparing #1 and #3). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel framework for multiview multi-person 3D pose estimation. We elaborately design three task-specific graph neural network models to exploit multi-view features. We propose Multi-view Matching Graph Module (MMG) and Center Refinement Graph Module (CRG) to detect human centers by proposal-andrefinement, and Pose Regression Graph Module (PRG) to produce accurate pose estimation results. Comprehensive experiments demonstrate that the proposed approach significantly outperforms the previous approaches.</p><p>In this section, we evaluate the generalization to the different number of camera views. Specifically, we train our graph-based models on the five-camera setup (camera id: 3, 6, 12, 13, 23), and directly evaluate these models with different number of camera views, i.e. the five-camera setup (camera id: 3, 6, 12, 13, 23), four-camera setup (camera id: 6, 12, 13, 23) and the three-camera setup (camera id: 6, 12, 23).</p><p>Transferring the pre-trained models to a reduced number of camera views is challenging. First, reducing the number of cameras increases the ambiguity of occluded human poses. Second, the information in the fused features is less complete. Third, the feature distribution may vary in different camera setups. We find that Tu et al. <ref type="bibr" target="#b34">[35]</ref> does not produce reliable prediction results when transferring to a reduced number of camera views. For example, when the number of camera views (# Views) is reduced to 3, the mAP drops dramatically from 96.73 to 68.14, and the MPJPE increases from 17.56mm to 37.14mm. Retraining the models with the test-time camera setups will mitigate this problem (marked with ?). In comparison, our approach can better generalize to different camera setups without any fine-tuning. Although reducing the number of camera views will reduce the accuracy, we show that we still achieve reasonably good results, demonstrating that our proposed approach has strong generalization ability. For example, with only 3 camera views, we achieve 91.60mAP and 94.14mAR. We also show that our approach consistently outperforms the state-of-the-art approach <ref type="bibr" target="#b34">[35]</ref> on generalization to different camera setups. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architecture</head><p>In this section, we illustrate the detailed graph model architectures of MMG, CRG and PRG in <ref type="figure">Figure A1</ref>.</p><p>As shown in <ref type="figure">Figure A1</ref> (a), the graph model of MMG consists of two layers of EdgeConv-E followed by two fully-connected (FC) layers. The input visual features R 512 extracted in the image plane are first updated by the EdgeConv-E layers. Then the fully-connected layers, whose input is the concatenation of target vertex feature and relative source vertex feature, predict whether an edge is connecting 2D centers of the same person.</p><p>As shown in <ref type="figure">Figure A1 (b)</ref>, the input features come from three sources: (1) the visual features R 512 extracted in the image plane (2) the normalized 3D coordinates R 3 of the query point (3) 2D center confidence score from the 2D backbone R 1 . They are processed by fully-connected layers and then concatenated to produce a feature vector R 545 for each vertex. The features are then processed by three layers of EdgeConv for cross-view feature message passing. A max-pooling layer is used for feature fusion and fullyconnected layers to predict the center confidence score. To facilitate training, we adopt residual connections in between the EdgeConv layers.</p><p>As shown in <ref type="figure">Figure A1</ref> (c), the input features also come from three sources: (1) the visual features R 512 extracted in the image plane (2) the normalized 3D coordinates R 3 of each joint in the initial pose (3) one-hot feature of the joint type R 15 . They are processed by fully-connected layers and concatenated to produce a feature vector R 559 for each vertex. The features are then processed by two layers of EdgeConv-E for cross-view message passing. Then a maxpooling layer is applied to aggregate the cross-view features and coarsen the graph. The max pooled features are updated by the following three EdgeConv layers via effective information flow between the body joints. Similar to CRG, we add some residual connections to help model training. Finally, the extracted features are passed to two parallel MLPs (multi-layer perceptrons) to respectively regress a refinement vector and predict a confidence score for each joint. Both MLPs are composed of two fully-connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Comparisons</head><p>In this section, we present more qualitative comparisons with Tu et al. <ref type="bibr" target="#b34">[35]</ref> on the CMU Panoptic dataset (a, b, c) and the Shelf dataset (d).</p><p>As shown in <ref type="figure" target="#fig_0">Figure A2 (a)</ref>, the arm of the man (purple) is only visible in two camera views, and is occluded by other people or by himself in most views. This results in large 3D pose errors for Tu et al. <ref type="bibr" target="#b34">[35]</ref>. Our proposed PRG can fix such kinds of pose errors, by considering both the geometric constraints and the human body structural relations.</p><p>As shown in <ref type="figure" target="#fig_0">Figure A2</ref> (b), many joints of the man (blue) <ref type="figure">Figure A1</ref>. The model architectures of (a) MMG, (b) CRG and (c) PRG. 'Linear' denotes the fully-connected layer, and 'MaxPool' denotes the graph max-pooling layer. The input feature dimensions and the output feature dimensions are illustrated.</p><p>are self-occluded by his own body in many camera views. This makes the visual features unreliable, leading to false negatives (FN) for Tu et al. <ref type="bibr" target="#b34">[35]</ref>. In comparison, our proposed MMG and CRG learn to detect human centers in a coarse-to-fine manner via GCN. We are able to obtain more robust human detection results. As shown in <ref type="figure" target="#fig_0">Figure A2</ref> (c), accurately predicting the poses of the little child (green) is challenging, due to insufficient training data. This example indicates that our proposed approach has better generalization ability towards rare poses. As shown in <ref type="figure" target="#fig_0">Figure A2 (d)</ref>, there is a false positive pose in the red circle estimated by Tu et al. <ref type="bibr" target="#b34">[35]</ref>. In comparison, our approach achieves better performance and gets fewer false positives. Our proposed CRG together with PRG can suppress these false positives, by considering the multi-view features as a whole via GCN. <ref type="figure" target="#fig_0">Figure A2</ref>. Qualitative analysis on CMU Panoptic dataset (a, b, c) and Shelf dataset (d). Estimated 3D poses and their 2D projections of ours, and Tu et al. <ref type="bibr" target="#b34">[35]</ref>. The ground-truth 3D poses are in black, while the predicted 3D poses are in other colors (red, blue etc.). Inaccurate poses, false negatives, and false positives are highlighted with circles. Best viewed in color.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our approach. The whole pipeline follows the top-down paradigm. It first applies Multi-view Matching Graph Module (MMG) to obtain coarse human center candidates, which are used to limit the search space. Center Refinement Graph Module (CRG) adaptively performs point-based prediction in the search space for more accurate human detection. Finally, Pose Regression Graph Module (PRG) is applied to each detected human proposal to predict the 3D poses in a coarse-to-fine manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Center Refinement Graph Module (CRG) iteratively applies point-based prediction on selected query points to detect human centers. The graph is constructed by linking the 2D projections of the 3D query in all camera views. Through a few graph convolutions, graph pooling, and MLP, we obtain the confidence score for each proposal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Overview of the 3D pose estimation stage. The initial 3D pose is projected to all camera views to construct the multi-view pose graph. With effective message passing and feature fusion, PRG predicts the regression offsets for 3D pose refinement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative analysis. Estimated 3D poses and their 2D projections of ours (the 1st row), and Tu et al. [35] (the 2nd row). The last column illustrates the ground-truth (black) and the predicted 3D poses (red, green, and blue). Missing poses are highlighted with circles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>is used. mAP ? mAR ? MPJPE ?</figDesc><table><row><cell>Tu et al. [35]</cell><cell>95.40  *</cell><cell>-</cell><cell>17.68mm</cell></row><row><cell cols="2">Tu et al. [35] (reproduce) 96.73</cell><cell>97.56</cell><cell>17.56mm</cell></row><row><cell>? Lin et al. [22]</cell><cell>97.68  *</cell><cell>-</cell><cell>16.75mm</cell></row><row><cell>Ours</cell><cell>98.10</cell><cell>98.70</cell><cell>15.84mm</cell></row><row><cell cols="4">sults by running the publicly available official codes 1 with</cell></row><row><cell cols="4">the recommended hyper-parameters. We find that our re-</cell></row><row><cell cols="4">implementation achieves a slightly better result (17.56mm</cell></row><row><cell cols="4">vs 17.68mm). We show that our approach significantly im-</cell></row><row><cell cols="4">proves upon Tu et al. [35] on mAP, mAR, and MPJPE.</cell></row><row><cell cols="4">Compared with Tu et al. [35], our approach has higher ac-</cell></row><row><cell cols="4">curacy (98.10 mAP vs 96.73 mAP) and also higher recall</cell></row><row><cell cols="4">(98.70 mAR vs 97.56 mAR). Especially, the MPJPE re-</cell></row><row><cell cols="4">markably decreases from 17.56mm to 15.84mm, demon-</cell></row><row><cell cols="4">strating the effectiveness of our proposed method in reduc-</cell></row><row><cell cols="4">ing the quantization error caused by space discretization.</cell></row><row><cell cols="4">The quantitative evaluation results on Shelf [3] dataset</cell></row><row><cell>are presented in</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Effect of MMG and CRG on human ceter detection and 3D human pose estimation. Pose results in this table are all obtained by PRG. 'Epi' means epipolar matching. 'GT' means using ground-truth matching results.</figDesc><table><row><cell>Method</cell><cell>Center</cell><cell>Pose</cell><cell>Pose</cell><cell>Pose</cell></row><row><cell></cell><cell cols="4">mAP ? mAP ? mAR ? MPJPE ?</cell></row><row><cell>Epi+Triangulation</cell><cell>61.65</cell><cell>86.02</cell><cell>91.08</cell><cell>24.46mm</cell></row><row><cell>MMG+Triangulation</cell><cell>75.91</cell><cell>95.11</cell><cell>97.60</cell><cell>16.99mm</cell></row><row><cell>GT+Triangulation</cell><cell>78.70</cell><cell>96.77</cell><cell>98.44</cell><cell>16.08mm</cell></row><row><cell cols="2">MMG+MLP-Baseline 81.38</cell><cell>97.82</cell><cell>97.89</cell><cell>16.06mm</cell></row><row><cell>Epi+CRG</cell><cell>79.80</cell><cell>95.68</cell><cell>95.68</cell><cell>16.03mm</cell></row><row><cell>MMG+CRG (final)</cell><cell>82.10</cell><cell>98.10</cell><cell>98.70</cell><cell>15.84mm</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Improvement of 3D pose estimation (MPJPE ?) when PRG is applied to different initial 3D pose regressors.</figDesc><table><row><cell></cell><cell cols="2">Before PRG After PRG</cell><cell>Improvement</cell></row><row><cell>(a)</cell><cell>18.12mm</cell><cell cols="2">16.63mm 1.49mm (8.2%)</cell></row><row><cell>(b)</cell><cell>17.78mm</cell><cell cols="2">16.44mm 1.34mm (7.5%)</cell></row><row><cell>(c)</cell><cell>17.09mm</cell><cell cols="2">15.84mm 1.25mm (7.3%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Memory and runtime analysis on CMU Panoptic dataset. Runtime is tested with one Titan X GPU. * denotes the cost of processing one person proposal.</figDesc><table><row><cell></cell><cell cols="2">CPN [35] PRN  *  [35]</cell><cell>MMG</cell><cell>CRG</cell><cell>PRG  *</cell></row><row><cell>Memory</cell><cell>1.10GB</cell><cell>2.38GB</cell><cell cols="3">7.10MB 1.08MB 20.3MB</cell></row><row><cell>Runtime</cell><cell>26ms</cell><cell>52ms</cell><cell>2.4ms</cell><cell>5.6ms</cell><cell>6.8ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Runtime comparison. N is the number of persons. 'avg is the average runtime (ms) when N = 4. '#bins' is the number of bins (voxel granularity) for PRN.</figDesc><table><row><cell>#</cell><cell>Method</cell><cell>#bins</cell><cell>Computational cost</cell><cell>avg MPJPE ?</cell></row><row><cell cols="2">1 Tu et al. [35]</cell><cell>64 3</cell><cell>26 + 52 ? N</cell><cell>234 17.68mm</cell></row><row><cell>2</cell><cell>Ours</cell><cell>64 3</cell><cell cols="2">8 + (52 + 6.8) ? N 243 15.84mm</cell></row><row><cell cols="2">3 Tu et al. [35]</cell><cell>32 3</cell><cell>26 + 7.3 ? N</cell><cell>55 19.19mm</cell></row><row><cell>4</cell><cell>Ours</cell><cell>32 3</cell><cell>8 + (7.3 + 6.8) ? N</cell><cell>64 15.95mm</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table A1 .</head><label>A1</label><figDesc>Generalization to different number of camera views. All results are obtained using ResNet-50 as the backbone. ? means the higher score the better, while ? means the lower the better. ? means fine-tuning models under the test-time camera setups. #Views mAP ? mAR ? MPJPE ?</figDesc><table><row><cell>Tu et al. [35]</cell><cell>5</cell><cell>96.73</cell><cell>97.56</cell><cell>17.56mm</cell></row><row><cell>Ours</cell><cell>5</cell><cell>98.10</cell><cell>98.70</cell><cell>15.84mm</cell></row><row><cell>Tu et al. [35]</cell><cell>4</cell><cell>94.54</cell><cell>95.97</cell><cell>20.06mm</cell></row><row><cell>Tu et al. [35]  ?</cell><cell>4</cell><cell>95.60</cell><cell>96.80</cell><cell>18.63mm</cell></row><row><cell>Ours</cell><cell>4</cell><cell>97.65</cell><cell>97.89</cell><cell>17.87mm</cell></row><row><cell>Tu et al. [35]</cell><cell>3</cell><cell>68.14</cell><cell>72.14</cell><cell>37.14mm</cell></row><row><cell>Tu et al. [35]  ?</cell><cell>3</cell><cell>89.26</cell><cell>93.91</cell><cell>24.02mm</cell></row><row><cell>Ours</cell><cell>3</cell><cell>91.60</cell><cell>94.14</cell><cell>22.69mm</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Generalization to Different Number of</head><p>Camera Views</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multi-view pictorial structures for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<editor>Brit. Mach. Vis. Conf.</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision. Kybernetes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex M</forename><surname>Andrew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bernt Schiele, Nassir Navab, and Slobodan Ilic. 3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bernt Schiele, Nassir Navab, and Slobodan Ilic. 3d pictorial structures revisited: Multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1929" to="1942" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiple human pose estimation with temporally consistent 3d pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Slobodan Ilic, and Nassir Navab</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-person 3d pose estimation and tracking in sports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Bridgeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Yves</forename><surname>Guillemaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d human pose esti-mation= 2d pose estimation+ matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7035" to="7043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning implicit fields for generative shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5939" to="5948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast and robust multi-person 3d pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7792" to="7801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Ershadi-Nasab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erfan</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohreh</forename><surname>Kasaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esmaeil</forename><surname>Sanaei</surname></persName>
		</author>
		<title level="m">Multiple human 3d pose estimation from multiview images. Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph convolutional tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4649" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end dynamic matching network for multiview multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congzhentao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Traish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard Yi Da</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Differentiable hierarchical graph grouping for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="718" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A generalizable approach for multi-view 3d human pose regression. Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdolrahim</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, and Ross Girshick. Pointrend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-view multi-person 3d pose estimation with plane sweep stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3d reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4460" to="4470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3d multiperson pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10133" to="10142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Bruce Xiaohan Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3467" to="3475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Peeking into occluded joints: A novel framework for crowd pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingteng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="488" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lightweight multi-view 3d pose estimation through camera-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Remelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6040" to="6049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Voxelpose: Towards multi-camera 3d human pose estimation in wild environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyue</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10965" to="10974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep network for the integrated 3d sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="8410" to="8419" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning 3d human shape and pose from dense body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Human pose estimation with spatial contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01760</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pymaf: 3d human pose and shape regression with pyramidal mesh alignment feedback loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yating</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">4d association graph for realtime multi-person motion capture using multiple video cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Highorder graph convolutional networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Brit. Mach. Vis. Conf</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
