<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fully Convolutional Cross-Scale-Flows for Image-based Defect Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Rudolph</surname></persName>
							<email>rudolph@tnt.uni-hannover.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Leibniz University Hannover</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Wehrbein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Leibniz University Hannover</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Leibniz University Hannover</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fully Convolutional Cross-Scale-Flows for Image-based Defect Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In industrial manufacturing processes, errors frequently occur at unpredictable times and in unknown manifestations. We tackle the problem of automatic defect detection without requiring any image samples of defective parts. Recent works model the distribution of defect-free image data, using either strong statistical priors or overly simplified data representations. In contrast, our approach handles fine-grained representations incorporating the global and local image context while flexibly estimating the density. To this end, we propose a novel fully convolutional cross-scale normalizing flow (CS-Flow) that jointly processes multiple feature maps of different scales. Using normalizing flows to assign meaningful likelihoods to input samples allows for efficient defect detection on image-level. Moreover, due to the preserved spatial arrangement the latent space of the normalizing flow is interpretable which enables to localize defective regions in the image. Our work sets a new stateof-the-art in image-level defect detection on the benchmark datasets Magnetic Tile Defects and MVTec AD showing a 100% AUROC on 4 out of 15 classes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>During the industrial production of components, defects occur over time. They must be detected to ensure safety standards and product quality. Since manual inspection by humans is very costly and error-prone, reliable and efficient automatic defect detection is highly demanded. In most real-world scenarios, however, there exist no examples of such defects. Moreover, even if a small set of known defects is available, new and formerly unseen types of defects occur at unpredictable times which makes it impossible to apply standard classification approaches. Instead, it is inevitable to let the defect detector learn only from non-defective examples. This problem is commonly called semi-supervised anomaly detection (AD), novelty detection or one-class classification.</p><p>These terms describe the objective of deciding whether a data sample belongs to the class of the given set X of ...</p><p>x <ref type="bibr" target="#b0">(1)</ref> x <ref type="bibr" target="#b1">(2)</ref> x <ref type="bibr" target="#b2">(3)</ref> y <ref type="bibr" target="#b0">(1)</ref> y <ref type="bibr" target="#b1">(2)</ref> y <ref type="bibr" target="#b2">(3)</ref> z <ref type="bibr" target="#b0">(1)</ref> z <ref type="bibr" target="#b1">(2)</ref> z <ref type="bibr" target="#b2">(3)</ref> resize f fe f fe <ref type="figure">Figure 1</ref>. Our method detects and localizes defects based on the density estimation of feature maps from the differently sized input images. We process the multi-scale feature maps jointly, using a fully convolutional normalizing flow with cross-connections between scales.</p><p>normal (in our case non-defective) data. The problem is interpreted in terms of whether a data sample lies out of the distribution p X of the set of normal images X, also named out-of-distribution (OOD) detection. It is assumed that de-fectsX are out-of-distribution, i.e. have a small likelihood given p X . We propose a method that models the distribution on feature level with a normalizing flow.</p><p>Most research <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b31">32]</ref> in the AD field focuses on image datasets with high intra-class and high inter-classvariance. The setting in defect detection is different: Since the non-defective components are similar to themselves and to the defects, there is a small intra-class and a small interclass-variance. Hence, most AD approaches are not suitable for defect detection. Common approaches based on autoencoders <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15]</ref> or generative adversarial networks (GANs) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7]</ref> perform poorly in this setting, which is described in detail in Section 2. Thus, recent works rely on density estimation of image features obtained from models pretrained on ImageNet <ref type="bibr" target="#b8">[9]</ref>, e.g. ResNet <ref type="bibr" target="#b16">[17]</ref> or Efficient-Net <ref type="bibr" target="#b37">[38]</ref>. However, either information is lost due to the averaging of feature maps <ref type="bibr" target="#b30">[31]</ref> or strong statistical priors are required limiting their flexibility in density estimation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b7">8]</ref>. To alleviate these issues, we propose a normalizing flow (NF) that is able to process multi-scale feature maps to estimate their density, as shown in <ref type="figure">Figure 1</ref>. NFs are generative models that transform the training set distri-bution p X to a latent space with a predefined distribution p Z via maximum-likelihood-optimization. In contrast to other generative models, for instance VAEs <ref type="bibr" target="#b20">[21]</ref> and GANs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4]</ref>, the likelihoods of latent space vectors in NFs are directly interpreted as likelihoods of the input data, since the network maps bijectively. Thus, the regions in the latent space with high likelihood represent the normal examples while defective examples are projected to latent variables outside of the learned distribution. Conversely, the injective mapping of autoencoders potentially results in projecting untrained anomalies to indeterminate latent space regions, which may overlap with the regions of the normal samples.</p><p>However, applying NFs to images for OOD detection is not straightforward as shown by Kirichenko et al. <ref type="bibr" target="#b21">[22]</ref>. With RGB data, the network fails to learn a useful distribution, focusing on local pixel correlations instead of semantics. For this reason, we perform the density estimation on feature maps obtained by pretrained feature extractors which provide compressed semantic information. Our cross-scale flow (CS-Flow) simultaneously processes the features of the image at different scales by propagating them in parallel through the NF while interacting with each other. Keeping in mind that the discriminability regarding defectiveness is unknown during training, our model utilizes the full potential of the information and correlations in both local and global contexts to learn the distribution precisely to identify defective examples. In addition to identification, the fully convolutional architecture also preserves spatial arrangement which allows for a visualization of the defective regions on the image. In contrast to models using densely connected layers and thus many parameters <ref type="bibr" target="#b30">[31]</ref>, our approach still achieves good performance even with a low number of training samples.</p><p>We summarize our contributions as follows:</p><p>? Our novel cross-scale normalizing flow (CS-Flow) detects defects by jointly estimating likelihoods on multiscale feature maps.</p><p>? Our method maintains the image structure to obtain an interpretable latent space, which enables precise defect detection.</p><p>? We set a new state-of-the-art in image-level defect detection on the MVTec AD and Magnetic Tile Defects dataset.</p><p>? Code is available on GitHub 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the following, we review previous work in the field of anomaly detection and normalizing flows as the basis of our methodology.   <ref type="bibr" target="#b37">[38]</ref>. Each histogram contains the values from the same position of one feature map. The blue line shows the best fitting normal distribution. Assuming a normal distribution of the features, as done by <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b7">8]</ref>, appears to be insufficient to capture the feature distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Anomaly Detection</head><p>State-of-the-art work can be roughly divided into approaches that are based on generative models or pretrained networks. Alternative methods that do not fall into one of these categories are described separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Generative Models</head><p>Many anomaly detection methods are based on generative models, such as autoencoders <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref> and GANs <ref type="bibr" target="#b15">[16]</ref>, which are optimized to generate the normal data. These approaches detect anomalies by the inability of the generative model to reconstruct them. In the simplest case, the input and the reconstruction of an autoencoder is compared <ref type="bibr" target="#b41">[42]</ref>. In this context, a high reconstruction error is interpreted as an indicator of an anomaly. Bergmann et al. <ref type="bibr" target="#b5">[6]</ref> replace the common l 2 error with SSIM to have a better metric for visual similarity. Gong et al. <ref type="bibr" target="#b14">[15]</ref> use memory modules in the latent space to prevent the autoencoder from generalizing to anomalous data. Zhai et al. <ref type="bibr" target="#b40">[41]</ref> combine energy-based models and regularized autoencoders to model the data distribution. Denoising autoencoders are used by Huang et al. <ref type="bibr" target="#b11">[12]</ref> by letting autoencoders learn to restore transformed images.</p><p>Similar to the decoding part of autoencoders, generators of GANs are utilized for anomaly detection. Schlegl et al. <ref type="bibr" target="#b33">[34]</ref> propose to learn an inverse generator after training a GAN, utilizing both together for reconstruction and the error consideration. A combination of autoencoders and GANs is proposed by Akcay et al. <ref type="bibr" target="#b0">[1]</ref>. They apply the autoencoder directly as the GAN's generator to ensure the generation of normal data only.</p><p>As shown in Section 4.3, autoencoders and GANs perform poorly on defect detection tasks.Since different types of anomalies with individual size, shape and structure have inconsistent characteristics regarding reconstruction errors,  <ref type="figure">Figure 3</ref>. Architecture of one block inside the normalizing flow: After a fixed random permutation, every input tensor is split into two parts across the channel dimension where each ensemble is used to estimate scale and shift parameters that transform the respective counterpart. Symbols and ? denote element-wise multiplication and addition, respectively.</p><formula xml:id="formula_0">y (1) in y (3) in y (2) in fixed</formula><p>they are not widely applicable. For example, structures with high frequency cannot be represented and reconstructed accurately in general and small defect areas cause smaller errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Methods Based on Pretrained Networks</head><p>Instead of working on the image directly, many methods perform defect detection on features of pretrained networks.</p><p>Pretraining on a large-scale database, such as ImageNet, ensures the extraction of universal features that are expected to differ in the presence of defects. In this way, discriminant features are considered which cannot be learned from non-defective data, since they do not necessarily occur in it. Detecting defects in the feature space commonly is done using traditional statistical approaches. Andrews et al. <ref type="bibr" target="#b1">[2]</ref> fit a one-class Support Vector Machine to the feature distribution. Rippel et al. <ref type="bibr" target="#b28">[29]</ref> model the features as an unimodal Gaussian distribution and utilizes the Mahalanobis distance as scoring function. This approach was further refined by Defard et al. <ref type="bibr" target="#b7">[8]</ref> by applying it to image patches utilizing feature maps at different semantic levels. However, these approaches are limited to normal distributions which are inappropriate in many cases as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. In contrast, we do not assume any predefined feature distribution, but learn the true distribution via maximum likelihood estimation (MLE). Assuming that distances within the feature space are semantically expressive, the distance to the nearest neighbour is used as an anomaly score in <ref type="bibr" target="#b26">[27]</ref>. The only deep-learning-based image feature density estimation method by Rudolph et al. <ref type="bibr" target="#b30">[31]</ref>, which is the most comparable to our work, is also based on normalizing flows. However, they do not process fullsized feature maps, but rather vectors after applying average pooling. As a result, important contextual and positional information is lost. The authors partially compensate this weakness by passing 64 different rotations of each image through the network, which, however, significantly increases computational complexity. In contrast, our method utilizes the fine-grained information of the full-sized feature maps while requiring only a single pass and outperforms DifferNet <ref type="bibr" target="#b30">[31]</ref> in almost all experiments by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Other Approaches</head><p>Besides generative and pretrained models, there are alternative approaches to perform anomaly detection. Liznerski et al. <ref type="bibr" target="#b25">[26]</ref> propose a learnable hypersphere classifier using exemplar outlier exposures as anomaly substitute. Contrastive learning on augmentations of the same image is used by Tack et al. <ref type="bibr" target="#b36">[37]</ref> by defining in-distribution and outof-distribution transformations. In contrast, Golan and El-Yaniv <ref type="bibr" target="#b13">[14]</ref> augment images to classify the specific transformation, assuming that this does not work as clearly on anomalies as it does on normal data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Normalizing Flows</head><p>A normalizing flow (NF) <ref type="bibr" target="#b27">[28]</ref> is a generative model that transforms data into tractable distributions. Unlike conventional neural networks, their mapping is bijective, which allows them to train and evaluate in both directions <ref type="bibr" target="#b38">[39]</ref>. The forward pass projects data into a latent space to calculate exact likelihoods for the data given the predefined latent distribution. Conversely, data sampled from the predefined distribution can be mapped back into the original space to generate data. Bijectivity and bidirectional execution are ensured by using invertible affine transformations. There are different types of normalizing flows, which differ in the architecture of the affine transformations in order to efficiently enable the forward or backward direction. The affine blocks are realized either by learning fixed or autoregressive transformations. A popular type of autoregressive flows is MADE (Germain et al. <ref type="bibr" target="#b12">[13]</ref>). The density calculation based on the Bayesian chain rule is efficient in this case. However, sampling is costly. In contrast, inverse autoregressive flows (Kingma et al. <ref type="bibr" target="#b19">[20]</ref>) are usually efficient at sampling, but not at computing likelihoods. Real-NVP <ref type="bibr" target="#b10">[11]</ref>, a variant of inverse autoregressive flows, simplifies both passes to be efficient in both directions. We enhanced Real-NVP to operate on multiple scales that can interact with each other. This leverages NFs for defect detection by introducing fully convolutional cross-scale flows, whose architecture is explained in detail in Section 3.1. Normalizing flows are successfully used for anomaly detection on non-image data <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b9">10]</ref>. With image data, the problem arises that the network mainly focuses on local pixel correlation without taking semantics into account. Recent works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b21">22]</ref> found that semantic information is better captured when working on image features instead of full images. In contrast to <ref type="bibr" target="#b21">[22]</ref>, we use features from multiple scales and refrain from the usage of fully connected layers and squeeze layers 2 . In this way, our latent space preserves the spatial arrangement and therefore enables precise defect localization. Furthermore, we lower the number of parameters which enables us to process high dimensional feature maps and train with few data samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>To detect defects in images, we first learn a statistical model of features y ? Y of defect-free images x ? X similar to DifferNet <ref type="bibr" target="#b30">[31]</ref>. During inference, we assign a likelihood to the input image x by using a density estimation on image features y, assuming a low likelihood is an indicator for a defect. The density estimation is learned via a bijective mapping of the unknown distribution p Y of the feature space Y to a latent space Z with a Gaussian distribution p Z . Thus, as shown in <ref type="figure">Figure 1</ref>, our method is divided into the steps feature extraction X ? Y and density estimation Y ? Z.</p><p>From the input image x we extract the features y by us-2 Squeeze layers reshape the tensor, e.g. by aggregating the channels of 4 neighboring pixels to one pixel with fourfold channel number. ing a pretrained neural network f fe (x) = y which will remain unchanged during training. To have a more descriptive representation of x, feature maps of different scales are included in y via extracting features from s different resolutions of the image. In contrast to <ref type="bibr" target="#b30">[31]</ref>, our proposed NFarchitecture is able to perform density estimation on different scaled full-sized feature maps in parallel instead of on concatenated feature vectors. Thus, important fine-grained positional and contextual information is maintained. We define y = [y <ref type="bibr" target="#b0">(1)</ref> , ..., y (s) ] with y (i) as the 3D feature tensor of the image x (i) at scale i ? {1, ..., s}. Our proposed crossscale-flow f csf transforms the feature tensors bijectively and in parallel to f csf (y <ref type="bibr" target="#b0">(1)</ref> , ..., y (s) ) = [z <ref type="bibr" target="#b0">(1)</ref> , ..., z (s) ] = z ? Z</p><p>with the same dimensionality 3 as y. The likelihood p Z (z) is measured according to the target distribution which in our case is a multivariate standard normal distribution N (0, I).</p><p>We use the likelihood of p Z (z) to decide whether x is anomalous according to a threshold ?:</p><formula xml:id="formula_2">A(x) = 1 for p Z (z) &lt; ? 0 else .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Cross-Scale Flow</head><p>We extend the traditional normalizing flows with our novel cross-scale flow to allow for effective defect detection on images. It processes feature maps of different sizes which interact with each other. In this way, information between the scales is shared to obtain a likelihood for the compound of y = [y <ref type="bibr" target="#b0">(1)</ref> , ..., y (s) ]. Moreover, we design it fully convolutional and preserve the spatial dimensions. This allows to determine the positions of the anomalies in Z as shown in Section 3.3. An additional benefit of our approach compared to <ref type="bibr" target="#b30">[31]</ref> is a practicable handling of very highdimensional input spaces while having few training samples as shown in Section 4.</p><p>The cross-scale flow is a chain of so-called coupling blocks, each performing affine transformations. As basis for the frame architecture of the coupling block we chose Real-NVP <ref type="bibr" target="#b10">[11]</ref>. The detailed structure of one block with s = 3 is shown in <ref type="figure">Figure 3</ref>. Inside, each input tensor y (i) in is first randomly permuted and evenly split across its channel dimension into the two parts y y out,2 = y in,2 e ?1 s1(yin,1) + ? 1 t 1 (y in,1 ) y out,1 = y in,1 e ?1 s2(yout,2) + ? 2 t 2 (y out,2 ),</p><p>with as the element-wise product. To initialize the model in a stable way, we introduce the learnable block-individual scalar coefficients ? 1 and ? 2 . They are initialized to 0 and thus cause y out = y in . The affinity property is preserved by having non-zero scaling coefficients with the exponentiation in Equation 3. The internal networks r 1 and r 2 do not need to be invertible and can be any differentiable function, which in our case is implemented as a fully convolutional network that regresses both components by splitting the output (see <ref type="figure" target="#fig_3">Figure 4</ref> for details of the architecture). Features are processed with one hidden layer per scale on which the number of channels is increased. Motivated by HRNet <ref type="bibr" target="#b35">[36]</ref>, we adjust the size of individual feature maps of different scales by bilinear upsampling or strided convolutions before aggregation by summation. We apply soft-clamping to the scale components s, as proposed by Ardizzone et al. <ref type="bibr" target="#b2">[3]</ref>, to preserve model stability in spite of the exponentiation. This clamping is applied as the last layer to the outputs s 1 and s 2 by the activation</p><formula xml:id="formula_4">? ? (h) = 2? ? arctan h ? .<label>(4)</label></formula><p>This prevents extreme scaling components by restricting the values to the interval (??, ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Objective</head><p>During training, we want the cross-scale flow f csf to maximize the likelihoods of feature tensors p Y (y) which we obtain by mapping them to the latent space Z where we model a well-defined density p Z . Using the change-ofvariables formula Eq. 5 and z = f NF (y), this likelihood is defined by</p><formula xml:id="formula_5">p Y (y) = p Z (z) det ?z ?y .<label>(5)</label></formula><p>We optimize the log-likelihood, since it is equivalent and more convenient for a density p Z of a Gaussian distribution.</p><p>Thus, we formulate our objective as the minimization of the negative log-likelihood ? log p Y (z):</p><p>log p Y (y) = log p Z (z) + log det ?z ?y</p><formula xml:id="formula_6">L(y) = ? log p Y (y) = z 2 2 2 ? log det ?z ?y .<label>(6)</label></formula><p>with det ?z ?y denoting the absolute determinant of the Jacobian. The logarithm of this term simplifies in our case to the sum of all values of s since the Jacobian of the elementwise product operator in Equation 3 is a diagonal matrix. The training is conducted over a fixed number of epochs. To stabilize it further, we limit the l 2 -norm of the gradients to 1. Section 4.2 describes the training in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Localization</head><p>In previous work <ref type="bibr" target="#b30">[31]</ref>, the latent space of the normalizing flow has only been used such that all entries of z are considered to produce a score at the image level. Since our method processes feature maps fully-convolutional, positional information is preserved. This allows for the interpretation of the output in terms of the likelihood of individual image regions, which in our application is the localization of the defect.</p><p>Analogous to the definition of the anomaly score of the entire image, we define an anomaly score for each local position (i, j) of the feature map y s by aggregating the values along the channel dimension with z s i,j 2 2 . Thus, we can localize the defect by marking image regions with high norm in the output feature tensors z s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate our method on a wide range of realistic defect detection scenarios to demonstrate the advantage of our contributions and the superiority over previous approaches. For this purpose, we measure the performance on the challenging and diverse MVTec AD <ref type="bibr" target="#b4">[5]</ref> and Magnetic Tile Defects (MTD) <ref type="bibr" target="#b17">[18]</ref> datasets.</p><p>MVTec AD comprises 10 object and 5 texture classes with overall 3629 defect-free training and 1725 testing images. Each class contains 60 to 320 high-resolution images with a range from 700 ? 700 to 1024 ? 1024 pixels. The test set includes defects of different sizes, shapes and types such as cracks, scratches and displacements, with up to 8 different defect types per class and 70 defect types in total. To the best of our knowledge, MVTec AD acts currently as the only dataset with multi-object and multi-defect-data for anomaly detection.</p><p>As a common choice, we also evaluate on the MTD dataset, which includes gray-scale images of magnetic tiles  <ref type="bibr" target="#b13">[14]</ref> 75.5 GANomaly <ref type="bibr" target="#b0">[1]</ref> 76.6 DSEBM <ref type="bibr" target="#b40">[41]</ref> 57.2 Mahalanobis <ref type="bibr" target="#b28">[29]</ref> 98.0 1-NN <ref type="bibr" target="#b26">[27]</ref> 97.8 DifferNet <ref type="bibr" target="#b30">[31]</ref> 97.7 PaDiM <ref type="bibr" target="#b7">[8]</ref> 98.7 CS-Flow (ours) 99.3 <ref type="table">Table 2</ref>. Area under ROC in % for detecting anomalies on MTD.</p><p>with and without defects. The contained defects, e.g. breaks and blowholes, can cause problems in engines due to an unequal magnetic potential. It is notable that this dataset shows a large variance within the defect-free examples due to the differences in illumination and other non-defect characteristics. Following <ref type="bibr" target="#b30">[31]</ref>, we use all 392 defect images and one fifth of the 952 defect-free images for testing and train on the remaining defect-free data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We utilize the output of layer 36 of EfficientNet-B5 <ref type="bibr" target="#b37">[38]</ref> as the feature extractor for all experiments as it provides feature maps having a good balance between level of feature semantic and spatial resolution. The feature extractor remains fixed during training after being pretrained on ImageNet <ref type="bibr" target="#b8">[9]</ref>. For MVTec AD, we use features at s = 3 scales with input image sizes of 768 ? 768, 384 ? 384 and 192 ? 192 pixels -resulting in feature maps with spatial dimensions 24 ? 24, 12 ? 12 and 6 ? 6 and each 304 channels. Due to the smaller original image size of MTD samples, we resized the images to 384 ? 384, 192 ? 192 and 96 ? 96 pixels. We use n blocks = 4 coupling blocks inside CS-Flow non-defects defects <ref type="figure">Figure 5</ref>. Distribution of negative log-likelihood for test images of MTD as a normalized histogram. By this criterion, the defective samples are almost completely separable from the non-defective samples. Note that for clarity, the rightmost bar summarizes all scores above 3. using 3 ? 3 convolutional kernels in internal networks for the first 3 blocks and 5 ? 5 kernels for the last block. The clamping parameter is set to ? = 3 and the negative slope of the leaky ReLU is set to 0.1. For optimization, we use Adam <ref type="bibr" target="#b18">[19]</ref> with a learning rate of 2 ? 10 ?4 , a weight decay of 10 ?5 and momentum values ? 1 = 0.5 and ? 2 = 0.9. We train our models with a batch size of 16 for a fixed number of 240 epochs for MVTec AD and 60 epochs for MTD, respectively, since there is no validation set to define a stopping criterion. A training run of one class of MVTec AD takes about 45 minutes on average using a NVIDIA RTX 2080 Ti.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Detection</head><p>In order to measure and compare the defect detection performance of our models, we follow <ref type="bibr" target="#b30">[31]</ref> and calculate the area under ROC (AUROC) at image-level on the respective test sets. The ROC (Receiver Operating Characteristics) curve relates the true positive rate to the false positive rate with respect to a parameter (in our case the threshold ?). Thus, it is invariant to the ratio of anomalies in the set and is therefore representative for realistic settings. <ref type="table" target="#tab_1">Table 1</ref> shows the defect detection performance of our method and other state-of-the-art works on the individual categories of MVTec AD. For a fair comparison, we evaluated <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b28">[29]</ref> on the same multi-scale features as our method which improved their performance in every case. Here, we averaged the feature maps of different scales individually, resulting in a feature vector with 3?304 = 912 dimensions. Note that PaDiM <ref type="bibr" target="#b7">[8]</ref> is originally based on EfficientNet-B5 <ref type="bibr" target="#b37">[38]</ref>. Since the results of <ref type="bibr" target="#b30">[31]</ref> dropped heavily with this backbone, we report the papergiven results with AlexNet <ref type="bibr" target="#b22">[23]</ref>. We outperform or match the competitors on 12 of 15 categories with an average AU-ROC of 98.7%, which considerably closes the gap to the optimum of 100% compared to competitors. CS-Flow works reliably on a wide range of defects having an AUROC over 97% in 14 of 15 categories. Our method remains competitive when training on only 16 samples per category, with even showing roughly the same performance on the texture categories.</p><p>We also set a new state of the art of 99.3% AUROC on MTD as shown in <ref type="table">Table 2</ref>. As shown in <ref type="figure">Figure 5</ref>, the likelihood assigned by our model clearly distinguishes the defective from the non-defective parts, with only a few exceptions. Being just 0.7% AUROC close to an optimal ROC, we want to emphasize that in this metric a margin of a few percents compared to competitors is a relatively strong increase in performance as visualized in <ref type="figure" target="#fig_7">Figure 6</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Localization</head><p>Although the objective of our approach is to detect defects on image level, it can also be used to localize defective regions in images, due to its global and local feature preserving nature. In this section we study the localization, as described in Section 3.3. Our goal is to give a quick visual feedback to an operator. <ref type="figure" target="#fig_8">Figure 7</ref> shows the visualization of the highest scale outputs z <ref type="bibr" target="#b0">(1)</ref> . These were scaled up with a bilinear interpolation after summing up the squared values along the channel dimension. It can be seen that the magnitude of the output values is directly related to the occurrence of anomalous regions at the respective position. Therefore, our method localizes anomalies of various sizes with respect to color, pattern and shape. Except for dilations due to the convolutional receptive field, defective regions are determined properly. We do not aim to provide pixelprecise segmentations as the method is not optimized for it and processes small-resolution feature maps. Nevertheless, this visualization helps in the interpretation of the output in practice to quickly find or assess the potential error. We refer to the supplemental material for more detailed analysis of the localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>To quantify the influence of the individual design decisions of our model, we report results obtained when varying the hyperparameters of our method. <ref type="table" target="#tab_3">Table 3</ref> shows the results of these experiments. We measure the impact of the multi-scale approach on the defect detection performance. To this end, we train models on feature maps from one of the three scales at a time (denoted as single scale NF). The results confirm that the features of a single scale are weaker with respect to the discriminability between defective and non-defective samples. Furthermore, we set another baseline by adding the log likelihoods provided by the networks from every scale (denoted as separate multi-scale). The increase in AUROC compared to the individual performance for the single scale models demonstrates that the features of different scales complement each other well to obtain a more robust score. Nevertheless, this method is 0.5% AU-ROC below the performance of our joint training of the individual scales with CS-Flow. To test our architecture against a naive approach of joint training, we feed a single-scale NF the concatenation of differently sized feature maps along the channel dimension after upscaling each of them to the highest feature map size with bilinear interpolation, comparable to <ref type="bibr" target="#b7">[8]</ref>. This setup (denoted as concat mutiscale) results in a performance drop of 0.7%, which justifies our cross-convolutional multi-scale procedure.</p><p>In another experiment, we studied the influence of the number of coupling blocks. The results in <ref type="table">Table 4</ref> show that the performance improves with increasing number of coupling blocks up to n blocks = 4 and then saturates.</p><p>To test our model on a setting with more intra-class variance in the normal data, we additional experiment training simultaneously with all 15 classes of MVTec AD as normal data. The average detection AUROC is 98.2% which shows that our model can handle multi-modal distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a semi-supervised method to effectively detect and localize defects on feature tensors of different scales using normalizing flows. We utilize the context within and between multi-scale feature maps by integrating cross-convolution blocks inside the normalizing flow to assign likelihoods and detect unlikely samples as defects. This addresses weaknesses of previous methods that struggle either due to restrictions of overly simplified data representations or limited distribution models and enables our method to set state-of-the-art performance on MVTec AD and MTD. In the future, the concept could be refined for video anomaly detection <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b24">25]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1</head><label></label><figDesc>https://github.com/marco-rudolph/cs-flow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Histogram of different features from MVTec AD images extracted with EfficientNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Architecture of the internal networks r inside the coupling blocks. Convolutions are performed at two levels, with crossconnections between scales at the second level. Feature map resizing is implemented by upsampling and strided convolutions. Aggregation is implemented by summation. The output is split across the channel dimension to obtain the scale and shift parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 .</head><label>2</label><figDesc>These parts manipulate each other by regressing element-wise scale and shift parameters which are successively applied to their respective counterparts to obtain the output [y The scale and shift parameters are estimated by coupling blockindividual subnetworks r 1 and r 2 whose output is split into [s 1 , t 1 ] and [s 2 , t 2 ] and is then used as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Comparison of defect detection performance of different methods on MTD. The graphs are the ROC-Curves of the individual methods. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Defect localization of one defective example per category of MVTec AD and MTD. The rows each show the original image, the localization and the overlay of both images, from top to bottom. The localization maps show the sum of squares along the channel dimension of the networks output at the highest scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Area under ROC in % for detecting defects of all categories of MVTec AD<ref type="bibr" target="#b4">[5]</ref> on image-level grouped into textures and objects. Best results are in bold. 16 shots denotes that a subset of only 16 random images per category was used in training. Beside the average value, detailed results of PaDiM<ref type="bibr" target="#b7">[8]</ref> were not provided by the authors.</figDesc><table><row><cell></cell><cell>Category</cell><cell>ARNet</cell><cell cols="4">Geom. GAN DSEBM Mahal.</cell><cell cols="3">1-NN DifferNet PaDiM</cell><cell>CS-Flow (ours)</cell></row><row><cell></cell><cell></cell><cell>[12]</cell><cell>[14]</cell><cell>[1]</cell><cell>[41]</cell><cell>[29]</cell><cell>[27]</cell><cell>[31]</cell><cell>[8]</cell><cell>(16 shots/full set)</cell></row><row><cell></cell><cell>Grid</cell><cell>88.3</cell><cell>61.9</cell><cell>70.8</cell><cell>71.7</cell><cell>93.7</cell><cell>81.8</cell><cell>84.0</cell><cell>-</cell><cell>93.3 99.0</cell></row><row><cell>Textures</cell><cell>Leather Tile Carpet Wood</cell><cell>86.2 73.5 70.6 92.3</cell><cell>84.1 41.7 43.7 61.1</cell><cell>84.2 79.4 69.9 83.4</cell><cell>41.6 69.0 41.3 95.2</cell><cell>100 100 99.6 99.3</cell><cell>100 100 98.5 95.8</cell><cell>97.1 99.4 92.9 99.8</cell><cell>----</cell><cell>100 100 99.9 100 100 100 99.5 100</cell></row><row><cell></cell><cell>Avg. Text.</cell><cell>82.2</cell><cell>59.6</cell><cell>77.5</cell><cell>63.8</cell><cell>98.5</cell><cell>96.1</cell><cell>94.6</cell><cell>99.0</cell><cell>98.5 99.8</cell></row><row><cell></cell><cell>Bottle</cell><cell>94.1</cell><cell>74.4</cell><cell>89.2</cell><cell>81.8</cell><cell>99.0</cell><cell>99.6</cell><cell>99.0</cell><cell>-</cell><cell>100 99.8</cell></row><row><cell></cell><cell>Capsule</cell><cell>68.1</cell><cell>67.0</cell><cell>73.2</cell><cell>59.4</cell><cell>96.3</cell><cell>89.4</cell><cell>86.9</cell><cell>-</cell><cell>83.1 97.1</cell></row><row><cell></cell><cell>Pill</cell><cell>78.6</cell><cell>63.0</cell><cell>74.3</cell><cell>80.6</cell><cell>91.4</cell><cell>79.9</cell><cell>88.8</cell><cell>-</cell><cell>90.9 98.6</cell></row><row><cell></cell><cell>Transistor</cell><cell>84.3</cell><cell>86.9</cell><cell>79.2</cell><cell>74.1</cell><cell>98.2</cell><cell>95.4</cell><cell>91.1</cell><cell>-</cell><cell>98.0 99.3</cell></row><row><cell>Objects</cell><cell>Zipper Cable Hazelnut</cell><cell>87.6 83.2 85.5</cell><cell>82.0 78.3 35.9</cell><cell>74.5 75.7 78.5</cell><cell>58.4 68.5 76.2</cell><cell>98.8 99.1 100</cell><cell>97.1 95.1 98.2</cell><cell>95.1 95.9 99.3</cell><cell>---</cell><cell>95.3 99.7 94.4 99.1 97.9 99.6</cell></row><row><cell></cell><cell>Metal Nut</cell><cell>66.7</cell><cell>81.3</cell><cell>70.0</cell><cell>67.9</cell><cell>97.4</cell><cell>91.1</cell><cell>96.1</cell><cell>-</cell><cell>99.1 99.1</cell></row><row><cell></cell><cell>Screw</cell><cell>100</cell><cell>50.0</cell><cell>74.6</cell><cell>99.9</cell><cell>94.5</cell><cell>91.4</cell><cell>96.3</cell><cell>-</cell><cell>65.2 97.6</cell></row><row><cell></cell><cell>Toothbrush</cell><cell>100</cell><cell>97.2</cell><cell>65.3</cell><cell>78.1</cell><cell>94.1</cell><cell>94.7</cell><cell>98.6</cell><cell>-</cell><cell>85.6 91.9</cell></row><row><cell></cell><cell>Avg. Obj.</cell><cell>84.8</cell><cell>71.6</cell><cell>75.5</cell><cell>74.5</cell><cell>96.9</cell><cell>93.2</cell><cell>94.7</cell><cell>97.2</cell><cell>91.0 98.2</cell></row><row><cell></cell><cell>Average</cell><cell>83.9</cell><cell>67.2</cell><cell>76.2</cell><cell>70.9</cell><cell>97.5</cell><cell>93.9</cell><cell>94.7</cell><cell>97.9</cell><cell>93.5 98.7</cell></row></table><note>Method AUROC [%] ? Geom.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on MVTec AD with varying strategies regarding the usage of scales.</figDesc><table><row><cell>n blocks</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell cols="2">AUROC [%] ? 94.6</cell><cell>97.8</cell><cell>98.5</cell><cell cols="2">98.7 98.7</cell><cell>98.6</cell></row><row><cell cols="7">Table 4. Ablation study on MVTec AD for a different number of</cell></row><row><cell>coupling blocks.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For better readability, in the following z without any index represents a vector which is the concatenation of the flattened tensors [z<ref type="bibr" target="#b0">(1)</ref> , ..., z (s) ].</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ganomaly: Semi-supervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2018</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transfer representation-learning for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerone</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Griffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynton</forename><surname>Ardizzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>L?th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ullrich</forename><surname>K?the</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02392</idno>
		<title level="m">Guided image generation with conditional invertible neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Toad-gan: coherent style level generation from a single example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maren</forename><surname>Awiszus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</title>
		<meeting>the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="10" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sindy</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VISIGRAPP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adgan: A scalable gan-based architecture for image anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (IT-NEC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="987" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Padim: a patch distribution modeling framework for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Defard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Setkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelique</forename><surname>Loesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romaric</forename><surname>Audigier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">pattern Recognition, ICPR International Workshops and Challenges</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Madson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?sar</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lincoln</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ticiana Lc Da</forename><surname>Mattos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ant?nio F De Macedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Wellington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05958,2020.4</idno>
		<title level="m">Anomaly detection in trajectory data with normalizing flows</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attribute restoration framework for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Jinkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Made: Masked autoencoder for distribution estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="881" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9758" to="9769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Surface defect saliency of magnetic tile. The Visual Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="85" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes. CoRR, abs/1312</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6114</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Why normalizing flows fail to detect out-of-distribution data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Kirichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalization and network design strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Connectionism in Perspective</title>
		<editor>R. Pfeifer, Z. Schreter, F. Fogelman, and L. Steels</editor>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gaussian process for activity modeling and anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Photogrammetry and Remote Sensing ISA workshop</title>
		<meeting><address><addrLine>La Grande Motte, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Explainable deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Liznerski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Billy</forename><forename type="middle">Joe</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Franks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>M?ller</surname></persName>
		</author>
		<idno>ICLR, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Are pretrained cnns good feature extractors for anomaly detection in surveillance videos?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Nazare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moacir</forename><surname>Ponti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08495</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<idno>PMLR, 2015. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Modeling the distribution of normal data in pre-trained deep features for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorit</forename><surname>Merhof</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14140</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structuring autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Same same but differnet: Semi-supervised defect detection with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="1907" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Normalizing flows for deep anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Ryzhikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Borisyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Ustyuzhanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Derkach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09323</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">f-anogan: Fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Erfurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Normalizing flows for novelty detection in industrial time series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Simic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06904</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Csi: Novelty detection via contrastive learning on distributionally shifted instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Probabilistic monocular 3d human pose estimation with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Wehrbein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Video event recognition and anomaly detection by combining gaussian process and hierarchical dirichlet process models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Photogrammetric Engineering &amp; Remote Sensing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep structured energy based models for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weining</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Anomaly detection with robust deep autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Randy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paffenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
