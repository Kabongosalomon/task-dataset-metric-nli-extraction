<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relational Self-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkai</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Chen</forename><surname>Qian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Chang</forename><surname>Xu</surname></persName>
						</author>
						<title level="a" type="main">Relational Self-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Contrastive Learning</term>
					<term>Unsupervised Learning</term>
					<term>Self-Supervised Learning</term>
					<term>Representation Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised Learning (SSL) including the mainstream contrastive learning has achieved great success in learning visual representations without data annotations. However, most methods mainly focus on the instance level information (i.e., the different augmented images of the same instance should have the same feature or cluster into the same class), but there is a lack of attention on the relationships between different instances. In this paper, we introduce a novel SSL paradigm, which we term as relational self-supervised learning (ReSSL) framework that learns representations by modeling the relationship between different instances. Specifically, our proposed method employs sharpened distribution of pairwise similarities among different instances as relation metric, which is thus utilized to match the feature embeddings of different augmentations. To boost the performance, we argue that weak augmentations matter to represent a more reliable relation, and leverage momentum strategy for practical efficiency. The designed asymmetric predictor head and an InfoNCE warm-up strategy enhance the robustness to hyper-parameters and benefit the resulting performance. Experimental results show that our proposed ReSSL substantially outperforms the state-of-the-art methods across different network architectures, including various lightweight networks (e.g., EfficientNet and MobileNet).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>R ECENTLY, self-supervised learning (SSL) has shown its superiority and achieved promising results for unsupervised visual representation learning in computer vision tasks <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b50">[50]</ref>, <ref type="bibr" target="#b68">[68]</ref>, <ref type="bibr" target="#b78">[78]</ref>. The purpose of a typical self-supervised learning algorithm is to learn general visual representations from a large amount of data without human annotations, which can be transferred or leveraged in downstream tasks (e.g., classification, detection, and segmentation). Some previous works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b35">[35]</ref> even have proven that a good unsupervised pretraining can lead to a better downstream performance than supervised pretraining.</p><p>Among various SSL algorithms, contrastive learning <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b74">[74]</ref>, <ref type="bibr" target="#b78">[78]</ref> serves as a state-of-the-art framework, which mainly focuses on learning an invariant feature from different views. For example, instance discrimination is a widely adopted pre-text task as in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b78">[78]</ref>, which utilizes the noisy contrastive estimation (NCE) to encourage two augmented views of the same image to be pulled closer on the embedding space but pushes apart all the other images away. Deep Clustering <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b81">[81]</ref> is an alternative pretext task that forces different augmented views of the same instance to be clustered into the same class. However, in- stance discrimination based methods will inevitably induce a class collision problem <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b54">[54]</ref>, where similar images should be pulled closer instead of being pushed away.</p><formula xml:id="formula_0">? Mingkai</formula><p>Deep clustering based methods cooperated with traditional clustering algorithms to assign a label for each instance, which relaxed the constraint of instance discrimination, but most of these algorithms adopt a strong assumption, i.e., the labels must induce an equipartition of the data, which might introduce some noise and hurt the learned representations.</p><p>In this paper, we introduce a novel Relational Self-Supervised Learning framework (ReSSL), which does not encourage explicitly to push away different instances, but uses relation as a manner to investigate the inter-instance relationships and highlight the intra-instance invariance. Concretely, we aim to maintain the consistency of pairwise similarities among different instances for two different augmentations. For example, if we have three instances x 1 , x 2 , y and z where x 1 , x 2 are two different augmentations of x, y, and z is a different sample. Then, if x 1 is similar to y but different to z, we wish x 2 can maintain such relationship and vice versa. In this way, the relation can be modelled as a similarity distribution between a set of augmented images, and then use it as a metric to align the same images with different augmentations, so that the relationship between different instances could be maintained across different augmented views.</p><p>However, this simple manner induces unexpectedly horrible performance if we follow the same training recipe as other contrastive learning methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b36">[36]</ref>. We argue that construction of a proper relation matters for ReSSL; aggressive data augmentations as in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b69">[69]</ref> are usually leveraged by default to generate diverse positive pairs that increase the difficulty of the pre-text task. However, this hurts the reliability of the target relation. Views generated by aggressive augmentations might cause the loss of semantic information, so the target relation might be noisy and not that reliable. In this way, we propose to leverage weaker augmentations to represent the relation, since much lesser disturbances provide more stable and meaningful relationships between different instances. Besides, we also sharpen the target distribution to emphasize the most important relationship and utilize the memory buffer with a momentumupdated network to reduce the demand of large batch size for more efficiency. With such simple relation metric, our ReSSL <ref type="bibr" target="#b89">[89]</ref> achieves 69.9% Top-1 linear evaluation accuracy with 200 epochs ImageNet pretraining, which is 2.4% better than our baseline (MoCo V2).</p><p>To further improve ReSSL <ref type="bibr" target="#b89">[89]</ref>, we leverage an asymmetric structure as in BYOL <ref type="bibr" target="#b35">[35]</ref>, and SimSiam <ref type="bibr" target="#b11">[12]</ref> which adopts an additional prediction head on top of the projector. We show that breaking the symmetry could improve the performance and make the system robust to selecting the hyper-parameters. Moreover, we found that the target relation is not very reliable in the early stage of the training. To resolve this issue, we propose a warm-up with InfoNCE strategy, which adopts the InfoNCE from the beginning of the training process, and gradually shifts the objective to our relational metric along with the training. Note the InfoNCE objective will introduce clear positive and negative pairs, which we do not encourage in this paper. However, it can provide better guidance in the early stage of the training. In this way, the model will first perform an instance discrimination task and then gradually relax the constrain and pay more attention to maintain the relationships among the samples. With the predictor and Warm-up strategy, the linear evaluation accuracy of ReSSL can be further boosted from 69.9% to 72.0%. Notabaly, when working with the Multi-Crop strategy (200 epochs), ReSSL achieved new state-of-the-art performance 76.0% Top-1 accuracy, which is 2.7% higher than CLSA-Multi <ref type="bibr" target="#b75">[75]</ref>.</p><p>On the other hand, most aforementioned contrastive learning algorithms <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref> only works for large networks (e.g.ResNet50 <ref type="bibr" target="#b38">[38]</ref> or even larger) but has unsatisfying performance on small networks. For instance, the linear evaluation accuracy on ImageNet using MoCo V2 is only about 42.2% with EfficientNet-B0 <ref type="bibr" target="#b65">[65]</ref>, which is 34.9% lower than its supervised performance 77.1%. With MobileNet-V3-Large <ref type="bibr" target="#b41">[41]</ref>, the performance gaps become even larger (36.3% vs 75.2%). To resolve this issue, some knowledge distillation (KD) methods has been proposed to transfer the pretrained SSL features from a large network to a small network. In this paper, we show that ReSSL is very friendly to lightweight architectures since the linear evaluation performance can directly surpass previous KD algorithms (e.g.SEED <ref type="bibr" target="#b29">[30]</ref>, DisCo <ref type="bibr" target="#b31">[32]</ref> and BINGO <ref type="bibr" target="#b80">[80]</ref>) without a large teacher. For example, with 200 epochs of pre-training and multi-crops strategy, ReSSL achieves 66.5% / 71.0% Top-1 linear evaluation accuracy with ResNet18 / ResNet34. This result is 8.6% / 12.5% higher than SEED, 5.9% / 8.5% higher than DisCo, and 4.1% / 7.5% better than BINGO. (All the KD methods has a ResNet-50 teacher with 67.5% Top-1 Accuracy.)</p><p>Our contributions can be summarized as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We proposed a novel SSL paradigm, which we term it as relational self-supervised learning (ReSSL). ReSSL maintains the relational consistency between the instances under different augmentations instead of explicitly pushing different instances away.</p><p>? Our proposed weak augmentation and sharpening distribution strategy provide a stable and high quality target similarity distribution, which makes the framework works well.</p><p>? ReSSL is a simple and effective SSL framework since it simply replaces the widely adopted contrastive loss with our proposed relational consistency loss. With a simple modification on MoCo, ReSSL achieved state-of-the-art performance with much lesser training cost.</p><p>? ReSSL is a lightweight friendly framework. Unlike previous contrastive learning algorithms that have a poor performance on small architectures, ReSSL consistently improves the performance for various lightweight networks and achieves better performance than KD algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Self-Supervised Learning with Pretext Tasks. Selfsupervised learning is a general representation learning framework that relies on the pretext tasks that can be formulated using only unlabeled data. The concept is "Obtain labels from the data itself by using a semi-automatic process" and "Predict part of the data from other parts" <ref type="bibr" target="#b57">[57]</ref>. For this reason, many previous works focused on how to utilize the structural features of the images to design better pretext tasks that can learn more general representations. Many pretext tasks were found to be conducive to learn image features. For example, context prediction <ref type="bibr" target="#b21">[22]</ref>, jigsaw puzzles <ref type="bibr" target="#b62">[62]</ref>, and rotation angle prediction <ref type="bibr" target="#b32">[33]</ref>. These methods can learn desired and transferable representations that achieve promising results in downstream tasks. Some of these tasks can even be used as an auxiliary task to improve the performance of the semi-supervised and supervised learning <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b53">[53]</ref>, <ref type="bibr" target="#b85">[85]</ref>. Generative Based Model. Image generation is always one of the most popular idea for self-supervised learning. Early generative base model aims to encode the images to a latent vector and then decode it to reconstruct the original images <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b47">[47]</ref>. With the development of GAN <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b33">[34]</ref>, some works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b23">[24]</ref> start to leverage the power of the discriminator to jointly differentiate both images and latent vectors. In this way, the generator not only maps latent samples to generated data, but also has an inverse mapping from data to the latent representation. There are also some works aims to reconstruct the original image from the corrupted image, e.g.image colorization <ref type="bibr" target="#b86">[86]</ref>, superresolution <ref type="bibr" target="#b52">[52]</ref>, and image inpainting <ref type="bibr" target="#b26">[27]</ref>.</p><p>Instance Discrimination. The recent contrastive learning methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr" target="#b50">[50]</ref>, <ref type="bibr" target="#b60">[60]</ref>, <ref type="bibr" target="#b68">[68]</ref>, <ref type="bibr" target="#b69">[69]</ref> have made a lot of progress in the field of self-supervised learning. Most of the previous contrastive learning methods are based on the instance discrimination <ref type="bibr" target="#b78">[78]</ref> task in which positive pairs are defined as different views of the same image, while negative pairs are formed by sampling views from different images. SimCLR <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> shows that image augmentation (e.g.Grayscale, Random Resized Cropping, Color Jittering, and Gaussian Blur), nonlinear projection head and large batch size plays a critical role in contrastive learning. Since large batch size usually requires a lot of GPU memory, which is not very friendly to most of researchers. MoCo <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b36">[36]</ref> proposed a momentum contrast mechanism that forces the query encoder to learn the representation from a slowly progressing key encoder and maintain a memory buffer to store a large number of negative samples, it achieves better performance with smaller batch size. NNCLR <ref type="bibr" target="#b25">[26]</ref> reverses the purpose of the memory buffer, where the goal is to find the most similar samples from the memory buffer and treat it as the positive pair to further increase the difficulty of the pretext task. InfoMin <ref type="bibr" target="#b69">[69]</ref> proposed a set of stronger augmentation that reduces the mutual information between views while keeping task-relevant information intact. Alig-nUniform <ref type="bibr" target="#b74">[74]</ref> shows that alignment and uniformity are two critical properties of contrastive learning.</p><p>Deep Clustering. In contrast to instance discrimination which treats every instance as a distinct class, deep clustering <ref type="bibr" target="#b5">[6]</ref> adopts the traditional clustering method (e.g.KMeans) to label each image iteratively. Eventually, similar samples will be clustered into the same class. Simply apply the KMeans algorithm might lead to a degenerate solution where all data points are mapped to the same cluster; SeLa <ref type="bibr" target="#b81">[81]</ref> solved this issue by adding the constraint that the labels must induce equipartition of the data and proposed a fast version of the Sinkhorn-Knopp to achieve this. SwAV <ref type="bibr" target="#b6">[7]</ref> further extended this idea and proposed a scalable online clustering framework by maintaining a set of prototypes and assign the samples equally to each prototype. DINO <ref type="bibr" target="#b7">[8]</ref> is another types of clustering framework which has very similar objective. It replace the Sinkhorn-Knopp with the sharpning and centering tricks, where the sharpning aims to minimize the entropy so that each sample will be assigned into a prototype, centering aims to maximize the entropy to prevent all samples assigned into one prototype.</p><p>Class Collision Problem PCL <ref type="bibr" target="#b54">[54]</ref> reveals the class collision problem in contrastive learning, since instance discrimination treats every sample as a single class, the similar samples will be undesirably pushed apart by the contrastive loss. PCL solved this problem by performing the Expectation-Maximization algorithm iteratively and performing the contrastive loss and clustering loss simultaneously, although it gets the same linear classification accuracy with MoCo V2 <ref type="bibr" target="#b10">[11]</ref>, it has better performance on downstream tasks. CLD <ref type="bibr" target="#b76">[76]</ref> proposed a very similar idea where the only difference is that the clustering process will be applied on the fly in each batch. WCL <ref type="bibr" target="#b88">[88]</ref> proposed a novel clustering algorithm by generate the connected component from a nearest neighbor graph. Different from the clustering based method, AdpCLR <ref type="bibr" target="#b87">[87]</ref> and FNC <ref type="bibr" target="#b44">[44]</ref> directly finds the top-K closest instances on the embedding space and treats these instances as their positive pairs to learn a much more compact representation for similar samples.</p><p>Contrastive Learning Without Negative Samples. Most previous contrastive learning methods prevent the model collapse in an explicit manner (e.g. push different instances away from each other or force different instances to be clustered into different groups.) BYOL <ref type="bibr" target="#b35">[35]</ref> can learn a high-quality representation without negatives. Specifically, it trains an online network to predict the target network representation of the same image under a different augmented view and uses an additional predictor network on top of the online encoder to avoid the model collapse.</p><p>SimSiam <ref type="bibr" target="#b11">[12]</ref> shows that simple Siamese networks can learn meaningful representations even without the use of negative pairs, large batch size, and momentum encoders. AdCo <ref type="bibr" target="#b42">[42]</ref> shows that a set of negative features can replace the negative samples, and these features can be adversarially learned by maximizing the contrastive loss. Barlow Twins <ref type="bibr" target="#b84">[84]</ref> proposed a redundancy-reduction principle that aims to make the cross-correlation matrix computed from twin representations as close to the identity matrix as possible. This objective can be regarded as a dual form of CL, since CL aims to decouple the features for each instance, and Barlow Twins aims to decouple the feature dimensions.</p><p>Contrastive Learning for Lightweight Architectures. Although contrastive learning has achieved remarkable performance for large networks (e.g.ResNet50 or larger), however, most of these methods have a horrible performance on lightweight architecture since it is pretty hard for smaller models to learn instance-level discriminative representation with a large amount of data. SEED <ref type="bibr" target="#b29">[30]</ref> first proposed a knowledge distillation (KD) framework for contrastive learning; it requires the student network to mimic the similarity distribution inferred by the teacher over the memory buffer. DisCo <ref type="bibr" target="#b31">[32]</ref> introduced a different KD method which aims to constrain the last embedding of the student to be consistent with that of the teacher; it also alleviates the distilling bottleNeck problem and presents a large projection head can increase the capability of the model generalization. BINGO <ref type="bibr" target="#b80">[80]</ref> utilized the teacher network to construct a group of similar instances; the objective is to aggregate compact representations over the student with respect to instances in the same group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we will first revisit the preliminary works on contrastive learning; then, we will introduce our proposed relational self-supervised learning framework (ReSSL). After that, the algorithm and the implementation details will also be explained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries on Self-supervised Learning</head><p>Given N unlabeled samples x, we randomly apply a composition of augmentation functions T (?) to obtain two different views x 1 and x 2 through T (x, ? 1 ) and T (x, ? 2 ) where ? is the random seed for T . Then, a convolutional neural network based encoder F(?) is employed to extract the information from these samples, i.e., h = F(T (x, ?)). Finally, a two-layer non-linear projection head g(?) is utilized to map h into embedding space, which can be written as: z = g(h). SimCLR <ref type="bibr" target="#b8">[9]</ref> and MoCo <ref type="bibr" target="#b36">[36]</ref> style framework adopt the noise contrastive estimation (NCE) objective for discriminating different instances in the dataset. Suppose z 1 i and z 2 i are the representations of two augmented views of x i and z k is a different instance. The NCE objective can be expressed by Eq. (1), where ? is the temperature parameter. Noted that we take the L 2 normalized vectors for representation z by default. <ref type="figure">Fig. 1</ref>. The overall framework of our proposed method. We adopt the student-teacher framework where the student is trained to predict the representation of the teacher, and the teacher is updated with a "momentum update" (exponential moving average) of the student. The relationship consistency is achieve by align the conditional distribution for student and teacher model. Please see more details in our method part.</p><formula xml:id="formula_1">L N CE = ? log exp(z 1 ? z 2 /? ) exp(z 1 i ? z 2 i /? ) + N k=1 exp(z 1 i ? z k /? ) . (1) 2 1 Contrastive Augmentation 2 = ? t ( 2 ) 1 = ? s ( 1 ) Weak Augmentation Exponential Moving Average Memory Buffer ? t ( 2 ) ? s ( 1 ) z 2 z Append SoftMax SoftMax Target No Gradient Backprop Sharper q(z) z 1</formula><p>BYOL <ref type="bibr" target="#b35">[35]</ref> and SimSiam <ref type="bibr" target="#b11">[12]</ref> style framework add an additional non-linear predictor head q(?) which further maps z to p. The model will minimize the negative cosine similarity (equivalent to minimize the L2 distance) between z to p.</p><formula xml:id="formula_2">L cos = ? p 1 p 1 ? z 2 z 2 , L mse = p 1 ? z 2 2 2 . (2)</formula><p>Tricks like stop-gradient and momentum teacher are often applied to avoid model collapsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relational Self-Supervised Learning</head><p>In classical self-supervised learning, different instances are to be pushed away from each other, and augmented views of the same instance is expected to be of exactly the same features. However, both constrains are too restricted because of the existence of similar samples and the distorted semantic information if aggressive augmentation is adopted. In this way, we do not encourage explicit negative instances (those to be pushed away) for each instance; instead, we leverage the pairwise similarities as a manner to explore their relationships. And we pull the features of two different augmentations in this sense of relation metric. As a result, our method relaxes both <ref type="formula">(1)</ref> and <ref type="formula">(2)</ref>, where different instances do not always need to be pushed away from each other; and augmented views of the same instance only need to share the similar but not exactly the same features. Concretely, given a image x in a batch of samples , two different augmented views can be obtained by x 1 = T (x, ? 1 ), x 2 = T (x, ? 2 ) and calculate the corresponds embedding z 1 = q(g(F(x 1 ))), z 2 = g(F(x 2 )). Then, we calculate the similarities between the instances of the first augmented images. Which can be measured by z 1 ? z i . A softmax layer can be adopted to process the calculated similarities, which then produces a relationship distribution:</p><formula xml:id="formula_3">p 1 i = exp(z 1 ? z i /? s ) K k=1 exp(z 1 ? z k /? s ) .<label>(3)</label></formula><p>where ? s is the temperature parameter. At the same time, we can calculate the relationship between x 2 and the i-th instance as z 2 ? z i . The resulting relationship distribution can be written as:</p><formula xml:id="formula_4">p 2 i = exp(z 2 ? z i /? t ) K k=1 exp(z 2 ? z k /? t ) .<label>(4)</label></formula><p>where ? t is a different temperature parameter. We propose to push the relational consistency between p 1 i and p 2 i by minimizing the Kullback-Leibler divergence, which can be formulated as:</p><formula xml:id="formula_5">L relation = D KL (p 2 ||p 1 ) = H(p 2 , p 1 ) ? H(p 2 ). (5)</formula><p>Since the H(p 2 ) will be a constant value, we only minimize H(p 2 , p 1 ) in our implementation.</p><p>More efficiency with Momentum targets. However, the quality of the target similarity distribution p 2 is crucial, to make the similarity distribution reliable and stable, we usually require a large batch size which is very unfriendly to GPU memories. To resolve this issue, we utilize a "momentum update" network as in <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b36">[36]</ref>, and maintain a large memory buffer Q of K past samples {z k |k = 1, ..., K} (following the FIFO principle) for storing the feature embeddings from the past batches, which can then be used for simulating the large batch size relationship and providing a stable similarity distribution.</p><formula xml:id="formula_6">F t ? mF t + (1 ? m)F s , g t ? mg t + (1 ? m)g s ,<label>(6)</label></formula><p>where F s and g s denote the most latest encoder and head, respectively, so we name them as the student model with a subscript s. On the other hand, F t and g t stand for ensembles of the past encoder and head, respectively, so we name them as the teacher model with a subscript t. m represents the momentum coefficient which controls how fast the teacher F t will be updated.</p><p>Sharper Distribution as Target. Note, the value of ? t has to be smaller than ? s since ? t will be used to generate the target distribution. A smaller ? will result in a "sharper" distribution which can be interpreted as highlighting the most similar feature for z 2 , since the similar features will have a higher value and the dissimilar features will have a lower value. Align p 1 with p 2 can be regarded as pulling z 1 towards the features that are similar with z 2 . In this way, ReSSL makes the similar samples more similar, and dissimilar ones be more dissimilar.</p><p>Weak Augmentation Strategy for Teacher. To further improve the quality and stability of the target distribution, we adopt a weak augmentation strategy for the teacher model since the standard contrastive augmentation is too aggressive, which introduced too many disturbances and will mislead the student network. Please refer to more details in our empirical study.</p><p>Compare with SEED and CLSA. SEED <ref type="bibr" target="#b29">[30]</ref> follows the standard Knowledge Distillation (KD) paradigm <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b82">[82]</ref> where it aims to distill the knowledge from a larger network into a smaller architecture. The knowledge transfer happens in the same view but between different models. In our framework, we are trying to maintain the relational consistency between different augmentations; the knowledge transfer happens between different views but in the same network. CLSA <ref type="bibr" target="#b75">[75]</ref> also introduced the concept of using weak augmentation to guide a stronger augmentation. However, the "weak" augmentation in CLSA is equivalent to the "strong" augmentation in our method (We do not use any stronger augmentations such as <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> in our default setting). On the other hand, CLSA requires at least one additional sample during training, which requires a lot more GPU memories and slow down the training speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Improving ReSSL with Predictor and InfoNCE</head><p>Asymmetric Head for Student. For the student network, we adopt an additional predictor head as in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b35">[35]</ref>, which breaks the symmetry of the Siamese network. <ref type="bibr" target="#b11">[12]</ref> demonstrates that the typical contrastive learning frameworks are just an Expectation-Maximization like algorithm; the predictor head helps fill the gaps between an augmented view and the expectation. In ReSSL, Breaking the symmetry could make the system become robuster to the selection of the temperature, and also slightly improve the performance. Please see more empirical analysis in Section 4.</p><p>Warm-up with InfoNCE. As we have described that the sharpening strategy is a crucial point in ReSSL, it can be interpreted as pulling z 1 towards to features that are similar with z 2 . However, the network is not able to capture highquality similar samples especially in the early stage of the training, which makes the target distribution p 2 unreliable. To resolve this issue, we add an additional InfoNCE loss to cooperate with our relational consistency loss, which can be expressed by Eq <ref type="formula" target="#formula_7">(7)</ref> L total = ?L relation + (1 ? ?)L Inf oN CE</p><p>Where ? is the hyper-parameter to control the weight of the two losses. Here, we set ? = current step/warmup step.</p><p>In this case, ? will increase from 0 to 1 along our optimization step. Basically, z 1 will be pulled towards to z 2 in the early stage since p 2 is not reliable, then, the objective function will gradually shit from Eq. (1) to Eq. (5). Although InfoNCE still introduces the clear positives and negatives, the weight will decrease along with the training. Experimental results shows that warm-up with InfoNCE significantly improves the results of the linear evaluation. Please see more details in Section 4 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EMPIRICAL STUDY OF RESSL</head><p>In this section, we will empirically study our ReSSL on 4 popular self-supervised learning benchmarks. Specifically, we ablate the effectiveness of the sharpening, weak augmentation, predictor, and the InfoNCE warm-up strategy. Small Dataset. CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b49">[49]</ref>. The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. CIFAR-100 is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class.</p><p>Medium Dataset. STL-10 <ref type="bibr" target="#b16">[17]</ref> and Tiny ImageNet <ref type="bibr" target="#b51">[51]</ref>. STL10 <ref type="bibr" target="#b16">[17]</ref> dataset is composed of 96x96 resolution images with 10 classes. It contains 5K labeled training images, 8K validation images, and 100K unlabeled images. The Tiny ImageNet dataset is composed of 64x64 resolution images with 200 classes, which has 100K training images and 10k validation images.</p><p>Implementation Details We adopt the ResNet18 <ref type="bibr" target="#b38">[38]</ref> as our backbone network. Because most of our dataset contains low-resolution images, we replace the first 7x7 Conv of stride 2 with 3x3 Conv of stride 1 and remove the first max pooling operation for a small dataset. For data augmentations, we use the random resized crops (the lower bound of random crop ratio is set to 0.2), color distortion (strength=0.5) with a probability of 0.8, and Gaussian blur with a probability of 0.5. The images from the small and medium datasets will be resized to 32x32 and 64x64 resolution respectively. Our method is based on MoCoV2 <ref type="bibr" target="#b10">[11]</ref>; we use the global BN and shuffle BN for with and without predictor setting. The momentum value and memory buffer size are set to 0.99 and 4096/16384 for small and medium datasets respectively. Moreover, The model is trained using SGD optimizer with a momentum of 0.9 and weight decay of 5e ?4 . We linear warm up the learning rate for 10 epochs until it reaches 0.06 ? BatchSize/256, then switch to the cosine decay scheduler <ref type="bibr" target="#b58">[58]</ref>.</p><p>Evaluation Protocol. All the models will be trained for 200 epochs. For testing the representation quality, we evaluate the pre-trained model on the widely adopted linear evaluation protocol -We will freeze the encoder parameters and train a linear classifier on top of the average pooling features for 100 epochs. To test the classifier, we use the center crop of the test set and computes accuracy according to predicted output. We train the classifier with a learning rate of 10, no weight decay, and momentum of 0.9. The learning rate will be times 0.1 in 60 and 80 epochs. Note, for STL-10; the pretraining will be applied on both labeled and unlabeled images. During the linear evaluation, only the labeled 5K images will be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A Properly Sharpened Relation is A Better Target</head><p>The temperature parameter is very crucial in most contrastive learning algorithms. To verify the effective of ? s and ? t for our proposed method, we fixed ? s = 0.1 or 0.2, and sweep over ? t = {0.01, 0.02, ..., 0.10}. The result is shown in <ref type="table">Table 1</ref>. Note, here we show the result for both with and without predictor setting. For ? t , the optimal value is 0.03 and 0.04 across all different datasets. As we can see, the performance is increasing when we increase ? t from 0. After the optimal value, the performance will start to decrease. Note, ? t ? 0 correspond to the Top-1 or argmax operation, which produce a one-hot distribution as the target; this actually makes our relational consistent loss degrades to a standard InfoNCE objective where the positive pair is the most similar sample from the memory buffer instead of the augmentation of the original image. In this case, the system will be pretty similar with NNCLR <ref type="bibr" target="#b25">[26]</ref>, the only difference is that NNCLR does not adopt the weak augmentation strategy, and the negative samples come from the current batch instead of the memory buffer. On the other hand, when ? t ? 0.1, the target will be a much flatter distribution that cannot highlight the most similar features for students. Hence, ? t can not be either too small or too large, but it has to be smaller than ? s (i.e.p 2 has to be sharper than p 1 ), therefore the target distribution can provide effective guidance to the student network.</p><p>For ? t , it is clearly to see that the result of ? s = 0.1 can always result a much higher performance than ? s = 0.2, which is different to MoCoV2 where ? s = 0.2 is the optimal value. According to <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b71">[71]</ref>, <ref type="bibr" target="#b73">[73]</ref>, a greater temperature will result in a larger angular margin in the hypersphere. Since MoCoV2 adopts instance discrimination as the pretext task, a large temperature can enhance the compactness for the same instance and discrepancy for different instances. In contrast to instance discrimination, our method can be interpreted as pulling similar instances closer on the hypersphere; the similar instances might not be very reliable, especially in the early stage of the training process. Thus, the large angular margin might hurt the performance when the ground truth label is not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Asymmetric Structure Makes Robuster System</head><p>Our default setting adopts the predictor structure as in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b35">[35]</ref> which breaks the symmetry of the system. It is however interesting to evaluate the performance with the typically symmetric system by removing the predictor structure from ReSSL. Thus, we follow the same experiment setting and present the result in <ref type="table">Table 2</ref>. As we can observe that in the case of ? s = 0.1 and ? t ? 0.08 (the gray numbers), the symmetric system will collapse since the network can simply output a constant vector to minimize our loss function. Although ReSSL also works well without the predictor, we would like to keep it since it makes the system more robust to the temperature selection. We can also notice that the performance will be slightly better with the predictor in various temperature settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Working with InfoNCE Warm-up</head><p>Although a stand-alone relation metric already works well, the obvious problem is that the teacher network cannot capture high-quality relationships among different instances in the early stage of the training process. We addressed this issue by including the InfoNCE during the warmup stage, as described in Section 3.3. We show the result in <ref type="table" target="#tab_2">Table 3</ref> below. As can be seen, the warm-up strategy significantly improves the performance on CIFAR-100, STL-10, and TinyImageNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Weak Augmentation Makes Better Relation</head><p>As we have mentioned, the weaker augmentation strategy for the teacher model is the key to the success of our framework. Here, We implement the weak augmentation as a random resized crop (the random ratio is set to <ref type="figure">(0.2, 1)</ref>) and a random horizontal flip. For temperature parameter, we simply adopt the same setting as in <ref type="table">Table 1</ref> and report the performance of the best setting. The result is shown in <ref type="table" target="#tab_4">Table 5</ref>, as we can see that when we use the weak augmentation for the teacher model, the performance is significantly boosted across all datasets. We believe that this phenomenon is because relatively small disturbances in the  <ref type="figure">Fig. 2</ref>. Visualization of the 10 nearest neighbour of the query image. The top half is the result when we apply the weak augmentation. The bottom half is the case when the typical contrastive augmentation is adopted. Note, we use the red square box to highlight the images that has different ground truth label with the query image.</p><p>teacher model can provide more accurate similarity guidance to the student model. To further verify this hypothesis, we random sampled three image from STL-10 training set as the query images, and then find the 10 nearest neighbour based on the weak / contrastive augmented query. We visualized the result in <ref type="figure">Figure 2</ref>, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">More Experiments on Weak Augmentation</head><p>Since the weak augmentation for the teacher model is one of the crucial points in ReSSL, we further analyze the effect of applying different augmentations on the teacher model. In this experiment, we simply adopt the best temperature setting from <ref type="table">Table 1</ref> for each datasets and report the linear evaluation performance across the four benchmark dataset. The results are shown in <ref type="table" target="#tab_3">Table 4</ref>. The first row is the baseline, where we simply resize all images to the same resolution (no extra augmentation is applied). Then, we applied random resized crops, random flip, color jitter, grayscale, gaussian blur, and various combinations. We empirically find that if we use no augmentation (e.g., no random resized crops) for the teacher model, the performance tends to degrade. This might result from that the gap of features between two views is way too smaller, which undermines the learning of representations. However, too strong augmentations of teacher model will introduce too much noise and make the target distribution inaccurate (see <ref type="figure">Figure 2</ref>). Thus mildly weak augmentations are better option for the teacher, and random resized crops with random flip is the combination with the highest performance as <ref type="table" target="#tab_3">Table 4</ref> shows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Dimension of the Relation</head><p>Since we also adopt the memory buffer as in MoCo <ref type="bibr" target="#b36">[36]</ref>, the buffer size will be equivalent to the dimension of the distribution p 1 p 2 . Thus, it will be one of the crucial points in our framework. To verify the effect of the memory buffer size, we simply adopt the best temperature setting from <ref type="table">Table 1</ref> for each dataset, then varying the memory buffer size from 256 to 32768. The result is shown in <ref type="table" target="#tab_5">Table 6</ref>, as we can see that the performance could be significantly improved when we increase the buffer size from 256 to 8192. However, a further increase in the buffer size (i.e.16384) can only bring a marginal improvement when the buffer is large enough. We can also observe that the performance will be slightly worse when K = 32768. This is possibly due to a too large memory buffer involving a lot of stale embeddings, which hurts the performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Visualization of Learned Representations</head><p>We also show the t-SNE <ref type="bibr" target="#b70">[70]</ref> visualizations of the representations learned by ReSSL with contrastive augmentation, ReSSL with weak augmentation, and MoCo V2 on the test set of CIFAR-10. Obviously, ReSSL with weak augmentation leads to better class separation than the contrastive loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results on Large-scale Datasets</head><p>We also performed our algorithm on the large-scale ImageNet-1k dataset <ref type="bibr" target="#b19">[20]</ref>. In the experiments, we use a batch size of 1024, and adopt the LARS <ref type="bibr" target="#b83">[83]</ref> optimizer with learning rate equals to 0.6 * BatchSize/256, momentum 0.9, and weight decay 1e ?6 . The optimizer will be scheduled by a cosine learning rate scheduler <ref type="bibr" target="#b58">[58]</ref> with 10 epochs of Warm-up. We also adopt a memory buffer to save 65536 past examples. The momentum coefficient is m = 0.996 and increases to 1 with a cosine schedule. For the projector, we use a 2-layer non-linear MLP network with BN and ReLU; the hidden dimension is 4096, and the output dimension is 256. The predictor consists of the same structure as the projector, but input dimension is 256. For ? t and ? s , we simply adopt the best setting from <ref type="table">Table 1</ref> where ? t = 0.03 and ? s = 0.1.</p><p>For contrastive augmentation, we take the strategy from <ref type="bibr" target="#b35">[35]</ref> with a few modifications on the parameters. Specifically, we set the probability of blurring / solarization to 50% / 10%, and also change the minimum crop area to 14%. For weak augmentation, we follow the best setting in <ref type="table" target="#tab_3">Table 4</ref>. Linear Evaluation. For the linear evaluation of ImageNet-1k, we follow the same setting in SimCLR <ref type="bibr" target="#b6">[7]</ref>. The results are shown in <ref type="table" target="#tab_6">Table 7</ref>. Specifically, we train the linear classification layer for 90 epochs using a SGD optimizer with a Nesterov momentum value of 0.9, weight decay of 0, learning rate with 0.1 * BatchSize/256, and a cosine-annealed learning rate scheduler. The result is shown in <ref type="table" target="#tab_6">Table 7</ref>. For a fair comparison, we divide previous methods into 1x backprop and 2x backprop settings since 2x backprop methods generally require more GPU memories and longer training. As we can see clearly that ReSSL consistently outperforms previous methods on both setting. For 1x setting, ReSSL surpass previous state-of-the-art (InfoMin) 1.9%. For 2x setting, ReSSL has 0.4% improvements over MoCo v3 with only 2/3 training costs and smaller batch size. We also report the performance when working without the predictor and InfoNCE warm-up. In this case, the performance drops 2.1% and 1.8% for 1x and 2x setting, respectively, which further verified the effectiveness of these two strategies. Working with Multi-Crops and Stronger Augmentations. We also performed ReSSL with multi-Crop strategy and stronger augmentations. Specifically, we follow the strategy from <ref type="bibr" target="#b42">[42]</ref> which adopts 5 augmented views with the resolution of 224 ? 224, 192 ? 192, 160 ? 160, 128 ? 128, 96 ? 96, and set the min / max crop area to (0.14, 0.117, 0.095, 0.073, 0.05) / (1.0, 0.86, 0.715, 0.571, 0.429) for the 5 views respectively, the rest of augmentation strategy is consistent with our 1x and 2x setting. The result is shown in <ref type="table" target="#tab_7">Table 8</ref>. Notably, with 200 epochs of training, ReSSL achieves 76.0% Top-1 accuracy on ImageNet, which is signif- Transfer learning on linear classification performance using ResNet-50 pretrained with ImageNet. The performance of SimCLR, BYOL, and NNCLR are directly copy from the original paper. Following the evaluation protocol from <ref type="bibr" target="#b8">[9]</ref>, we report Top-1 accuracy except Pets, Flowers, and Caltech101 for which we report mean per-class accuracy. ReSSL outperforms previous methods on 6 out of the 9 datasets. icantly better than previous methods with even 800 or 1000 epochs of training. We have tried to train our ReSSL with more epochs, but we found that the performance appears to saturate from 200 epochs, demonstrating that ReSSL is easier to converge than other methods. Meanwhile, we also explored the effective of stronger augmentation to ReSSL. Concretely, we follow the same augmentation setting in <ref type="bibr" target="#b75">[75]</ref> and optimize ReSSL for 200 epoch. In this case, ReSSL achieves 76.3% Top-1 accuracy which is almost on pair with the supervised baseline (76.5%). Transfer learning on Linear Classification. We show representations learned by ReSSL are effective for transfer learning on multiple downstream classification tasks. We follow the linear evaluation setup described in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b35">[35]</ref>. Specifically, we trained an L2-regularized multinomial logistic regression classifier on features extracted from the frozen pretrained network, then we used L-BFGS <ref type="bibr" target="#b56">[56]</ref> to optimize the softmax cross-entropy objective, we did not apply data augmentation. We selected the best L2-regularization parameter and learning rate from validation splits and applied it to the test sets. The datasets used in this benchmark are as follows: CIFAR-10 <ref type="bibr" target="#b49">[49]</ref>, CIFAR-100 <ref type="bibr" target="#b49">[49]</ref>, Food101 <ref type="bibr" target="#b3">[4]</ref>, Cars <ref type="bibr" target="#b48">[48]</ref>, DTD <ref type="bibr" target="#b15">[16]</ref>, Oxford-IIIT-Pets <ref type="bibr" target="#b63">[63]</ref>, Aircrat <ref type="bibr" target="#b59">[59]</ref>, Oxford-Flowers <ref type="bibr" target="#b61">[61]</ref>, and Caltech-101 <ref type="bibr" target="#b30">[31]</ref>. We present the results in <ref type="table" target="#tab_8">Table 9</ref>. It is clearly to see that ReSSL achieves stateof-the-art performance on CIFAR-10, Food-101, Cars, DTD, Aircraft, and Caltech101, which is significantly better than SimCLR, BYOL and NNCLR. Semi-Supervised Learning. Next, we evaluate the performance obtained when fine-tuning the model representation using only 1% and 10% labeled examples. We directly adopt the labeled and unlabeled file list from <ref type="bibr" target="#b8">[9]</ref> for fair comparison. For 1%, we freeze the backbone layers and set the learning rate to 0.1 for the final linear layer. For 10%, we use a learning rate of 0.02 and 0.2 for the backbone layers and the final linear layer. The model will be optimized for 60 and 20 epochs for 1% and 10% setting with a cosine learning rate scheduler. We do not apply any weight decay in this experiments. The result has been shown in <ref type="table" target="#tab_10">Table 10</ref>  Transfer Learning on Low-shot Classification. We further evaluate the quality of the learned representations by transferring them to the low-shot classification task. Following <ref type="bibr" target="#b54">[54]</ref>, we perform linear classification on the PASCAL VOC2007 dataset <ref type="bibr" target="#b28">[29]</ref>. Specifically, we resize all images to 256 pixels along the shorter side and taking a 224 ? 224 center crop. Then, we train a linear SVM on top of corresponding global average pooled final representations. To study the transferability of the representations in few-shot scenarios, we vary the number of labeled examples k. and report the mAP. <ref type="table" target="#tab_12">Table 11</ref> shows the comparison between our method with previous works. We report the average performance over 5 runs (except for k=full). Notably, ReSSL consistently has a higher performance than other methods across different settings, and it also surpasses the supervised setting when k is greater than 64. Object Detection and Instance Segmentation. We finally evaluate the learned representations for the localization based tasks of object detection and instance segmentation on COCO <ref type="bibr" target="#b55">[55]</ref> dataset by following the experiment setting in <ref type="bibr" target="#b36">[36]</ref>. Specifically, we use the ReSSL pretrained weight to initialize the Mask R-CNN <ref type="bibr" target="#b37">[37]</ref> C4 backbone, the model will be finetuned on the COCO 2017 train split and report the results on the val split. We use a learning rate of 0.04 and keep the other parameters the same as in the default 2x schedule in detectron2 <ref type="bibr" target="#b77">[77]</ref>. The results in <ref type="table" target="#tab_1">Table 12</ref> demonstrates that our ReSSL has a competitive performance with the state-of-the-art contrastive learning methods for these localization tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on Lightweight Architectures</head><p>We also applied our ReSSL on various lightweight architectures to further demonstrate the generality. By following the experimental setting in <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b31">[32]</ref>, we performed ReSSL on ResNet-18, ResNet-34 <ref type="bibr" target="#b38">[38]</ref>, EfficientNet-B0, EfficientNet-B1 <ref type="bibr" target="#b65">[65]</ref>, and MobileNetV3-Large <ref type="bibr" target="#b41">[41]</ref>. The linear evaluation result has been shown in <ref type="table" target="#tab_2">Table 13</ref>. We first present the result for SimCLR, MoCo V2, BYOL and SwAV on ResNet-18.</p><p>Obviously, these methods have significant gaps with the supervised performance even with 800 epochs of training. Although the Knowledge distillation (KD) based methods (e.g.SEED, Compress, DisCo and BINGO) can somewhat improve the performance, our ReSSL is much more efficient since it does not require a large teacher. More importantly, the performance of ReSSL is consistently better than these KD methods especially on those extremely small networks (e.g.EfficientNet-B0 and Mobilenet-V3-Large). Apart from the linear evaluation, we also report the performance for semi-supervised learning. We follow the same training strategy as our ResNet-50 experiments. <ref type="table" target="#tab_3">Table  14</ref> shows that our ReSSL achieves promising results for various lightweight architectures and outperforms the latest self-supervised KD methods by a clear margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we propose relational self-supervised learning (ReSSL), a new paradigm for unsupervised visual representation learning framework that maintains the relational consistency between instances under different augmentations. Our proposed ReSSL relaxes the typical constraints in contrastive learning where different instances do not always need to be pushed away on the embedding space, and the augmented views do not need to share exactly the same feature. An extensive empirical study shows the effect of each component in our framework. The experiments on largescaled datasets demonstrate the efficiency and state-of-theart performance for unsupervised representation learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>t-SNE visualizations on CIFAR-10. Classes are indicated by colors. Here we show the visualization result for ReSSL with contrastive augmentation, Standard ReSSL with weak augmentation, and MoCo V2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Zheng and Chang Xu are with the School of Computer Science, Faculty of Engineering, The University of Sydney, Australia. E-mail: mzhe4001@uni.sydney.edu.au, c.xu@sydney.edu.au ? Shan You and Chen Qian are with the SenseTime Research Centre. E-mail: {youshan,qianchen}@sensetime.com ? Fei Wang is with the University of Science and Technology of China. Email: wangfei91@mail.ustc.edu.cn</figDesc><table /><note>? Shan You and Changshui Zhang are with the Department of Automa- tion, Tsinghua University, Institute for Artificial Intelligence, Tsinghua University (THUAI), Beijing National Research Center for Information Science and Technology (BNRist). E-mail: zcs@mail.tsinghua.edu.cn? Xiaogang Wang is with Department of Electronic Engineering, The Chinese University of Hong Kong. E-mail: xgwang@ee.cuhk.edu.hk</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 TABLE 2</head><label>12</label><figDesc>Effect of different ?t and ?s for ReSSL with predictor Dataset ?s ?t = 0.01 ?t = 0.02 ?t = 0.03 ?t = 0.04 ?t = 0.05 ?t = 0.06 ?t = 0.07 ?t = 0.08 ?t = 0.09 ?t = 0.10 Effect of different ?t and ?s for ReSSL without predictor.Dataset ?s ?t = 0.01 ?t = 0.02 ?t = 0.03 ?t = 0.04 ?t = 0.05 ?t = 0.06 ?t = 0.07 ?t = 0.08 ?t = 0.09 ?t = 0.10</figDesc><table><row><cell>CIFAR-10</cell><cell>0.1</cell><cell>89.72</cell><cell>90.16</cell><cell>90.62</cell><cell>90.76</cell><cell>90.46</cell><cell>90.21</cell><cell>89.95</cell><cell>89.67</cell><cell>89.61</cell><cell>89.25</cell></row><row><cell>CIFAR-10</cell><cell>0.2</cell><cell>90.18</cell><cell>90.42</cell><cell>90.42</cell><cell>89.91</cell><cell>89.97</cell><cell>90.00</cell><cell>89.84</cell><cell>89.19</cell><cell>88.88</cell><cell>88.63</cell></row><row><cell>CIFAR-100</cell><cell>0.1</cell><cell>63.63</cell><cell>64.61</cell><cell>65.03</cell><cell>64.60</cell><cell>64.26</cell><cell>63.19</cell><cell>62.54</cell><cell>60.56</cell><cell>60.17</cell><cell>58.18</cell></row><row><cell>CIFAR-100</cell><cell>0.2</cell><cell>63.12</cell><cell>63.09</cell><cell>62.08</cell><cell>61.93</cell><cell>61.20</cell><cell>60.48</cell><cell>59.25</cell><cell>58.52</cell><cell>58.11</cell><cell>56.36</cell></row><row><cell>STL-10</cell><cell>0.1</cell><cell>87.82</cell><cell>88.54</cell><cell>88.89</cell><cell>88.41</cell><cell>87.60</cell><cell>87.24</cell><cell>86.87</cell><cell>84.04</cell><cell>83.67</cell><cell>83.59</cell></row><row><cell>STL-10</cell><cell>0.2</cell><cell>87.66</cell><cell>87.28</cell><cell>86.51</cell><cell>86.19</cell><cell>85.75</cell><cell>85.75</cell><cell>85.58</cell><cell>85.61</cell><cell>85.37</cell><cell>84.52</cell></row><row><cell cols="2">Tiny ImageNet 0.1</cell><cell>45.66</cell><cell>45.62</cell><cell>46.64</cell><cell>46.06</cell><cell>45.36</cell><cell>41.22</cell><cell>37.02</cell><cell>36.03</cell><cell>32.54</cell><cell>32.28</cell></row><row><cell cols="2">Tiny ImageNet 0.2</cell><cell>45.64</cell><cell>45.74</cell><cell>43.72</cell><cell>42.62</cell><cell>42.02</cell><cell>40.72</cell><cell>38.24</cell><cell>34.74</cell><cell>34.12</cell><cell>30.10</cell></row><row><cell>CIFAR-10</cell><cell>0.1</cell><cell>89.35</cell><cell>89.74</cell><cell>90.09</cell><cell>90.04</cell><cell>90.20</cell><cell>90.18</cell><cell>88.67</cell><cell>10.00</cell><cell>10.00</cell><cell>10.10</cell></row><row><cell>CIFAR-10</cell><cell>0.2</cell><cell>89.52</cell><cell>89.67</cell><cell>89.24</cell><cell>89.50</cell><cell>89.22</cell><cell>89.40</cell><cell>89.50</cell><cell>89.58</cell><cell>89.43</cell><cell>89.43</cell></row><row><cell>CIFAR-100</cell><cell>0.1</cell><cell>62.34</cell><cell>62.79</cell><cell>62.71</cell><cell>63.79</cell><cell>63.46</cell><cell>63.20</cell><cell>61.31</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>CIFAR-100</cell><cell>0.2</cell><cell>60.37</cell><cell>60.05</cell><cell>60.24</cell><cell>60.09</cell><cell>59.09</cell><cell>59.12</cell><cell>59.76</cell><cell>59.97</cell><cell>59.69</cell><cell>59.08</cell></row><row><cell>STL-10</cell><cell>0.1</cell><cell>86.65</cell><cell>86.96</cell><cell>87.16</cell><cell>87.32</cell><cell>88.25</cell><cell>87.83</cell><cell>87.08</cell><cell>10.00</cell><cell>10.00</cell><cell>10.00</cell></row><row><cell>STL-10</cell><cell>0.2</cell><cell>85.17</cell><cell>86.12</cell><cell>85.01</cell><cell>85.67</cell><cell>85.21</cell><cell>85.51</cell><cell>85.28</cell><cell>85.93</cell><cell>85.56</cell><cell>85.58</cell></row><row><cell cols="2">Tiny ImageNet 0.1</cell><cell>45.20</cell><cell>45.40</cell><cell>46.30</cell><cell>46.60</cell><cell>45.08</cell><cell>45.24</cell><cell>44.18</cell><cell>0.50</cell><cell>0.50</cell><cell>0.50</cell></row><row><cell cols="2">Tiny ImageNet 0.2</cell><cell>43.28</cell><cell>42.98</cell><cell>43.58</cell><cell>42.12</cell><cell>42.70</cell><cell>42.76</cell><cell>42.60</cell><cell>41.46</cell><cell>41.08</cell><cell>40.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 Working</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">with InfoNCE Warm-up</cell></row><row><cell>Dataset</cell><cell cols="2">w/o Warm-up w/ Warm-up</cell></row><row><cell>CIFAR-10</cell><cell>90.76</cell><cell>89.98</cell></row><row><cell>CIFAR-100</cell><cell>65.03</cell><cell>65.33</cell></row><row><cell>STL-10</cell><cell>88.89</cell><cell>90.00</cell></row><row><cell>Tiny ImageNet</cell><cell>46.64</cell><cell>48.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc>Effect of different augmentation for teacher model Random Resized Crops Random Flip Color Jitter GrayScale Gaussian Blur CIFAR-10 CIFAR-100 STL-10 Tiny ImageNet</figDesc><table><row><cell>79.33</cell><cell>52.77</cell><cell>86.01</cell><cell>36.76</cell></row><row><cell>90.62</cell><cell>64.98</cell><cell>88.62</cell><cell>46.38</cell></row><row><cell>80.38</cell><cell>51.60</cell><cell>86.24</cell><cell>38.20</cell></row><row><cell>78.47</cell><cell>51.73</cell><cell>85.92</cell><cell>36.64</cell></row><row><cell>78.44</cell><cell>49.76</cell><cell>87.12</cell><cell>36.32</cell></row><row><cell>78.54</cell><cell>50.65</cell><cell>85.34</cell><cell>36.20</cell></row><row><cell>90.76</cell><cell>65.03</cell><cell>88.89</cell><cell>46.64</cell></row><row><cell>89.80</cell><cell>62.87</cell><cell>87.45</cell><cell>43.98</cell></row><row><cell>89.56</cell><cell>62.61</cell><cell>88.65</cell><cell>42.52</cell></row><row><cell>89.18</cell><cell>62.75</cell><cell>87.34</cell><cell>46.18</cell></row><row><cell>90.15</cell><cell>63.55</cell><cell>87.21</cell><cell>44.00</cell></row><row><cell>89.63</cell><cell>62.95</cell><cell>88.47</cell><cell>41.88</cell></row><row><cell>89.43</cell><cell>62.37</cell><cell>87.22</cell><cell>45.30</cell></row><row><cell>86.88</cell><cell>58.60</cell><cell>85.05</cell><cell>38.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5</head><label>5</label><figDesc>Effect of weak augmentation guided ReSSL</figDesc><table><row><cell>Dataset</cell><cell cols="2">Contrastive Aug Weak Aug</cell></row><row><cell>CIFAR-10</cell><cell>86.88</cell><cell>90.76</cell></row><row><cell>CIFAR-100</cell><cell>58.60</cell><cell>65.03</cell></row><row><cell>STL-10</cell><cell>85.05</cell><cell>88.89</cell></row><row><cell>Tiny ImageNet</cell><cell>38.66</cell><cell>46.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6</head><label>6</label><figDesc>Effect of different memory buffer size on small and medium dataset</figDesc><table><row><cell>K</cell><cell cols="4">CIFAR-10 CIFAR-100 STL-10 Tiny ImageNet</cell></row><row><cell>256</cell><cell>89.52</cell><cell>62.21</cell><cell>85.05</cell><cell>42.80</cell></row><row><cell>512</cell><cell>90.38</cell><cell>63.06</cell><cell>85.76</cell><cell>43.40</cell></row><row><cell>1024</cell><cell>90.37</cell><cell>64.32</cell><cell>87.10</cell><cell>44.72</cell></row><row><cell>4096</cell><cell>90.76</cell><cell>65.03</cell><cell>87.90</cell><cell>45.47</cell></row><row><cell>8192</cell><cell>90.73</cell><cell>65.65</cell><cell>87.71</cell><cell>46.34</cell></row><row><cell>16384</cell><cell>90.82</cell><cell>65.62</cell><cell>88.89</cell><cell>46.64</cell></row><row><cell>32768</cell><cell>90.27</cell><cell>65.37</cell><cell>88.60</cell><cell>45.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7</head><label>7</label><figDesc>Top-1 accuracy under the linear evaluation on ImageNet with the ResNet-50 backbone. ReSSL with * denote the results without predictor and InfoNCE warm-up.</figDesc><table><row><cell>Method</cell><cell cols="4">Backprop Batch Size Epochs Top-1</cell></row><row><cell>Supervised</cell><cell>1x</cell><cell>256</cell><cell>120</cell><cell>76.5</cell></row><row><cell>1x Backprop Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NPID [78]</cell><cell>1x</cell><cell>256</cell><cell>200</cell><cell>58.5</cell></row><row><cell>LocalAgg [90]</cell><cell>1x</cell><cell>128</cell><cell>200</cell><cell>58.8</cell></row><row><cell>MoCo V2 [11]</cell><cell>1x</cell><cell>256</cell><cell>200</cell><cell>67.5</cell></row><row><cell>MoCHi [45]</cell><cell>1x</cell><cell>512</cell><cell>200</cell><cell>68.0</cell></row><row><cell>CPC v2 [50]</cell><cell>1x</cell><cell>512</cell><cell>200</cell><cell>63.8</cell></row><row><cell>PCL v2 [54]</cell><cell>1x</cell><cell>256</cell><cell>200</cell><cell>67.6</cell></row><row><cell>AdCo [42]</cell><cell>1x</cell><cell>256</cell><cell>200</cell><cell>68.6</cell></row><row><cell>InfoMin [69]</cell><cell>1x</cell><cell>256</cell><cell>200</cell><cell>70.1</cell></row><row><cell>ISD [67]</cell><cell>1x</cell><cell>256</cell><cell>200</cell><cell>69.8</cell></row><row><cell>ReSSL * (1 Crop) [89]</cell><cell>1x</cell><cell>256</cell><cell>200</cell><cell>69.9</cell></row><row><cell>ReSSL (1 Crop)</cell><cell>1x</cell><cell>1024</cell><cell>200</cell><cell>72.0</cell></row><row><cell>2x Backprop Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CLSA-Single [75]</cell><cell>2x</cell><cell>256</cell><cell>200</cell><cell>69.4</cell></row><row><cell>SimCLR [9]</cell><cell>2x</cell><cell>4096</cell><cell>200</cell><cell>66.8</cell></row><row><cell>SimSiam [12]</cell><cell>2x</cell><cell>256</cell><cell>200</cell><cell>70.0</cell></row><row><cell>MoCo V2 -Symmetric [12]</cell><cell>2x</cell><cell>256</cell><cell>200</cell><cell>69.9</cell></row><row><cell>WCL [88]</cell><cell>2x</cell><cell>4096</cell><cell>200</cell><cell>70.3</cell></row><row><cell>NNCLR [26]</cell><cell>2x</cell><cell>256</cell><cell>200</cell><cell>70.7</cell></row><row><cell>BYOL [35]</cell><cell>2x</cell><cell>4096</cell><cell>300</cell><cell>72.5</cell></row><row><cell>Barlow Twins [84]</cell><cell>2x</cell><cell>2048</cell><cell>300</cell><cell>71.4</cell></row><row><cell>MoCo V3 [13]</cell><cell>2x</cell><cell>4096</cell><cell>300</cell><cell>72.8</cell></row><row><cell>SwAV [7]</cell><cell>2x</cell><cell>4096</cell><cell>400</cell><cell>70.1</cell></row><row><cell>W-MSE 4 [28]</cell><cell>2x</cell><cell>1024</cell><cell>400</cell><cell>72.6</cell></row><row><cell>ReSSL * (2 Crop) [89]</cell><cell>2x</cell><cell>256</cell><cell>200</cell><cell>71.4</cell></row><row><cell>ReSSL (2 Crops)</cell><cell>2x</cell><cell>1024</cell><cell>200</cell><cell>73.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="4">Working with Multi-Crops Strategy and Stronger Augmentations.</cell></row><row><cell cols="3">(Linear Evaluation on ImageNet)</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Epochs Batch Size Top-1</cell></row><row><cell>With Contrastive Augmentation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SwAV [7]</cell><cell>200</cell><cell>4096</cell><cell>72.7</cell></row><row><cell>SwAV [7]</cell><cell>800</cell><cell>4096</cell><cell>75.3</cell></row><row><cell>AdCo [42]</cell><cell>200</cell><cell>256</cell><cell>73.2</cell></row><row><cell>AdCo [42]</cell><cell>800</cell><cell>1024</cell><cell>75.7</cell></row><row><cell>WCL [88]</cell><cell>200</cell><cell>4096</cell><cell>73.3</cell></row><row><cell>WCL [88]</cell><cell>800</cell><cell>4096</cell><cell>74.7</cell></row><row><cell>FCN [44]</cell><cell>1000</cell><cell>4096</cell><cell>74.4</cell></row><row><cell>truncated triplet [72]</cell><cell>200</cell><cell>4160</cell><cell>74.1</cell></row><row><cell>DINO [8]</cell><cell>800</cell><cell>4096</cell><cell>75.3</cell></row><row><cell>NNCLR [26]</cell><cell>1000</cell><cell>4096</cell><cell>75.6</cell></row><row><cell>UniGrad [66]</cell><cell>800</cell><cell>4096</cell><cell>75.5</cell></row><row><cell>ReSSL (Multi)</cell><cell>200</cell><cell>1024</cell><cell>76.0</cell></row><row><cell>With Stronger Augmentation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CLSA-Multi [75]</cell><cell>200</cell><cell>256</cell><cell>73.3</cell></row><row><cell>CLSA-Multi [75]</cell><cell>800</cell><cell>256</cell><cell>76.2</cell></row><row><cell>ReSSL (Strong)</cell><cell>200</cell><cell>1024</cell><cell>76.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 9</head><label>9</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 10 ImageNet</head><label>10</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">semi-supervised evaluation.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1%</cell><cell></cell><cell>10%</cell><cell></cell></row><row><cell>Method</cell><cell cols="5">Epochs Top-1 Top-5 Top-1 Top-5</cell></row><row><cell>Semi-supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>S4L [85]</cell><cell>200</cell><cell>-</cell><cell>53.4</cell><cell>-</cell><cell>83.8</cell></row><row><cell>UDA [79]</cell><cell>-</cell><cell>-</cell><cell>68.8</cell><cell>-</cell><cell>88.5</cell></row><row><cell>FixMatch [64]</cell><cell>300</cell><cell>-</cell><cell>-</cell><cell>71.5</cell><cell>89.1</cell></row><row><cell>Self-Supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NPID [78]</cell><cell>200</cell><cell>-</cell><cell>39.2</cell><cell>-</cell><cell>77.4</cell></row><row><cell>PCL [54]</cell><cell>200</cell><cell>-</cell><cell>75.6</cell><cell>-</cell><cell>86.2</cell></row><row><cell>PIRL [60]</cell><cell>800</cell><cell>30.7</cell><cell>60.4</cell><cell>57.2</cell><cell>83.8</cell></row><row><cell>SwAV [7]</cell><cell>800</cell><cell>53.9</cell><cell>78.5</cell><cell>70.2</cell><cell>89.9</cell></row><row><cell>SimCLR [9]</cell><cell>1000</cell><cell>48.3</cell><cell>75.5</cell><cell>65.6</cell><cell>87.8</cell></row><row><cell>BYOL [35]</cell><cell>1000</cell><cell>53.2</cell><cell>78.4</cell><cell>68.8</cell><cell>89.0</cell></row><row><cell>Barlow Twins [84]</cell><cell>1000</cell><cell>55.0</cell><cell>79.2</cell><cell>69.7</cell><cell>89.3</cell></row><row><cell>NNCLR [26]</cell><cell>1000</cell><cell>56.4</cell><cell>80.7</cell><cell>69.8</cell><cell>89.3</cell></row><row><cell>ReSSL (Ours)</cell><cell>200</cell><cell>57.8</cell><cell>81.6</cell><cell>71.2</cell><cell>90.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 11</head><label>11</label><figDesc>Transfer learning on low-shot image classification. We follow the implementation in<ref type="bibr" target="#b54">[54]</ref> and reproduce the result of BYOL and SwAV using the official checkpoints.</figDesc><table><row><cell>Method</cell><cell cols="4">Epochs k=4 k=8 k=16 k=32 k=64 Full</cell></row><row><cell>Random</cell><cell>-</cell><cell>10.1 10.4 10.8</cell><cell>11.3</cell><cell>11.9 12.4</cell></row><row><cell>Supervised</cell><cell>90</cell><cell>73.8 79.5 82.3</cell><cell>84.0</cell><cell>85.1 87.3</cell></row><row><cell>MoCo V2 [11]</cell><cell>200</cell><cell>64.9 72.5 76.1</cell><cell>79.2</cell><cell>81.5 84.6</cell></row><row><cell>PCL V2 [54]</cell><cell>200</cell><cell>66.2 74.5 78.3</cell><cell>80.7</cell><cell>82.7 85.4</cell></row><row><cell>SwAV [7]</cell><cell>800</cell><cell>64.0 73.0 78.7</cell><cell>82.3</cell><cell>84.9 88.1</cell></row><row><cell>BYOL [35]</cell><cell>1000</cell><cell>64.8 73.4 78.3</cell><cell>81.7</cell><cell>83.9 87.0</cell></row><row><cell>ReSSL (Ours)</cell><cell>200</cell><cell>66.8 75.3 80.5</cell><cell>83.8</cell><cell>86.0 88.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 12 Transfer</head><label>12</label><figDesc>Learning on Object Detection and Instance Segmentation.</figDesc><table><row><cell>Method</cell><cell>AP Box</cell><cell>AP M ask</cell></row><row><cell>Random</cell><cell>35.6</cell><cell>31.4</cell></row><row><cell>Supervised</cell><cell>40.0</cell><cell>34.7</cell></row><row><cell>Relative-Loc [23]</cell><cell>40.0</cell><cell>35.0</cell></row><row><cell>Rotation-Pred [53]</cell><cell>40.0</cell><cell>34.9</cell></row><row><cell>NPID [78]</cell><cell>39.4</cell><cell>34.5</cell></row><row><cell>MoCo V2 [11]</cell><cell>40.9</cell><cell>35.5</cell></row><row><cell>SimCLR [9]</cell><cell>39.6</cell><cell>34.6</cell></row><row><cell>BYOL [35]</cell><cell>40.3</cell><cell>35.1</cell></row><row><cell>ReSSL (Ours)</cell><cell>40.4</cell><cell>35.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 13 Top</head><label>13</label><figDesc>-1 accuracy under the linear evaluation on ImageNet with various lightweight backbones. The numbers for SimCLR, MoCo V2, SwAV, and BYOL on ResNet-18 are copied from<ref type="bibr" target="#b13">[14]</ref> </figDesc><table><row><cell>Method</cell><cell>Teacher Arch</cell><cell>Teacher Top-1</cell><cell>Epochs</cell><cell>Student Top-1</cell></row><row><cell>ResNet-18</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimCLR [9]</cell><cell>-</cell><cell>-</cell><cell>800</cell><cell>53.4</cell></row><row><cell>MoCo V2 [36]</cell><cell>-</cell><cell>-</cell><cell>800</cell><cell>56.1</cell></row><row><cell>SwAV [7]</cell><cell>-</cell><cell>-</cell><cell>800</cell><cell>64.9</cell></row><row><cell>BYOL [35]</cell><cell>-</cell><cell>-</cell><cell>800</cell><cell>61.9</cell></row><row><cell>SEED [30]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>200</cell><cell>57.9</cell></row><row><cell>DisCo [32]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>200</cell><cell>60.6</cell></row><row><cell>BINGO [80]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>200</cell><cell>61.4</cell></row><row><cell>CompRess [1]</cell><cell>ResNet-50</cell><cell>71.1</cell><cell>200</cell><cell>62.6</cell></row><row><cell>OSS [14]</cell><cell>ResNet-50</cell><cell>75.3</cell><cell>200</cell><cell>64.1</cell></row><row><cell>SimDis-Off [14]</cell><cell>ResNet-50</cell><cell>74.3</cell><cell>300</cell><cell>65.2</cell></row><row><cell>ReSSL (1 Crop)</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>62.4</cell></row><row><cell>ReSSL (2 Crops)</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>63.8</cell></row><row><cell>ReSSL (Multi)</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>66.5</cell></row><row><cell>ResNet-34</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MoCo V2 [11]</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>57.4</cell></row><row><cell>SEED [30]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>200</cell><cell>58.5</cell></row><row><cell>SEED [30]</cell><cell>ResNet-152</cell><cell>74.2</cell><cell>200</cell><cell>62.7</cell></row><row><cell>DisCo [32]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>200</cell><cell>62.5</cell></row><row><cell>DisCo [32]</cell><cell>ResNet-152</cell><cell>74.1</cell><cell>200</cell><cell>68.1</cell></row><row><cell>BINGO [80]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>200</cell><cell>63.5</cell></row><row><cell>BINGO [80]</cell><cell>ResNet-152</cell><cell>74.1</cell><cell>200</cell><cell>69.1</cell></row><row><cell>ReSSL (1 Crop)</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>66.3</cell></row><row><cell>ReSSL (2 Crops)</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>68.4</cell></row><row><cell>ReSSL (Multi)</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>71.0</cell></row><row><cell>EfficientNet-B0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MoCo V2 [11]</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>46.8</cell></row><row><cell>SEED [30]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>200</cell><cell>61.3</cell></row><row><cell>SEED [30]</cell><cell>ResNet-152</cell><cell>74.2</cell><cell>200</cell><cell>65.3</cell></row><row><cell>DisCo [32]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>200</cell><cell>66.5</cell></row><row><cell>DisCo [32]</cell><cell>ResNet-152</cell><cell>74.1</cell><cell>200</cell><cell>67.8</cell></row><row><cell>OSS [14]</cell><cell>ResNet-50</cell><cell>75.3</cell><cell>200</cell><cell>64.1</cell></row><row><cell>ReSSL (1 Crop)</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>69.7</cell></row><row><cell>ReSSL (2 Crops)</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>70.9</cell></row><row><cell>ReSSL (Multi)</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>72.2</cell></row><row><cell>EfficientNet-B1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MoCo V2 [11]</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>48.4</cell></row><row><cell>SEED [30]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>200</cell><cell>61.4</cell></row><row><cell>SEED [30]</cell><cell>ResNet-152</cell><cell>74.2</cell><cell>200</cell><cell>67.3</cell></row><row><cell>DisCo [32]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>200</cell><cell>66.6</cell></row><row><cell>DisCo [32]</cell><cell>ResNet-152</cell><cell>74.1</cell><cell>200</cell><cell>73.1</cell></row><row><cell>ReSSL (1 Crop)</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>71.1</cell></row><row><cell>ReSSL (2 Crops)</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>73.1</cell></row><row><cell>ReSSL (Multi)</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>74.6</cell></row><row><cell>MobileNet-V3-Large</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MoCo V2 [11]</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>36.2</cell></row><row><cell>SEED [30]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>200</cell><cell>55.2</cell></row><row><cell>SEED [30]</cell><cell>ResNet-152</cell><cell>74.2</cell><cell>200</cell><cell>61.4</cell></row><row><cell>DisCo [32]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>200</cell><cell>64.4</cell></row><row><cell>DisCo [32]</cell><cell>ResNet-152</cell><cell>74.1</cell><cell>200</cell><cell>63.7</cell></row><row><cell>ReSSL (1 Crop)</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>64.7</cell></row><row><cell>ReSSL (2 Crops)</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>66.6</cell></row><row><cell>ReSSL (Multi)</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>68.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 14</head><label>14</label><figDesc>Semi-supervised learning for lightweight architectures.</figDesc><table><row><cell>Method</cell><cell>Teacher Arch</cell><cell>Teacher Top-1</cell><cell>1% 10%</cell></row><row><cell>ResNet-18</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimCLR [9]</cell><cell>-</cell><cell>-</cell><cell>28.8 54.2</cell></row><row><cell>MoCo V2 [11]</cell><cell>-</cell><cell>-</cell><cell>25.2 54.1</cell></row><row><cell>BYOL [35]</cell><cell>-</cell><cell>-</cell><cell>25.6 52.3</cell></row><row><cell>SwAV [7]</cell><cell>-</cell><cell>-</cell><cell>39.7 60.4</cell></row><row><cell>Compress [1]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>41.2 47.6</cell></row><row><cell>SEED [30]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>39.1 50.2</cell></row><row><cell>DisCo [32]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>39.2 50.1</cell></row><row><cell>BINGO [32]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>42.8 57.5</cell></row><row><cell>ReSSL (Multi)</cell><cell>-</cell><cell>-</cell><cell>45.6 62.3</cell></row><row><cell>ResNet-34</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ReSSL (Multi)</cell><cell>-</cell><cell>-</cell><cell>55.4 67.8</cell></row><row><cell>EfficientNet-B0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SEED [30]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>37.7 53.3</cell></row><row><cell>SEED [30]</cell><cell>ResNet-152</cell><cell>74.1</cell><cell>41.8 56.7</cell></row><row><cell>DisCo [32]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>42.1 56.6</cell></row><row><cell>DisCo [32]</cell><cell>ResNet-152</cell><cell>74.1</cell><cell>52.0 63.1</cell></row><row><cell>ReSSL (Multi)</cell><cell>-</cell><cell>-</cell><cell>55.6 68.2</cell></row><row><cell>EfficientNet-B1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ReSSL (Multi)</cell><cell>-</cell><cell>-</cell><cell>59.5 70.9</cell></row><row><cell>MobileNet-V3-Large</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SEED [30]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>36.0 51.7</cell></row><row><cell>SEED [30]</cell><cell>ResNet-152</cell><cell>74.1</cell><cell>40.0 52.8</cell></row><row><cell>DisCo [32]</cell><cell>ResNet-50</cell><cell>67.5</cell><cell>38.8 51.3</cell></row><row><cell>DisCo [32]</cell><cell>ResNet-152</cell><cell>74.1</cell><cell>42.9 54.4</cell></row><row><cell>ReSSL (Multi)</cell><cell>-</cell><cell>-</cell><cell>50.9 62.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mingkai Zheng received his BSc degree in computer science and technology from the University of Sydney (USYD), Australia, in 2019, where he is currently pursuing the Ph.D. degree. His research interests include pattern recognition and machine learning fundamentals with a focus on contrastive learning, selfsupervised learning, unsupervised learning, and semi-supervised learning. Shan You is currently a Senior Researcher at SenseTime, and also a post doc at Tsinghua University. Before that, he received a Bachelor of mathematics and applied mathematics (elite class) from Xi'an Jiaotong University, and a Ph.D. degree of computer science from Peking University. His research interests include fundamental algorithms for machine learning and computer vision, such as AutoML, representation learning, light detector and face analysis. He has published his research outcomes in many top tier conferences and transactions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fei Wang</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Compress: Self-supervised learning by compressing representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajinkya</forename><surname>Soroush Abbasi Koohpayegani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Tejankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12980" to="12992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishikesh</forename><surname>Khandeparkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orestis</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikunj</forename><surname>Saunshi</surname></persName>
		</author>
		<idno>abs/1902.09229</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autoencoders, unsupervised learning and deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Conference on Unsupervised and Transfer Learning Workshop</title>
		<meeting>the 2011 International Conference on Unsupervised and Transfer Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="37" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>abs/1809.11096</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2104</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised representation transfer for small networks: I believe i can distill on-the-fly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><forename type="middle">Min</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoa</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dokwan</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Debiased contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8765" to="8775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sammy</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="11" to="13" />
			<date type="published" when="2011-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ima-geNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4685" to="4694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Agree to disagree: Adaptive ensemble knowledge distillation in gradient space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">With a little help from my friends: Nearest-neighbor contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9588" to="9597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image inpainting: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Elharrouss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Noor Almaadeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Al-M?adeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akbari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Whitening for self-supervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Ermolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3015" to="3024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">{SEED}: Self-supervised distillation for visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2004 conference on computer vision and pattern recognition workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="178" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Disco: Remedy selfsupervised learning on lightweight models with distilled contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia-Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09124</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1803.07728</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>Z. Ghahramani</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Adco: Adversarial contrast for efficient learning of unsupervised representations from self-trained negative adversaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianjiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08435</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Self-adaptive training: Bridging the supervised and self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08732</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Boosting contrastive self-supervised learning with false negative cancellation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Khademi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11765</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hard negative mixing for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bulent</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noe</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Supervised contrastive learning. ArXiv, abs/2004.11362, 2020</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio and Yann LeCun, editors, 2nd International Conference on Learning Representations</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Contrastive predictive coding based feature for automatic speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01575</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Tiny imagenet visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CS</title>
		<imprint>
			<biblScope unit="volume">231</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Self-supervised label augmentation via input transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hankook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5714" to="5724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On the limited memory bfgs method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical programming</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="503" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08218</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Exploring the equivalence of siamese self-supervised learning via a unified gradient framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahua</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05141</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Isd: Self-supervised learning by iterative similarity distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajinkya</forename><surname>Tejankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Soroush Abbasi Koohpayegani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9609" to="9618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<title level="m">What makes for good views for contrastive learning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Normface: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Solving inefficiency of self-supervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9505" to="9515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10242</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07713</idno>
		<title level="m">Contrastive learning with stronger augmentations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning by cross-level instance-group discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12586" to="12595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6256" to="6268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Bag of instances aggregation boosts self-supervised distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenrui</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Learning from multiple teacher networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1285" to="1294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Deny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12310" to="12320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">S4l: Selfsupervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiaohua Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Self-supervised representation learning via adaptive hard-positive mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Weakly supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="10042" to="10051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Relational selfsupervised learning with weak augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu Ressl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.09282</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
