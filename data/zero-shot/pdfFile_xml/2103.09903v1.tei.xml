<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformer-based ASR Incorporating Time-reduction Layer and Fine-tuning with Self-Knowledge Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Md</roleName><forename type="first">Akmal</forename><surname>Haidar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Rezagholizadeh</surname></persName>
						</author>
						<title level="a" type="main">Transformer-based ASR Incorporating Time-reduction Layer and Fine-tuning with Self-Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 Huawei Noah&apos;s Ark Lab, Montreal, Canada. Correspondence to: Md Akmal Haidar &lt;md.akmal.haidar@huawei.com&gt;.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>End-to-end automatic speech recognition (ASR), unlike conventional ASR, does not have modules to learn the semantic representation from speech encoder. Moreover, the higher frame-rate of speech representation prevents the model to learn the semantic representation properly. Therefore, the models that are constructed by the lower frame-rate of speech encoder lead to better performance. For Transformer-based ASR, the lower frame-rate is not only important for learning better semantic representation but also for reducing the computational complexity due to the self-attention mechanism which has O(n 2 ) order of complexity in both training and inference. In this paper, we propose a Transformerbased ASR model with the time reduction layer, in which we incorporate time reduction layer inside transformer encoder layers in addition to traditional sub-sampling methods to input features that further reduce the frame-rate. This can help in reducing the computational cost of the self-attention process for training and inference with performance improvement. Moreover, we introduce a fine-tuning approach for pre-trained ASR models using self-knowledge distillation (S-KD) which further improves the performance of our ASR model. Experiments on LibriSpeech datasets show that our proposed methods outperform all other Transformer-based ASR systems. Furthermore, with language model (LM) fusion, we achieve new state-of-the-art word error rate (WER) results for Transformer-based ASR models with just 30 million parameters trained without any external data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>End-to-end (E2E) automatic speech recognition (ASR) systems <ref type="bibr" target="#b8">(Graves et al., 2006;</ref><ref type="bibr" target="#b0">Amodei et al., 2016;</ref><ref type="bibr" target="#b1">Chan et al., 2016;</ref><ref type="bibr" target="#b17">Karita et al., 2019a)</ref> have shown great success recently because of their simple training and inference procedures over the traditional HMM-based methods <ref type="bibr" target="#b29">(Povey et al., 2016)</ref>. These end-to-end models learn a direct mapping of the input acoustic signal to the output transcription without needing to decompose the problem into different parts such as lexicon modeling, acoustic modeling and language modeling. The very first end-to-end ASR model is the connectionist temporal classification (CTC) <ref type="bibr" target="#b8">(Graves et al., 2006)</ref> model which independently maps the acoustic frames into the outputs. The CTC-based models which incorporate language model re-scoring could improve their output quality <ref type="bibr" target="#b7">(Graves &amp; Jaitly, 2014)</ref>. The conditional independence assumption in CTC was tackled by the recurrent neural network transducer (RNNT) model <ref type="bibr" target="#b6">(Graves, 2012;</ref><ref type="bibr" target="#b12">He et al., 2019)</ref>, which shows better performance in streaming scenarios. Other E2E ASR models are the attention-based encoder-decoder (AED) architectures which yield state-ofthe-art results in offline and online fashions <ref type="bibr" target="#b18">(Karita et al., 2019b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b1">Chan et al., 2016;</ref><ref type="bibr" target="#b25">Moritz et al., 2020;</ref><ref type="bibr" target="#b37">Wang et al., 2020a;</ref><ref type="bibr" target="#b35">Tsunoo et al., 2020)</ref>. The Transformer architecture, which uses self-attention mechanism to model temporal contextual information, has been shown to achieve lower word error rate (WER) compared to the recurrent neural network (RNN) based AED architectures <ref type="bibr" target="#b17">(Karita et al., 2019a)</ref>. However, Transformer architectures suffer from the decreased computational efficiency for longer input sequences because of quadratic time requirements of the self-attention mechanism.</p><p>For ASR systems, the number of time frames for an audio input sequence is significantly higher than the number of output text labels. For example, a task such as Lib-riSpeech which has long utterances (15s), contains 1500 speech frames (assuming 10 ms per frame) . Considering that adjacent speech frames can form a chunk to represent more meaningful units like phonemes, some preprocessing mechanisms are considered to capture the embedding of a group of speech frames to reduce the frame-rate in the encoder input. In <ref type="bibr" target="#b39">(Wang et al., 2020c)</ref>, different pre-processing strategies such as convolutional sub-sampling and frame stacking &amp; skipping techniques for Transformerbased ASR are discussed. Among these approaches, the convolutional approach for frame-rate reduction gives better WER results compared to other approaches. Recently, a VGG-like convolutional block <ref type="bibr" target="#b30">(Simonyan &amp; Zisserman, 2015)</ref> with max-pooling <ref type="bibr" target="#b14">(Hori et al., 2017)</ref> and layer normalization is used <ref type="bibr" target="#b37">(Wang et al., 2020a)</ref> before the Transformer encoder layers and outperforms the 2D-convolutional subsampling <ref type="bibr" target="#b17">(Karita et al., 2019a)</ref>. In <ref type="bibr" target="#b1">(Chan et al., 2016)</ref>, a pyramidal encoder structure for Bidirectional LSTM (BLSTM) is proposed using time-reduction layers to reduce the framerate of the input sequence by concatenating adjacent frames. In <ref type="bibr" target="#b12">(He et al., 2019)</ref>, a time-reduction layer is employed in the RNNT model to speed up the training and inference. However, time-reduction layer for Transformer architecture was never explored.</p><p>In this work, we hypothesize that further frame-rate reduction is possible on top of current approaches for Transformerbased architectures. In this regard, we introduce a timereduction layer to Transformer-based ASR models that further decreases the frame-rate by concatenating adjacent frames. Our proposed approach can speed up the Transformer model training and inference. Also, our model yields a new state-of-the art WER results for Transformer-based ASR models over traditional approaches. Furthermore, we introduce a fine-tuning approach for pre-trained ASR models incorporating the self-knowledge distillation (S-KD) approach <ref type="bibr" target="#b11">Haun &amp; Choi, 2019)</ref>, which further improves the performance of our ASR model. S-KD is recently investigated which shows the dark knowledge of a model can be progressively deployed through distillation to improve even its own performance <ref type="bibr" target="#b11">Haun &amp; Choi, 2019)</ref>. We summarize our contributions as following:</p><p>? We introduce a Transformer-based ASR model incorporating time-reduction layer;</p><p>? We deploy a fine-tuning approach for ASR model using S-KD which shows better performance than the conventional S-KD training from scratch. To best of our knowledge, we apply the S-KD approach for the first time in training ASR models;</p><p>? Our model outperforms the existing state-of-the-art Transformer-based ASR models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Transformer Architecture for ASR</head><p>In Transformer-based ASR <ref type="bibr" target="#b17">(Karita et al., 2019a;</ref>, the input sequence X is first mapped to a subsequence X 0 ? R n sub ?datt by CNN blocks <ref type="bibr" target="#b3">(Dong et al., 2018;</ref><ref type="bibr" target="#b17">Karita et al., 2019a)</ref> and then transformer layers of the encoder map X 0 to a sequence of encoded features X e . Here, n sub and d att describe the length of sub-sampled sequence and the dimensions of the features respectively. The layers of the encoder iteratively refine the representation of the input sequence with a combination of multi-head self-attention (MHA) and frame-level affine transformations. Specifically, the inputs to each layer are projected into keys K, queries Q, and values V . Scaled dot product attention is then used to compute a weighted sum of values for Q matrix <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref>:</p><formula xml:id="formula_0">Attention(Q, K, V ) = softmax( QK T ? d att )V<label>(1)</label></formula><p>where Q ? R nq?datt , K, V ? R n k ?datt , n q is the length of Q, and n k is the length of K, V . We obtain multi-head attention by performing this computation h times independently with different sets of projections, and concatenating:</p><formula xml:id="formula_1">MHA(Q, K, V ) = Concat(head 1 , . . . , head h )W O (2) head i = Attention(QW Q i , KW K i , V W V i ) (3)</formula><p>where Q, K, V are inputs of this multi-head attention layer, head i ? R nq?datt is the i th attention layer output (i = 1, . . . , h), W * i ? R datt?datt , W O ? R datth?datt are the learnable weight matrices and h is the number of attention heads in this layer <ref type="bibr" target="#b36">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b17">Karita et al., 2019a)</ref>. The outputs of multi-head attention go through a 2-layer position-wise feed-forward network with hidden size d f f :</p><formula xml:id="formula_2">FFN(X 0 [t]) = ReLU(X 0 [t]W d f f 1 + b d f f 1 )W d f f 2 + b d f f 2 (4) where X 0 [t] ? R datt is the t th frame of the sequence X 0 , W d f f 1 ? R datt?d f f , W d f f 2 ? R d f f ?datt are learnable weight matrices, b f f 1 ? R d f f and b f f 2 ? R datt are learnable bias vectors.</formula><p>The decoder generates a transcription sequence Y = (Y [1], ..., Y [t]) one token at a time. Each choice of output token Y [t] is conditioned on the encoder representations X e and previously generated tokens (Y [1], ..., Y [t ? 1]) through attention mechanisms. Each decoder layer performs two rounds of multi-head attention <ref type="bibr" target="#b17">(Karita et al., 2019a)</ref>. The multi-head attention is then followed by the positionwise FFN <ref type="bibr">(Equation 4</ref>). The output of the final decoder layer for token Y [t ? 1] is used to predict the following token Y [t]. Other components of the architecture such as sinusoidal positional encodings, residual connections and layer normalization are described in <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref>. The positional encodings are applied into X 0 and Y 0 when convolutional sub-sampling was used <ref type="bibr" target="#b17">(Karita et al., 2019a;</ref><ref type="bibr" target="#b40">Watanabe et al., 2018)</ref>. For VGG-like convolutional subsampling <ref type="bibr" target="#b37">(Wang et al., 2020a)</ref>, the positional encoding for X 0 was discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>Transformer ASR Several studies have focused on adapting Transformer networks for end-to-end speech recognition. In particular, <ref type="bibr" target="#b3">(Dong et al., 2018;</ref><ref type="bibr" target="#b24">Mohamed et al., 2019)</ref> present models augmenting Transformer networks with convolutions. <ref type="bibr" target="#b17">(Karita et al., 2019a)</ref> focuses on refining the training process, and show that Transformer-based end-toend ASR is highly competitive with state-of-the-art methods. In <ref type="bibr" target="#b37">(Wang et al., 2020a)</ref>, a regularization method based on semantic masking was introduced for Transformer ASR to force the decoder to learn a better language model. A hybrid Transformer model with deeper layers and iterative loss was introduced in <ref type="bibr" target="#b39">(Wang et al., 2020c)</ref> where a Transformerbased acoustic model outperforms the best hybrid results combined with an n-gram language model. In <ref type="bibr" target="#b31">(Synnaeve et al., 2020)</ref>, a semi-supervised model with pseudo-labeling using Transformer-based acoustic model was introduced. Recently a convolution augmented Transformer was proposed <ref type="bibr" target="#b9">(Gulati et al., 2020)</ref> where a convolution module with two macaron-like feed forward layers is augmented in the Transformer encoder layers. Moreover, Transformer architecture has shown better performance for streaming applications <ref type="bibr" target="#b25">Moritz et al., 2020;</ref><ref type="bibr" target="#b35">Tsunoo et al., 2020)</ref>. In this paper, we incorporate time-reduction layer in the vanilla Transformer architecture <ref type="bibr" target="#b36">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b17">Karita et al., 2019a</ref>) and achieve better performance over the baselines <ref type="bibr" target="#b17">(Karita et al., 2019a;</ref><ref type="bibr" target="#b37">Wang et al., 2020a;</ref><ref type="bibr" target="#b25">Moritz et al., 2020;</ref><ref type="bibr" target="#b31">Synnaeve et al., 2020;</ref><ref type="bibr" target="#b28">Park et al., 2019)</ref>.</p><p>Knowledge Distillation Knowledge distillation (KD) is a prominent neural model compression technique <ref type="bibr" target="#b13">(Hinton et al., 2015)</ref> in which the output of a teacher network is used as an auxiliary supervision besides the ground-truth training labels. Later on, it was shown that KD can be used for improving the performance of neural networks in the so-called born-again <ref type="bibr" target="#b5">(Furlanello et al., 2018)</ref> or self-distillation frameworks <ref type="bibr" target="#b42">Yun et al., 2020;</ref><ref type="bibr" target="#b10">Hahn &amp; Choi, 2019)</ref>. Self-distillation is a regularization technique trying to improve the performance of a network using its internal knowledge. In other words, in self-distillation scenarios, the student becomes its own teacher. While KD has shown great success in different ASR tasks <ref type="bibr" target="#b27">(Pang et al., 2018;</ref><ref type="bibr" target="#b15">Huang et al., 2018;</ref><ref type="bibr" target="#b32">Takashima et al., 2018;</ref><ref type="bibr" target="#b19">Kim et al., 2019;</ref><ref type="bibr" target="#b2">Chebotar &amp; Waters, 2016;</ref><ref type="bibr" target="#b4">Fukuda et al., 2017;</ref><ref type="bibr" target="#b41">Yoon et al., 2020)</ref>, self-distillation is more investigated in computer vision and natural language processing (NLP) domains <ref type="bibr" target="#b11">(Haun &amp; Choi, 2019;</ref><ref type="bibr" target="#b10">Hahn &amp; Choi, 2019)</ref>. To best of our knowledge, we incorporate the self-KD approach for the first time in training ASR models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Model for Transformer ASR</head><p>Given an input sequence X of log-mel filterbank speech features, Transformer predicts a target sequence Y of characters or SentencePiece <ref type="bibr" target="#b22">(Kudo &amp; Richardson, 2018)</ref>. The architecture of our proposed model is described in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Transformer Encoder</head><p>In <ref type="figure" target="#fig_0">Figure 1</ref>, EncPre(.) transforms the source sequence X into a sub-sampled sequence X 0 ? R n sub ?datt by using convolutional neural network (CNN) blocks <ref type="bibr" target="#b17">(Karita et al., 2019a;</ref><ref type="bibr" target="#b25">Moritz et al., 2020;</ref><ref type="bibr" target="#b37">Wang et al., 2020a</ref>) which reduce the sequence length of the output X 0 by a factor of 4 compared to the sequence length of X. In the baseline model, the EncPre(.) is followed by a stack of Transformer layers that transform X 0 into a sequence of encoded features X e ? R n sub ?datt for the CTC and decoder networks <ref type="bibr" target="#b17">(Karita et al., 2019a)</ref>. In this paper, we introduce a time-reduction (TR) layer in the stack of Transformer layers of the encoder. The position of the TR layer in the Transformer layers is a hyper-parameter. In <ref type="figure" target="#fig_0">Figure 1</ref>, we add a TR layer between two groups (Enc1Body(.) and Enc2Body(.)) of Transformer layers in the encoder. The Enc1Body(.) transforms X 0 into a sequence of encoded features X 1 ? R n sub ?datt for the TR layer. The TR layer transforms the sequence X 1 into X R ? R n sub2 ?datt by concatenating two adjacent time frames <ref type="bibr" target="#b1">(Chan et al., 2016)</ref>. This concatenation further reduces the length of the output sequence X R by a factor of 2 (i.e., the frame-rate of X R is reduced by a factor of 8 compared to X). Here, n sub2 represents the sequence length after applying a TR layer. A TR layer corresponding to the j th layer at the output of i th time step can be described as follows:</p><formula xml:id="formula_3">h j i = h j?1 2i , h j?1 2i+1<label>(5)</label></formula><p>where h j i represents the encoder representation of the j th layer at the i th time step. After the TR layer, Enc2Body(.) transforms X R into a sequence of encoded features X e ? R n sub2 ?datt for the CTC and decoder networks. The Enc1Body(.) and Enc2Body(.) can be defined as <ref type="bibr" target="#b17">(Karita et al., 2019a)</ref>:</p><formula xml:id="formula_4">X i = X i + M HA i (X i , X i , X i ) X i+1 = F F N i (X i )<label>(6)</label></formula><p>where i = 0, . . . , e 1 ? 1 or i = 0, . . . , e 2 ? 1 describe the index of the Transformer layers of the encoder before or after the TR layer respectively.</p><p>Since the time complexity of the self-attention mechanism is quadratic, our proposed time-reduction layer inside the Transformer layers of the encoder can reduce the computational cost for training and inference. To best of our knowledge, we are the first who applied time-reduction approach to Transformers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transformer Decoder</head><p>We keep the same decoder architecture as in <ref type="bibr" target="#b17">(Karita et al., 2019a;</ref>. The decoder receives the encoded sequence X e and the prefix of a target sequence Y [1 : t ? 1] of token IDs: characters or SentencePiece <ref type="bibr" target="#b22">(Kudo &amp; Richardson, 2018)</ref>. First, DecPre(.) embeds the tokens into learnable vectors Y 0 [1 : t ? 1]. Then, DecBody(.) and a single linear layer DecPost(.) predicts the posterior distribution of the next token prediction Y post [t] given X e and Y [1 : t ? 1]. For the decoder input Y [1 : t ? 1], we use ground-truth labels in the training stage, while we use generated output in the decoding stage. The DecBody(.) can be described as:</p><formula xml:id="formula_5">Y j [t] = Y j [t] + M HA self j (Y j [t], Y j [1 : t ? 1], Y j [1 : t ? 1]) Y j = Y j + M HA src j (Y j , X e , X e ) Y j+1 = Y j + F F N j (Y j )<label>(7)</label></formula><p>where j = 0, . . . , d ? 1 represents the index of the Transformer layers of the decoder. M HA src j (Y j , X e , X e ) and M HA self</p><formula xml:id="formula_6">j (Y j [t], Y j [1 : t ? 1], Y j [1 : t ?<label>1</label></formula><p>]) are defined as the 'encoder-decoder attention' and the 'decoder selfattention' respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training and Decoding</head><p>During ASR training, the frame-wise posterior distribution of P s2s (Y |X) and P ctc (Y |X) are predicted by the decoder and the CTC module respectively. The training loss function is the weighted sum of the negative log-likelihood of these values <ref type="bibr" target="#b17">(Karita et al., 2019a)</ref>:</p><formula xml:id="formula_7">L ASR = ?(1??) log P s2s (Y |X)?? log P ctc (Y |X) (8)</formula><p>where? log P ctc (Y |X) and ? log P s2s (Y |X) describe the CTC and cross-entropy (CE) losses respectively. ? is a hyper-parameter which determines the CTC weight. In the decoding stage, given the speech feature X and the previous predicted token, the next token is predicted using beam search combining the scores of s2s and ctc with/without language model score as <ref type="bibr" target="#b17">(Karita et al., 2019a)</ref>:</p><formula xml:id="formula_8">Y = arg max Y ?y * {(1 ? ?) log P s2s (Y |X e )+ ? log P ctc (Y |X e ) + ? log P lm (Y )} (9)</formula><p>where y * is a set of hypotheses of the target sequence Y , and ?, ? are hyperparameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Encoder sub-sampling</head><p>We apply 2D-convolutional sub-sampling (Karita et al., 2019a) (CONV2D4) for EncPre(.) which is a block of two CNN layers. Each CNN layer uses stride 2, a kernel size of 3 ? 3 and a ReLU activation function. For EncPre(.), we also use VGG-like convolutional block (VGGCONV2D4) with layer normalization and a max-pooling function <ref type="bibr" target="#b37">(Wang et al., 2020a)</ref>. For both cases, the sequence length reduces by a factor of 4 compared to the length of X. To compare with our proposed approach, we also define two other EncPre(.) namely CONV2D8 <ref type="bibr" target="#b40">(Watanabe et al., 2018)</ref> and VGGCONV2D8 which reduce the sequence length by 8 times compared to the original sequence length. All the sub-sampling approaches are depicted in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Fine-tuning using Self-Knowledge Distillation</head><p>We further improve the performance of our proposed model by incorporating a self-knowledge distillation (S-KD) approach . S-KD has recently been investigated for natural language processing (NLP) applications, where the soft labels generated by the model are used to improve itself in a progressive manner <ref type="bibr" target="#b11">(Haun &amp; Choi, 2019;</ref><ref type="bibr" target="#b20">Kim et al., 2020)</ref>. S-KD is a framework of KD where the student becomes its own teacher. Bear in mind, since ASR training takes longer time to converge, starting from scratch with S-KD as in  would increase the training time significantly. In this paper, we incorporate S-KD in fine-tuning a pre-trained ASR model and will</p><p>show that it will be more efficient than training ASR models using S-KD from scratch. For fine-tuning using S-KD, let P T s2s (Y [t]|Y [1 : t ? 1], X e ) be the probability distribution of the decoder of a pre-trained ASR model as a teacher. We initialized the student with the teacher model and at iteration i ? 1, the student model tries to mimic the teacher probability distribution with cross-entropy:</p><formula xml:id="formula_9">L S?KD = ? Y [t]?V (P T s2s (Y [t]|Y [1 : t ? 1], X e )? log P s2s (Y [t]|Y [1 : t ? 1], X e ))<label>(10)</label></formula><p>where V is a set of vocabulary. At iteration i, the student probability distribution P s2s (Y [t]|Y [1 : t ? 1], X e ) of the decoder becomes the teacher probability distribution</p><formula xml:id="formula_10">P T s2s (Y [t]|Y [1 : t ? 1], X e</formula><p>) and the student model is trained to mimic the teacher with L S?KD . A schematic of the S-KD approach is described in <ref type="figure">Figure 3</ref>. The new loss function for fine-tuning (FT) the ASR model can be described as:</p><formula xml:id="formula_11">L ASR?F T = ?L ctc + (1 ? ?)(? KD L S?KD +(1 ? ? KD )L s2s )<label>(11)</label></formula><p>where L ctc = ? log P ctc (Y |X), L s2s = ? log P s2s (Y |X), and ? KD is a controlled parameter, and is typically ? KD = 0.5 <ref type="bibr" target="#b35">(Tsunoo et al., 2020)</ref>.</p><p>To show the effectiveness of our proposed fine-tuning approach using S-KD, we also perform S-KD experiments to <ref type="figure">Figure 3</ref>. Fine-tuning ASR using the S-KD approch. The student at i ? 1 becomes the teacher at iteration i and the model is trained with the loss LASR?F T train our model from scratch. In this regard, we update the KD loss for each epoch similar to :</p><formula xml:id="formula_12">? KDt = ? KD T ? t T (12)</formula><p>where T is the total number of training epochs, ? KD T is the ? KDt for the last epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experimental Setup</head><p>We use the open-source, ESPnet toolkit <ref type="bibr" target="#b40">(Watanabe et al., 2018)</ref> for our experiments. We conduct our experiments on LibriSpeech dataset <ref type="bibr" target="#b26">(Panayotov et al., 2015)</ref>, which is a speech corpus of reading English audio books. It has 960 hours of training data, 10.7 hours of development data, and 10.5 hours of test data, whereby the development and the test datasets are both split into approximately two halves named "clean" and "other". To extract the input features for speech, we follow the same setup as in <ref type="bibr" target="#b40">(Watanabe et al., 2018;</ref><ref type="bibr" target="#b17">Karita et al., 2019a)</ref>: using 83-dimensional log-Mel filterbanks frames with pitch features <ref type="bibr" target="#b17">(Karita et al., 2019a;</ref><ref type="bibr" target="#b25">Moritz et al., 2020)</ref>. The output tokens come from a 5K subword vocabulary created with sentencepiece <ref type="bibr" target="#b22">(Kudo &amp; Richardson, 2018)</ref>"unigram" <ref type="bibr" target="#b40">(Watanabe et al., 2018)</ref>. We perform experiments using 12 (e 1 + e 2 = 12) and 6 (d = 6) Transformer layers for the encoder and decoder respectively, with d f f = 2048, d att = 256, and h = 4. The total number of model parameters are about 30 M.We apply default settings of SpecAugmentation <ref type="bibr" target="#b28">(Park et al., 2019)</ref> of ESPnet <ref type="bibr" target="#b40">(Watanabe et al., 2018)</ref> and no gradient accumulation for all of our experiments. We use Adam optimizer with learning rate scheduling similar to <ref type="bibr" target="#b36">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b40">Watanabe et al., 2018)</ref> and other settings (e.g., dropout, warmup steps, ?, label smoothing penalty <ref type="bibr" target="#b28">(Park et al., 2019)</ref>) following <ref type="bibr" target="#b25">(Moritz et al., 2020;</ref><ref type="bibr" target="#b40">Watanabe et al., 2018)</ref>. We set the initial learning rate of 5.0 <ref type="bibr" target="#b40">(Watanabe et al., 2018)</ref>. We train all of our models for 150 epochs. We report our results based on averaging the best five checkpoints. For fine-tuning with self-knowledge distillation (FS-KD) experiments, we use our trained best average model for initialization. We train the FS-KD experiments for 50 epochs with Adam optimizer <ref type="bibr" target="#b21">(Kingma &amp; Ba, 2015)</ref> with fixed learning rate of 0.0001 <ref type="bibr" target="#b40">(Watanabe et al., 2018)</ref> and fixed ? KD = 0.5. We also perform S-KD experiments from scratch for 150 epochs with ? KD T = 0.5 and 200 epochs with ? KD T = 0.7 respectively. For S-KD experiments, We use Adam optimizer with learning rate scheduling similar to <ref type="bibr" target="#b36">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b40">Watanabe et al., 2018)</ref>.</p><p>For decoding, the CTC weight ?, the LM weight ? and beam size are 0.5, 0.7 and 20 respectively <ref type="bibr" target="#b25">(Moritz et al., 2020;</ref><ref type="bibr" target="#b40">Watanabe et al., 2018)</ref>. Moreover, we apply insertionpenalty of 2.0 during decoding. For S-KD and FS-KD experiments, we note that the CTC weight of ? = 0.3 gives better results. This might be because of S-KD applied on the decoder output of the ASR model. For LM fusion <ref type="bibr" target="#b33">(Toshniwal et al., 2018)</ref>, we use a pre-trained Transformer LM 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">New Baseline Results</head><p>We compare our proposed models with state-of-the-art models <ref type="bibr" target="#b25">(Moritz et al., 2020;</ref><ref type="bibr" target="#b28">Park et al., 2019;</ref><ref type="bibr" target="#b17">Karita et al., 2019a;</ref><ref type="bibr" target="#b37">Wang et al., 2020a;</ref><ref type="bibr" target="#b23">Luscher et al., 2019;</ref><ref type="bibr" target="#b31">Synnaeve et al., 2020)</ref>. We also introduce the latest baseline models and run experiments to compare them with our proposed approaches. We create a pyramidal structure <ref type="bibr" target="#b1">(Chan et al., 2016)</ref> in the first three Transformer layers of the encoder, where we concatenate the outputs at consecutive steps (Equation 5) of each layer before feeding it to the next layer. After the first three layers, the sequence length would reduce by a factor of 8 than the sequence length of X. We define the model as Pyramidal Transformer. We also use CONV2D8 sub-sampling <ref type="bibr" target="#b40">(Watanabe et al., 2018)</ref> and VGGCONV2D8 which are explained in <ref type="figure" target="#fig_1">Figure 2</ref> for EncPre(.) to reduce the sequence length 8 times than the length of X and then apply all the Transformer layers. We define them as CONV2D8 and VGGCONV2D8 respectively. From <ref type="table" target="#tab_0">Table 1</ref>, we can see that our proposed new baseline models give comparable results to the existing models with LM fusion, which are much larger than our model. Among these baselines, the VGGCONV2D8 gives the better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results using TR layer</head><p>We apply the TR layer after CONV2D4 or VGGCONV2D4 to further reduce the sequence length by a factor of two. First, we apply the TR layer after CONV2D4 and VG-GCONV2D4 approaches (i.e., TR0: e 1 = 0 and e 2 = 12). We define the models as CONV2D4+TR0 and VG-GCONV2D4+TR0 respectively, which reduce the framerate by a factor of 8 than the frame-rate of X. From <ref type="table" target="#tab_0">Table 1</ref>, we can note that VGGCONV2D4+TR0 outperforms or gives comparable results to the existing hybrid results <ref type="bibr" target="#b38">(Wang et al., 2020b;</ref><ref type="bibr" target="#b23">Luscher et al., 2019)</ref>, E2E LSTM <ref type="bibr" target="#b28">(Park et al., 2019)</ref> and E2E Transformer baselines <ref type="bibr" target="#b25">(Moritz et al., 2020;</ref><ref type="bibr" target="#b17">Karita et al., 2019a;</ref><ref type="bibr" target="#b31">Synnaeve et al., 2020;</ref><ref type="bibr" target="#b37">Wang et al., 2020a)</ref> which are trained with more parameters. Also, it shows better results over the VGGCONV2D8 model. Next, we apply the TR layer after the second Transformer layer (i.e., TR2: e 1 = 2 and e 2 = 10). From both TR0 and TR2 experiments, we can see that both experiments give comparable results and VGGCONV2D4+TR0/TR2 yields better results over the CONV2D4+TR0/TR2 models. Without LM fusion, VGGCONV2D4+TR2 gives better results over VGGCONV2D4+TR0. For the other experiments, we apply the VGGCONV2D4 for the EncPre(.) and perform TR2 experiments. From the table 1, we see that the VG-GCONV2D4+TR0/TR2 model with LM fusion outperforms over the best E2E Transformer ASR models using the semantic mask <ref type="bibr" target="#b37">(Wang et al., 2020a)</ref> and <ref type="bibr" target="#b31">(Synnaeve et al., 2020)</ref> which are trained with 75 and 270 M parameters respectively. Compared to the semantic mask technique <ref type="bibr" target="#b37">(Wang et al., 2020a)</ref> and <ref type="bibr" target="#b31">(Synnaeve et al., 2020)</ref>, our proposed approach achieves relative WER reductions of 10.7% and 14.1% for the test-clean and 2.3% and 3.2% for the test-other respectively. Also, we can note that our approach shows comparable results to Transformer Transducer  which is trained with RNN-T loss <ref type="bibr" target="#b34">(Tripathi et al., 2019)</ref> and 139 M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Results using Self-KD</head><p>We report the results for self-KD (S-KD) experiments in Table 2. We conduct experiments VGGCONV2D4+TR2+FS-KD for fine-tuning the pre-trained ASR model VG-GCONV2D4+TR2 using S-KD. We can note that with FS-KD, the proposed method gives further WER reductions for both without and with LM fusion. With LM fusion, our model outperforms the Transformer transducer model for the test-clean dataset and gives comparable results for the test-other dataset. We also train an ASR model by applying S-KD from scratch to show the effectiveness of our finetuning approach on a pre-trained ASR model. We conduct two experiments by applying self-KD from scratch for 150 epochs and 200 epochs respectively. From <ref type="table" target="#tab_1">Table 2</ref>, we can see that the S-KD experiments from scratch give similar or better results than the VGGCONV2D4+TR2 model but lower performance than our proposed VGGCONV2D4+FS-KD approach. Also, the self-KD approach from scratch requires longer training time than our proposed approaches. Furthermore, to make the same number of epochs, we apply self-KD for fine-tuning the VGGCONV2D4+TR2 S-KD* model which is trained using the self-KD from scratch for 150 epochs. From <ref type="table" target="#tab_1">Table 2</ref>, it can be seen that the model VGGCONV2D4+TR2 S-KD*+FS-KD does not give better results than our proposed model VGGCONV2D4+TR2+FS-KD. Moreover, we report the best five validation accuracy for our best model with the time-reduction layer and the models with self-KD in <ref type="table" target="#tab_1">Table 2</ref>. We observe that our proposed finetuning approach using self-KD (VGGCONV2D4+TR2+FS-KD) gives better validation accuracy over the other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Time Complexity Analysis</head><p>Transformers are recently the most efficient models in speech and semantic fields which were suffered from a (1 + m l ) 2 decay in computational efficiency for the self-attention mechanism if the sequence length l is increased by m steps. Our proposed approach could release the burden of computational efficiency in Transformers by applying time reduction layer. For example, if the sequence length l is decreased by a factor of 2, the computation for selfattention can be reduced by ( l l 2 ) 2 times. Therefore, the proposed method can reduce the computational cost of the self-attention mechanism by k 2 times over the traditional approach, where k is the time reduction ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Future Work</head><p>Convolutional sub-sampling approaches are essential for efficient use of self-attention mechanism in Transformerbased ASR. These sub-sampling approaches reduce the speech frame-rate to form meaningful units like phoneme, word-piece, etc. The reduced frame-rate can help in proper training of Transformer ASR. In this paper, we incorporated a time-reduction layer for Transformer-based ASR which can further reduce the frame-rate and improve the performance over traditional convolutional sub-sampling approaches. It can also gives faster training and inference during the self-attention computation of the Transformer ASR model. Our approach yields state-of-the-art results using the LibriSpeech benchmark dataset for Transformer architectures. Furthermore, we introduced fine-tuning of a pretrained ASR model using self-knowledge distillation which further improves the performance of our ASR model. Moreover, we showed our proposed fine-tuning approach with self-KD outperformed the conventional self-KD training from scratch. We performed experiments on LibriSpeech datasets and show that our proposed method with 30 M parameters achieves word error rate (WER) of 1.9%, 4.8%, 1.8% and 4.6% results for the test-clean, test-other, devclean and dev-other respectively using language model (LM) fusion and outperformed over the other Transformer-based ASR models. For future work, we will investigate our approaches for conformer <ref type="bibr" target="#b9">(Gulati et al., 2020)</ref> architecture and explore more experimental settings with incorporating gradient accumulation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Proposed Transformer ASR with time-reduction layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Different types of Encoder sub-sampling. (a) CONV2D4, (b) CONV2D8, (c) VGGCONV2D4, and (d) VGGCONV2D8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>WER results for E2E ASR models. TR0 and TR2 represent the time-reduction (TR) layer applied before all and after second Transformer layers of the encoder respectively. Best results are depicted in bold.</figDesc><table><row><cell>MODEL</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>WER results of the proposed Transformer-based ASR models using self-KD. FS-KD describes the results for fine-tuning the pre-trained VGGCONV2D4+TR2 model with Self-KD. TR2 S-KD* and TR2 S-KD** represent the models training from scratch using self-KD with time-reduction layer after second Transformer layer for 150 and 200 epochs respectively. Best results are described in bold.</figDesc><table><row><cell>MODEL</cell><cell cols="4">TEST-CLEAN TEST-OTHER DEV-CLEAN DEV-OTHER</cell></row><row><cell>VGGCONV2D4+TR2 +FS-KD</cell><cell>3.1</cell><cell>7.9</cell><cell>3.0</cell><cell>8.0</cell></row><row><cell>+ LM</cell><cell>1.9</cell><cell>4.8</cell><cell>1.8</cell><cell>4.6</cell></row><row><cell>VGGCONV2D4+TR2 S-KD*</cell><cell>3.2</cell><cell>8.1</cell><cell>3.2</cell><cell>8.3</cell></row><row><cell>+ LM</cell><cell>2.0</cell><cell>4.8</cell><cell>1.9</cell><cell>4.7</cell></row><row><cell>VGGCONV2D4+TR2 S-KD*+FS-KD</cell><cell>3.3</cell><cell>8.1</cell><cell>3.2</cell><cell>8.3</cell></row><row><cell>+ LM</cell><cell>2.0</cell><cell>4.8</cell><cell>1.8</cell><cell>4.7</cell></row><row><cell>VGGCONV2D4+TR2 S-KD**</cell><cell>3.2</cell><cell>8.0</cell><cell>3.1</cell><cell>8.2</cell></row><row><cell>+ LM</cell><cell>2.0</cell><cell>4.9</cell><cell>1.9</cell><cell>4.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Rows</figDesc><table><row><cell></cell><cell cols="4">1-5 represent the best five validation accu-</cell></row><row><cell cols="5">racy for the VGGCONV2D4+TR2, VGGCONV2D4+TR2+FS-</cell></row><row><cell cols="5">KD, VGGCONV2D4+TR2+S-KD*, VGGCONV2D4+TR2?S-</cell></row><row><cell cols="5">KD*+FS-KD, and VGGCONV2D4+TR2?S-KD**. Best results</cell></row><row><cell>are in bold.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell cols="5">0.95053 0.94900 0.94793 0.94780 0.94735</cell></row><row><cell cols="5">0.95518 0.95475 0.95436 0.95434 0.95418</cell></row><row><cell cols="5">0.95043 0.95011 0.95002 0.94999 0.94966</cell></row><row><cell cols="5">0.95209 0.95184 0.95072 0.95049 0.94973</cell></row><row><cell cols="5">0.95266 0.95216 0.95189 0.95187 0.95146</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/espnet/espnet/blob/ master/egs/librispeech/asr1/RESULTS.md</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distilling knowledge from ensembles of neural networks for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speech-transformer: a norecurrence sequence-to-sequence model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Efficient knowledge distillation froman ensemble of teachers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fukuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Born again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anandkumar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<title level="m">Sequence transduction with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08100</idno>
		<title level="m">Convolution-augmented transformer for speech recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01851</idno>
		<title level="m">Self-knowledge distillation in natural language processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-knowledge distillation in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Natural Language Processing (RANLP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Streaming end-to-end speech recognition for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Advances in joint ctc-attention based end-to-end speech recognition with a deep cnn encoder and rnn-lm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowledge distillation for sequence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In INTERSPEECH</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the choice of modeling unit for sequence-to-sequence speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruguier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A comparative study on transformer vs rnn in speech applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Someki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2019 Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving transformerbased end-to-end speech recognition with connectionist temporal classification and language model integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Knowledge distillation using output errors for self-attention end-to-end models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Self-knowledge distillation: A simple way for better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12000v1</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Rwth asr systems for librispeech: Hybrid vs attention -w/o data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kitza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schluter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03072v3</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11660</idno>
		<title level="m">Transformers with convolutional context for asr</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Streaming automatic speech recognition with the transformer model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Compression of end-toend models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In INTERSPEECH</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Specaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Purely sequence-trained neural networks for asr based on latticefree mmi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">End-to-end asr: From supervised to semisupervised learning with modern architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08460</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An investigation of a knowledge distillation method for ctc acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Takashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A comparison of techniques for language model integration in encoder-decoder speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2018 Spoken Language Technology (SLT) Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Monotonic recurrent neural network transducer and de-coding strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2019 Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tsunoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kashiwagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14941</idno>
		<title level="m">Streaming transformer asr with blockwise synchronous inference</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Semantic mask for transformer based end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03010</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Low latency end-to-end streaming speech recognition with a scout network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10369</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transformerbased acoustic modeling for hybrid speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Renduchintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ochiai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Espnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00015</idno>
		<title level="m">End-to-end speech processing toolkit</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">I</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tutornet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00671</idno>
		<title level="m">Towards flexible knowledge distillation forend-to-end speech recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Regularizing classwise predictions via self-knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13876" to="13885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02562</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
