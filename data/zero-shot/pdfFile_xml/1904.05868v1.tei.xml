<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved training of binary networks for human pose estimation and image recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
							<email>adrian.bulat@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
							<email>georgios.t@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Kossaifi</surname></persName>
							<email>j.kossaifi@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
							<email>maja.pantic@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved training of binary networks for human pose estimation and image recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Big neural networks trained on large datasets have advanced the state-of-the-art for a large variety of challenging problems, improving performance by a large margin. However, under low memory and limited computational power constraints, the accuracy on the same problems drops considerable. In this paper, we propose a series of techniques that significantly improve the accuracy of binarized neural networks (i.e networks where both the features and the weights are binary). We evaluate the proposed improvements on two diverse tasks: fine-grained recognition (human pose estimation) and large-scale image recognition (ImageNet classification). Specifically, we introduce a series of novel methodological changes including: (a) more appropriate activation functions, (b) reverse-order initialization, (c) progressive quantization, and (d) network stacking and show that these additions improve existing state-ofthe-art network binarization techniques, significantly. Additionally, for the first time, we also investigate the extent to which network binarization and knowledge distillation can be combined. When tested on the challenging MPII dataset, our method shows a performance improvement of more than 4% in absolute terms. Finally, we further validate our findings by applying the proposed techniques for large-scale object recognition on the Imagenet dataset, on which we report a reduction of error rate by 4%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent methods based on Convolutional Neural Networks (CNNs) have been shown to produce results of high accuracy for a wide range of challenging Computer Vision tasks like image recognition <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b15">16]</ref>, object detection <ref type="bibr" target="#b28">[29]</ref>, semantic segmentation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b13">14]</ref> and human pose estimation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b24">25]</ref>. Two fundamental assumptions made by these methods are that: 1) very large and diverse labelled datasets are available for training, and 2) that at least one high-end GPU is available for model training and inference. While it can be assumed that for training both labelled training data and computational resources are available, from a a practical perspective, in many applications (e.g. object recognition or human sensing on mobile devices and robots), it is unreasonable to assume that dedicated high-end GPUs are available for inference. The aim of this paper is to enable highly accurate and efficient convolutional networks on devices with limited memory, storage and computational power. Under such constraints, the accuracy and performance of existing methods rapidly drops, and the problem is considered far from being solved.</p><p>Perhaps, the most promising method for model compression and efficient model inference is network binarization, especially when both activations and weights are binary <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28]</ref>. In this case, the binary convolution operation can be efficiently implemented with the bitwise XNOR, resulting in speed-up of ? 58? on CPU (this speed-up on FPGAs can be even higher) and model compression ratio of ? 32? <ref type="bibr" target="#b27">[28]</ref>. Although no other technique can achieve such impressive speed-ups and compression rates, this also comes at the cost of reduced accuracy. For example, there is ? 18% difference in top-1 accuracy between a real-valued ResNet-18 and its binary counterpart on ImageNet <ref type="bibr" target="#b27">[28]</ref>, and ? 9% difference between a real-valued state-of-the-art network for human pose estimation and its binary counterpart on MPII <ref type="bibr" target="#b2">[3]</ref>.</p><p>Motivated by the above findings, in this work, we focus on improving the training of binary networks by proposing a series of methodological improvements. In particular, we make the following contributions:</p><p>? We motivate, provide convincing evidence and describe a series of methodological changes for training binary neural networks including (a) more appropriate non-linear activation functions (Sub-section 3.2), (b) reverse-order initialization (Sub-section 3.3), (c) progressive quantization (Sub-section 3.4), and (d) network stacking (Sub-section 3.5) that, individually and combined, are shown to improve existing state-of-theart network binarization techniques, significantly. (e) We also show to what extent network binarization and knowledge distillation can be combined (Section 3.6).</p><p>? We show that our improved training of binary networks is task and network agnostic by applying it on two diverse tasks: fine-grained recognition and, in particular, human pose estimation and classification, specifically ImageNet classification.</p><p>? Exhaustive experiments conducted on the challenging MPII dataset show that our method offers an improvement of more than 4% in absolute terms over the stateof-the-art (Section 4).</p><p>? On ImageNet we report a reduction of error rate by 4% over the current state-of-the-art (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section, we review related prior work including network quantization and knowledge distillation for image classification, and methods for efficient human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Network Quantization</head><p>Network quantization refers to quantizing the weights and/or the features of a neural network. It is considered the method of choice for model compression and efficient model inference and a very active topic of research. Seminal work in this area goes back to <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref> who introduced techniques for 16-and 8-bit quantization. The method of <ref type="bibr" target="#b45">[46]</ref> proposed a technique which allocates different numbers of bits (1-2-6) for the network parameters, activations and gradients. For more recent work see <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>The focus of the first methods proposed in this work is on binarization of both weights and features which is the extreme case aiming to quantizing to {?1, 1}, thus offering the largest possible compression and speed gains. The work of <ref type="bibr" target="#b8">[9]</ref> introduced a technique for training a CNN with binary weights. A follow-up work <ref type="bibr" target="#b9">[10]</ref> demonstrates how to binarize both parameters and activations. This has the advantage that, during the forward pass, multiplications can be replaced with binary operations. The method of <ref type="bibr" target="#b27">[28]</ref> proposes to model the weights with binary numbers multiplied by a scaling factor. Using this simple modification which does not sacrifice the beneficial properties of binary networks, <ref type="bibr" target="#b27">[28]</ref> was the first to report good results on a large scale dataset (ImageNet <ref type="bibr" target="#b10">[11]</ref>).</p><p>Our method proposes several extensions to <ref type="bibr" target="#b27">[28]</ref>, including more appropriate activation functions, reverse-order initialization, progressive quantization, and network stacking, which are shown to produce large improvements of more than % 4 (in absolute terms) for human pose estimation over the state-of-the art <ref type="bibr" target="#b2">[3]</ref>. We also report similar improvements for large-scale image classification on ImageNet, in particular, we report a reduction of error rate by 4% over the current state-of-the-art <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Knowledge Distillation</head><p>Recent works <ref type="bibr" target="#b17">[18]</ref> have shown that, at least for realvalued networks, the performance of a smaller network can be improved by "distilling the knowledge" of another one, where "knowledge distillation" refers to transferring knowledge from one CNN (the so-called "teacher") to another (the so-called "student"). Typically, the teacher is a highcapacity model of great accuracy, while the student is a compact model with much fewer parameters (thus also requiring much less computation). Thus, the goal of knowledge distillation is to use the teacher to train a compact student model with similar accuracy to that of the teacher. The term "knowledge" refers to the soft outputs of the teacher. Such soft outputs provide extra supervisory signals of intraclass and inter-class similarities learned by teacher. Further extensions include transferring from intermediate representations of the teacher network <ref type="bibr" target="#b29">[30]</ref> and from attention maps <ref type="bibr" target="#b42">[43]</ref>. While most of the prior work focuses on distilling real-valued neural networks little to no-work has been done on studying the effectiveness of such approaches for binarized neural networks. In this work, we propose to adapt such techniques to binary networks, showing through empirical evidence their positive effect on improving accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Human pose estimation</head><p>A large number of works have been recently proposed for both single-person <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b5">6]</ref> and multiperson <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12]</ref> human pose estimation. We note that the primary focus of these works is accuracy (especially for the single-person case) rather than efficient inference under low memory and computational constraints which is the main focus of our work.</p><p>Many of the aforementioned methods use the so-called HourGlass (HG) architecture <ref type="bibr" target="#b24">[25]</ref> and its variants. While we also used the HG in our work, our focus was to enhance its efficiency while maintaining as much as possible its high accuracy which makes our work different to all aforementioned works. To our knowledge, the only papers that have similar aims are the works of <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b34">[35]</ref>. <ref type="bibr" target="#b2">[3]</ref> and its extension <ref type="bibr" target="#b3">[4]</ref> aim to improve binary neural networks for human pose estimation by introducing a novel residual block. <ref type="bibr" target="#b34">[35]</ref> aims to improve quantized neural networks by introducing a new HG architecture. In contrast, in this work, we focus on improving binary networks for human pose estimation by (a) improving the binarization process per se, and (b) combining binarization with knowledge distillation. Our method is more general than the improvements proposed in <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b34">[35]</ref>. We illustrate this by showing the benefits of the proposed method for also improving ImageNet classification with binary networks. <ref type="figure">Figure 1</ref>: The residual binary block of <ref type="bibr" target="#b3">[4]</ref> used in our work. The module has a hierarchical, parallel, multi-scale structure comprised of three 3 ? 3 convolutional layers with input-output channel ratio equal to 1:2, 1:4 and 1:4. Each convolution layer is preceded by a BatchNorm and the binarization function (sgn(x)) and followed by a non-linearity. See Section 3 for the changes introduced in our work for improving its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>This section presents the proposed methodological changes for improving the network binarization process. Throughout this section, we validated the performance gains offered by our method on the single person human pose estimation dataset, MPII. We note that we chose human pose estimation as the main dataset to report the bulk of our results as the dataset is considerably smaller and training is much faster (compared to ImageNet).</p><p>Sub-section 3.1 describes the strong baseline used in our work, briefly explaining the binarization process proposed in <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b2">[3]</ref>, while the proposed improvements are described in Sub-sections 3.2, 3.3, 3.4, 3.5 and 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Baseline</head><p>All results reported herein are against the state-of-the-art method of <ref type="bibr" target="#b2">[3]</ref> which we used as a strong baseline to report the performance improvements introduced by our method. The method of <ref type="bibr" target="#b2">[3]</ref> combines the HourGlass (HG) architecture of <ref type="bibr" target="#b24">[25]</ref> with the a newly proposed residual block that was specifically designed for binary CNNs (see <ref type="figure">Fig. 1</ref>). The network was binarized using the approach described in <ref type="bibr" target="#b27">[28]</ref> as follows:</p><formula xml:id="formula_0">T * W ? (sgn(T) sgn(W))K?,<label>(1)</label></formula><p>where T is the input tensor, W is the layer's weight tensor, K a matrix containing the scaling factors for all the sub-tensors of T, and ? ? R + is a scaling factor for the weights. denotes the binary convolution operation which can be efficiently implemented with the bitwise XNOR, resulting in speed-up of ? 58? and model compression ratio of ? 32? <ref type="bibr" target="#b27">[28]</ref>. Note than in practice, we follow <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref> and drop K since this speed-ups the network at a negligible performance drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Leaky non-linearities</head><p>Previous work <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3]</ref> has shown that adding a nonlinearity after each convolutional layer can be used to increase the performance of binarized CNNs. In the context of real-valued networks there exists a plethora or works that explore their effect on the overall network accuracy, however, in contrast to this, there is little to no work avaialble for binary networks. Herein, we rigorously explore the choice of the non-linearity and its impact on the overall performance for the task of human pose estimation showing empirically in the process the negative impact of the previously proposed ReLU. Instead of using a ReLU, we propose to use the recently introduced PReLU <ref type="bibr" target="#b14">[15]</ref> function, an adaptation of the leaky ReLU that has a learnable negative slope, which we find it to perform better than both the ReLU and the leaky ReLU.</p><p>There are two main arguments for justifying our findings. Firstly, with the help of the sgn function, the binarization process restricts the possible states of the filters and features to {?1, 1}. As such, the representational power of the network resides on these two states, and removing one of them during training using a ReLU for each convolutional layer makes the training unstable. See also <ref type="figure" target="#fig_1">Fig. 3</ref>. Secondly, this instability is further amplified by the fact that the implementation of the sign function is "leaky" at 0, introducing a third unwanted spurious state and the subsequent iterations can cause easy jumps between the two states. See also <ref type="figure" target="#fig_0">Fig. 2</ref>. Note, that despite the fact that the Batch Normalisation <ref type="bibr" target="#b18">[19]</ref> layer mitigates some of this effects by re-centering the input distribution, as the experiments show, in practice, the network can achieve significantly better accuracy if the non-linearity function allows negative values to pass. On the other hand, we know that non-linearities should be used to increase the representational power of the network. We conclude that a PReLU can be safely used for this purpose removing also the aforementioned instabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Reverse-order initialization</head><p>Initialization of neural networks has been the subject of study of many recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31]</ref> where it was shown that an appropriate initialization is often required for achieving good performance <ref type="bibr" target="#b32">[33]</ref>. The same holds for quantized networks, where most of prior works either use an adaptation of the above mentioned initialization strategies, or start from a pretrained real-valued neural network. However, while the weight binarization alone can be done with little to no accuracy loss <ref type="bibr" target="#b27">[28]</ref>, quantizing the features has much higher detrimental effect <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b45">46]</ref>. In addition, since the output signal from sgn is very different to the output of a ReLU layer, the transition from a fully real-valued network  to a binary one causes a catastrophic loss in accuracy often comparable with training from scratch.</p><p>To alleviate this, we propose the opposite of what is currently considered the standard method to initialize a binary network from a real-valued one: we propose to firstly train a network with real weights and binary features (the features are binarized using the approach presented in Section 3.4) and only after this, it is fully trained to further binarize the weights. By doing so, we effectively split the problem into two sub-problems: weight and feature binarization which we then try to solve from the hardest to the easiest one. <ref type="figure">Fig. 4</ref> shows the advantage of the proposed initialization method against standard pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Smooth progressive quantization</head><p>Previous works have shown that incrementally quantizing the network either by gradually decreasing the precision or by partitioning and progressively increasing the amount <ref type="figure">Figure 4</ref>: Accuracy evolution on the validation set of MPII during training for different pre-initialization approaches. Our initialization provides a much better starting point. of quantized weights <ref type="bibr" target="#b43">[44]</ref> leads to decent performance improvements. While the later is more practical, it requires a careful fine-tuning of the quantization ratio at each step.</p><p>Instead, in this work, we follow a different route by proposing to approximate the quantization function sgn(x) with a smoother one, in which the estimation error is controlled by ?. By gradually increasing ? during training, we achieve a progressive binarization. This allows for a natural and smoother transition in which the selection of the weights to be binarized occurs implicitly and can be easily controlled by varying ? without the need to define a fixed scheduling for increasing the amount of quantized weights as in <ref type="bibr" target="#b43">[44]</ref>.</p><p>In the following, we present a few options we explored to approximate the sgn(x) function alongside their derivatives (see also <ref type="figure" target="#fig_3">Fig. 6</ref>):</p><p>Sigmoid:</p><formula xml:id="formula_1">sgn(x) ? 2 e ?x 1 + e ?x ? 1 d dx 2 e ?x 1 + e ?x ? 1 = 2?e ?x (e ?x + 1) 2 (2) SoftSign: sgn(x) ? ?x 1 + ?|x| d dx ?x 1 + ?|x| = ? (1 + ?|x|) 2<label>(3)</label></formula><p>Tanh:</p><formula xml:id="formula_2">sgn(x) ? tanh(?x) d dx tanh(?x) = ?(1 ? tanh 2 (?x))<label>(4)</label></formula><p>As ? ? ? the function converges to sgn(x). In a similar fashion, the derivative of the approximation function converges to the Dirac function ?. In practice, as most of the features are outside of the region with high approximation error (see <ref type="figure" target="#fig_2">Fig. 5</ref>), we started observing close-to-binary results starting with ? = 25. See <ref type="figure">Fig. 7</ref>.</p><p>In our tests we found that all the above approximation functions behaved similarly, however the best performance was obtained using the tanh, while the softsign offered slightly lower performance. As such, the final reported results are obtained using the tanh. During training we progressively increased the value of ? starting from 2 0 to 2 16 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Stacked binary networks</head><p>As shown in <ref type="bibr" target="#b24">[25]</ref>, using a stack of HG networks can be used to greatly improve human pose estimation accuracy, allowing the network to gradually refine its prediction at each stage. In a similar fashion, in this work we constructed a stack of binary HG networks also incorporating the improvements introduced in the previous subsections. We would like to verify to what extent stacking can further contribute on top of these improvements. In addition to these improvements, our method differs to <ref type="bibr" target="#b3">[4]</ref> in that all the intermediate layers used to join the stacks are also binarized. As the results from section 4 show, stacking further improves upon the improvements reported in the previous subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Combining binarization with distillation</head><p>Recent work on knowledge distillation has focused on real-valued networks <ref type="bibr" target="#b17">[18]</ref>, largely ignoring the quantized, and especially, the binarized case.</p><p>In this work, and in light of the methods proposed in the previous sub-sections, we also study the effect and effectiveness of knowledge distillation for the case of binary networks, evaluating in the process the following options: (a) using a real-valued teacher and a binary student and (b) using a binary teacher and a binary student with and without feature matching. During training, we used the output heatmaps of the teacher network as soft labels for the Binary Cross Entropy Loss. In addition, we found that the best results can be obtained by combining the ground truth and the soft labels with a weight equal to 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Human pose estimation experiments</head><p>In this section, we report our results on MPII, one of the most challenging datasets for single person human pose estimation <ref type="bibr" target="#b0">[1]</ref>. MPII contains approximately 25,000 images and more than 40,000 persons annotated with up to 16 landmarks and visibility labels. We use the same split for validation and training as in <ref type="bibr" target="#b36">[37]</ref> (3,000 for validation and 22,000 for training). We firstly report the performance improvements, using the PCKh metric <ref type="bibr" target="#b0">[1]</ref>, obtained by applying incrementally the proposed methods in the same order as these methods appear in the paper. We then evaluate the proposed improvements in isolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results</head><p>Baseline: The performance of our strong baseline <ref type="bibr" target="#b2">[3]</ref> using 1 HG with and without ReLU is shown in the first 2 rows of <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Leaky non-linearities (Section 3.2):</head><p>The performance improvement obtained by replacing the ReLU with Leaky ReLU and then PReLU as proposed in our work is shown in the 3-rd and 4-th rows of <ref type="table">Table 1</ref>. We observe a large improvement of 2.5% in terms of absolute error with the highest gains offered by the PReLU function. Note that we obtained similar accuracy between the variant that uses a single scale factor and the one that uses one per each channel for the negative slope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reverse-order initialization (Section 3.3):</head><p>We observe an additional improvement of 0.8% by firstly binarizing the features and then the weights, as proposed in our work and as shown in the 5-th row of <ref type="table">Table 1</ref>. This, alongside the results from <ref type="figure">Fig. 4</ref> show that the proposed strategy is an efficient way of improving the performance of binary networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Progressive binarization (Section 3.4):</head><p>We observe an additional improvement of 0.4% by the proposed progressive binarization as shown in the 6-th row of Table 1.  Stacked binary networks (Section 3.5): We observe an additional improvement of 1.5% and 1.9% by using 2-stack and 3-stack HG networks, respectively, as shown in the 4-th column of <ref type="table" target="#tab_1">Table 2</ref>. While significant improvements can be observed when going from 1 HG to a stack of 2, the gain in performance diminishes when 1 more binary HG is added to the network. A similar phenomenon is also observed (but to less extent though) for the case of real-valued networks.</p><p>Binarization plus distillation As shown in the last row of <ref type="table">Table 1</ref>, we obtained an improvement of 0.6% via combining binarization and distillation for a binary network with a single HG distilled using a high performing real-valued teacher. Note that the binary network already incorporates the improvements proposed in section 3. Also, the last column of <ref type="table" target="#tab_1">Table 2</ref> shows the improvements obtained by combining binarization with distillation for multi-stack binary HG networks. We observe an additional improvement of 1.5% and 1.9% by using 2-stack and 3-stack HG networks, respectively.</p><p>While we explored with using both a binary and a realvalued "teacher" given that finding a high performing binary teacher is challenging on its own, we obtained the best results using a real-valued one. However, in both cases the network converged to a satisfactory condition.  While the above results illustrate the accuracy gains by incrementally applying our improvements it is also important to evaluate the performance gains of each proposed improvement in isolation. As the results from <ref type="table">Table 3</ref> show, the proposed techniques also yield high improve-   <ref type="table">Table 3</ref>: PCK-h on the validation set of MPII that evaluates each proposed improvement in isolation. Each column points out to the section that presents the particular method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training</head><p>We trained all models for human pose estimation (both real-valued and binary) following the same procedure: they were trained for 120 epochs, using a learning rate of 2.5e?4 that was dropped every 40 epochs by a factor of 10. For data augmentation, we applied random flipping, scale (0.75? to 1.25?) jittering and rotation (?30 ? to 30 ? ). Instead of using the MSE loss, we followed the findings from <ref type="bibr" target="#b2">[3]</ref> and used the BCE loss defined as: <ref type="bibr" target="#b4">(5)</ref> where p n ij denotes the ground truth confidence map of the n?th part at pixel location (i, j) and p n ij is the corresponding predicted output at the same location. For distillation, we simply applied a BCE loss using as ground truth the predictions of the teacher network.</p><formula xml:id="formula_3">l h = 1 N N n=1 W i=1 H j=1 p n ij log p n ij + (1 ? p n ij ) log (1 ? p n ij ),</formula><p>The models were optimized using RMSProp <ref type="bibr" target="#b35">[36]</ref>. We implemented our models using PyTorch <ref type="bibr" target="#b26">[27]</ref>. Qualitative results produced by our best-performing binary model can be seen in <ref type="figure" target="#fig_7">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Imagenet classification experiments</head><p>To emphasize the generalization properties of the proposed improvements: (a) leaky non-linearities (Section 3.2), (b) reverse-order initialization (Section 3.3), (c) smooth progressive quantization (Section 3.4), and (d) knowledge distillation (Section 3.6), in this section, we show that they are largely task-, architecture-and blockindependent, by applying them on both of a more traditional architecture (i.e AlexNet <ref type="bibr" target="#b21">[22]</ref>) and a resnet-based one (ResNet-18) for the task of Imagenet <ref type="bibr" target="#b10">[11]</ref> classification.</p><p>AlexNet: Similarly to <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b9">10]</ref>, we removed the local normalization layer preserving the same structure, namely (from input to output):  <ref type="bibr" target="#b27">[28]</ref>, the first and last layer were kept real.</p><p>ResNet: We kept the original ResNet-18 macroarchitecture unchanged, using, as proposed in <ref type="bibr" target="#b27">[28]</ref>, the basic block from <ref type="bibr" target="#b16">[17]</ref> with pre-activation. Similarly to the previous works <ref type="bibr" target="#b27">[28]</ref>, the first and last layers were kept real.</p><p>Results: As <ref type="table">Table 4</ref> shows, when compared against the state-of-the-art method of <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b9">[10]</ref>, our approach offers a large improvement of up to 4% in terms of absolute error for both Top-1 and Top-5 error metrics using both AlexNet and ResNet-18 architectures. This further validate the generality of our method.</p><p>Training: We trained the binarized version of AlexNet <ref type="bibr" target="#b21">[22]</ref> and ResNet-18 <ref type="bibr" target="#b15">[16]</ref> using Adam <ref type="bibr" target="#b20">[21]</ref> starting with a learning rate of 1e ? 3 that is gradually    <ref type="bibr" target="#b21">[22]</ref> 56.6% 80.2% 69.3% 89.2% <ref type="table">Table 4</ref>: Top-1 and Top-5 classification accuracy using binary AlexNet and ResNet-18 on Imagenet. Notice that our method offers consistent improvements across multiple architectures: both traditional ones(AlexNet) and residual ones (ResNet-18). decreased every 25 epochs by 0.1. We simply augment the data by firstly resizing the images to have 256px over the smallest dimension and then randomly cropping them to 227 ? 227 for AlexNet and 224 ? 224px for ResNet. We believe that further performance gains can be achieved with more aggressive augmentation. At test time, instead of random-crop we center-crop the images. To alleviate problems introduced by the binarization process, and similarly to <ref type="bibr" target="#b27">[28]</ref>, we trained the network using a large batch size, specifically 400 for AlexNet and 256 for ResNet-18. All models were trained for 80 epochs. <ref type="figure" target="#fig_6">Fig. 8</ref> shows the top-1 and top-5 accuracy across training epochs for AlexNet (the network was initialized using the procedure proposed in Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this work, we proposed a series of novel techniques for highly efficient binarized convolutional neural network. We experimentally validated our results on the challenging problems of human pose estimation and large scale image classification. Mainly, we propose (a) more appropriate non-linear activation functions, (b) reverse-order initializa- tion, (c) progressive features quantization, and (d) network stacking that improve existing state-of-the-art network binarization techniques. Furthermore, we explore the effect and efficiency of knowledge distillation procedures in the context of binary networks using a real-valued "teacher" and binary "student".</p><p>Overall, our results show that a performance improvement of up to 5% in absolute terms is obtained on the challenging human pose estimation dataset of MPII. Finally, we show that our approach is architecture and task-agnostic and can increase the performance of arbitrary networks. In particular, by applying the proposed techniques to Imagenet classification, we report an absolute performance improvement of 4% over the current state-of-the-art using both AlexNet and ResNet architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Weight distribution for various layers from a network using PReLU (first row) and ReLU (second row) as we advance in the network (from left to right). The ReLU tends to push the weights closer to 0 making a jump between states more likely, thus causing the observed instabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Accuracy evolution on the validation set of MPII during training. Notice the high fluctuations introduced by the ReLU. Best performance is obtained with PReLU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Input distribution before the sgn function from 3 layers located at the bottom, middle and top of the network. Most values are in a range where the approximation function outputs values close to ?1 allowing the approximator to reach good estimates for relatively low values of ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>The quantization approximation functions used: sigmoid, softsign and tanh (first row) and their derivatives (second row) for various values of ? = {1, 5, 25, 625, 65536}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(a) ? = 1 (Figure 7 :</head><label>17</label><figDesc>b) ? = 5 (c) ? = 25 (d) ? = 625 Output distribution of tanh(?x) for ? = {1, 5, 25, 625}. Notice that starting with ? = 25, in practice, the function behaves close to sgn(x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Top-1 accuracy on ImageNet.(b) Top-5 accuracy on ImagenNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>ImageNet training and validation accuracy vs epoch for different variants of our binary AlexNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative results produced by our binary network on the validation set of MPII.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>PCK-h on the validation set of MPII for various number of binarized stacked HGs.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical binary cnns for landmark localization with limited resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adversarial posenet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1705.00389</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07432</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Training deep neural networks with low precision multiplications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>David</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detect-and-track: Efficient pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="350" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09894</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Talathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Annapureddy</surname></persName>
		</author>
		<title level="m">Fixed point quantization of deep convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards accurate multiperson pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Xnornet: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6120</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very deep convolutional networks for large-scale image recognition. arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deeply learned compositional models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Quantized densely connected u-nets for efficient landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>2012. 7</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Clip-q: Deep network compression learning by in-parallel pruning-quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Quantized convolutional neural networks for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Incremental network quantization: Towards lossless cnns with lowprecision weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03044</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Explicit losserror-aware quantization for low-bit deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<title level="m">Dorefanet: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
