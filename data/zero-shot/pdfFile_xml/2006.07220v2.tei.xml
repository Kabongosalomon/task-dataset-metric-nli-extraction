<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Second Order Behaviour in Augmented Neural ODEs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Norcliffe</surname></persName>
							<email>alex.norcliffe98@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bodnar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Day</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Simidjievski</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Physics</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On Second Order Behaviour in Augmented Neural ODEs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural Ordinary Differential Equations (NODEs) are a new class of models that transform data continuously through infinite-depth architectures. The continuous nature of NODEs has made them particularly suitable for learning the dynamics of complex physical systems. While previous work has mostly been focused on first order ODEs, the dynamics of many systems, especially in classical physics, are governed by second order laws. In this work, we consider Second Order Neural ODEs (SONODEs). We show how the adjoint sensitivity method can be extended to SONODEs and prove that the optimisation of a first order coupled ODE is equivalent and computationally more efficient. Furthermore, we extend the theoretical understanding of the broader class of Augmented NODEs (ANODEs) by showing they can also learn higher order dynamics with a minimal number of augmented dimensions, but at the cost of interpretability. This indicates that the advantages of ANODEs go beyond the extra space offered by the augmented dimensions, as originally thought. Finally, we compare SONODEs and ANODEs on synthetic and real dynamical systems and demonstrate that the inductive biases of the former generally result in faster training and better performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Residual Networks (ResNets) <ref type="bibr" target="#b7">[8]</ref> have been an essential tool for scaling the capabilities of neural networks to extreme depths. It has been observed that the skip layers that these networks employ can be seen as an Euler discretisation of a continuous transformation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref>. Neural Ordinary Differential Equations (NODEs) <ref type="bibr" target="#b2">[3]</ref> are a new class of models that consider the limit of this discretisation step, naturally giving rise to an ODE that can be optimised via black-box ODE solvers. Their continuous depth makes them particularly suitable for learning and modelling the unknown dynamics of complex systems, which often cannot be described analytically.</p><p>Since the introduction of NODEs, many variants have been proposed <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>. While a few of these models use second order dynamics <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24]</ref>, no in-depth study on second order behaviour in Neural ODEs exists even though most dynamical systems that arise in science, such as Newton's equations of motion and oscillators, are governed by second order laws. To fill this void, we take a deeper look at Second Order Neural ODEs (SONODEs) and the broader class of  <ref type="figure">Figure 1</ref>: Three learnt trajectories from the compact parity experiment (g 1d from the original <ref type="bibr" target="#b3">[4]</ref>). NODEs are not able to learn the mapping, ANODE(1) is able to learn it, SONODEs learn the simplest trajectory.</p><p>models formed by Augmented Neural ODEs (ANODEs). Unlike previous approaches, which mainly focus on classification tasks, we use low-dimensional physical systems, often with known analytic solutions, as our main arena of investigation. As we will show, the simplicity of these systems is useful in analysing the properties of these models.</p><p>To summarise our contributions, we begin by studying more closely the optimisation of SONODEs by generalising the adjoint sensitivity method to second order models. We continue by analysing how some of the properties of ANODEs extend to SONODEs and show that the latter can often find simpler solutions for the problems we consider. Our analysis also extends to ANODEs and demonstrates that they are capable of learning higher-order dynamics, sometimes with just a few additional dimensions. However, the way they do so has deeper implications for their functional loss landscape and their interpretability as a scientific tool. Finally, we compare SONODEs and ANODEs on real and synthetic second order dynamical systems. Our results reveal that the inductive biases in SONODEs are beneficial in this setting. Our code is available online at https://github.com/ a-norcliffe/sonode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>As discussed in the introduction, Neural ODEs (NODEs) can be seen as a continuous variant of ResNet models <ref type="bibr" target="#b7">[8]</ref>, whose hidden state evolves continuously according to a differential equatio?</p><formula xml:id="formula_0">x = f (v) (x, t, ? f ), x(t 0 ) = X 0 ,<label>(1)</label></formula><p>whose velocity is described by a neural network f <ref type="bibr">(v)</ref> with parameters ? f and initial position given by the points of a dataset X 0 . As shown by Chen et al. <ref type="bibr" target="#b2">[3]</ref>, the gradients can be computed through an abstract adjoint state r(t), once its dynamics are known.</p><p>Our investigations are mainly focused on Augmented Neural ODEs (ANODEs) <ref type="bibr" target="#b3">[4]</ref>, which append states a(t) to the ODE:</p><formula xml:id="formula_1">z = x a ,? = f (v) (z, t, ? f ), z(t 0 ) = X 0 g(X 0 , ? g ) .<label>(2)</label></formula><p>We note that, unlike the original formulation, we allow for the initial values of the augmented dimensions a(t 0 ) to be learned as a function of x(t 0 ) by a neural network g with parameters ? g . For the remainder of the paper, we use the ANODE(D) notation to signify the use of D augmented dimensions.</p><p>We are almost exclusively concerned with the problem of learning and modelling the behaviour of dynamical systems, given N + 1 sample points X t?T , t = (t 0 , . . . , t N ), from a fixed set of its trajectories at multiple time steps included in the set T . For such tasks, we use the mean squared error (MSE) between these points and the corresponding predicted location over all time steps for training the models. For the few toy classification tasks we include, we optimise only for the linear separability of the final positions via the cross-entropy loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Second Order Neural Ordinary Differential Equations</head><p>We consider Second Order Neural ODEs (SONODEs), whose initial position x(t 0 ), initial velocit? x(t 0 ), and acceleration? are given by</p><p>x(t 0 ) = X 0 ,?(t 0 ) = g(x(t 0 ), ? g ),? = f (a) (x,?, t, ? f ),</p><p>where f (a) is a neural network with parameters ? f . Alternatively, SONODEs can be seen as a system of coupled first-order neural ODEs with state z(t) = [x(t), a(t)]:</p><formula xml:id="formula_3">z = x a ,? = f (v) (z, t, ? f ) = a f (a) (x, a, t, ? f )</formula><p>, z(t 0 ) = X 0 g(X 0 , ? g )</p><p>.</p><p>This formulation makes clear that SONODEs are a type of ANODE with constraints on the structure of f <ref type="bibr">(v)</ref> , and offers a way to reuse NODE's first order adjoint method <ref type="bibr" target="#b2">[3]</ref> for training, as in previous work <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24]</ref>. However, a pair of questions remain about the optimisation of SONODEs: firstly, what is the ODE that the second order adjoint follows? And, consequently, how does the second order adjoint sensitivity method compare with first order adjoint-based optimisation? To address these questions, we show how the adjoint sensitivity method can be generalised to SONODEs. Proposition 3.1. The adjoint state r(t) of SONODEs follows the second order OD?</p><formula xml:id="formula_5">r = r T ?f (a) ?x ?? T ?f (a) ?? ? r T d dt ?f (a) ??<label>(5)</label></formula><p>The proof and boundary conditions for this ODE is given in Appendix B. As an additional contribution, we include an alternative proof to those of Chen et al. <ref type="bibr" target="#b2">[3]</ref> and Pontryagin <ref type="bibr" target="#b17">[18]</ref> for the first order adjoint. Given that the dynamics of the abstract adjoint vector are known, its state at all times t can be used to train the parameters ? f using the integral</p><formula xml:id="formula_6">dL d? f = ? t0 tn r T ?f (a) ?? f dt,<label>(6)</label></formula><p>where L denotes the loss function and t n is the timestamp of interest. The gradient with respect to the parameters of the initial velocity network, ? g , can be found in Appendix B. To answer the second question, we compare this gradient against that obtained through the adjoint of the first order coupled ODE from Equation <ref type="formula" target="#formula_4">(4)</ref>. Proposition 3.2. The gradient of ? f computed through the adjoint of the coupled ODE from (4) and the gradient from (6) are equivalent. However, the latter requires at least as many matrix multiplications as the former.</p><p>This result motivates the use of the first order coupled ODE as it presents computational advantages.</p><p>The proof in Appendix B shows that this is due to the dynamics of the adjoint from the coupled ODE, which contain entangled representations of the adjoint. This is in contrast to the disentangled representation in Equation <ref type="formula" target="#formula_5">(5)</ref>, where the adjoint state and velocity are separated. It is the entangled representation that permits the faster computation of the gradients for the coupled ODE. We will see in Section 5.3 that entangled representations in ANODEs are a reoccurring phenomenon, and their effects are not always beneficial, as in this case. We use the first order ODE optimisation for the remainder of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Properties of SONODEs</head><p>In this section, we analyse certain properties of SONODEs and illustrate them with toy examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Generalised parity problem</head><p>It is known that unique trajectories in NODEs cannot cross at the same time <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>. We extend this to higher order Initial Value Problems (IVP). Proofs are presented in Appendix A. While this shows SONODE trajectories cannot cross in phase space, they can cross in real space if they have different velocities. To illustrate this, we introduce a generalised parity problem, an extension to D dimensions of the g 1d function from Dupont et al. <ref type="bibr" target="#b3">[4]</ref>, which maps x ? ?x. We remark that SONODEs should be able to learn a parity flip in any number of dimensions, with a trivial solution</p><formula xml:id="formula_7">f (a) (x,?, t, ? f ) = 0, g(x(t 0 ), ? g ) = ? 2 t N ? t 0 x(t 0 )<label>(7)</label></formula><p>This is equivalent to all points moving in straight lines through the origin to ?x(t 0 ). We first visualise the learnt transformation in the one dimensional case <ref type="figure">(Figure 1</ref>), for points initialised at ?1. SONODEs learn the simplest trajectories for this problem.  <ref type="figure">Figure 2</ref>: The logarithm of the loss in each dimension for the generalised parity problem. SONODE has the lowest loss, while the NODE loss generally oscillates between dimensions as predicted.</p><p>For higher dimensions, we first remark that NODEs are able to produce parity flips for even dimensions by pairing off the dimensions and performing a 180 ? rotation in each pair. This solution does not apply to odd-dimensional cases because there is always an unpaired dimension that is not rotated. In addition to the dimensional-parity effect, as volume increases exponentially with the dimensionality, the density exponentially decreases (given the number of points in the dataset remains constant). This makes it easier to manipulate the points without trajectories crossing, and so, it is expected that the problem will become easier for NODEs as dimensionality increases.</p><p>In <ref type="figure">Figure 2</ref>, we investigate parity flips in higher dimensions, using 50 training points and 10 test points, randomly generated between -1 and 1 in each dimension. For NODEs, as predicted, the loss oscillates over dimensions and, for odd dimensions, the loss decreases with the number of dimensions. ANODEs perform better than NODES, especially in odd dimensions, where it can rotate the unpaired dimension through the additional space. SONODEs have the lowest loss in every generalisation, which can be associated with the existence of the trivial solution in any number of dimensions, given by Equation <ref type="formula" target="#formula_7">(7)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Nested n-spheres</head><p>Dupont et al. <ref type="bibr" target="#b3">[4]</ref> prove that a transformation under NODEs has to be a homeomorphism, preserving the topology of the input space, and as such, they cannot learn certain transformations. Similarly to ANODEs, SONODEs avoid this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NODE</head><p>ANODE(1) SONODE <ref type="figure">Figure 3</ref>: The trajectories learnt by NODEs, ANODEs and SONODEs for the nested-n-spheres problem in 2D. NODEs preserves the topology so the blue region cannot escape the red region. ANODEs, as expected, use the third dimension to separate the two regions. For SONODEs the points pass through each other in real space. The proof can be found in Appendix C. To illustrate this, we perform an experiment on the nested n-spheres problem <ref type="bibr" target="#b3">[4]</ref>, (the name is taken from <ref type="bibr" target="#b13">[14]</ref>, originally called g function <ref type="bibr" target="#b3">[4]</ref>), where the elements of the blue class are surrounded by the elements of the red class ( <ref type="figure">Figure 3</ref>) such that a homeomorphic transformation in that space cannot linearly separate the two classes. As expected, only ANODEs and SONODEs can learn a mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Second order behaviour in SONODEs and ANODEs</head><p>Previously, the benefits of ANODEs were attributed only to the extra space they have in which to move <ref type="bibr" target="#b3">[4]</ref>. However, in this section, we show that coupled first order ODEs, such as ANODEs, are also able to represent higher-order order behaviour. Additionally, we study the functional forms ANODEs can use to learn this. Unless stated we consider ANODEs in their original formulation where a(t 0 ) = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">How do ANODEs learn second order dynamics?</head><p>Consider a SONODE as in Equation <ref type="formula" target="#formula_2">(3)</ref>. Similarly to the coupled ODE from Equation <ref type="formula" target="#formula_4">(4)</ref>, ANODEs can represent this if the state, z = [x, a], is augmented such that a has the same dimensionality as x:</p><formula xml:id="formula_8">z(t 0 ) = x(t 0 ) 0 ,? = a +?(t 0 ) f (a) (x,?, t, ? f ) = a + g(x(t 0 ), ? g ) f (a) (x, a + g(x(t 0 ), ? g ), t, ? f ) ,<label>(8)</label></formula><p>where a differentiates to the acceleration and, because a(t 0 ) = 0, the initial velocity is added to it to obtain the correct dynamics. Generalising this, it is clear to see how ANODEs can also learn k-th order ODEs, by splitting up the augmented part a into k ? 1 vectors with the same dimensionality as x. However, if ANODEs were to learn higher order dynamics this way, x(t 0 ) is required as an input, just as in data-controlled neural ODEs <ref type="bibr" target="#b13">[14]</ref>. To show this is not usually the case, we let ANODE(1) learn two 1D functions at the same time with a shared ODE, using the same set of parameters, but different initial conditions. Specifically, we consider two damped harmonic oscillators x 1 (t) = e ??t sin(?t), x 2 (t) = e ??t cos(?t) (9) where ? can be zero so that there is no decay.</p><p>SONODEs can learn these using the functional form  It is not immediately obvious how ANODEs could solve this, especially if they follow Equation <ref type="formula" target="#formula_8">(8)</ref>, where x(t 0 ) is needed as an input to determine?(t 0 ). However, <ref type="figure" target="#fig_4">Figure  4</ref> shows that ANODEs are able to fit the two functions in the same training session. We observe that ANODEs approximate a solution of the form:</p><formula xml:id="formula_9">f (a) (x,?, t, ? f ) = ?(? 2 + ? 2 )x ? 2??, g(x(0), ? g ) = ?(? + ?)x(0) + ?<label>(10)</label></formula><formula xml:id="formula_10">? a = Ca ? ?x ? ?x + ? ?a ? ?a ? 1 C (2? 2 x + ?? ? ? 2 )<label>(11)</label></formula><p>Using a(0) = 0, this gives the correct ODE and initial conditions in Equation <ref type="formula" target="#formula_0">(10)</ref>, for all finite, non-zero C.</p><p>We remark that the state x and the augmented dimension a are entangled in the velocity of the state and? = a. This example gives an intuition about the way ANODEs can learn second order behaviour through an ODE as in Equation <ref type="bibr" target="#b10">(11)</ref>. We now formalise this intuition and give a general expression:</p><p>Proposition 5.1. The general form ANODEs learn second order behaviour is given by:</p><formula xml:id="formula_11">? a = F (x, a, t, ? F ) G(x, a, t, ? G ) , G = ?F ?a T ?1 left f (a) ? ?F ?x T F ? ?F ?t<label>(12)</label></formula><p>This result is derived in Appendix D. It shows that SONODEs and ANODEs learn second order dynamics in different ways. ANODEs learn an abstract function F that at t 0 is equal to the initial velocity, and another function G that couples to F giving it the right acceleration. In contrast, SONODEs are constrained to learn the acceleration and initial velocity directly. This also leads to several useful properties that we investigate next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Minimal augmentation</head><p>The first property we analyse is called minimal augmentation. It refers to the fact that ANODEs can learn second order dynamics even when the number of extra dimensions is less than the dimensionality of the real space. Corollary 5.1.1. When the system from Proposition 5.1 is overdetermined (i.e. dim(a) &lt; dim(x)) and has a solution, the Moore-Penrose left pseudo-inverse produces that solution, given by G. If no solution exists, G is the best least-squares approximation.  In effect, ANODE is learning a system of linear equations parametrised by deep neural networks.</p><p>To learn second order dynamics with minimal augmentation, it must learn an overdetermined linear system allowing a solution. Depending on the form of?, it is possible that an F with explicit a dependence that produces a degenerate system like this could be learned. In turn, this would allow a complementary G to be learned. In fact, systems like this can naturally arise when the dynamics are latent and lower-dimensional and many of the observed dimensions become redundant. For instance, two spatial dimensions suffice for a pendulum moving in a plane of the 3D space.</p><p>However, even if an overdetermined system allowing a solution could not be learned due to the additional constraints acting on F , the left Moore-Penrose pseudo-inverse from Proposition 5.1 would still produce a G that is a best least-squares approximation. If the matrix A = ?F ?a T has full rank, then the left inverse is given by (A T A) ?1 A T . In general, the closer dim(a) gets to dim(x), the better this approximation will be.</p><p>To demonstrate minimal augmentation, we consider a two dimensional second order ODE, whose starting conditions and respective ?'s and ?'s were chosen randomly such that</p><formula xml:id="formula_12">? y = ?(? 2 x + ? 2 x )x ? 2? x? ?(? 2 y + ? 2 y )y ? 2? y? , x y = e ?0.1t (3 sin(t) + cos(t)) e ?0.3t (2 sin(1.2t) ? 5 cos(1.2t))<label>(13)</label></formula><p>ANODE(1) is able to learn this function as shown in <ref type="figure" target="#fig_6">Figure 5</ref>. Moreover, the augmented dimension trajectory differs greatly from the velocity of the ODE in either of the two spatial dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Interpretability of ANODEs</head><p>The result from Proposition 5.1 also raises the issue of how interpretable ANODEs are. For example, when investigating the dynamics of physical systems it is useful to know the force equation. This is straightforward with SONODEs, which directly learn the acceleration as a function of position, velocity and time. However, ANODEs learn the dynamics through an abstract alternative ODE where the state and augmented dimensions are entangled. This is similar to the widely studied problem of entangled representations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>We then train both ANODE(2) and SONODE to learn the dynamics of the ODE from Equation <ref type="formula" target="#formula_0">(13)</ref>, and provide them both with the correct initial velocity. <ref type="figure">Figure 6</ref> shows the results for two  <ref type="figure">Figure 6</ref>: ANODEs and SONODEs successfully learn the trajectory in real space of a 2D ODE for two different random initialisations. However, the augmented trajectories of ANODE are in both cases widely different from the true velocity of the ODE. In contrast, SONODE converges in both cases to the true ODE.</p><p>different runs for both models. Though ANODE(2) is able to learn the true trajectory in real space, the augmented trajectories differ greatly from the true velocity of the underlying ODE. In contrast, SONODE learns the correct velocity for both runs. This simple experiment confirms that ANODEs might not be a suitable investigative tool for scientific applications, where the physical interpretability of the results is important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">The functional loss landscape</head><p>The functional forms the two models converge to in <ref type="figure">Figure 6</ref> are not a coincidence. Proposition 5.1 also has deeper implications for the ANODE's (functional) loss landscape when learning second order dynamics. Please refer to Appendix D for the proofs of the following results. Proposition 5.2. There are an infinity of (non-trivial) functional forms ANODEs can learn that model the true second order dynamics in real space.</p><p>This means that there is an infinite number of functions ANODEs can approximate and obtain a zero loss. This suggests that an infinite number of global minima, representing different functions, may exist in the loss landscape of ANODEs. In contrast, we show that the second order constraints imposed on SONODE enforce that any global minima in its loss landscape approximate the same function -the acceleration and, in some cases, the initial velocity. Proposition 5.3. There is a unique functional form SONODEs can learn that models the true second order dynamics in real space. This is confirmed by our experiment from the previous section, where ANODE always converges to another augmented trajectory for each random initialisation (only two shown in the <ref type="figure">Figure 6</ref>), while SONODE always converges to the correct underlying ODE.  To test our above predictions, we perform an extensive comparison of ANODE and SONODE on a set of more challenging real and synthetic modelling tasks. These experiments provide further evidence for the described theoretical findings. Additional experimental details regarding the models and additional results are given in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Synthetic harmonic oscillators and noise robustness</head><p>Harmonic oscillator The most obvious application of SON-ODEs is on dynamical data from classical physics. This was tested by looking at a damped harmonic oscillator? = ?(? 2 + ? 2 )x ? 2?? with ? = 0.1 and ? = 1 on 30 random pairs of initial positions and velocities. These were each evolved for 10 seconds, using one hundred evenly spaced time stamps. The loss depended on both position and velocity explicitly, therefore the models used the state z = [x, v] with the option of augmentation for ANODEs. NODEs and ANODEs learnt a general?, whereas SONODEs are given? = [v, f (a) ] and only learn f (a) . SONODEs leverage their inductive bias and converge faster than the other models. Note that, all models were able to reduce the loss to approximately zero, as shown in <ref type="figure" target="#fig_7">Figure 7</ref>.</p><p>Noise robustness We tested the models' abilities to learn a sine curve in varying noise regimes. The models were trained on fifty training points in the first ten seconds of x = sin(t), and then tested with ten points in the next five seconds. The train points had noise added to them, drawn from a normal distribution N (0, ? 2 ) for different standard deviations ? = (0, 0.1, 0.2, . . . , 0.7). The results presented in <ref type="figure" target="#fig_8">Figure 8</ref> show that SONODEs are more robust to noise. The dotted line separates training and testing regimes. SONODEs are able to extrapolate better than ANODEs because they are forced to learn second order dynamics, and therefore are less likely to overfit the training points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experiments on real-world dynamical systems</head><p>Airplane vibrations The dataset <ref type="bibr" target="#b15">[16]</ref> concerns real vibrations measurements of an airplane. A shaker was attached underneath the right wing, producing an acceleration a 1 . Additional accelerations at different points were measured including a 2 , which was examined in this experiment, the acceleration on the right wing, next to a non-linear interface of interest. This is a higher order system, therefore it pertains to be a challenging modelling task. The results presented in <ref type="figure">Figure 9</ref> show that while both methods can model the dynamics reasonably well, ANODEs perform marginally better. We conjecture that this is due to ANODEs not being restricted to second order behaviour, allowing them to partially access higher order dynamics. We test this conjecture in Appendix E.  <ref type="figure">Figure 9</ref>: ANODE(1) and SONODE on the Airplane Vibrations dataset. ANODEs are able to perform slightly better than SONODEs because they are able to access higher order dynamics. The models were trained on the first 1000 timestamps and then extrapolated to the next 4000.</p><p>Silverbox oscillator The Silverbox dataset <ref type="bibr" target="#b20">[21]</ref> is an electronic circuit resembling a Duffing Oscillator, with input voltage V 1 (t) and measured output V 2 (t). The non-linear model Silverbox represents isV 2 = aV 2 + bV 2 + cV 3 2 + dV 1 . To account for this, all models included a V 3 2 term. The results can be seen in <ref type="figure">Figure 10</ref>. On this second order system, SONODEs extrapolate better than ANODEs and are able to capture the increase in the amplitude of the signal exceptionally well.  <ref type="figure">Figure 10</ref>: ANODE(1) and SONODE on the Silverbox dataset. SONODEs are able to reduce the loss faster and to a lower value than ANODEs, as expected when second order behaviour is built in. The models were trained on the first 1000 timestamps and extrapolated to the next 4000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and related work</head><p>SONODEs vs ANODEs SONODEs can be seen as a special case of ANODEs, whose phase space dynamics are restricted to model second order behaviour. We believe that for tasks where the trajectory is unimportant, and performance depends only on the endpoints (such as classification), ANODEs might perform better because they are unconstrained in how they use their capacity (see Appendix E.4). In contrast, we expect SONODEs to outperform ANODEs both in terms of accuracy and convergence rate on time series data whose underlying dynamics is assumed (or known) to be second order. In this setting, SONODEs have a unique functional solution and fewer local minima compared to ANODEs. At the same time, they have higher parameter efficiency since? = v requires no parameters, so all parameters are in the acceleration. Finally, we expect SONODEs to be more appropriate for application in the natural sciences, where second order dynamics are common and it is useful to recover the force equation.</p><p>Second Order Models Concurrent to our work, SONODEs have been briefly evaluated on MNIST by Massaroli et al. <ref type="bibr" target="#b13">[14]</ref> as part of a wider study on Neural ODEs. In contrast, our study is focused on the theoretical understanding of second order behaviour. At the same time, our investigations are largely based on learning the dynamics of physical systems rather than classification tasks. Second order models have also been considered in Graph Differential Equations <ref type="bibr" target="#b16">[17]</ref> and ODE 2 VAE <ref type="bibr" target="#b23">[24]</ref>.</p><p>Physics Based Models In the same way SONODEs assert Newtonian mechanics, other models have been made to use physical laws, guaranteeing physically plausible results, in discrete and continuous cases. Lutter et al. <ref type="bibr" target="#b12">[13]</ref> apply Lagrangian mechanics to cyber-physical systems, while Greydanus et al. <ref type="bibr" target="#b5">[6]</ref> and Zhong et al. <ref type="bibr" target="#b22">[23]</ref> use Hamiltonian mechanics to learn dynamical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we took a closer look at how Neural ODEs (NODEs) can learn second order dynamics.</p><p>In particular, we considered Second Order NODEs (SONODEs), a model constructed with this inductive bias in mind, and the more general class of Augmented Neural ODEs (ANODEs). We began by shedding light on the optimisation of SONODEs by generalising the adjoint sensitivity method from NODEs and comparing it with the training procedure of the equivalent coupled ODE. We also studied the theoretical properties of SONODEs and how they manifest in modelling toy problems.</p><p>We showed that, despite lacking the physics-based inductive biases of SONODEs, ANODEs are flexible enough to learn second order dynamics in practice. However, we also demonstrated, analytically and empirically, that they do this by learning to approximate an abstract coupled ODE where the state and augmented dimensions become entangled in the velocity. We proved that this has implications for interpretability in scientific applications as well as the 'shape' of the loss landscape. Our experiments on synthetic and real second order dynamical systems validate these concerns and reveal that the inductive biases of SONODE are generally beneficial in this setting. Although this work investigates second order dynamics, the underlying principles of SONODEs can be readily extended to higher orders (a proof-of-principle is given in Appendix E.2). This, in turn, allows for modelling richer and more complex behaviour, while retaining the benefits of faster training and better modelling performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Neural ODEs are relatively new models and we are yet to see their full potential. We anticipate NODEs will see particular success in time-series data, which have a wide variety of real-world applications. Examples given by Jia and Benson <ref type="bibr" target="#b9">[10]</ref> include the evolution of individuals' medical records and earthquake monitoring. Poli et al. <ref type="bibr" target="#b16">[17]</ref> look at traffic forecasting and Greydanus et al. <ref type="bibr" target="#b5">[6]</ref> show how a Neural ODE inspired by Hamiltonian mechanics can be applied to classical physics. Our work concerns Second Order Neural ODEs which can also be applied to classical physics, where Newton's second law describes the forces on an object.</p><p>Our theoretical work was concerned with demonstrating how best to use the adjoint method on SONODEs, and showing how the coupled ODE perspective of ANODEs leads to them being able to learn second order behaviour. Naturally, any impacts from this work will come from the applications of SONODEs.</p><p>We directly investigated two potential real-world applications of SONODEs. The Silverbox dataset, an electronic implementation of a damped spring with a non-linear spring constant. This naturally applies to circuits with oscillators, and damped elements, opening new directions to monitor circuits and signals. The dynamics can also be encountered in mechanical systems, including car suspension, which could be used to improve car safety. Note that, in our experiments, we also investigated the task of modelling the vibration dynamics of an aeroplane, which might lead to better and optimal aeroplane designs. Though contributions to civil mechanical engineering such as these have parallel applications in the design of weapons, it is not the case that our investigation expands technological capabilities in such a way as to enable new forms of warfare or to significantly improve current technologies (at this stage.)</p><p>As stated, Neural ODEs are relatively new, and we are yet to see their full potential. We anticipate more applications to time series data in the future, which have many positive and negative applications, though at most we should think of our contribution as incremental in this regard and covered by existing institutions and norms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Phase Space Trajectory Proofs</head><p>Here we present the proofs for the propositions from Section 4, concerning a k-th order initial value problem. Lemma A.1. For a k-th order IVP, where the k-th derivative is Lipschitz continuous, a solution cannot have discontinuities in the time derivative of its phase space trajectory.</p><p>Proof. Consider the phase space trajectory</p><formula xml:id="formula_13">z(t) = x(t), dx dt (t), ..., d k?1 x dt k?1 (t) . Let f be the k-th time derivative of x(t). Then the time derivative of z(t) is d dt ? ? ? ? ? ? ? x dx dt ... d k?1 x dt k?1 ? ? ? ? ? ? ? = ? ? ? ? ? ? ? dx dt d 2 x dt 2 ... f (z) ? ? ? ? ? ? ?</formula><p>If for one set of finite arguments, z 1 , f (z 1 ) is also finite, then because the gradients of f are all bounded (due to Lipschitz continuity), for any other finite arguments, z n , f (z n ) will remain finite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now consider</head><formula xml:id="formula_14">d k?1 x dt k?1 , its time derivative is f (z(t))</formula><p>, which is finite for all finite z. Therefore, d k?1 x dt k?1 , can't have discontinuities with a finite derivative, and also must be finite for finite z. Now consider d k?2 x dt k?2 , its time derivative is finite for all finite z, and therefore it can't have discontinuities and also must be finite for all finite z. This line of argument continues up to x. The state x and all of its time derivatives up to the k-th have no discontinuities and are finite. Therefore as long as the initial conditions z(t 0 ) are finite, there can be no discontinuities in the time derivative of the phase space trajectory at finite time. Proof. Consider two trajectories z 1 (t) and z 2 (t) that have different initial conditions z 1 (t 0 ) = h 1 and z 2 (t 0 ) = h 2 . Assume the trajectories cross at a point in phase space at an angle, z 1 (t 1 ) = z 2 (t 2 ) =h. If they intersect at an angle, then evolving the two states by a small time ?t &lt;&lt; 1, and using the Lipschitz continuity of f , meaning that the trajectories cannot have kinks in them (as shown in Lemma A.1), z 1 (t 1 + ?t) = z 2 (t 2 + ?t). However, if they are at the same point in phase space, then they must have the same k-th order derivative, f . All other derivatives are equal, so by evolving the states by the same small time ?t &lt;&lt; 1, z 1 (t 1 + ?t) = z 2 (t 2 + ?t). There is a contradiction and therefore the assumption is wrong, unique trajectories cannot cross at an angle in phase space when f is Lipschitz continuous and has no t dependence. Now consider the single trajectory z(t). Assume it intersects itself at an angle, at t 1 and t 2 . Now consider two particles on this trajectory, starting at t 1 ? ? and t 2 ? ? such that t 2 ? ? &gt; t 1 . These two particles have different initial conditions and cross at an angle. However, the above shows that cannot happen. Therefore, the assumption that z(t) can intersect itself at an angle must be wrong. Trajectories cannot intersect themselves in phase space at an angle.</p><p>Trajectories can, however, feed into each other representing the same particle path at different times. Single phase space trajectories can feed into themselves representing periodic motion. This requires a Lipschitz continuous f , and for there to be no explicit time dependence. If there was time dependence then two trajectories can cross at different times, and a trajectory can self intersect. Effectively an additional dimension is added to phase space, which is time. The propositions above would still hold because dt dt = 1 which is Lipschitz continuous. Therefore, with time included as a phase space dimension, intersections in space are only forbidden if they occur at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Adjoint Sensitivity Method</head><p>We present a proof to both the first and second order Adjoint method, using a Lagrangian style approach <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref>. We also prove that when the underlying ODE is second order, using the first order method on a concatenated state, z = [x, v], produces the same results as the second order method but does so more efficiently. All parameters, ?, are time-dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 First Order Adjoint Method</head><p>Let L denote a scalar loss function, L = L(x(t n )), the gradient with respect to a parameter ? is</p><formula xml:id="formula_15">dL d? = ?L ?x(t n ) T dx(t n ) d?<label>(14)</label></formula><p>The vector ?L ?x(t n ) T is found using backpropagation. For dynamical data the loss will depend on multiple time stamps, there is also a sum over timestamps, t n . Therefore</p><formula xml:id="formula_16">dx(t n ) d? is needed. x(t n ) follows x(t n ) = tn t0? (t)dt + x(t 0 )<label>(15)</label></formula><p>subject</p><formula xml:id="formula_17">to? = f (v) (x, t, ? f ), x(t 0 ) = s(X 0 , ? s )<label>(16)</label></formula><p>where X 0 is the data going into the network and is constant. The functions f (v) and s describe the ODE and the initial conditions. Here we allow X 0 to first go through the transformation, s(x 0 , ? s ). This maintains generality and allows NODEs to be used as a component of a larger model. For example, X 0 could go through a ResNet before the NODE, and then through a softmax classifier at the end (which is accounted for in the term ?L ?x(t n ) T ). Introduce the new variable F </p><formula xml:id="formula_18">? + A(t)(? ? f (v) ) dt + B(x(t 0 ) ? s)<label>(17)</label></formula><p>These are equivalent because (? ? f (v) ) and (x(t 0 ) ? s) are both zero. This means the matrices, A(t) and B, can be chosen freely (as long as they are well behaved, finite etc.), to make the computation easier. The gradients of x(t n ) with respect to the parameters are</p><formula xml:id="formula_19">dx(t n ) d? f = dF d? f , dx(t n ) d? s = dF d? s + ds(X 0 , ? s ) d? s<label>(18)</label></formula><p>Differentiating F with respect to a general parameter ? </p><formula xml:id="formula_20">dF d? = tn t0 d? d? dt + tn t0 A(t) d? d? ? ?f (v) ?? ? ?f (v) ?x T dx d? dt + B dx(t 0 ) d? ? ds d?<label>(19)</label></formula><formula xml:id="formula_21">dF d? = dx d? + A(t) dx d? tn ? dx d? + A(t) dx d? t0 ? tn t0 A(t) ?f (v) ?? dt ? tn t0 ? (t) + A(t) ?f (v) ?x T dx d? dt + B dx d? t0 ? ds d?<label>(21)</label></formula><p>Using the freedom of choice of A(t), let it follow the OD?</p><formula xml:id="formula_22">A(t) = ?A(t) ?f (v) ?x T , A(t n ) = ?I<label>(22)</label></formula><p>Where I is the identity matrix. Then the first term and second integral in Equation <ref type="formula" target="#formula_0">(21)</ref> become zero,</p><formula xml:id="formula_23">yielding dF d? = (B ? I ? A(t 0 )) dx d? t0 + t0 tn A(t) ?f (v) ?? dt ? B ds d?<label>(23)</label></formula><p>Now using the freedom of choice of B, let it obey the equation</p><formula xml:id="formula_24">B = I + A(t 0 )<label>(24)</label></formula><p>This makes the first term in Equation <ref type="formula" target="#formula_1">(23)</ref> zero. This gives the final form of</p><formula xml:id="formula_25">dF d? dF d? = t0 tn A(t) ?f (v) ?? dt ? (I + A(t 0 )) ds d?<label>(25)</label></formula><p>Subbing into Equation <ref type="formula" target="#formula_0">(18)</ref> and using the fact that f (v) has no ? s dependence and s has no ? f dependence</p><formula xml:id="formula_26">dx(t n ) d? f = t0 tn A(t) ?f (v) (x, t, ? f ) ?? f dt, dx(t n ) d? s = ?A(t 0 ) ds(X 0 , ? s ) d? s<label>(26)</label></formula><p>This leads to the gradients of the loss</p><formula xml:id="formula_27">dL d? f = ?L ?x(t n ) T t0 tn A(t) ?f (v) (x, t, ? f ) ?? f dt, dL d? s = ? ?L ?x(t n ) T A(t 0 ) ds(X 0 , ? s ) d? s<label>(27)</label></formula><p>Subject to the ODE for A(t)</p><formula xml:id="formula_28">A(t) = ?A(t) ?f (v) (x, t, ? f ) ?x , A(t n ) = ?I<label>(28)</label></formula><p>Now introduce the adjoint state r(t)</p><formula xml:id="formula_29">r(t) = ?A(t) T ?L ?x(t n ) , r(t) T = ? ?L ?x(t n ) T A(t)<label>(29)</label></formula><p>Using the fact that ?L ?x(t n )</p><p>is constant with respect to time, the adjoint equations are obtained by applying the definition of the adjoint in Equation <ref type="formula" target="#formula_1">(29)</ref>, to the gradients in Equation <ref type="formula" target="#formula_1">(27)</ref>, and multiplying the ODE in Equation <ref type="formula" target="#formula_1">(28)</ref>  </p><formula xml:id="formula_30">dL d? f = ? t0 tn r(t) T ?f (v) (x, t, ? f ) ?? f dt, dL d? s = r(t 0 ) T ds(X 0 , ? s ) d? s<label>(30)</label></formula><p>Where the adjoint a(t) follows the OD?</p><formula xml:id="formula_31">r(t) = ?r(t) T ?f (v) (x, t, ? f ) ?x , r(t n ) = ?L ?x(t n )<label>(31)</label></formula><p>The gradients are found by integrating the adjoint state, r, and the real state, x, backwards in time, which requires no intermediate values to be stored, using constant memory, a major benefit over traditional backpropagation.</p><p>These are the same equations that were derived by Chen et al. <ref type="bibr" target="#b2">[3]</ref>, however this includes the addition of letting x(t 0 ) = s(X 0 , ? s ) giving the corresponding gradient, dL d? s . Additionally, the derivation used by Chen et al. <ref type="bibr" target="#b2">[3]</ref> is simpler but does not present an obvious way to extend the adjoint method to second order ODEs, which this derivation method can do, as shown next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Second Order Adjoint</head><p>Using the same derivation method, but with a second order differential equation, a second order adjoint method is derived, according to the proposition from the main text:</p><p>Proposition 3.1. The adjoint state r(t) of SONODEs follows the second order OD?</p><formula xml:id="formula_32">r = r T ?f (a) ?x ?? T ?f (a) ?? ? r T d dt ?f (a) ??<label>(32)</label></formula><p>and the gradients of the loss with respect to the parameters of the acceleration, ? f are</p><formula xml:id="formula_33">dL d? f = ? t0 tn r T ?f (a) ?? f dt,<label>(33)</label></formula><p>Proof. In general, the loss function, L, depends on x and?</p><formula xml:id="formula_34">dL d? = ?L ?x(t n ) T dx(t n ) d? + ?L ??(t n ) T d?(t n ) d?<label>(34)</label></formula><p>The gradients from the positional part and the velocity part are found separately and added. Firstly the position</p><formula xml:id="formula_35">x(t n ) = tn t0? (t)dt + x(t 0 )<label>(35)</label></formula><p>Subject to the second order OD?</p><formula xml:id="formula_36">x = f (a) (x,?, t, ? f ), x(t 0 ) = s(X 0 , ? s ),?(t 0 ) = g(x(t 0 ), ? g )<label>(36)</label></formula><p>Following the same procedure as in first order, but including the initial condition for the velocity as well</p><formula xml:id="formula_37">F = tn t0? + A(t)(? ? f (a) )dt + B(?(t 0 ) ? g) + C(x(t 0 ) ? s)<label>(37)</label></formula><p>As before, the vectors, (? ? f (a) ), (?(t 0 ) ? g) and (x(t 0 ) ? s) are zero, which gives freedom to choose the matrices A(t), B and C to make the calculation easier. The gradients of x(t n ) with respect to the parameters ? are</p><formula xml:id="formula_38">dx(t n ) d? f = dF d? f , dx(t n ) d? g = dF d? g , dx(t n ) d? s = dF d? s + ds(X 0 , ? s ) d? s<label>(38)</label></formula><p>Differentiating F from equation 37 with respect to a general parameter </p><formula xml:id="formula_39">dF d? = dx d? tn t0 ? tn t0 A(t) ?f (a) ?? dt + tn t0 A(t) d? d? ? ?f (a) ?x T dx d? ? ?f (a) ?? T d? d? dt + B d? d? t0 ? ?g ?? ? ?g ?x(t 0 ) T dx(t 0 ) d? + C dx d? t0 ? ds d?<label>(39)</label></formula><formula xml:id="formula_40">+ tn t0 ? (t) ? A(t) ?f (a) ?x T + d dt A(t) ?f (a) ?? T dx d? dt + t0 tn A(t) ?f (a) ?? dt + B d? d? t0 ? ?g ?? ? ?g ?x(t 0 ) T dx(t 0 ) d? + C dx d? t0 ? ds d? (42)</formula><p>Using the freedom to choose A(t), let it follow the second order OD?</p><formula xml:id="formula_41">A(t) = A(t) ?f (a) ?x T ? d dt A(t) ?f (a) ?? T , A(t n ) = 0,?(t n ) = I (43)</formula><p>This makes the first term and first integral in Equation (42) zero, yielding</p><formula xml:id="formula_42">dF d? = t0 tn A(t) ?f (a) ?? dt+ ? (t) + A(t) ?f (a) ?? T ? I ? B ?g ?x(t 0 ) T + C dx d? t0 + (B ? A) d? d? t0 ? B ?g ?? ? C ds d? (44)</formula><p>Now using the freedom of choice in B and C</p><formula xml:id="formula_43">B = A(t 0 ), C = ??(t 0 ) ? A(t 0 ) ?f (a) ?? t0 + I + A(t 0 ) ?g ?x(t 0 ) T (45)</formula><p>This makes the second and third terms in Equation <ref type="formula" target="#formula_4">(44)</ref>  These give the final gradients of x(t n ) with respect to the parameters, by subbing the results for B, C and dF d? above into Equation (38), using the fact that f (a) , g and s only depend on the parameters ? f , ? g and ? s respectively</p><formula xml:id="formula_44">dx(t n ) d? f = t0 tn A(t) ?f (a) ?? f dt, dx(t n ) d? g = ?A(t 0 ) ?g ?? g dx(t n ) d? s = ? (t 0 ) + A(t 0 ) ?f (a) ?? T t0 ? ?g ?x(t 0 ) T ds d? s (47)</formula><p>As before, introduce the adjoint state r x (t):</p><formula xml:id="formula_45">r x (t) = ?A(t) T ?L ?x(t n ) , r x (t) T = ? ?L ?x(t n ) T A(t)<label>(48)</label></formula><p>Using the fact that ?L ?x(t n )</p><p>is constant with respect to time, all the results above, and the ODE and initial conditions for A(t) in Equation (43) can be multiplied by ? ?L ?x(t n ) T , to get the gradients dL d? in terms of r x (t)</p><formula xml:id="formula_46">dL d? f = ? t0 tn r x (t) T ?f (a) ?? f dt, dL d? g = r x (t 0 ) T ?g ?? g dL d? s = ?? x (t 0 ) T ? r x (t 0 ) T ?f (a) ?? T t0 ? ?g ?x(t 0 ) T dx(t 0 ) d? s<label>(49)</label></formula><p>Subject to the second order ODE for r x (t)</p><formula xml:id="formula_47">r x (t) = r x (t) T ?f (a) ?x ? d dt r x (t) T ?f (a) ?? , r x (t n ) = 0,? x (t n ) = ? ?L ?x(t n ) (50)</formula><p>Where after differentiating with the product rule the ODE in Equation <ref type="formula" target="#formula_5">(50)</ref> becomes</p><formula xml:id="formula_48">r x (t) = r x (t) T ?f (a) ?x ?? x (t) T ?f (a) ?? ? r x (t) T d dt ?f (a) ??<label>(51)</label></formula><p>Where doing the full time derivative gives</p><formula xml:id="formula_49">d dt ?f (a) ?? = [? T , f (a)T , 1] ? x ?? ? t ?f (a) ??<label>(52)</label></formula><p>Where the fact that? = f (a) has been used. This is only when the loss depends on the position. The same method is used to look at the velocity part in Equation (34)</p><formula xml:id="formula_50">dL d? = ?L ??(t n ) T d?(t n ) d? (53) Where? (t n ) = tn t0? (t)dt +?(t 0 ) (54)</formula><p>The general method is to take this expression and add zeros, in the form of A(t), B and C multiplied by the ODE and initial conditions, (? ? f (a) ), (?(t 0 ) ? g) and (x(t 0 ) ? s). Then differentiate with respect to a general parameter ? and integrate by parts to get any integrals containing d? d? or d? d? in terms of dx d? . Then choose the ODE for A(t) to remove any dx d? terms in the integral, and the initial conditions of A(t n ) to remove the boundary terms at t n . Then B and C are chosen to remove the boundary terms at t 0 . After doing this the gradients of? with respect to the parameters are</p><formula xml:id="formula_51">d?(t n ) d? f = t0 tn A(t) ?f (a) ?? f dt, d?(t n ) d? g = ?A(t 0 ) ?g ?? g d?(t n ) d? s = ? (t 0 ) + A(t 0 ) ?f (a) ?? T t0 ? A(t 0 ) ?g ?x(t 0 ) T ds d? s (55)</formula><p>Subject to the second order ODE for A(t)</p><formula xml:id="formula_52">A(t) = A(t) ?f (a) ?x T ? d dt A(t) ?f (a) ?? T , A(t n ) = ?I,?(t n ) = ?f (a) ?? T tn (56) Now introduce the state r v (t) r v (t) = ? ?L ??(t n ) T A(t), r v (t) = ?A(t) T ?L ??(t n )<label>(57)</label></formula><p>Which allows the gradients of the loss with respect to the parameters to be written as</p><formula xml:id="formula_53">dL d? f = ? t0 tn r v (t) T ?f (a) ?? f dt, dL d? g = r v (t 0 ) T ?g ?? g dL d? s = r v (t 0 ) T ?g ?x(t 0 ) T ?? v (t 0 ) T ? r v (t 0 ) T ?f (a) ?? T t0 ds d? s<label>(58)</label></formula><p>Where r v follows the second order ODE and initial conditions</p><formula xml:id="formula_54">r v (t) = r v (t) T ?f (a) ?x ?? v (t) T ?f (a) ?? ? r v (t) T d dt ?f (a) ?? r v (t n ) = ?L ??(t n ) ,? v (t n ) = ? ?L ??(t n ) T ?f (a) ?? tn (59)</formula><p>Now adding the gradients from the x dependence and the? dependence together. It can be seen that the gradients are the same in Equations (49) and (58), but just swapping r x and r v . Additionally, it can be seen from the ODEs for r x and r v in Equations (51) and (59), that they are governed by the same, linear, second order ODE, with different initial conditions. Therefore the gradients, dL d? , can be written in terms of a new adjoint state,</p><formula xml:id="formula_55">r = r x + r v dL d? f = ? t0 tn r(t) T ?f (a) (x,?, t, ? f ) ?? f dt, dL d? g = r(t 0 ) T ?g(x(t 0 ), ? g ) ?? g dL d? s = r(t 0 ) T ?g(x(t 0 ), ? g ) ?x(t 0 ) T ??(t 0 ) T ? r(t 0 ) T ?f (a) (x,?, t, ? f ) ?? T t0 ds(X 0 , ? s ) d? s (60)</formula><p>Where a follows the second order ODE with initial conditions</p><formula xml:id="formula_56">r(t) = r(t) T ?f (a) (x,?, t, ? f ) ?x ??(t) ?f (a) (x,?, t, ? f ) ?? ? r(t) T d dt ?f (a) (x,?, t, ? f ) ?? r(t n ) = ?L ??(t n ) ,?(t n ) = ? ?L ?x(t n ) ? ?L ??(t n ) T ?f (a) (x,?, t, ? f ) ?? tn (61)</formula><p>The full derivative, d t (??f (a) ), is given by Equation <ref type="formula" target="#formula_1">(52)</ref>. The ODE can also be written compactly asr</p><formula xml:id="formula_57">(t) = r(t) T ?f (a) (x,?, t, ? f ) ?x ? d dt r(t) T ?f (a) (x,?, t, ? f ) ?? (62)</formula><p>Just as in the first order method, a sum over times stamps t n may be required. This matches and extends on the gradients and ODE given by proposition 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Equivalence between the two Adjoint methods</head><p>When acting on a concatenated state, z(t) = [x(t), v(t)], the first order adjoint method will produce the same gradients as the second order adjoint method. However, it is more computationally efficient to use the first order method. This is also given in the main text as the following proposition:</p><formula xml:id="formula_58">Proposition 3.2.</formula><p>The gradient of ? f computed through the adjoint of the coupled ODE from (4) and the gradient from (6) are equivalent. However, the latter requires at least as many matrix multiplications as the former.</p><p>Intuitively, the first order method will produce the same gradients because second order dynamics can be thought of as two coupled first order ODEs, where the first order dynamics happen in phase space.</p><p>However, this provides no information about computational efficiency. We prove the equivalence and compare the computational efficiencies below.</p><p>Proof. The first order formulation of second order dynamics can be written as</p><formula xml:id="formula_59">z(t) = x(t) v(t) ,? = v f (a) (x, v, t, ? f ) , z(t 0 ) = x(t 0 ) v(t 0 ) = s(X 0 , ? s ) g(s(X 0 , ? s ), ? g )<label>(63)</label></formula><p>When using index notation, x i and v i are concatenated to make z i. . For x i and v i , the index, i, ranges from 1 to d, whereas for z i it ranges from 1 to 2d accounting for the concatenation. This is represented below</p><formula xml:id="formula_60">z i = x i , if i ? d v (i?d) , if i ? d + 1 (64) Relabelling the first index (i ? d) ? ? i r B i = ? d j=1 r A j ?v j ?v i ? d j=1 r B j ?f (a) j ?v i (75)</formula><p>Noting that, ?v j ?v i = ? ij , the time derivatives can be written in vector matrix notation a?</p><formula xml:id="formula_61">r A (t) = ?r B (t) T ?f (a) (x, v, t, ? f ) ?x (76) r B (t) = ?r A (t) ? r B (t) T ?f (a) (x, v, t, ? f ) ?v<label>(77)</label></formula><p>Differentiating Equation (77)</p><formula xml:id="formula_62">r B (t) = r B (t) T ?f (a) (x, v, t, ? f ) ?x ? d dt r B (t) T ?f (a) (x, v, t, ? f ) ?v<label>(78)</label></formula><p>This matches the ODE for the second order method in Equation (62). Now applying the initial conditions, using index notation again</p><formula xml:id="formula_63">r i (t n ) = ?L ?z i (t n ) (79) For i ? d r i = r A i (t n ) = ?L ?x i (t n )<label>(80)</label></formula><formula xml:id="formula_64">For i ? d + 1 r i (t n ) = r B (i?d) (t n ) = ?L ?v (i?d) (t n ) ? ? r B i (t n ) = ?L ?v i (t n )<label>(81)</label></formula><p>Applying these initial conditions in r A and r B to Equation (77)</p><formula xml:id="formula_65">r B i (t n ) = ? ?L ?x i (t n ) ? ?L ?v j (t n ) ?f (a) j ?v i tn<label>(82)</label></formula><p>By looking at the ODE and initial conditions, it is clear r B is equivalent to the second order adjoint, in Equation (61). Now looking at the gradients, and including an explicit sum over the index</p><formula xml:id="formula_66">dL d? f = ? t0 tn 2d i=1 r i ?f (v) i ?? f dt ? ? = ? t0 tn d i=1 r A i ?v i ?? f dt ? t0 tn 2d i=d+1 r B (i?d) ?f (a) (i?d) ?? f dt (83)</formula><p>The first term is zero because v has no explicit ? dependence. The second term, after relabelling and using summation convention becomes</p><formula xml:id="formula_67">dL d? f = ? t0 tn r B i (t) ?f (a) i ?? f dt = ? t0 tn r B (t) T ?f (a) ?? f dt<label>(84)</label></formula><p>Where? f = ? f has been used, as they are both the parameters for the acceleration. This matches the result for gradients of parameters in the acceleration term ? f , when using the second order adjoint method, because r B is the adjoint.</p><p>Looking at the gradients related to the initial conditions dL d? s = r(t 0 ) T ds(X 0 ,? s ) d? s (85)</p><p>After going through the previous process of separating out the sums from 1 ? ? d and d + 1 ? ? 2d, then relabelling the indices on r B , this becomes</p><formula xml:id="formula_68">= r A i (t 0 ) ds i (X 0 , ? s ) d? s + r B i (t 0 ) dg i (s(X 0 , ? s ), ? g ) d? s<label>(86)</label></formula><p>Using the expression for r A by rearranging Equation <ref type="formula" target="#formula_7">(77)</ref>, this can be written as</p><formula xml:id="formula_69">dL d? s = ?? B i (t 0 ) ? r B j (t 0 ) ?f (a) j ?v i t0 ds i d? s + r B i (t 0 ) dg i d? s<label>(87)</label></formula><p>The parameters? s contain both ? s and ? g . Looking at ? g first, where s(X 0 , ? s ) has no dependence</p><formula xml:id="formula_70">dL d? g = r B i (t 0 ) ?g i (s(X 0 , ? s ), ? g ) ?? g = r B (t 0 ) T ?g(s(X 0 , ? s ), ? g ) ?? g<label>(88)</label></formula><p>where dg d? g can be written as a partial derivative, because X 0 and ? s have no dependence on ? g at all. This expression is equivalent to dL d? g found using the second order adjoint method. Now looking at the parameters ? s , these parameters are in s(X 0 , ? s ) explicitly and g(s, ? g ), implicitly through s. Subbing? s = ? s into Equation <ref type="formula" target="#formula_7">(87)</ref> gives</p><formula xml:id="formula_71">dL d? s = ?? B i (t 0 ) ? r B j (t 0 ) ?f (a) j (x, v, t, ? f ) ?v i t0 + r B j (t 0 ) ?g j (s(X 0 , ? s ), ? g ) ?s i ds i (X 0 , ? s ) d? s<label>(89)</label></formula><p>Using the fact that x(t 0 ) = s, this is the same result for dL d? s found using the second order adjoint method:</p><formula xml:id="formula_72">dL d? s = r B (t 0 ) T ?g(x(t 0 ), ? g ) ?x(t 0 ) T ?? B (t 0 ) T ? r B (t 0 ) T ?f (a) (x, v, t, ? f ) ?v T t0 ds(X 0 , ? s ) d? s<label>(90)</label></formula><p>The time derivatives and intial conditions for the second order adjoint representation are</p><formula xml:id="formula_73">d dt r(t) =?(t) d dt? (t) = r(t) T ?f (a) (x, v, t, ? f ) ?x ??(t) T ?f (a) (x, v, t, ? f ) ?v ? r(t) T d dt ?f (a) (x, v, t, ? f ) ?v r(t n ) = ?L ?v(t n ) r(t n ) = ? ?L ?x(t n ) ? ?L ?v(t n ) T ?f (a) (x, v, t, ? f ) ?v tn (92) Where d dt ?f (a) ?v = [v T , f (a)T , 1] ? x ? v ? t ?f (a) ?v<label>(93)</label></formula><p>Looking at Equations (91) and (92), the second order method has the additional term, r ? d t (? v (f (a) )), in the ODE, and the additional term, (? v L) ? (? v f (a) ) in the initial conditions. The first order method acting on the concatenated state, [x, v], requires equal or fewer matrix multiplications than the second order method acting on x, to find the gradients at each step and the initial conditions. This is in the general case, but also for all specific cases, it is as efficient or more efficient. The same is also true for calculating the final gradients.</p><p>The reason for the difference in efficiencies is the state, r B , is the adjoint, and the state, r A , contains a lot of the complex information about the adjoint. It is an entangled representation of the adjoint, contrasting with the disentangled second order representation [r,?]. This is similar to how ANODEs can learn an entangled representation of second order ODEs and SONODEs learn the disentangled representation, seen in Section 5.3. However, entangled representations are more useful here, because they do not need to be interpretable, they just need to produce the gradients, and the entangled representation can do this more efficiently.</p><p>This analysis provides useful information on the inner workings of the adjoint method. It shows a second order specific method does exist, but the first order method acting on a state z = [x, v] will produce the same gradients more efficiently, due to how it represents the complexity. This was specific to second order ODEs, however, the first order adjoint will work on any system of ODEs, because any motion can be thought of as being first order motion in phase space. Additionally, the first order method may be the most efficient adjoint method. The complexity going from the first order to the second order was seen based on the calculation, so this is only likely to get worse as the system of ODEs becomes more complicated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Second Order ODEs are not Homeomorphisms</head><p>One of the conditions for a transformation to be a homeomorphism is for the transformation to be bijective (one-to-one and onto). In real space, a transformation that evolves according to a second order ODE does not have to be one-to-one. This is demonstrated using a one-dimensional</p><formula xml:id="formula_74">counter-example? = 0 ? ? x(t) = x 0 + v 0 t x 0 = [0] [1] , v 0 = ?x 0 + 2 = [2] [1]</formula><p>If t 0 = 0 and t N = 1</p><formula xml:id="formula_75">x(1) = [2] [2]</formula><p>So the transformation in real space is not always one-to-one, and therefore, not always a homeomorphism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ANODEs learning 2nd Order</head><p>Here we present the proofs for the propositions from Section 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Functional Form Proofs</head><p>Proposition 5.1.The general form ANODEs learn second order behaviour is given by:</p><formula xml:id="formula_76">? a = F (x, a, t, ? F ) G(x, a, t, ? G ) , G = ?F ?a T ?1 left f (a) ? ?F ?x T F ? ?F ?t (94)</formula><p>Proof. Let z(t) be the state vector [x(t), a(t)]. The time derivatives can be written as</p><formula xml:id="formula_77">?(t) a(t) = F (x, a, t, ? F ) G(x, a, t, ? G ) (95) Let x(t) follow the second order ODE,? =? = f (a) (x,?, t, ? f ). Differentiating F with respect to time? = ?F ?x T? + ?F ?a T? + ?F ?t = f (a) (x,?, t, ? f ) (96) Using? = F and? = G f (a) (x, F, t, ? f ) = ?F ?x T F + ?F ?a T G + ?F ?t (97) Rearranging for G G(x, a, t, ? G ) = ?F ?a T ?1 left f (a) (x, F, t, ? f ) ? ?F ?x T F ? ?F ?t<label>(98)</label></formula><p>In order for the solution of G to exist, the matrix ?F ?a T must be invertible. Either the dimension of a matches F , x and f (a) , so that ?F ?a T is square, or ?F ?a T has a left inverse. Crucially, F must have explicit a dependence, or the inverse does not exist. Intuitively, in order for real space to couple to augmented space, there must be explicit dependence.</p><p>Using the equation for G(x, a, t, ? G ), there is a gauge symmetry in the system, which proves proposition 5.2.</p><p>Proposition 5.2. ANODEs can learn an infinity of (non-trivial) functional forms to learn the true dynamics of a second order ODE in real space.</p><p>Proof. Assume a solution for F (x, a, t, ? F ) and G(x, a, t, ? G ) has been found such that,? = f (a) and F (x 0 , a 0 , t 0 , ? F ) =? 0 . If an arbitrary function of</p><formula xml:id="formula_78">x, ?(x), is added to F , where ?(x 0 ) = 0 F (x, a, t, ? F ) = F (x, a, t, ? F ) + ?(x)<label>(99)</label></formula><p>The initial velocity is still the same. The dynamics are preserved if there is a corresponding change in GG</p><formula xml:id="formula_79">(x, a, t, ? G ) = ?(F + ?) ?a T ?1 f (a) (x, F + ?, t, ? f ) ? ?(F + ?) ?x T (F + ?) ? ?(F + ?) ?t</formula><p>(100) The proof can end here, however this can be simplified. ?(x) has no explicit a or t dependence, so this equation simplifies t?</p><formula xml:id="formula_80">G = ?F ?a T ?1 f (a) (x, F + ?, t, ? f ) ? ?F ?x T F ? ?F ?t ? ?F ?x T ? ? ?? ?x T F ? ?? ?x T ?<label>(101)</label></formula><p>The term f (a) (x, F + ?, t, ? f ) can be Taylor expanded (assuming convergence)</p><formula xml:id="formula_81">f (a) (x, F + ?, t, ? f ) = f (a) (x, F, t, ? f ) + ? n=1 ? n f (a) (x,?, t, ? f ) ?? T n ?=F ? n n! (102)</formula><p>Which gives the corresponding change in G</p><formula xml:id="formula_82">G = G(x, a, t, ? G )+ ?F ?a T ?1 ? n=1 ? n f (a) ?? T n ?=F ? n n! ? ?F ?x T ? ? ?? ?x T F ? ?? ?x T ? (103)</formula><p>This demonstrates that there are infinite functional forms that ANODEs can learn. This only considered perturbing functions ?(x). More complex functions can be added that have a or t dependence, which lead to a more complex change in G. By contrast, we now show SONODEs have a unique functional form.</p><p>Proposition 5.3. SONODEs learn to approximate a unique functional form to learn the true dynamics of a second order ODE in real space.</p><p>Proof. Consider a dynamical system</p><formula xml:id="formula_83">d 2 x dt 2 = f (x, v, t), x(t 0 ) = x 0 , v(t 0 ) = v 0<label>(104)</label></formula><p>For these problems we let the loss only depend on the position, if it depends on position and velocity there would be more restrictions. So if it is true when loss only depends on the position, it is also true when it depends on both position and velocity.</p><p>Assume that there is another system, that has the same position as a function of time</p><formula xml:id="formula_84">d 2x dt 2 =f (x,?, t),x(t 0 ) =x 0 ,x(t 0 ) =? 0<label>(105)</label></formula><p>Where f (x, v, t) =f (x,?, t). Because the initial conditions are given the position and velocity are defined at all times, and therefore position, velocity and acceleration can all be written as explicit functions of time.</p><p>x ? x(t), v ? v(t). This allows for the acceleration to be written as a function of t only, f (x, v, t) = f ? (t) for all t. The same applies for the second system,x ?x(t),? ??(t) and</p><formula xml:id="formula_85">f (x,?, t) =f ? (t)</formula><p>For all t, x(t) =x(t), therefore, for any time increment, ?t, x(t + ?t) =x(t + ?t). Taking the full time derivative of x andx(t)</p><formula xml:id="formula_86">dx(t) dt = v(t) = lim ?t?0 x(t + ?t) ? x(t) ?t (106) dx(t) dt =?(t) = lim ?t?0x (t + ?t) ?x(t) ?t<label>(107)</label></formula><p>Using these two equations and the fact that x(t) =x(t), it is inferred that v(t) =?(t) for all t.</p><p>Taking the full time derivative of v(t) and?(t)</p><formula xml:id="formula_87">dv(t) dt = f ? (t) = lim ?t?0 v(t + ?t) ? v(t) ?t (108) d?(t) dt =f ? (t) = lim ?t?0? (t + ?t) ??(t) ?t<label>(109)</label></formula><p>Using these two equation and the fact that v(t) =?(t) for all t, it is also inferred that f ? (t) =f ? (t) for all t.</p><p>Using these three facts, x(t) =x(t), v(t) =?(t) and f ? (t) =f ? (t). It must also be true that</p><formula xml:id="formula_88">f (x(t), v(t), t) =f (x(t),?(t), t) ? ? f (x, v, t) =f (x, v, t).</formula><p>Therefore the assumption that f (x, v, t) =f (x,?, t) is incorrect, there can only be one functional form for f (x, v, t).</p><p>Additionally, using v(t) =?(t) for all t, the initial velocities must also be the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 ANODEs Learning Two Functions</head><p>In Section 5.1, it was shown that ANODEs were able to learn two functions at the same time</p><formula xml:id="formula_89">x 1 (t) = e ??t sin(?t), x 2 (t) = e ??t cos(?t)<label>(110)</label></formula><p>using the solution</p><formula xml:id="formula_90">? a = Ca ? ?x ? ?x + ? ?a ? ?a ? 1 C (2? 2 x + ?? ? ? 2 ) ,<label>(111)</label></formula><p>This is a specific case of the general formulation given by Equation <ref type="formula" target="#formula_8">(8)</ref>. When the problem is generalised to have mixed amounts of sine and cosine in each function</p><formula xml:id="formula_91">x 1 (t) = e ??t (A 1 sin(?t) + B 1 cos(?t)), x 2 (t) = e ??t (A 2 sin(?t) + B 2 cos(?t)) (112)</formula><p>ANODEs are still able to learn these functions, shown in the first plot of <ref type="figure">Figure 11</ref>. As shown previously, if F (x, a, t, ? F ) gets the addition, ?x + ?, then the ODE is preserved if G(x, a, t, ? G )</p><p>also gets the addition</p><formula xml:id="formula_92">?1 C ((? ? ? + ?)(?x + ?) + ?(Ca ? ?x ? ?x + ?))</formula><p>, given by Equation <ref type="formula" target="#formula_0">(103)</ref>.</p><p>This gauge change preserves the ODE, but gives a new expression for the initial velocit?</p><formula xml:id="formula_93">x(0) = ??x(0) ? ?x(0) + ? + ?x(0) + ? =?x(0) +?<label>(113)</label></formula><p>which can be written in matrix-vector notation as</p><formula xml:id="formula_94">x 1 (0) 1 x 2 (0) 1 ? ? = ? 1 (0) x 2 (0)<label>(114)</label></formula><p>There are two equations and two unknowns,? and?, so this is possible to solve, and for ANODEs to learn. <ref type="bibr" target="#b1">2</ref> To test this even further we added a third function to be learnt. ANODEs were able to do this, shown in the second plot of <ref type="figure">Figure 11</ref>. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Experimental Setup and Additional Results</head><p>We anticipate two main uses for SONODEs. One is using an experiment in a controlled environment, where the aim is to find values such as the coefficient of friction. The other use is when data is observed, and the aim is to extrapolate in time, but the experiment is not controlled, for example, observing weather. We would expect for the former, a simple model with only a single linear layer would be useful, to find those coefficients, and for the latter, a deeper model may be more appropriate.</p><p>Additionally, Neural ODEs may be used in classification or other tasks that only involve the start and endpoints of the flow. For all of these tasks we used t 0 = 0 and t 1 = 1, and accelerations that were not time-dependent. For tasks depending on the start and endpoint only, a deeper neural network is more useful for the acceleration.</p><p>For all experiments, except the MNIST experiment, we optimise using Adam with a learning rate of 0.01. We also train on the complete datasets and do not minibatch. All the experiments were repeated 3 times to obtain a mean and standard deviation. Depending on the task at hand, we used two different architectures for NODEs, ANODEs and SONODEs. The first is a simple linear model,  <ref type="figure">Figure 11</ref>: ANODE(1) learning two functions and three functions, with a shared ODE, but different initial conditions. The real trajectories are seen going through their sampled data points, and the corresponding augmented trajectories are also plotted. ANODE(1) is able to learn the trajectories.</p><p>one weight matrix and bias without activations. This architecture, in the case of NODEs, ANODEs and SONODEs, was used on Silverbox, Airplane Vibrations and Van-Der-Pol Oscillator, with the aim of extracting coefficients from the models, for these tasks we also allowed ANODEs to learn the initial augmented position. The second architecture is a fully connected network with two hidden layers of size 20, it uses ELU activations in? and tanh activations in the initial conditions. ELU and tanh were used because they allow for negative values in the ODE <ref type="bibr" target="#b13">[14]</ref>.</p><p>When considering ANODEs, they are in a higher-dimensional space than the problem, and the result must be projected down to the lower dimensions. This projection was not learnt as a linear layer, instead, the components were directly selected, using an identity for the real dimensions, and zero for the augmented dimensions. This was done because a final (or initial) learnt linear layer would hide the advantages of certain models. For example, the parity problem can be solved easily if NODEs are given a final linear layer, do not move the points and then multiply by -1. For this reason, no models used a linear layer at the end of the flow. Equally, they do not initialise with a linear layer as they again hide advantages. For example, the nested n-spheres problem, NODEs can solve this with an initial linear layer, if they were to go into a higher-dimensional space the points may already be linearly separated, as shown by Massaroli et al. <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Van Der Pol Oscillator</head><p>ANODEs and SONODEs were tested on a forced Van Der Pol (VDP) Oscillator that exhibits a chaotic behaviour. More specfically, the parameters and equations of the particular VDP oscillator are:   <ref type="figure">Figure 13</ref>: Repeating the Airplane Vibrations task with third order NODEs (TONODEs). We see that, in this case, TONODEs are not as successful at modelling these dynamics as SONODEs and ANODEs, having a larger error both on the training data and the extrapolation.</p><formula xml:id="formula_95">x = 8.53(1 ? x 2 )? ? x + 1.2 cos(0.2?t), x 0 = 0.1,? 0 = 0<label>(115)</label></formula><p>As shown in <ref type="figure" target="#fig_11">Figure 12</ref>, while ANODEs achieve a lower training loss than SONODEs, their test loss is much greater. We conjecture that, in the case of ANODEs, this is a case of overfitting. SONODEs, on the other hand, can better approximate the dynamics, therefore they exhibit better predictive performance. Note that, neither model can learn the VDP oscillator particularly well, which may be attributed to chaotic behaviour of the system at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Third Order NODEs on Airplane Vibrations</head><p>We test Third Order Neural ODEs (TONODEs) on the Airplane Vibrations task from section 6.2. The results are in <ref type="figure">Figure 13</ref>.</p><p>We see that TONODEs vastly underperform compared to ANODEs and SONODEs. In each of the 3 repetitions of the experiment, the different initialisation found the best solution to be at zero. Therefore, whilst the loss stays constant, the error remains large. We hypothesise that despite theoretically being able to perform at least as well as SONODEs, TONODEs avoid exponentially growing at any point by exponentially decaying towards zero. It is likely that by rescaling the time to be between 0 and 1, TONODE would approach a more accurate solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 First Order Dynamics and Interpolation</head><p>SONODEs contain a subset of models that is NODEs. Consider first order dynamics that is approximated by the NODE? = f (v) (x, t,? f ) (116) Carrying out the full time derivative of Equation (116):</p><formula xml:id="formula_96">x = ?f (v) (x, t,? f ) ?x T? + ?f (v) (x, t,? f ) ?t ,?(t 0 ) = f (v) (x(t 0 ), t 0 ,? f )<label>(117)</label></formula><p>Which yields the SONODE equivalent of the learnt dynamics:</p><formula xml:id="formula_97">f (a) (x, v, t, ? f ) = ?f (v) (x, t,? f ) ?x T v + ?f (v) (x, t,? f ) ?t , g(x(t 0 ), ? g ) = f (v) (x(t 0 ), t 0 ,? f )<label>(118)</label></formula><p>Additionally, it was shown in Equation (4) that SONODEs are a specific case of ANODEs that learn the initial augmented position. Therefore, anything that NODEs can learn, SONODEs should also be able to learn, and anything SONODEs can learn, ANODEs should be able to learn. To demonstrate that SONODEs and ANODEs can also learn first order dynamics, we task them with learning an exponential with no noise, x(t) = exp(0.1667t). All models, as expected, are able to learn the function, as shown in <ref type="figure" target="#fig_4">Figure 14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Performance on MNIST</head><p>NODEs, SONODEs and ANODEs were tested on MNIST <ref type="bibr" target="#b10">[11]</ref> to investigate their ability on classification tasks. The networks used convolutional layers, which in the case of SONODEs were used for  <ref type="figure" target="#fig_4">Figure 14</ref>: The different models learning an exponential, simple first order dynamics, and interpolating between two observation sections. As expected, all models are able to learn the function.</p><p>both the acceleration and the initial velocity. ANODEs were augmented with one additional channel as is suggested by Dupont et al. <ref type="bibr" target="#b3">[4]</ref>. The models used a training batch size of 128 and test batch size of 1000, as well as group normalisation. SGD optimiser was used with a learning rate of 0.1 and momentum 0.9. The cross-entropy loss was used. The experiment was repeated 3 times with random initialisations to obtain a mean and standard deviation. The results are given in table 1 and <ref type="figure" target="#fig_6">Figure 15</ref>. In terms of test accuracy, SONODEs and ANODEs perform marginally better than NODEs. ANODEs can achieve the same accuracy with fewer parameters than SONODEs because the dynamics are not limited to second order and it is only the final state that is of concern in classification. However, SONODEs are able to achieve the same accuracy with a lower number of function evaluations (NFE). NFE denotes how many function evaluations are made by the ODE solver, and represents the complexity of the learnt solution. It is a continuous analogue of the depth of a discrete layered network. In the case of NODEs and ANODEs, the NFE gradually increases meaning that the complexity of the flow also increases. However, in the case of SONODEs, the NFE stays constant, suggesting that the initial velocity was associated with larger gradients (otherwise we would expect NFE to increase for SONODEs with training).  <ref type="figure" target="#fig_6">Figure 15</ref>: Comparing the performance of SONODEs and NODEs on the MNIST dataset. SONODEs converge to the same training accuracy and a higher test accuracy with a lower NFE than NODEs. NODEs had 208266 parameters, SONODEs had 283658 and ANODEs had 210626. Additional parameters were associated with the initial velocity, or the augmented channel.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Proposition 4 . 2 .</head><label>42</label><figDesc>SONODEs are not restricted to homeomorphic transformations in real space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>ANODE(1) learning two functions using the same parameters, for ? = 1 and ? = 0.1667. The real trajectories are going through their sampled data points. Augmented trajectories are plotted over their theoretical trajectories given by Equation(11)for C = 1.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>ANODE(1) learning a 2D second order function. ANODE(1) is able to learn the function, showing that it does not necessarily need double the dimensions to learn second order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>NODE, ANODE(1) and SONODE training on harmonic oscillators. SONODEs already have the second order behaviour built in as an architectural choice, so they are able to learn the dynamics in fewer iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>How SONODEs and ANODEs perform learning a sine curve in different noise regimes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Proposition 4 . 1 .</head><label>41</label><figDesc>For a k-th order IVP, if the k-th derivative of x is Lipschitz continuous and has no explicit time dependence, then unique phase space trajectories cannot intersect at an angle. Similarly, a single phase space trajectory cannot intersect itself at an angle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>ANODE(1) and SONODE learning a Van-Der-Pol Oscillator. ANODEs are able to converge to a lower training loss, however they diverge when extrapolating. The models were trained on the first 70 points and extrapolated to 200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Proposition 4.1. For a k-th order IVP, if the k-th derivative of x is Lipschitz continuous and has no explicit time dependence, then unique phase space trajectories cannot intersect at an angle. Similarly, a single phase space trajectory cannot intersect itself at an angle.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 1 :</head><label>1</label><figDesc>Results for the MNIST experiments at convergence. SONODE converges to a higher test accuracy than NODEs with a lower NFE. ANODEs converge to the same higher test accuracy with a higher NFE, but with a lower parameter count than SONODEs. ? 0.0004 26.2 ? 0.0 SONODE 0.9963 ? 0.0001 20.1 ? 0.0 ANODE 0.9963 ? 0.0001 32.2 ? 0.0</figDesc><table><row><cell>Model</cell><cell>Test Accuracy</cell><cell>NFE</cell></row><row><cell>NODE</cell><cell>0.9961</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">There are trivial cases where this would be impossible. For example if the two functions were ?sin(?t), they would have the same initial position, but different initial velocities. Corresponding to the matrix in Equation (114) having zero determinant.<ref type="bibr" target="#b2">3</ref> The figure also shows that when trajectories cross in real space they do not in augmented space, and when they cross in augmented space they do not in real space, supporting Proposition 4.1.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We would like to thank C?t?lina Cangea, Jacob Deasy and Duo Wang for their helpful comments. We would like to also thank the reviewers for their constructive feedback and efforts towards improving our paper. The authors declare no competing interests.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It also extends to? i and z i (t 0 ), where f (a) i , s i and g i also have the index range from 1 to d, but the index of? i goes from 1 to 2d just like for z i .</p><p>Using the first order adjoint method, Equations (30) and (31), and using index notation with repeated indices summed over, the gradients are</p><p>Where the adjoint follows the OD?</p><p>Where just like in z i , the index, i, ranges from 1 to 2d in the adjoint r i (t). When writing the sum over the index explicitl?</p><p>Now split up the adjoint state, r, into two equally sized vectors, r A and r B , where their indices only range from 1 to d, like x, v, f (a) , g and s.</p><p>Using Equations (64), (65), (66) and (70), and subbing them into Equation <ref type="formula">(69)</ref>, the derivative can be written as?</p><p>Relabelling the indices in the second sum (j ? d) ? ? j</p><p>Looking at specific values of i:</p><p>All of the gradients match, so the first order adjoint method acting on z(t) = [x(t), v(t)] will produce the same gradients as the second order adjoint method acting on x(t). Given by Equation (60).</p><p>Looking at the efficiencies of each method and how they would be implemented.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">PDE-constrained optimization and the adjoint method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
		<ptr target="https://cs.stanford.edu/~ambrad/adjoint_tutorial.pdf" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Augmented neural odes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3134" to="3144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ANODE: Unconditionally accurate memory-efficient gradients for neural odes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholaminejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Biros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Hamiltonian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Greydanus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misko</forename><surname>Dzamba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stable architectures for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14004</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">beta-VAE: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lerchner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junteng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<title level="m">Neural jump stochastic differential equations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxiao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep lagrangian networks: Using physics as model prior for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Peters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkyoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Asama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08071</idno>
		<title level="m">Dissecting neural odes</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Disentangling disentanglement in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emile</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">F-16 aircraft benchmark based on ground vibration test data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>No?l</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Schoukens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Nonlinear System Identification Benchmarks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="19" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Graph neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Asama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkyoo</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mathematical theory of optimal processes. Routledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Semenovich Pontryagin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep neural networks motivated by partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Neural stochastic differential equations: Deep latent gaussian models in the diffusion limit. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Tzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Raginsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Three free data sets for development and benchmarking in nonlinear system identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torbj?rn</forename><surname>Wigren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Schoukens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Control Conference (ECC)</title>
		<imprint>
			<biblScope unit="page" from="2933" to="2938" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">ANODEV2: A coupled neural ode evolution framework. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Biros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mahoney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaofeng Desmond</forename><surname>Zhong</surname></persName>
		</author>
		<title level="m">Biswadip Dey, and Amit Chakraborty. Symplectic ODE-Net: Learning hamiltonian dynamics with control</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">ODE 2 VAE: Deep generative second order odes with bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?agatay</forename><surname>Y?ld?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>L?hdesm?ki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
