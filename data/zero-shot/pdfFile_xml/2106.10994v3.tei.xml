<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BernNet: Learning Arbitrary Graph Spectral Filters via Bernstein Approximation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingguo</forename><surname>He</surname></persName>
							<email>mingguo@ruc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
							<email>zhewei@ruc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
							<email>huangzf@fudan.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongteng</forename><surname>Xu</surname></persName>
							<email>hongtengxu@ruc.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BernNet: Learning Arbitrary Graph Spectral Filters via Bernstein Approximation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many representative graph neural networks, e.g., GPR-GNN and ChebNet, approximate graph convolutions with graph spectral filters. However, existing work either applies predefined filter weights or learns them without necessary constraints, which may lead to oversimplified or ill-posed filters. To overcome these issues, we propose BernNet, a novel graph neural network with theoretical support that provides a simple but effective scheme for designing and learning arbitrary graph spectral filters. In particular, for any filter over the normalized Laplacian spectrum of a graph, our BernNet estimates it by an order-K Bernstein polynomial approximation and designs its spectral property by setting the coefficients of the Bernstein basis. Moreover, we can learn the coefficients (and the corresponding filter weights) based on observed graphs and their associated signals and thus achieve the BernNet specialized for the data. Our experiments demonstrate that BernNet can learn arbitrary spectral filters, including complicated band-rejection and comb filters, and it achieves superior performance in real-world graph modeling tasks. Code is available at https://github.com/ivam-he/BernNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph neural networks (GNNs) have received extensive attention from researchers due to their excellent performance on various graph learning tasks such as social analysis <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29]</ref>, drug discovery <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref>, traffic forecasting <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6]</ref>, recommendation system <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b31">32]</ref> and computer vision <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b3">4]</ref>. Recent studies suggest that many popular GNNs operate as polynomial graph spectral filters <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b34">35]</ref>. Specifically, we denote an undirected graph with node set V and edge set E as G = (V, E), whose adjacency matrix is A. Given a signal x = [x] ? R n on the graph, where n = |V | is the number of nodes, we can formulate its graph spectral filtering operation as K k=0 w k L k x, w k 's are the filter weights, L = I ? D ?1/2 AD ?1/2 is the symmetric normalized Laplacian matrix of G, and D is the diagonal degree matrix of A. Another equivalent polynomial filtering operation is  We can broadly categorize the GNNs applying the above filtering operation into two classes, depending on whether they design the filter weights or learn them based on observed graphs. Some representative models in these two classes are shown below.</p><formula xml:id="formula_0">V V F g Q a N 0 x M K I j X p n b u f 7 M V d E q M = " &gt; A A A C I n i c b V B J S 8 N A G J 3 U r d Y t 6 t F L s A g V p C R F X G 5 F L 4 K X C n a B J I b J Z N o O n U z C z E Q o I b / F i 3 / F i w d F P Q n + G K d p B G 1 9 M P B 4 7 9 v m + T E l Q p r m p 1 Z a W F x a X i m v V t b W N z a 3 9 O 2 d j o g S j n A b R T T i P R 8 K T A n D b U k k x b 2 Y Y x j 6 F</formula><p>? The GNNs driven by designing filters: GCN <ref type="bibr" target="#b12">[13]</ref> uses a simplified first-order Chebyshev polynomial, which is proven to be a low-pass filter <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref>. APPNP <ref type="bibr" target="#b13">[14]</ref> utilizes Personalized PageRank (PPR) to set the filter weights and achieves a low-pass filter as well <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b40">41]</ref>. GNN-LF/HF <ref type="bibr" target="#b40">[41]</ref> designs filter weights from the perspective of graph optimization functions, which can simulate high-and low-pass filters.</p><p>? The GNNs driven by learning filters: ChebNet <ref type="bibr" target="#b6">[7]</ref> approximates the filtering operation with Chebyshev polynomials, and learns a filter via trainable weights of the Chebyshev basis. GPR-GNN <ref type="bibr" target="#b4">[5]</ref> learns a polynomial filter by directly performing gradient descent on the filter weights, which can derive high-or low-pass filters. ARMA <ref type="bibr" target="#b1">[2]</ref> learns a rational filter via the family of Auto-Regressive Moving Average filters <ref type="bibr" target="#b20">[21]</ref>.</p><p>Although the above GNNs achieve some encouraging results in various graph modeling tasks, they still suffer from two major drawbacks. Firstly, most existing methods focus on designing or learning simple filters (e.g., low-and/or high-pass filters), while real-world applications often require much more complex filters such as band-rejection and comb filters. To the best of our knowledge, none of the existing work supports designing arbitrary interpretable spectral filters. The GNNs driven by learning filters can learn arbitrary filters in theory, but they cannot intuitively show what filters they have learned. In other words, their interpretability is poor. For example, GPR-GNN <ref type="bibr" target="#b4">[5]</ref> learns the filter weights w k 's but only proves a small subset of the learnt weight sequences corresponds to low-or high-pass filters. Secondly, the GNNs often design their filters empirically or learn the filter weights without any necessary constraints. As a result, their filter weights often have poor controllability. For example, GNN-LF/HF <ref type="bibr" target="#b40">[41]</ref> designs its filters with a complex and non-intuitive polynomial with difficult-to-tune hyperparameters. The multi-layer GCN/SGC <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">31]</ref> leads to "ill-posed" filters (i.e., those deriving negative spectral responses). Additionally, the filters learned by GPR-GNN <ref type="bibr" target="#b4">[5]</ref> or ChebNet <ref type="bibr" target="#b6">[7]</ref> have a chance to be ill-posed as well.</p><p>To overcome the above issues, we propose a novel graph neural network called BernNet, which provides an effective algorithmic framework for designing and learning arbitrary graph spectral filters. As illustrated in <ref type="figure">Figure 1</ref>, for an arbitrary spectral filter h : [0, 2] ? [0, 1] over the spectrum of the symmetric normalized Laplacian L, our BernNet approximates h by a K-order Bernstein polynomial approximation, i.e., h(?)</p><formula xml:id="formula_1">= K k=0 ? k b K k (?). The non-negative coefficients {? k } K k=0</formula><p>of the Bernstein basis {b K k (?)} K k=0 work as the model parameter, which can be interpreted as h(2k/K), k = 0, . . . , K (i.e., the filter values uniformly sampled from [0, 2]). By designing or learning the ? k 's, we can obtain various spectral filters, whose filtering operation can be formulated as K k=0 ? k</p><formula xml:id="formula_2">1 2 K K k (2I ? L) K?k L k x, where</formula><p>x is the graph signal. We further demonstrate the rationality of our BernNet from the perspective of graph optimization -any valid polynomial filers, i.e., those polynomial functions mapping [0, 2] to [0, 1], can always be expressed by our BernNet, and accordingly, the filters learned by our BernNet are always valid. Finally, we conduct experiments to demonstrate that 1) BernNet can learn arbitrary graph spectral filters (e.g., band-rejection, comb, low-band-pass, etc.), and 2) BernNet achieves superior performance on real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BernNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bernstein approximation of spectral filters</head><p>Given an arbitrary filter function h : [0, 2] ? [0, 1], let L = U?U T denote the eigendecomposition of the symmetric normalized Laplacian matrix L, where U is the matrix of eigenvectors and ? = diag[? 1 , ..., ? n ] is the diagonal matrix of eigenvalues. We use</p><formula xml:id="formula_3">h(L)x = Uh(?)U T x = Udiag[h(? 1 ), ..., h(? n )]U T x<label>(1)</label></formula><p>to denote a spectral filter on graph signal x. The key of our work is approximate h(L) (or, equivalently, h(?)). For this purpose, we leverage the Bernstein basis and Bernstein polynomial approximation defined below. Definition 2.1 ( [10]). (Bernstein polynomial approximation) Given an arbitrary continuous function f (t) on t ? [0, 1], the Bernstein polynomial approximation (of order K) for f is defined as</p><formula xml:id="formula_4">p K (t) := K k=0 ? k ? b K k (t) = K k=0 f k K ? K k (1 ? t) K?k t k .<label>(2)</label></formula><p>Here, for k = 0, ...,</p><formula xml:id="formula_5">K, b K k (t) = K k (1 ? t) K?k t k is the k-th Bernstein base, and ? k = f ( k K ) is the function value at k/K, which works as the coefficient of b K k (t). Lemma 2.1 ( [10]). Given an arbitrary continuous function f (t) on t ? [0, 1], let p K (t) denote the Bernstein approximation of f (t) as defined in Equation (2). We have p K (t) ? f (t) as K ? ?.</formula><p>For the filter function h :</p><formula xml:id="formula_6">[0, 2] ? [0, 1], we let t = ? 2 and f (t) = h(2t), so that the Bernstein polynomial approximation becomes applicable, where ? k = f (k/K) = h(2k/K) and b K k (t) = b K k ( ? 2 ) = K k (1 ? ? 2 ) K?k ( ? 2 ) k for k = 1, ..., K. Consequently, we can approximate h(?) by p K (?/2) = K k=0 ? k K k (1 ? ? 2 ) K?k ? 2 k = K k=0 ? k 1 2 K K k (2 ? ?) K?k ? k , and Lemma 2.1 ensures that p K (?/2) ? h(?) as K ? ?. Replacing {h(? i )} n i=1 with {p K (? i /2)} n i=1</formula><p>, we approximate the spectral filter h(L) in Equation <ref type="formula" target="#formula_3">(1)</ref> as Udiag[p K (? 1 /2), ..., p K (? n /2)]U T and derive the proposed BernNet. In particular, given a graph signal x, the convolutional operator of our BernNet is defined as follows:</p><formula xml:id="formula_7">z = Udiag[p K (? 1 /2), ..., p K (? n /2)]U T BernNet x = K k=0 ? k 1 2 K K k (2I ? L) K?k L k x<label>(3)</label></formula><p>where each coefficient ? k can be either set to h(2k/K) to approximate a predetermined filter h, or learnt from the graph structure and signal in an end-to-end fashion. As a natural extension of Lemma 2.1, our BernNet owns the following proposition. Proposition 2.1. For an arbitrary continuous filter function h :</p><formula xml:id="formula_8">[0, 2] ? [0, 1], by setting ? k = h(2k/K), k = 0, . . . , K, the z in Equation (3) satisfies z ? h(L)x as K ? ?.</formula><p>Proof. According to the above derivation, we have</p><formula xml:id="formula_9">p K (?/2) = K k=0 ? k K k (1 ? ? 2 ) K?k ? 2 k = K k=0 ? k 1 2 K K k (2 ? ?) K?k ? k , and Lemma 2.1 ensures that p K (?/2) ? h(?) as ? k = h(2k/K) and K ? ?. Consequently, we have z = Udiag[p K (? 1 /2), ..., p K (? n /2)]U T x ? Udiag[h(? 1 ), ..., h(? n )]U T x = h(L)</formula><p>as ? k = h(2k/K) and K ? ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Realizing existing filters with BernNet.</head><p>As shown in Proposition 2.1, our BernNet can approximate arbitrary continuous spectral filters with sufficient precision. Below we give some representative examples of how our BernNet exactly realizes existing filters that are commonly used in GNNs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filter types</head><p>Filter h(?) ? k for k = 0, . . . , K Bernstein approximation p K ( ? 2 ) BernNet All-pass 1</p><formula xml:id="formula_10">? k = 1 1 I Linear low-pass 1 ? ?/2 ? k = 1 ? k/K 1 ? ?/2 I ? 1 2 L Linear high-pass ?/2 ? k = k/K ?/2 1 2 L Impulse low-pass ? 0 (?) ? 0 = 1 and other ? k = 0 (1 ? ?/2) K 1 2 K (2I ? L) K Impulse high-pass ? 2 (?) ? K = 1 and other ? k = 0 (?/2) K 1 2 K L K Impulse band-pass ? 1 (?) ? K/2 = 1 and other ? k = 0 K K/2 (1 ? ?/2) K/2 (?/2) K/2 1 2 K K K/2 (2I ? L) K/2 L K/2</formula><p>? All-pass filter h(?) = 1. We set ? k = 1 for k = 0, . . . , K, and the approximation p K ( ? 2 ) = 1 is exactly the same with h(?). Accordingly, our BernNet becomes an identity matrix, which realizes the all-pass filter perfectly.</p><p>? Linear low-pass filter h(?) = 1??/2. We set ? k = 1?k/K for k = 0, . . . , K and obtain</p><formula xml:id="formula_11">p K ( ? 2 ) = 1 ? ?/2. The BernNet becomes K k=0 (K?k) K 1 2 K K k (2I ? L) K?k L k = I ? 1 2</formula><p>L, which achieves the linear low-pass filter exactly. Note that I ? 1 2 L = 1 2 (I + P) is also the same as the graph convolutional network (GCN) before renormalization <ref type="bibr" target="#b12">[13]</ref>.</p><p>? Linear high-pass filter h(?) = ?/2. Similarly, we can set ? k = k/K for k = 0, . . . , K to get a perfect approximation p K ( ? 2 ) = ? 2 , and the BernNet becomes 1 2 L.</p><p>Note that even for those non-continuous spectral filters, e.g., the impulse low/high/band-pass filters, our BernNet can also provide good approximations (with sufficient large K).</p><p>? Impulse low-pass filter h(?) = ? 0 (?). ? We set ? 0 = 1 and ? k = 0 for k = 0, and p K ( ? 2 ) = (1 ? ? 2 ) K . Accordingly, the BernNet becomes 1 2 K (2I ? L) K , deriving an K-layer linear low-pass filter.</p><p>? Impulse high-pass filter h(?) = ? 2 (?). We set ? K = 1 and ? k = 0 for k = K, and p K ( ? 2 ) = ( ? 2 ) K . The BernNet becomes 1 2 K L K , i.e., an K-layer linear high-pass filter. ? Impulse band-pass filter h(?) = ? 1 (?). Similarly, we set ? K/2 = 1 and ? k = 0 for k = K/2, and p K ( ? 2 ) = K K/2 (1 ? ?/2) K/2 (?/2) K/2 . The BernNet becomes 1 2 K K K/2 (2I ? L) K/2 L K/2 , which can be explained as stacking a K/2-layer linear low-pass filter and a K/2-layer linear high-pass filter. Obviously, K should be an even number in this case. <ref type="table" target="#tab_0">Table 1</ref> summarizes the design of the BernNet for the filters above. We can find that an appealing advantage of our BernNet is that its coefficients are highly correlated with the spectral property of the target filter. In particular, we can determine to pass or reject the spectral signal with ? ? 2k K by using a large or small ? k because each Bernstein base b K k (?) corresponds to a "bump" located at 2k K . This property provides useful guidance when designing filters, which enhances the interpretability of our BernNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning complex filters with BernNet</head><p>Besides designing the above typical filters, our BernNet can express more complex filters, such as band-pass, band-rejection, comb, low-band-pass filters, etc. Moreover, given the graph signals before and after applying such filters (i.e., the x's and the corresponding z's), our BernNet can learn their approximations in an end-to-end manner. Specifically, given the pairs {x, z}, we learn the coefficients {? k } K k=0 of the BernNet by gradient descent. More implementation details can be found at the experimental section below. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates the four complex filters and the approximations we learned (The low-band pass filter is h(?) = I [0,0.5] (?) + exp (?100(? ? 0.5) 2 )I (0.5,1) (?) + exp (?50(? ? 1.5) 2 )I <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> (?), where I ? (?) = 1 when ? ? ?, otherwise I ? (?) = 0). In general, our BernNet can learn a smoothed approximation of these complex filters, and the approximation precision improves with the increase of the order K. Note that although the BernNet cannot pinpoint the exact peaks of the comb filter or drop to 0 for the valleys of comb or low-band-pass filters due to the limitation of K, it still significantly outperforms other GNNs for learning such complex filters. <ref type="bibr">?</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BernNet in the Lens of Graph Optimization</head><p>In this section, we motivate BernNet from the perspective of graph optimization. In particular, we show that any polynomial filter that attempts to approximate a valid filter has to take the form of BernNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A generalized graph optimization problem</head><p>Given a n-dimensional graph signal x, we consider a generalized graph optimization problem</p><formula xml:id="formula_12">min z f (z) = (1 ? ?)z T ?(L)z + ? z ? x 2 2<label>(4)</label></formula><p>where ? ? [0, 1) is a trade-off parameter, z ? R n denotes the propagated representation of the input graph signal x, and ?(L) denotes an energy function of L, determining the rate of propagation <ref type="bibr" target="#b27">[28]</ref>. Generally, ?(?) operates on the spectral of L, and we have ?(L) = Udiag[?(? 1 ), ..., ?(? n )]U T .</p><p>We can model the polynomial filtering operation of existing GNNs with the optimal solution of Equation <ref type="bibr" target="#b3">(4)</ref>. For example, if we set ?(L) = L, then the optimization function (4) becomes f (z) = (1 ? ?)z T Lz + ? z ? x 2 2 , a well-known convex graph optimization function proposed by Zhou et al. <ref type="bibr" target="#b39">[40]</ref>. f (z) takes the minimum when the derivative ?f (z) ?z = 2(1 ? ?)Lz + 2? (z ? x) = 0, which solves to</p><formula xml:id="formula_13">z * = ? (I ? (1 ? ?)(I ? L)) ?1 x = ? k=0 ?(1 ? ?) k (I ? L) k x = ? k=0 ?(1 ? ?) k P k x.</formula><p>By taking a suffix sum K k=0 ?(1 ? ?) k P k x, we obtain the polynomial filtering operation for APPNP <ref type="bibr" target="#b13">[14]</ref>. Zhu et al. <ref type="bibr" target="#b40">[41]</ref> further show that GCN <ref type="bibr" target="#b12">[13]</ref>, DAGNN <ref type="bibr" target="#b18">[19]</ref>, and JKNet <ref type="bibr" target="#b35">[36]</ref> can be interpreted by the optimization function (4) with ?(L) = L.</p><p>The generalized form of Equation (4) allows us to simulate more complex polynomial filtering operation. For example, let ? = 0.5 and ?(L) = e tL ? I, a heat kernel with t as the temperature parameter. Then f (z) takes the minimum when the derivative ?f (z) ?z = e tL ? I z + z ? x = 0, which solves to</p><formula xml:id="formula_14">z * = e ?tL x = e ?t(I?P) x = ? k=0 e ?t t k k! P k x.</formula><p>By taking a suffix sum K k=0 e ?t t k k! P k x, we obtain the polynomial filtering operation for the heat kernal based GNN such as GDC <ref type="bibr" target="#b14">[15]</ref> and GraphHeat <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Non-negative constraint on polynomial filters</head><p>A natural question is that, does an arbitrary energy function ?(L) correspond to a valid or ill-posed spectral filter? Conversely, does any polynomial filtering operation K k=0 w k L k x correspond to the optimal solution of the optimization function (4) for some energy function ?(L)?</p><p>As it turns out, there is a "minimum requirement" for the energy function ?(L); ?(L) has to be positive semidefinite. In particular, if ?(L) is not positive semidefinite, then the optimization function f (z) is not convex, and the solution to ?f (z) ?z = 0 may corresponds to a saddle point. Furthermore, without the positive semidefinite constraint on ?(L), f (z) may goes to ?? as we set z to be a multiple of the eigenvector corresponding to the negative eigenvalue.</p><p>Non-negative polynomial filters. Given a positive semidefinite energy function ?(L), we now consider how the corresponding polynomial filtering operation K k=0 w k L k x should look like. Recall that we assume ?(L) = Udiag[?(? 1 ), ..., ?(? n )]U T . By the positive semidefinite constraint, we have ?(?) ? 0 for ? ? [0, 2]. Since the objective function f (z) is convex, it takes the minimum when ?f (z) ?z = 2(1 ? ?)?(L)z + 2? (z ? x) = 0. Accordingly, the optimum z * can be derived as </p><formula xml:id="formula_15">? (?I + (1 ? ?)?(L)) ?1 x = Udiag ? ? + (1 ? ?)?(? 1 ) , ..., ? ? + (1 ? ?)?(? n ) U T x.</formula><formula xml:id="formula_16">0 ? g(?) = K k=0 w k ? k ? 1, ? ? ? [0, 2].<label>(6)</label></formula><p>While Constraint 3.1 seems to be simple and intuitive, some of the existing GNN may not satisfies this constraint. For example, GCN uses z = Px = (I ? L) x, which corresponds to a polynomial filter g(?) = 1 ? ? that takes negative value when ? &gt; 1, violating Constraint 3.1. As shown in <ref type="bibr" target="#b30">[31]</ref>, the renormalization trickP = (I + D) ?1/2 (I + A) (I + D) ?1/2 shrinks the spectral and thus reliefs the problem. However, g(?) may still take negative value as the maximum eigenvalue ofL = I ?P is still larger than 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Non-negative polynomials and Bernstein basis</head><p>Constraint 3.1 motivates us to design polynomial filters g(?) = K k=0 w k ? k such that 0 ? g(?) ? 1 when ? ? [0, 2]. The g(?) ? 1 part is trivial, as we can always rescale each w k by a factor of K k=0 |w k |2 k . The g(?) ? 0 part, however, requires more elaboration. Note that we can not simply set w k ? 0 for each k = 0 . . . , K, since it is shown in <ref type="bibr" target="#b4">[5]</ref> that such polynomials only correspond to low-pass filters.</p><p>As it turns out, the Bernstein basis has the following nice property: a polynomial that is non-negative on a certain interval can always be expressed as a non-negative linear combination of Bernstein basis. Specifically, we have the following lemma. </p><formula xml:id="formula_17">p(x) = K k=0 ? k b K k (x) = K k=0 ? k K k (1 ? x) K?k x k</formula><p>Lemma 3.1 suggests that to approximate a valid filter, the polynomial filter g(?) has to be a nonnegative linear combination of Bernstein basis. Specifically, by setting x = ?/2, the filter g(?) that satisfies g(?) ? 0 for ? ? [0, 2] can be expressed as</p><formula xml:id="formula_18">g(?) := p ? 2 = K k=0 ? k 1 2 K K k (2 ? ?) K?k ? k .</formula><p>Consequently, any valid polynomial filter that approximate the optimal solution of (4) with positive semidefinite energy function ?(L) has to take the following form: z = K k=0 ? k</p><formula xml:id="formula_19">1 2 K K k (2I ? L) K?k L k x.</formula><p>This observation motivates our BernNet from the perspective of graph optimizationany valid polynomial filers, i.e., the g : [0, 2] ? [0, 1], can always be expressed by BernNet, and accordingly, the filters learned by our BernNet are always valid.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Graph neural networks (GNNs) can be broadly divided into spectral-based GNNs and spatial-based GNNs <ref type="bibr" target="#b32">[33]</ref>.</p><p>Spectral-based GNNs design spectral graph filters in the spectral domain. ChebNet <ref type="bibr" target="#b6">[7]</ref> uses Chebyshev polynomial to approximate a filter. GCN <ref type="bibr" target="#b12">[13]</ref> simplifies the Chebyshev filter with the first-order approximation. GraphHeat <ref type="bibr" target="#b33">[34]</ref> uses heat kernel to design a graph filter. APPNP <ref type="bibr" target="#b13">[14]</ref> utilizes Personalized PageRank (PPR) to set the filter weights. GPR-GNN <ref type="bibr" target="#b4">[5]</ref> learns the polynomial filters via gradient descent on the polynomial coefficients. ARMA <ref type="bibr" target="#b1">[2]</ref> learns a rational filter via the family of Auto-Regressive Moving Average filters <ref type="bibr" target="#b20">[21]</ref>. AdaGNN <ref type="bibr" target="#b8">[9]</ref> learns simple filters across multiple layers with a single parameter for each feature channel at each layer. As aforementioned, these methods mainly focus on designing low-or high-pass filters or learning filters without any constraints, which may lead to misspecified even ill-posed filters.</p><p>On the other hand, spatial-based GNNs directly propagate and aggregate graph information in the spatial domain. From this perspective, GCN <ref type="bibr" target="#b12">[13]</ref> can be explained as the aggregation of the one-hop neighbor information on the graph. GAT <ref type="bibr" target="#b29">[30]</ref> uses the attention mechanism to learn aggregation weights. Recently, Balcilar et al. <ref type="bibr" target="#b0">[1]</ref> bridge the gap between spectral-based and spatial-based GNNs and unify GNNs in the same framework. Their work shows that the GNNs can be interpreted as sophisticated data-driven filters. This motivates the design of the proposed BernNet, which can learn arbitrary non-negative spectral filters from real-world graph signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we conduct experiments to evaluate BernNet's capability to learn arbitrary filters as well as the performance of BernNet on real datasets. All the experiments are conducted on a machine with an NVIDIA TITAN V GPU (12GB memory), Intel Xeon CPU (2.20 GHz), and 512GB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Learning filters from the signal</head><p>We conduct an empirical analysis on 50 real images with the resolution of 100?100 from the Image Processing Toolbox in Matlab. We conduct independent experiments on these 50 images and report the average of the evaluation index. Following the experimental setting in <ref type="bibr" target="#b0">[1]</ref>, we regard each image as a 2D regular 4-neighborhood grid graph. The graph structure translates to an 10, 000 ? 10, 000 adjacency matrix while the pixel intensity translates to a 10, 000-dimensional signal vector.  <ref type="table" target="#tab_0">Nodes  2708  3327  19717  13752  7650  2277  5201  7600  183  183  Edges  5278  4552  44324  245861  119081  31371  198353 26659  279  277  Features 1433  3703  500  767  745  2325  2089  932  1703  1703  Classes  7  6  5  10  8  5  5  5  5  5</ref> For each of the 50 images, we apply 5 different filters (low-pass, high-pass, band-pass, band-rejection and comb) to the spectral domain of its signal. The formula of each filter is shown in <ref type="table" target="#tab_1">Table 2</ref>. Recall that applying a low-pass filter exp(?10? 2 ) to the spectral domain L = Udiag [? 1 , . . . , ? n ] U means applying Udiag exp(?10? 2 1 ), . . . , exp(?10? 2 n ) U to the graph signal. <ref type="figure" target="#fig_5">Figure 3</ref> shows the one of the input image and the corresponding filtering results.</p><p>In this task, we use the original graph signal as the input and the filtering signal to supervise the training process. The goal is to minimize the square error between output and the filtering signal by learning the correct filter. We evaluate BernNet against five popular GNN models: GCN <ref type="bibr" target="#b12">[13]</ref>, GAT <ref type="bibr" target="#b29">[30]</ref>, GPR-GNN <ref type="bibr" target="#b4">[5]</ref>, ARMA <ref type="bibr" target="#b1">[2]</ref> and ChebNet <ref type="bibr" target="#b6">[7]</ref>. To ensure fairness, we use two convolutional units and a linear output layer for all models. We train all models with approximately 2k trainable parameters and tune the hidden units to ensure they have similar parameters. Following <ref type="bibr" target="#b0">[1]</ref>, we discard any regularization or dropout and simply force the GNN to learn the input-output relation. For all models, we set the maximum number of epochs to 2000 and stop the training if the loss does not drop for 100 consecutive times and use Adam optimization with a 0.01 learning rate without decay. Models do not use the position information of the picture pixels. We use a mask to cover the edge nodes of the picture, so the problem can be regarded as a simple regression problem. For BernNet, we use a two-layer model, with each layer sharing the same set of ? k for k = 0, . . . , K and set K = 10. For GPR-GNN, we use the officially released code (see the supplementary materials for URL and commit numbers) and set the order of polynomial filter K = 10. Other baseline models are based on Pytorch Geometric implementation <ref type="bibr" target="#b10">[11]</ref>. The more detailed experiments setting can be found in the Appendix. <ref type="table" target="#tab_1">Table 2</ref> shows the average of the sum of squared error (lower the better) and the R 2 scores (higher the better). We first observe that GCN and GAT can only handle low-pass filters, which concurs with the theoretical analysis in <ref type="bibr" target="#b0">[1]</ref>. GPR-GNN, ARMA and ChebNet can learn different filters from the signals. However, BernNet consistently outperformed these models by a large margin on all tasks in terms of both metrics. We attribute this quality to BernNet's ability to tune the coefficients ? k 's, which directly correspond to the uniformly sampled filter values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Node classification on real-world datasets</head><p>We now evaluate the performance of BernNet against the competitors on real-world datasets. Following <ref type="bibr" target="#b4">[5]</ref>, we include three citation graph Cora, CiteSeer and PubMed <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37]</ref>, and the Amazon co-purchase graph Computers and Photo <ref type="bibr" target="#b19">[20]</ref>. As shown in <ref type="bibr" target="#b4">[5]</ref> these 5 datasets are homophilic graphs on which the connected nodes tend to share the same label. We also include the Wikipedia graph Chameleon and Squirrel <ref type="bibr" target="#b25">[26]</ref>, the Actor co-occurrence graph, and webpage graphs Texas and Cornell from WebKB ? <ref type="bibr" target="#b21">[22]</ref>. These 5 datasets are heterophilic datasets on which connected nodes tend to have different labels. We summarize the statistics of these datasets in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>Following <ref type="bibr" target="#b4">[5]</ref>, we perform full-supervised node classification task with each model, where we randomly split the node set into train/validation/test set with ratio 60%/20%/20%. For fairness, we generate 10 random splits by random seeds and evaluate all models on the same splits, and report the average metric for each model.</p><p>We compare BernNet with 6 baseline models: MLP, GCN <ref type="bibr" target="#b12">[13]</ref>, GAT <ref type="bibr" target="#b29">[30]</ref>, APPNP <ref type="bibr" target="#b13">[14]</ref>, ChebNet <ref type="bibr" target="#b6">[7]</ref>, and GPR-GNN <ref type="bibr" target="#b4">[5]</ref>. For GPR-GNN, we use the officially released code (see the supplementary materials for URL and commit numbers) and set the order of polynomial filter K = 10. For other models, we use the corresponding Pytorch Geometric library implementations <ref type="bibr" target="#b10">[11]</ref>. For BernNet, we ? http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/  use the following propagation process:</p><formula xml:id="formula_20">Z = K k=0 ? k 1 2 K K k (2I ? L) K?k L k f (X) ,<label>(7)</label></formula><p>where f (X) is a 2-layer MLP with 64 hidden units on the feature matrix X. Note that this propagation process is almost identical to that of APPNP or GPR-GNN. The only difference is that we substitute the Generalized PageRank polynomial with Bernstein polynomial. We set the K = 10 and use different learning rate and dropout for the linear layer and the propagation layer. For all models, we optimal leaning rate over {0.001, 0.002, 0.01, 0.05} and weight decay {0.0, 0.0005}. More detailed experimental settings are discussed in Appendix.</p><p>We use the micro-F1 score with a 95% confidence interval as the evaluation metric. The relevant results are summarized in <ref type="table" target="#tab_3">Table 4</ref>. Boldface letters indicate the best result for the given confidence interval. We observe that BernNet provides the best results on seven out of the ten datasets. On the other three datasets, BernNet also achieves competitive results against SOTA methods.</p><p>More interestingly, this experiment also shows BernNet can learn complex filters from real-world datasets with only the supervision of node labels. <ref type="figure" target="#fig_6">Figure 4</ref> plots some of the filters BernNet learnt in the training process. On Actor, BernNet learns an all-pass-alike filter, which concurs with the fact that MLP outperforms all other baselines on this dataset. On Chameleon and Squirrel, BernNet learns two comb-alike filters. Given that BernNet outperforms all competitors by at least 1% on these two datasets, it may suggest that comb-alike filters are necessary for Chameleon and Squirrel. <ref type="figure" target="#fig_3">Figure 5</ref> shows the Coefficients ? k learnt from real-world datasets by BernNet. When comparing Figures 4 and 5, we observe that the curves of filters and curves of coefficients are almost the same. This is because BernNet's coefficients are highly correlated with the spectral property of the target filter, which indicates BernNet Bernnet has strong interpretability.</p><p>Finally, we present the train time for each method in <ref type="table" target="#tab_4">Table 5</ref>. BernNet is slower than other methods due to its quadratic dependence on the degree K. However, compared to the SOTA method GPR-GNN, the margin is generally less than 2, which is often acceptable in practice. In theory, both ChebNet <ref type="bibr" target="#b6">[7]</ref> and GPR-GNN <ref type="bibr" target="#b4">[5]</ref> are linear time complexity related to propagation step K, but BernNet is quadratic time complexity related to K. Delgado et al. <ref type="bibr" target="#b7">[8]</ref> show that Bernstein approximation can be evaluated in linear time related to K using the corner cutting algorithm. However, BernNet can not use this algorithm directly, because we need to multiply signal x. How to convert BernNet to linear complexity will be a problem worth studying in the future.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper proposes BernNet, a graph neural network that provides a simple and intuitive mechanism for designing and learning an arbitrary spectral filter via Bernstein polynomial approximation. Compared to previous methods, BernNet can approximate complex filters such as band-rejection and comb filters, and can provide better interpretability. Furthermore, the polynomial filters designed and learned by BernNet are always valid. Experiments show that BernNet outperforms SOTA methods in terms of effectiveness on both synthetic and real-world datasets. For future work, an interesting direction is to improve the efficiency of BernNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>The proposed BernNet algorithm addresses the challenge of designing and learning arbitrary spectral filters on graphs. We consider this algorithm a general technical and theoretical contribution, without any foreseeable specific impacts. For applications in bioinformatics, computer vision, and natural language processing, applying the BernNet algorithm may improve the performance of existing GNN models. We leave the exploration of other potential impacts to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional experimental details</head><p>A.1 Learning filters from the signal (Section 5.1)</p><p>For all models, we use two convolutional layers and a linear output layer that projects the final node representation onto the single output for each node. We train all models with approximately 2k trainable parameters and tune the hidden units to ensure they have similar parameters. We discard any regularization or dropout and simply force the GNN to learn the input-output relation. We stop the training if the loss does not drop for 100 consecutive epochs with a maximum limit of 2000 epochs and use Adam optimization with a 0.01 learning rate without decay. For GPR-GNN, we use the officially released code (URL and commit number in <ref type="table" target="#tab_5">Table 6</ref>), and other baseline models are based on Pytorch Geometric implementation <ref type="bibr" target="#b10">[11]</ref>.</p><p>For GCN, we set the hidden units to 32 and set the linear units to 64. For GAT, the first layer has 4 attention heads and each head has 8 hidden; the second layer has 8 attention heads and each head has 8 hidden. And we set the linear units to 64. For ARMA, we set the ARMA layer and ARMA stacks to 1, and set the hidden units to 32 and set the linear units to 64. For ChebNet, we use 3 steps propagation for each layer with 32 hidden units and set the linear units to 64. For GPR-GNN, we set the hidden units to 32 with 10 steps propagation and set the linear units to 64, and use the random initialization for the GPR part. For BernNet, we use same set of ? k for k = 0, . . . , K in each layer and set K = 10. Regarding the analysis experiment in section 2.3, BenNet's settings are the same as above and the image used is <ref type="figure" target="#fig_5">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Node classification on real-world datasets (Section 5.2)</head><p>In this experiment, we also use the officially released code (URL and commit number in <ref type="table" target="#tab_5">Table 6</ref>) for GPR-GNN and Pytorch Geometric implementation <ref type="bibr" target="#b10">[11]</ref> for other models. For all models, we use two convolutional layers and use early stopping 200 with a maximum of 1000 epochs for all datasets. We use the Adam optimizer to train the models and optimal leaning rate over {0.001, 0.002, 0.01, 0.05} and weight decay {0.0, 0.0005}.</p><p>For GCN, we use 64 hidden units for each GCN convolutional layer. For MLP, we use the 2-layer full connected network with 64 hidden units. For GAT, we make the first layer have 8 attention heads and each heads have 8 hidden units, the second layer have 1 attention head and 64 hidden units. For APPNP, we use 2-layer MLP with 64 hidden units and set the propagation steps K to be 10. We search the optimal ? within {0.1, 0.2, 0.5, 0.9}. For ChebNet, we set the propagation steps K to be 2 and use 32 hidden units for each layer, which the number of equivalent hidden units is 64 for this case. For GPR-GNN, we use 2-layer MLP with 64 hidden units and set the propagation steps K to be 10, and use PPR initialization with ? ? {0.1, 0.2, 0.5, 0.9} for the GPR weights. For BernNet, we use 2-layer MLP with 64 hidden units and set the order K = 10, and we optimize the learning rate for the linear layer and the propagation layer. We fix the dropout rate for the convolutional layer or the linear layer to be 0.5 for all models, and optimize the dropout rate for the propagation layer for GPR-GNN and BernNet. The <ref type="table" target="#tab_6">Table 7</ref> shows the hyperparameters of BernNet on real-world datasets. <ref type="figure" target="#fig_8">Figure 6</ref> plots the filters learnt from real-world datasets by BernNet. Besides some special filters discussed in section 5.2, we find that BernNet learns a low-pass filter on Cora and CiteSeer which concurs the analysis in <ref type="bibr" target="#b0">[1]</ref>. <ref type="figure" target="#fig_9">Figure 7</ref> shows the Coefficients ? k learnt from real-world datasets by BernNet. We observe that the curves of filters and curves of coefficients are almost the same, this is because BernNet's coefficients are highly correlated with the spectral property of the target filter which indicates BernNet Bernnet has strong interpretability.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>K</head><label></label><figDesc>k=0 c k P k x, where P = D ?1/2 AD ?1/2 is the normalized adjacency matrix and c k 's are the filter weights. BernNet &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a r O / D a 0 q o g P 8 u Z a y 7 t v m / c / t s w A = " &gt; A A A C b n i c d V H L S g M x F M 2 M 7 / q q C i 4 s Y r A I F a Q k Y 5 + 7 o h u X C r Y V 2 q F k 0 r Q N z T x I M k I Z Z u k P u v M b 3 P g J p i / Q o h c C h 3 P P P f c R L x J c a Y Q + L H t t f W N z a 3 s n s 7 u 3 f 3 C Y P T p u q T C W l D V p K E L 5 4 h H F B A 9 Y U 3 M t 2 E s k G f E 9 w d r e + H 6 a b 7 8 y q X g Y P O t J x F y f D A M + 4 J R o Q / W y b 0 l 3 Z t K R Q 8 9 N U B E 5 t X L J u U F F p 4 z q u G 5 A G e F 6 p Z S O C u m K t F I r V 3 H N K N A s D M A Y 3 e J S 2 h W m f 5 + s 6 v + 1 v k 5 7 2 f z S D y 7 9 4 N I P 4 g W T B 4 t 4 7 G X f u / 2 Q x j 4 L N B V E q Q 5 G k X Y T I j W n g q W Z b q x Y R O i Y D F n H w I D 4 T L n J b J w U X h m m D w e h N C / Q c M b + r E i I r 9 T E 9 4 z S J 3 q k V n N T 8 q 9 c J 9 a D m p v w I I o 1 C + i 8 0 S A W U I d w e n v Y 5 5 J R L S Y G E C q 5 m R X S E Z G E a v N D G X O E 5 a b w f 9 B y i r h S d J 5 K + c b d 4 h z b I A c u Q Q F g U A U N 8 A A e Q R N Q 8 G k d W W d W z v q y T + 1 z + 2 I u t a 1 F z Q n 4 F X b h G 6 H 1 s N s = &lt; / l a t e x i t &gt; h( ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 Q c v R + k R e f / x S F Y f 1 Z b 5 5 o X r L f c = " &gt; A A A C G X i c d V D L S g M x F L 1 T X 7 W + q i 7 d B I v g Q s p M r a P d F d 2 4 r G A f M D O U T J q 2 o Z k H S U Y o Q 3 / D j b / i x o U i L n X l 3 5 i 2 U 1 D R A 4 H D u e c + c v y Y M 6 l M 8 9 P I L S 2 v r K 7 l 1 w s b m 1 v b O 8 X d v Z a M E k F o k 0 Q 8 E h 0 f S 8 p Z S J u K K U 4 7 s a A 4 8 D l t + 6 O r a b 1 9 R 4 V k U X i r x j H 1 A j w I W Z 8 R r L T U L Z q p O x v i i I H v p W b Z r l V P a 5 U T s 2 z O o I l l 2 l X 7 b O J y P b S H J 9 1 i a e F C C x d a u J C V K S X I 0 O g W 3 9 1 e R J K A h o p w L K V j m b H y U i w U I 5 x O C m 4 i a Y z J C A + o o 2 m I A y q 9 d H b U B B 1 p p Y f 6 k d A v V G i m f u 9 I c S D l O P C 1 M 8 B q K H / X p u J f N S d R / Q s v Z W G c K B q S + a J + w p G K 0 D Q m 1 G O C E s X H m m A i m L 4 V k S E W m C g d Z k G H s P g p + p + 0 K m X L L l d u q q X 6 Z R Z H H g 7 g E I 7 B g n O o w z U 0 o A k E 7 u E R n u H F e D C e j F f j b W 7 N G V n P P v y A 8 f E F l U K c 7 Q = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " F Y k 0 B u + J T Y 7 D q L q D G J 0 l 5 E P 9 K 5 Y = " &gt; A A A C K H i c b V D L S g M x F M 3 4 r P V V d e l m s A g u p M w U U T d i 0 Y 3 g p o J 9 w M w 4 Z N K 0 D Z N 5 k N w R S p j P c e O v u B F R p F u / x P Q h a O u B w O G c c 5 O b E 6 S c S b C s o b G w u L S 8 s l p Y K 6 5 v b G 5 t l 3 Z 2 m z L J B K E N k v B E t A M s K W c x b Q A D T t u p o D g K O G 0 F 4 f X I b z 1 S I V k S 3 8 M g p V 6 E e z H r M o J B S 3 7 p U r n j S x z R C z x l V a w x j u d I 7 i o X + h S w H 7 q 5 r 8 I L O 3 9 Q t 3 n u l 8 o / E X O e 2 F N S R l P U / d K b 2 0 l I F t E Y C M d S O r a V g q e w A E Y 4 z Y t u J m m K S Y h 7 1 N E 0 x h G V n h r v m J u H W u m Y 3 U T o E 4 M 5 V n 9 P K B x J O Y g C n Y w w 9 O W s N x L / 8 5 w M u u e e Y n G a A Y 3 J 5 K F u x k 1 I z F F r Z o c J S o A P N M F E M L 2 r S f p Y Y A K 6 2 6 I u w Z 7 9 8 j x p V i v 2 a a V 6 d 1 K u X U 3 r K K B 9 d I C O k I 3 O U A 3 d o D p q I I K e 0 A t 6 R x / G s / F q f B r D S X T B m M 7 s o T 8 w v r 4 B M p q j F Q = = &lt; / l a t e x i t &gt; {? k } K k=0 Model Parameter &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 KFigure 1 :</head><label>21</label><figDesc>H f 9 0 e X E 7 9 5 j L k j E b u U 4 x m 4 I B 4 z 0 C Y J S S Z 5 + n j r 5 E J s P f D c 1 6 2 a O o z m S + Z 5 5 l 1 5 n N Y e q 4 Q E 8 z D y 9 + m M a 8 8 Q q S B U U a H n 6 u x N E K A k x k 4 h C I W z L j K W b Q i 4 J o j i r O I n A M U Q j O M C 2 o g y G W L h p f l 1 m H C g l M P o R V 4 9 J I 1 d / d 6 Q w F G I c + q o y h H I o Z r 2 J + J 9 n J 7 J / 5 q a E x Y n E D E 0 X 9 R N q y M i Y 5 G U E h G M k 6 V g R i D h R t x p o C D l E U q V a U S F Y s 1 + e J 5 1 G 3 T q p N 2 6 O q 8 2 L I o 4 y 2 A P 7 o A Y s c A q a 4 A q 0 Q B s g 8 A C e w A t 4 1 R 6 1 Z + 1 N + 5 i W l r S i Z x f 8 g f b 1 D S P U n 7 w = &lt; / l a t e x i t &gt; b K 0 ( ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P b a a 2 z e P L A Z c f m b s 8 j A O C 9 l D F 7 Q = " &gt; A A A C I n i c b V B J S 8 N A G J 3 U r d Y t 6 t F L s A g V p C R F X G 5 F L 4 K X C n a B J I b J Z N o O n U z C z E Q o I b / F i 3 / F i w d F P Q n + G K d p B G 1 9 M P B 4 7 9 v m + T E l Q p r m p 1 Z a W F x a X i m v V t b W N z a 3 9 O 2 d j o g S j n A b R T T i P R 8 K T A n D b U k k x b 2 Y Y x j 6 F H f 9 0 e X E 7 9 5 j L k j E b u U 4 x m 4 I B 4 z 0 C Y J S S Z 5 + n j r 5 E J s P f D c 1 6 2 a O o z m S + Z 5 1 l 1 5 n N Y e q 4 Q E 8 z D y 9 + m M a 8 8 Q q S B U U a H n 6 u x N E K A k x k 4 h C I W z L j K W b Q i 4 J o j i r O I n A M U Q j O M C 2 o g y G W L h p f l 1 m H C g l M P o R V 4 9 J I 1 d / d 6 Q w F G I c + q o y h H I o Z r 2 J + J 9 n J 7 J / 5 q a E x Y n E D E 0 X 9 R N q y M i Y 5 G U E h G M k 6 V g R i D h R t x p o C D l E U q V a U S F Y s 1 + e J 5 1 G 3 T q p N 2 6 O q 8 2 L I o 4 y 2 A P 7 o A Y s c A q a 4 A q 0 Q B s g 8 A C e w A t 4 1 R 6 1 Z + 1 N + 5 i W l r S i Z x f 8 g f b 1 D S V m n 7 0 = &lt; / l a t e x i t &gt; b K 1 ( ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x T / z d L k p Q 2 g X Z H u V + O 2 h a i v w L z 4 = " &gt; A A A C J n i c b V D L S s N A F J 3 U V 6 2 v q E s 3 w S J U 0 J I U U T d C 0 Y 3 Q T Q X 7 g C S G y W T S D p 0 8 m J k I J e R r 3 P g r b l x U R N z 5 K U 7 T C N p 6 Y O B w 7 r m P O W 5 M C R e 6 / q m U l p Z X V t f K 6 5 W N z a 3 t H X V 3 r 8 u j h C H c Q R G N W N + F H F M S 4 o 4 g g u J + z D A M X I p 7 7 u h m W u 8 9 Y s Z J FN 6 L c Y z t A A 5 C 4 h M E h Z Q c 9 S q 1 8 i E m G 7 h 2 q t f 1 H C c L J H O d t H V q Z A 9 p K 6 t Z V C 7 w 4 H H m q N U f g 7 Z I j I J U Q Y G 2 o 0 4 s L 0 J J g E O B K O T c N P R Y 2 C l k g i C K s 4 q V c B x D N I I D b E o a w g B z O 8 0 v z L Q j q X i a H z H 5 Q q H l 6 u + O F A a c j w N X O g M o h n y + N h X / q 5 m J 8 C / t l I R x I n C I Z o v 8 h G o i 0 q a Z a R 5 h G A k 6 l g Q i R u S t G h p C B p G Q y V Z k C M b 8 l x d J t 1 E 3 z u u N u 7 N q 8 7 q I o w w O w C G o A Q N c g C a 4 B W 3 Q A Q g 8 g R c w A W / K s / K q v C s f M 2 t J K X r 2 w R 8 o X 9 8 u c 6 F V &lt; / l a t e x i t &gt; b K K 1 ( ) &lt; l at e x i t s h a 1 _ b a s e 6 4 = " k t Q b q a o o 3 5 C i 0 T 5 c J 9 i I T Q t W R o 4 = " &gt; A A A C I n i c b V B J S 8 N A G J 2 4 1 r p F P X o Z L E I F K U k R l 1 v R i 9 B L B b t A E s N k M m 2 H T h Z m J k I J + S 1 e / C t e P C j q S f D H O E 0 j a O u D g c d 7 3 z b P i x k V 0 j A + t Y X F p e W V 1 d J a e X 1 j c 2 t b 3 9 n t i C j h m L R x x C L e 8 5 A g j I a k L a l k p B d z g g K P k a 4 3 u p r 4 3 X v C B Y 3 C W z m O i R O g Q U j 7 F C O p J F e / S O 1 8 i M U H n p M a N S P H 8 R z J P L d 5 l z a z q s 3 U c B 8 d Z a 5 e + T H h P D E L U g E F W q 7 + b v s R T g I S S s y Q E J Z p x N J J E Z c U M 5 K V 7 U S Q G O E R G h B L 0 R A F R D h p f l 0 G D 5 X i w 3 7 E 1 Q s l z N X f H S k K h B g H n q o M k B y K W W 8 i / u d Z i e y f O y k N 4 0 S S E E 8 X 9 R M G Z Q Q n e U G f c o I l G y u C M K f q V o i H i C M s V a p l F Y I 5 + + V 5 0 q n X z N N a / e a k 0 r g s 4 i i B f X A A q s A E Z 6 A B r k E L t A E G D + A J v I B X 7 V F 7 1 t 6 0 j 2 n p g l b 0 7 I E / 0 L 6 + A U 4 6 n 9 c = &lt; / l a t e x i t &gt; b K K ( ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s g f K o e + Z i 5 X i B t D k s 1 X I r x 6 / 5 V U = " &gt; A A A B 7 n i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I t Q Q c p u E f V Y F N F j B f s B 7 V K y a b Y N z W a X J C u U p T / C i w d F v P p 7 v P l v T N s 9 a O u D g c d 7 M 8 z M 8 2 P B t X G c b 5 R b W V 1 b 3 8 h v F r a 2 d 3 b 3 i v s H T R 0 l i r I G j U S k 2 j 7 R T H D J G o Y b w d q x Y i T 0 B W v 5 o 5 u p 3 3 p i S v N I P p p x z L y Q D C Q P O C X G S q 2 7 c v M M 3 5 7 2 i i W n 4 s y A l 4 m b k R J k q P e K X 9 1 + R J O Q S U M F 0 b r j O r H x U q I M p 4 J N C t 1 E s 5 j Q E R m w j q W S h E x 7 6 e z c C T 6 x S h 8 H k b I l D Z 6 p v y d S E m o 9 D n 3 b G R I z 1 I v e V P z P 6 y Q m u P J S L u P E M E n n i 4 J E Y B P h 6 e + 4 z x W j R o w t I V R x e y u m Q 6 I I N T a h g g 3 B X X x 5 m T S r F f e i U n 0 4 L 9 W u s z j y c A T H U A Y X L q E G 9 1 C H B l A Y w T O 8 w h u K 0 Q t 6 R x / z 1 h z K Z g 7 h D 9 D n D 1 m V j k Y = &lt; / l a t e x i t &gt; G(V, E) Bernstein Approximation &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 t N D d n Z v f Y X l l w s W 3 a J y 4 f D k Z D 0 = " &gt; A A A C Y X i c b V F N T x s x F P R u + U y B b u H I x S K q B A e i 3 Q i V X p B Q e 6 G C A 5 U I I G W T l d f x E m t t 7 8 p + W 5 F a / p O 9 c e H S P 4 I T g i g f T 7 I 0 n n l j P 4 / z W n A D c X w X h B 8 W F p e W V 1 Z b H 9 f W N z 5 F n z c v T d V o y n q 0 E p W + z o l h g i v W A w 6 C X d e a E Z k L d p W X P 6 b 6 1 W + m D a / U B U x q N p D k R v G C U w K e y q L b V B I Y 5 4 X 9 4 4 5 S 0 8 h U c M n B Z L Y 8 i t 3 Q n r o U x g x I V u K 0 0 I T a N O e q k p 6 3 p X O 2 O z x 1 e L f 7 d M Z P t / 8 E z 9 y e d + + X 7 p k Y 2 u f d r c u i d t y J Z 4 X f g m Q O 2 m h e 5 1 n 0 N x 1 V t J F M A R X E m H 4 S 1 z C w R A O n g r l W 2 h h W E 1 q S G 9 b 3 U B H J z M D O E n L 4 i 2 d G u K i 0 X w r w j P 3 f Y Y k 0 Z i J z 3 z m d 0 L z W p u R 7 W r + B 4 t v A c l U 3 w B R 9 v K h o B I Y K T + P G I 6 4 Z B T H x g F D N / a y Y j o l P E v y n t H w I y e s n v w W X 3 U 7 y t X P w 6 6 B 9 / H 0 e x w r a R j t o F y X o E B 2 j E 3 S O e o i i + 2 A h W A 8 2 g n / h a h i F m 4 + t Y T D 3 b K E X F W 4 / A P n Y u c g = &lt; / l a t e x i t &gt; (2I L) K k L k x &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 V j o D p W 4 a 4 X U a c P E u k + M X 2 3 O q G s = " &gt; A A A B 8 X i c b V C 7 S g N B F L 3 r M 8 Z X 1 N J m M A h W Y T e I W g Z t L C w i m A c m I c x O 7 i Z D Z m e X m V k h L P k L G w t F b P 0 b O / / G 2 W Q L T T w w c D j n X u b c 4 8 e C a + O 6 3 8 7 K 6 t r 6 x m Z h q 7 i 9 s 7 u 3 X z o 4 b O o o U Q w b L B K R a v t U o + A S G 4 Y b g e 1 Y I Q 1 9 g S 1 / f J P 5 r S d U m k f y w U x i 7 I V 0 K H n A G T V W e u y G 1 I z 8 I L 2 b 9 k t l t + L O Q J a J l 5 M y 5 K j 3 S 1 / d Q c S S E K V h g m r d 8 d z Y 9 F K q D G c C p 8 V u o j G m b E y H 2 L F U 0 h B 1 L 5 0 l n p J T q w x I E C n 7 p C E z 9 f d G S k O t J 6 F v J 7 O E e t H L x P + 8 T m K C q 1 7 K Z Z w Y l G z + U Z A I Y i K S n U 8 G X C E z Y m I J Z Y r b r I S N q K L M 2 J K K t g R v 8 e R l 0 q x W v I t K 9 f 6 8 X L v O 6 y j A M Z z A G X h w C T W 4 h T o 0 g I G E Z 3 i F N 0 c 7 L 8 6 7 8 z E f X X H y n S P 4 A + f z B 7 y n k P c = &lt; / l a t e x i t &gt; L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A T g U p w f G W o Z q d P 7 2 T 2 b j W z F e P X U = " &gt; A A A B 8 X i c b V D L S g M x F L 3 j s 9 Z X 1 a W b Y B F c l Z k i 6 r L o x m U F + 8 C 2 l E x 6 p w 3 N Z I Y k I 5 a h f + H G h S J u / R t 3 / o 2 Z d h b a e i B w O O d e c u 7 x Y 8 G 1 c d 1 v Z 2 V 1 b X 1 j s 7 B V 3 N 7 Z 3 d s v H R w 2 d Z Q o h g 0 W i U i 1 f a p R c I k N w 4 3 A d q y Q h r 7 A l j + + y f z W I y r N I 3 l v J j H 2 Q j q U P O C M G i s 9 d E N q R n 6 Q P k 3 7 p b J b c W c g y 8 T L S R l y 1 P u l r + 4 g Y k m I 0 j B B t e 5 4 b m x 6 K V W G M 4 H T Y j f R G F M 2 p k P s W C p p i L q X z h J P y a l V B i S I l H 3 S k J n 6 e y O l o d a T 0 L e T W U K 9 6 G X i f 1 4 n M c F V L + U y T g x K N v 8 o S A Q x E c n O J w O u k B k x s Y Q y x W 1 W w k Z U U W Z s S U V b g r d 4 8 j J p V i v e R a V 6 d 1 6 u X e d 1 F O A Y T u A M P L i E G t x C H R r A Q M I z v M K b o 5 0 X 5 9 3 5 m I + u O P n O E f y B 8 / k D / 4 O R I w = = &lt; / l a t e x i t &gt; An illustration of the proposed BernNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The impulse function ?x(?) = 1 if ? = x, otherwise ?x(?) = 0 Illustrations of four complex filters and their approximations learnt by BernNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 5 )</head><label>5</label><figDesc>Let h(?) = ? ?+(1??)?(?) denote the exact spectral filter, and g(?) = K k=0 w k ? k denote a polynomial approximation of h(?) (e.g. the suffix sum of h(?)'s taylor expansion). Since?(?) ? 0 when ? ? [0, 2], we have 0 ? h(?) ? ? ?+(1??)?0 = 1 for ? ? [0, 2].Consequently, it is natural to assume the polynomial filter g(?) = K k=0 w k ? k also satisfies 0 ? g(?) ? 1. Constraint 3.1. Assuming the energy function ?(L) is positive semidefinite, a polynomial filter g(?) = K k=0 w k ? k approximating the optimal solution to Equation (4) has to satisfy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Lemma 3 . 1 (</head><label>31</label><figDesc><ref type="bibr" target="#b22">[23]</ref>). Assume a polynomial p(x) = K k=0 ? k x k satisfies p(x) ? 0 for x ? [0, 1]. Then there exists a sequence of non-negative coefficients ? k , k = 0, . . . , K, such that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>A input image and the filtering results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Filters learnt from real-world datasets by BernNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Coefficients ? k learnt from real-world datasets by BernNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Filters learnt from real-world datasets by BernNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Coefficients ? k learnt from real-world datasets by BernNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Realizing commonly used filters with BernNet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average sum of squared error and R 2 score in parentheses.</figDesc><table><row><cell></cell><cell>Low-pass</cell><cell>High-pass</cell><cell>Band-pass</cell><cell>Band-rejection</cell><cell>Comb</cell></row><row><cell></cell><cell>exp(?10? 2 )</cell><cell cols="3">1 ? exp(?10? 2 ) exp(?10(? ? 1) 2 ) 1 ? exp(?10(? ? 1) 2 )</cell><cell>| sin(??)|</cell></row><row><cell>GCN</cell><cell>3.4799(.9872)</cell><cell>67.6635(.2364)</cell><cell>25.8755(.1148)</cell><cell>21.0747(.9438)</cell><cell>50.5120(.2977)</cell></row><row><cell>GAT</cell><cell>2.3574(.9905)</cell><cell>21.9618(.7529)</cell><cell>14.4326(.4823)</cell><cell>12.6384(.9652)</cell><cell>23.1813(.6957)</cell></row><row><cell cols="2">GPR-GNN 0.4169(.9984)</cell><cell>0.0943(.9986)</cell><cell>3.5121(.8551)</cell><cell>3.7917(.9905)</cell><cell>4.6549(.9311)</cell></row><row><cell>ARMA</cell><cell>1.8478(.9932)</cell><cell>1.8632(.9793)</cell><cell>7.6922(.7098)</cell><cell>8.2732(.9782)</cell><cell>15.1214(.7975)</cell></row><row><cell>ChebNet</cell><cell>0.8220(.9973)</cell><cell>0.7867(.9903)</cell><cell>2.2722(.9104)</cell><cell>2.5296(.9934)</cell><cell>4.0735(.9447)</cell></row><row><cell>BernNet</cell><cell>0.0314(.9999)</cell><cell>0.0113(.9999)</cell><cell>0.0411(.9984)</cell><cell>0.9313(.9973)</cell><cell>0.9982(.9868)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Cora CiteSeer PubMed Computers</cell><cell>Photo</cell><cell>Chameleon Squirrel Actor Texas Cornell</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results on real world benchmark datasets: Mean accuracy (%) ? 95% confidence interval.</figDesc><table><row><cell></cell><cell>GCN</cell><cell>GAT</cell><cell>APPNP</cell><cell>MLP</cell><cell>ChebNet</cell><cell>GPR-GNN</cell><cell>BernNet</cell></row><row><cell>Cora</cell><cell cols="5">87.14?1.01 88.03?0.79 88.14?0.73 76.96?0.95 86.67?0.82</cell><cell>88.57 ?0.69</cell><cell>88.52?0.95</cell></row><row><cell>CiteSeer</cell><cell cols="5">79.86?0.67 80.52 ?0.71 80.47?0.74 76.58?0.88 79.11?0.75</cell><cell>80.12?0.83</cell><cell>80.09?0.79</cell></row><row><cell>PubMed</cell><cell cols="5">86.74?0.27 87.04?0.24 88.12?0.31 85.94?0.22 87.95?0.28</cell><cell>88.46?0.33</cell><cell>88.48 ?0.41</cell></row><row><cell cols="6">Computers 83.32?0.33 83.32?0.39 85.32?0.37 82.85?0.38 87.54?0.43</cell><cell>86.85?0.25</cell><cell>87.64 ?0.44</cell></row><row><cell>Photo</cell><cell cols="5">88.26?0.73 90.94?0.68 88.51?0.31 84.72?0.34 93.77?0.32</cell><cell>93.85 ?0.28</cell><cell>93.63?0.35</cell></row><row><cell cols="6">Chameleon 59.61?2.21 63.13?1.93 51.84?1.82 46.85?1.51 59.28?1.25</cell><cell>67.28?1.09</cell><cell>68.29 ?1.58</cell></row><row><cell>Actor</cell><cell cols="5">33.23?1.16 33.93?2.47 39.66?0.55 40.19?0.56 37.61?0.89</cell><cell>39.92?0.67</cell><cell>41.79 ?1.01</cell></row><row><cell>Squirrel</cell><cell cols="5">46.78?0.87 44.49?0.88 34.71?0.57 31.03?1.18 40.55?0.42</cell><cell>50.15?1.92</cell><cell>51.35 ?0.73</cell></row><row><cell>Texas</cell><cell cols="5">77.38?3.28 80.82?2.13 90.98?1.64 91.45?1.14 86.22?2.45</cell><cell>92.95?1.31</cell><cell>93.12 ?0.65</cell></row><row><cell>Cornell</cell><cell cols="5">65.90?4.43 78.21?2.95 91.81?1.96 90.82?1.63 83.93?2.13</cell><cell>91.37?1.81</cell><cell>92.13 ?1.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Average running time per epoch (ms)/average total running time (s).</figDesc><table><row><cell></cell><cell>GCN</cell><cell>GAT</cell><cell>APPNP</cell><cell>MLP</cell><cell>ChebNet</cell><cell>GPR-GNN</cell><cell>BernNet</cell></row><row><cell>Cora</cell><cell>4.59/1.62</cell><cell>9.56/2.03</cell><cell cols="2">7.16/2.32 3.06/0.93</cell><cell>6.25/1.76</cell><cell>9.94/2.21</cell><cell>19.71/5.47</cell></row><row><cell>CiteSeer</cell><cell>4.63/1.95</cell><cell>9.93/2.21</cell><cell cols="2">7.79/2.77 2.95/1.09</cell><cell>8.28/2.56</cell><cell cols="2">11.16/2.37 22.36/6.32</cell></row><row><cell>PubMed</cell><cell cols="7">5.12/1.87 16.16/3.41 8.21/2.63 2.91/1.61 18.04/3.03 10.45/2.81 22.02/8.19</cell></row><row><cell cols="8">Computers 5.72/2.52 30.91/7.85 9.19/3.48 3.47/1.31 20.64/9.64 16.05/4.38 28.83/8.69</cell></row><row><cell>Photo</cell><cell cols="7">5.08/2.63 19.97/5.41 8.69/4.18 3.67/1.66 13.25/7.02 13.96/3.94 24.69/7.37</cell></row><row><cell cols="8">Chameleon 4.93/0.99 13.11/2.66 7.93/1.62 3.14/0.63 10.92/2.25 10.93/2.41 22.54/4.75</cell></row><row><cell>Actor</cell><cell cols="4">5.43/1.09 11.94/2.45 8.46/1.71 3.82/0.77</cell><cell>7.99/1.62</cell><cell cols="2">11.57/2.35 23.34/5.81</cell></row><row><cell>Squirrel</cell><cell cols="5">5.61/1.13 22.76/4.91 8.01/1.61 3.41/0.69 38.12/7.78</cell><cell>9.87/5.56</cell><cell>25.58/9.23</cell></row><row><cell>Texas</cell><cell>4.58/0.92</cell><cell>9.65/1.96</cell><cell cols="2">7.83/1.63 3.19/0.65</cell><cell>6.51/1.34</cell><cell cols="2">10.45/2.16 23.35/4.81</cell></row><row><cell>Cornell</cell><cell>4.83/0.97</cell><cell>9.79/1.99</cell><cell cols="2">8.23/1.68 3.25/0.66</cell><cell>5.85/1.22</cell><cell>9.86/2.05</cell><cell>22.23/5.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>URL and commit number of GPR-GNN codes</figDesc><table><row><cell>URL</cell><cell>Commit</cell></row><row><cell cols="2">GRP-GNN https://github.com/jianhao2016/GPRGNN 2507f10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters for BernNet on real-world datasets.</figDesc><table><row><cell>Datasets</cell><cell>Linear layer learning rate</cell><cell>Propagation layer learning rate</cell><cell>Hidden dimension</cell><cell>Propagation layer dropout</cell><cell>Linear layer dropout</cell><cell>K</cell><cell>Weight decay</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the expressive power of graph neural networks in a spectral perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammet</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>H?roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Ga?z?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Graph neural networks with convolutional arma filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alippi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A graph cnn-lstm neural network for short and long-term traffic forecasting based on trajectory data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toon</forename><surname>Bogaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">S</forename><surname>Masegosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Angarita-Zapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Onieva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hellinckx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Zhao-Min Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Traffic graph convolutional recurrent neural network: A deep learning framework for network-scale traffic learning and forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Henrickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhai</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-ITS</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4883" to="4894" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A linear complexity algorithm for the bernstein basis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Manuel</forename><surname>Pena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GMP</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="162" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph neural networks with adaptive frequency response filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Jalaian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The bernstein polynomial basis: A centennial retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farouki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Aided Geometric Design</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="379" to="419" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Could graph neural networks learn better molecular representation for drug discovery? a comparison study of descriptor-based and graph-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Yu</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingjun</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Encoding social information with graph convolutional networks forpolitical perspective detection in news media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Qinfeng Shi, and Anton Van Den Hengel. Image-based recommendations on styles and substitutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Signal processing techniques for interpolation in graph structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Sunil K Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP. IEEE</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Polynomials that are positive on an interval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Reznick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">352</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4677" to="4692" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepinf: Social influence prediction with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In KDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Practical high-quality electrostatic potential surfaces for drug discovery using a graph-convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Prakash Chandra Rathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><forename type="middle">L</forename><surname>Ludlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verdonk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="8778" to="8790" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Spectral and algebraic graph theory. Yale lecture notes, draft of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Spielman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-12-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Leveraging domain context for question answering over knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="323" to="335" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Session-based recommendation with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph convolutional networks using heat kernel for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keting</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph wavelet neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In KDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Interpreting and unifying graph neural networks with an optimization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<imprint>
			<publisher>WWW</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
