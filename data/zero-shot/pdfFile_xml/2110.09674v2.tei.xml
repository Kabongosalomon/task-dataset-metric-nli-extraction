<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Distillation: Aggregating Knowledge from Multiple Paths for Efficient Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumanth</forename><surname>Chennupati</surname></persName>
							<email>schennupati@wyze.com</email>
							<affiliation key="aff0">
								<orgName type="institution">WYZE Labs AI Team</orgName>
								<address>
									<settlement>Kirkland</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Mahdi</forename><surname>Kamani</surname></persName>
							<email>mkamani@wyze.com</email>
							<affiliation key="aff0">
								<orgName type="institution">WYZE Labs AI Team</orgName>
								<address>
									<settlement>Kirkland</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwei</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">WYZE Labs AI Team</orgName>
								<address>
									<settlement>Kirkland</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Chen</surname></persName>
							<email>zchen@wyze.com</email>
							<affiliation key="aff0">
								<orgName type="institution">WYZE Labs AI Team</orgName>
								<address>
									<settlement>Kirkland</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Distillation: Aggregating Knowledge from Multiple Paths for Efficient Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge Distillation is becoming one of the primary trends among neural network compression algorithms to improve the generalization performance of a smaller student model with guidance from a larger teacher model. This momentous rise in applications of knowledge distillation is accompanied by the introduction of numerous algorithms for distilling the knowledge such as soft targets and hint layers. Despite this advancement in different techniques for distilling the knowledge, the aggregation of different paths for distillation has not been studied comprehensively. This is of particular significance, not only because different paths have different importance, but also due to the fact that some paths might have negative effects on the generalization performance of the student model. Hence, we need to adaptively adjust the importance of each path to maximize the impact of distillation on the student model. In this paper, we explore different approaches for aggregating these different paths and introduce our proposed adaptive approach based on multitask learning methods. We empirically demonstrate the effectiveness of the proposed approach over other baselines on the applications of knowledge distillation in classification, semantic segmentation, and object detection tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The promising advancements of deep learning models in various AI tasks are dominantly depending on their large and complex model structures, which grants them boosted generalization capabilities on test data. However, the benefits of these achievements are limited in resourceconstrained systems such as mobile devices, low power robots, etc. Different model compression algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> have been proposed to reduce the complexity of such larger models for these systems. Among different compression algorithms, distilling the knowledge from a larger model to a smaller one has shown to be Feats Box mAP mAP 50  <ref type="table">Table 1</ref>. Comparing a hand-tuned versus adaptive approach to aggregate distillation paths for an object detection task on Cityscapes <ref type="bibr" target="#b8">[9]</ref>. For more details refer to <ref type="bibr">Section 4.</ref> highly effective and advantageous <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b56">57]</ref>. Beyond compression capabilities, knowledge distillation has also been a primary motive for different techniques that transfer the knowledge between models such as domain adaptation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b46">47]</ref>. With the upsurge in the applications of knowledge distillation in different domains, various methods were introduced for distilling the knowledge from a teacher to a student. These approaches are applied to different parts (i.e. distillation paths) of the models such as output logits <ref type="bibr" target="#b21">[22]</ref> and hidden layers' feature maps <ref type="bibr" target="#b58">[59]</ref>. Although most of these distillation paths could improve the generalization performance of the student model, a naive combination of these paths might have negative effects on the performance of the student model. For instance, when we use knowledge distillation for an object detector, we can distill the knowledge from their backbone feature maps <ref type="bibr" target="#b74">[75]</ref> or its bounding box generator <ref type="bibr" target="#b76">[77]</ref>. As it can be inferred from <ref type="table">Table 1</ref>, naively combining backbone features' distillation paths with those from the bounding box generator can degrade the performance when compared to the bounding box generator alone. This phenomenon indicates the necessity for a systematic approach towards the aggregation of different paths to gain the most from the knowledge distillation process.</p><p>In this paper, we propose an adaptive approach to learn the importance of each path during the distillation and train-ing process. This approach is inspired by multitask learning methods <ref type="bibr" target="#b4">[5]</ref>, where we consider each path as a separate task that we want to optimize the model for. Using our adaptive approach, we can mitigate the negative effects mentioned above and benefit the most while aggregating from multiple paths of distillation. As it can be seen in <ref type="table">Table 1</ref>, our proposed Adaptive approach can aggregate these two paths and surpasses both of them in improving the performance of the student model. In addition to our adaptive approach, we propose another baseline method based on multiobjective optimization, where it reduces the aggregation problem to a multi-criteria optimization and intends to find its Pareto stationary solutions. This approach seems to be more effective than naive aggregation methods, but cannot outperform our proposed adaptive distillation.</p><p>The main contributions of this paper can be summarized as follows:</p><p>? We propose an adaptive distillation approach, inspired by multitask learning methods for efficient aggregation of different distillation paths. We provide a general optimization formulation for this problem and reduce different methods using this formulation.</p><p>? We introduce multiobjective optimization for this problem as a baseline approach, which can be more effective than naive aggregation approaches.</p><p>? We conduct extensive comparison between our approach and other baseline methods in different tasks of image classification, semantic segmentation, and object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Knowledge distillation has become one of the prevailing approaches in model compression to improve the generalization performance of a smaller student model using the knowledge from a richer teacher model. In this section, we provide an overview of proposals in this domain, as well as methods we utilize to aggregate different paths of knowledge distillation.</p><p>Knowledge Distillation The concept of knowledge distillation for neural networks was first introduced by Hinton et al. <ref type="bibr" target="#b21">[22]</ref> to distill the knowledge from a teacher to a student model by minimizing the distance between their soft targets. This idea has been expanded to the hidden layers' feature maps by the introduction of Hint layers by Romero et al. <ref type="bibr" target="#b58">[59]</ref>. The idea of knowledge distillation and its variants has extensively employed in various problems such as compression <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b66">67]</ref>, knowledge transfer <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b72">73]</ref>, and federated learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b61">62]</ref>. It has also been explored in different tasks such as image classification <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b71">72]</ref>, object detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b74">75]</ref>, semantic segmentation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b69">70]</ref>, and graph neural net-works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b48">49]</ref> to name but a few. More detailed discussions regarding knowledge distillation approaches and state-ofthe-art methods in this domain can be found in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b64">65]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensemble of Teachers</head><p>The problem of aggregating different knowledge paths from multiple teachers has been the primary topic of several studies. Most studies investigate how to effectively combine the ensemble of teachers' outputs (logits) for distillation to a student model using weighted averaging <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37]</ref>. Different approaches have been proposed for finding these weights such as the teachers' confidence score <ref type="bibr" target="#b68">[69]</ref> or the similarity between two models' inputs <ref type="bibr" target="#b73">[74]</ref>. Zhang et al. <ref type="bibr" target="#b75">[76]</ref> introduce the reverse problem, where they can improve the teacher with an ensemble of student models using symmetrical KL divergence. Piao et al. <ref type="bibr" target="#b55">[56]</ref> introduce A2dele to combine depth and RGB networks using a confidence-based weights for distillation paths. In some other studies, the problem of an ensemble of teachers on hint layers' feature maps has been studied <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b54">55]</ref>. However, the number of these proposals is limited due to the challenging nature of this problem because of the non-aligning size of feature maps in different teachers. As it is mentioned by <ref type="bibr" target="#b64">[65]</ref>, finding a systematic approach to calculate the degree of efficiency for each distillation path in this problem remains open and challenging. To the best of our knowledge this is one of the first attempts to adaptively combine distillation paths with different nature in a training procedure.</p><p>Multiobjective Optimization An efficient way to aggregate different distillation paths is to consider the problem as multiobjective optimization, and hence, benefit from approaches proposed in this domain. There are various methods to solve a multiobjective optimization, however, for the sake of efficiency of the approach, we will use a first-order gradient-based method like the ones proposed in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b60">61]</ref>. Using these approaches, we can converge to the Pareto stationary of the problem, where no other solution can dominate that solution.</p><p>Multitask Learning Another way of aggregating knowledge from different distillation paths is to adjust the importance of each path by scaling their losses. Early works in multitask learning (MTL) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b63">64]</ref> use a weighted arithmetic sum of individual task losses. These weights are hand-tuned or manually searched and remain static throughout the training process. Later, Liu et al. <ref type="bibr" target="#b45">[46]</ref> and Guo et al. <ref type="bibr" target="#b14">[15]</ref> propose to adjust these weights by inspecting the change in loss or difficulty of each task as the training progresses. GradNorm <ref type="bibr" target="#b6">[7]</ref> proposes to normalize gradients from different losses to a common scale during backpropagation. Kendall et al. <ref type="bibr" target="#b28">[29]</ref> and Leang et al. <ref type="bibr" target="#b38">[39]</ref> propose to consider different task weights as parameters and learn them using backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>Distilling knowledge from a teacher to a student can take place at any stage of a model from initial feature maps to final logits of the output. Not all of them have the same effect on boosting the generalization performance of the student model. In fact, in practice, it can be seen that some of these distillation paths might hurt the generalization of the student models. Hence, it is of paramount importance to take these effects into consideration when updating the parameters based on each of these paths. In this section, we first introduce the problem formulation. Next, using this formulation, we provide few baseline approaches on how to aggregate different paths of distillation during the training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>In order to better formulate the multiple paths distillation problem at hand, we first describe the main learning task. Universally, in many forms of supervised learning tasks, the primary objective is to find an optimal prediction mapping, given a training dataset T , with N training samples. The mapping is between the input feature space X and the target space Y, whether it is a classification, semantic segmentation, or object detection task. In this case, each sample is presented with the intersection of these two spaces denoted by (</p><formula xml:id="formula_0">x (i) , y (i) ) ? X ? Y, i ? [N ]</formula><p>. Deep neural networks aim at representing this mapping using an M -layer neural network model, where each layer l is represented by a parameter set of w l ? R d l and applies the transformation of f l (.; w l ) on its input. The set of all parameters of the network is denoted by</p><formula xml:id="formula_1">w = {w 1 , . . . , w M } ? R d , where d = i?[M ] d i .</formula><p>Thus the main objective of this supervised task is to minimize the empirical risk on the training data, which is:</p><formula xml:id="formula_2">L (T ; w) = 1 N i?[N ] (x i , y i ; w) ,<label>(1)</label></formula><p>where (., .; .) is the loss on each sample such as cross entropy loss. In knowledge distillation frameworks the ultimate goal is for the student to imitate the teacher's output features in different layers. For instance, in the primary form of the knowledge distillation using soft targets <ref type="bibr" target="#b21">[22]</ref>, these outputs are the soft logits of the two models. Whereas in hint layers <ref type="bibr" target="#b58">[59]</ref> and attention transfer <ref type="bibr" target="#b33">[34]</ref>, these features are the middle layers' outputs. It should be noted that each distillation path only affects a subset of the parameters in the student model (unless for the soft target or equivalent, where all the parameters of the student model are affected). Hence, the general form of knowledge distillation loss for each path between the j-th layer on the student model and the k-th layer on the teacher model can be formulated as:</p><formula xml:id="formula_3">L jk KD X ; w S , w T = 1 N i?[N ] jk KD x i ; w S ?j , w T ?k ,<label>(2)</label></formula><p>where w S and w T are the student and teacher model parameters, respectively. The set of parameters for the layers up to layer j is denoted by w ?j . The loss for each sample is calculated based on the loss function jk KD (.; ., .). For instance, if the path is the soft target, the loss is the KL-divergence between two soft logits, and if it is a hint layer, the loss could be a simple euclidean distance <ref type="bibr" target="#b58">[59]</ref>. For hint layers an adaptation layer w A might be necessary to match the spatial or channel size of the feature maps on the student model to that of the teacher model. The parameters of this layer will be tuned using the distillation loss defined in Eq. <ref type="bibr" target="#b1">(2)</ref>. Thus, if we consider K paths for distillation with their own defined empirical loss function as in Eq. (2), we can create a distillation loss vector denoted by f KD X ; w S , w T ? R K . Then, the overall optimization can be written as:</p><formula xml:id="formula_4">min w S ?R ds L T ; w S + ? ? v f KD X ; w S , w T<label>(3)</label></formula><p>where ? ? [0, 1] is the weight indicating the importance of the distillation loss compared to the main empirical loss. v ? R K is the vector indicating the weight for each distillation path. For the current setting, we only consider the linear combination of losses between these paths; however, the nonlinear combination can be investigated in future studies. Next, we will describe how we can combine these losses during the training to distill the knowledge from the teacher model to the student model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Equal Weights</head><p>The most naive and common form of combining these distillation paths is to consider all of them equally weighted during the optimization. Considering the overall objective of this problem in Eq. (3), for this case, we should consider all distillation paths' weights equal, which is v = [1, . . . 1] . Although this might work in some cases, due to different scales of these losses, in most cases we need to adjust the weights accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Hyperparameter Optimization</head><p>Another baseline for this problem is to consider the weights for each distillation path as a new hyperparameter that we need to optimize before the training procedure as shown in <ref type="figure" target="#fig_1">Figure 1</ref>(b). In this case, the weight vector v is considered to be a hyperparameter that needs to be tuned. This can be done using different hyperparameter optimization approaches such as grid search, random search, or bilevel optimization <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multiobjective Optimization</head><p>If we look closely at the objective of the optimization in Eq. (3), it can be evidently reformulated into a multiobjective optimization as shown in <ref type="figure" target="#fig_1">Figure 1</ref>(c). In multiobjective optimization, the goal is to find a Pareto stationary  point, where no other local solution can dominate that solution in any of the objectives in the task. Most of the firstorder gradient-based approaches, based on this notion, aim at finding the direction at every step that is not harming any of the objectives until such direction cannot be found. In this way, all the objectives are treated equally and we try to find a Pareto stationary point of the problem. The main challenge of using multiobjective optimization in our task is that the fundamental goal of the main empirical risk and distillation losses are not the same in the essence, and the former is more important than the latter. A solution is to use preference-based approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b49">50]</ref>, to put more emphasis on the main objective of the learning. Another way is to consider the optimization problem of multiple paths distillation as a multiobjective optimization to find the best weights v and then combine it with the main empirical risk with the weight of ?. In this way, at every iteration, we first find a descent direction for all distillation losses, and then combine it with the gradients of the main learning objective using the weight ?. To find the descent direction for distillation losses at every iteration we use approaches introduced in different studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b60">61]</ref>.</p><p>To do so, for each distillation path, we compute the gradients of the parameters of the student model affected by that loss on the mini-batch ?. Then for each path i we combine the gradients of the parameters from different layers together in a vector g i ?; w S ?ji , w T ?ki , where j i and k i are the layer indices of student and teacher models, respectively, for the i-th distillation path. Since in each distillation path not all of the parameters in the student model are involved, we will use zero gradients for the parameters not involved in a path when gathering all the gradients. Then, by solving the following quadratic optimization, we will find the optimal weights of v for the current mini-batch as:</p><formula xml:id="formula_5">v * ? arg min v?? K i?[K] v i g i ?; w S ?ji , w T ?ki 2 2 ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">? K = p i |0 ? p i ? 1, i?[K] p i = 1 is a K- dimensional simplex.</formula><p>It has been shown that using the v * from Eq. (4) the resulting direction is descent for all distillation paths <ref type="bibr" target="#b60">[61]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Adaptive Distillation</head><p>Inspired by multitask learning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b38">39]</ref>, we intend to learn the importance of each distillation path adaptively by considering each path as a separate task in the learning. To do so, in addition to the parameters in Eq. (3), we introduce a new set of proxy parameters z = [z 1 , . . . , z K ] to estimate v = [e ?z1 , . . . , e ?z K ] as shown in <ref type="figure" target="#fig_1">Figure 1(d)</ref>. Thus, we update the objective of the optimization in Eq. (3) as:</p><formula xml:id="formula_7">min w S ?R ds ,z?R K L T ; w S (5) + ? ? (v f KD X ; w S , w T + i?[K] z i fd(X ;w S ,w T ,z) , )</formula><p>where the last two terms define the distillation loss in terms of the model parameters and the proxy parameters z. Expressing v i as e ?zi ensures v i &gt; 0 ?z i ? R. The term i?[K] z i acts as a regularization to prevent z i to converge to larger values that decreases v i , and thereby, vanishes the second loss term. To understand how z i gets updated, we inspect the gradients of the distillation loss in Eq. (5) with respect to z i on mini-batch of ? denoted by:</p><formula xml:id="formula_8">?f d X ; w S , w T , z ?z i ? = ? 1 ? e ?zi ? jiki KD ?; w S ?ji , w T ?ki (6)</formula><p>Hence, if we have access to the true loss (full batch), based on the first-order optimality condition resulting from Eq. (6), we can infer that the optimal value for v i would be equal to the inverse of its corresponding loss. This means that scaling the gradients of each loss at every iteration with the inverse of their true loss will give us the optimal results. However, due to the infeasibility of the true loss at every iteration, and since we are using stochastic gradient descent for the optimization, we will use the stochastic gradient in Eq. (6) to update z i values at every iteration. We expect this will converge to the optimal values for each path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Empirical Studies</head><p>In this section, we explore knowledge distillation with multiple paths from a teacher using the approaches mentioned in Section 3, in comparison with our proposed adaptive distillation. For most of the experiments, unless specified, we use ResNet50 <ref type="bibr" target="#b18">[19]</ref> as a teacher and ResNet18 as a student. We chose attention transfer (AT) <ref type="bibr" target="#b33">[34]</ref> for feature distillation paths, and soft target (ST) <ref type="bibr" target="#b21">[22]</ref> for logit distillation, as two paths for distillation. For attention transfer, we use the sum of squared differences between attention maps of features from layers 2 through 5 of ResNet50 and ResNet18. We </p><formula xml:id="formula_9">use v = [v AT , v ST ] =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results</head><p>Image Classification: We perform experiments on CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b34">[35]</ref>, as well as ImageNet-200 <ref type="bibr" target="#b67">[68]</ref> and ImageNet-1K <ref type="bibr" target="#b59">[60]</ref> for the image classification task. We evaluate top1 classification error on the validation dataset along with the top1 agreement error as suggested by Stanton et al. <ref type="bibr" target="#b62">[63]</ref> for knowledge distillation methods (i.e, classification error between teacher and student). We train our student for 200 epochs with batch size 128 on CIFAR datasets and 100 epochs with batch size 256 on ImageNet-200, ImageNet-1K datasets. We chose an SGD optimizer with an initial learning rate of 0.1 for all datasets. We reduce the learning rate by a factor of 0.1 at [100, 150] epochs for CIFAR-10, <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr">90]</ref> for both ImageNet datasets. For CIFAR-100, we reduce the learning rate by 0.2 at [60, 120, 160] epochs. We use random flip augmentation during training in experiments.</p><p>As it can be seen in <ref type="table">Table 2</ref>, we report the performance of teacher and student models without any knowledge distillation, student with single knowledge distillation path using hand-tuned weights, and baselines with multiple knowledge distillation paths which include hand-tuned weights, equal weights, and multiobjective optimization. We observe that adaptive distillation outperforms all other baselines in all three datasets. Among the baseline methods, the multiobjective optimization achieved better results on CIFAR-10 than others while hand-tuned and equal weights achieved better results on CIFAR-100 and ImageNet-200 respectively. Our proposed adaptive distillation method always outperforms the baseline methods and also the teacher model in terms of top1 classification error, as well as top1 agreement error between the teacher and student networks, which demonstrate the efficacy of our proposed aggregation approach for the knowledge distillation process.</p><p>Comparisons with state-of-the-arts: In <ref type="figure" target="#fig_3">Figure 2</ref>, we study the performance of our adaptive distillation methods when added to a combination of existing state-of-theart distillation methods for image classification task on CIFAR-10, CIFAR-100 and ImageNet-200 datasets. In addition to Attention Transfer (AT) <ref type="bibr" target="#b33">[34]</ref> and Soft Target (ST) <ref type="bibr" target="#b21">[22]</ref> distillation methods, we use the recent and advanced distillation methods like Overhaul of Feature Distillation (OFD) <ref type="bibr" target="#b20">[21]</ref>, Feature Map (FM) <ref type="bibr" target="#b70">[71]</ref> and Softmax Regression and Representation Learning (SR) <ref type="bibr" target="#b70">[71]</ref>. For each combination, we chose two distillation paths that include distillation at intermediate features (AT, OFD, FM) and logits (ST, SR). We observed that adding adaptive methods to the existing methods consistently improved the performance. This experiment shows that our Adaptive Distillation is orthogonal to other knowledge distillation methods and can be used on top of those approaches to boost their performance.</p><p>Ablations: In <ref type="table">Table 3</ref>, we present how adaptive distillation performs when we add more knowledge distillation paths. We add neural selective transfer (NST) <ref type="bibr" target="#b23">[24]</ref> for feature level distillation and regression logits ( 2 -Logit) <ref type="bibr" target="#b2">[3]</ref> for output layers in addition to existing paths. We use v = [v AT , v ST , v N ST , v 2?Logit ] = [1000, 0.1, 10, 0.1] for handtuned baselines. We construct three variants of adaptive distillation methods by choosing [AT, ST], [NST, 2 -Logit] and [AT, ST, NST, 2 -Logit]. Finally, we also explore the robustness of adaptive distillation by treating each of the 4 residual layers in the backbone as a unique path for distillation. This results in 5 (4+1) paths for [AT, ST] and [NST, 2 -Logit] experiments, and 10 (8+2) paths for [AT, ST, NST, 2 -Logit]. We refer to these models as 'Adaptivelayerwise'. For more information on different structures refer to Appendix A.</p><p>We observed that our proposed adaptive methods continue to outperform their single baselines in terms of both top1 classification error and top1 agreement error. Adaptive methods with [AT, ST] paths yielded the best results in terms of top1 classification error compared to other counterparts. However, the adaptive method with [AT, ST, NST,  <ref type="table">Table 2</ref>. Validation results for different knowledge distillation methods on CIFAR-10, CIFAR-100, ImageNet-200 and ImageNet-1K. We report top1 classification error (%) for the student model and top1 agreement error (%) defined in <ref type="bibr" target="#b62">[63]</ref> (i.e, classification error between teacher and student). Adaptive distillation achieves the best results, even better than the teacher model, in all datasets. (?) indicates lower the better. reduce negative effects caused by [NST, 2 -Logit] to some degree. The same patterns can be observed with layerwise approaches as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Importance of distillation loss (?)</head><p>As it was mentioned in Section 4, when the number of distillation paths increases, the alignment between teacher and student increases (the agreement error decreases), but the performance of the student model does not necessarily improve. This is mainly due to the higher importance of the distillation when the number of paths increases. To remedy this, we suggest to decrease the weight of distillation using its parameter ?. In <ref type="table" target="#tab_2">Table 4</ref>, we change ?, the relative importance of distillation loss compared to main empirical loss. We observed that higher value of alphas led to lower top1 agreement error between teacher and student networks. We also observed that as the number of distillation paths are increased, the norm of the gradients from distillation loss increases, which may improve top1 agreement error but might not always improve the top1 classification error. A relatively lower ?, reduces the weight of distillation gradients and bolster to improve top1 classification error when more distillation paths are used. For example, changing ? to 0.75 instead of 1.0 helped improve the top1 classification error when all [AT, ST, NST and 2 -Logit] distillation paths were used on CIFAR-100 and ImageNet-200 datasets. We observe a similar trend with the layerwise methods in <ref type="table" target="#tab_4">Table 8</ref>. On the other hand, it can be seen that using lower ? values increase both top1 agreement and top1 classification errors. <ref type="table">Table 5</ref>, we experiment with smaller ResNet model with only 10 layers. We observe that adaptive distillation with multiple paths are outperforming their single distillation counterparts even when the student model is smaller and the gap between teacher and student is higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smaller Student Models: In</head><p>Semantic Segmentation: For this task, we perform experiments on Cityscapes <ref type="bibr" target="#b8">[9]</ref> and ADE20K <ref type="bibr" target="#b77">[78]</ref>  evaluate mean intersection over union (mIoU) and mean accuracy (mAcc). We use semantic fpn <ref type="bibr" target="#b31">[32]</ref> network with ResNet <ref type="bibr" target="#b18">[19]</ref> backbone(s). We initialize our backbone(s) with ImageNet pretrained weights. We use attention transfer (AT) for backbone features and soft target (ST) for outputs. We flatten the semantic segmentation output to generate soft targets. We chose an SGD optimizer with an initial learning rate of 0.01, polynomial learning rate policy (power=0.9) and train for 80K iterations with a batch size 8 (512x1024 crop) images. <ref type="table">Table 6</ref> indicates the performance of knowledge distillation with multiple paths for this task. We observe that whenever the gap between teacher and student models' performance is large, adaptive distillation outperforms other approaches in this task. This can be inferred from results in ADE 20K dataset, where adaptive distillation outperforms hand-tuned ones. When this gap is small, adaptive distillation can achieve very close performance to the best hand-tuned model, which essentially reduces the time for exhaustive search for best hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Detection:</head><p>We perform experiments on Cityscapes <ref type="bibr" target="#b8">[9]</ref> and COCO <ref type="bibr" target="#b41">[42]</ref> datasets and evaluate mean average precision(mAP). We use RetinaNet <ref type="bibr" target="#b42">[43]</ref> with Gen-eralized Focal Loss <ref type="bibr" target="#b39">[40]</ref> built using ResNet <ref type="bibr" target="#b18">[19]</ref> backbone(s). We use ResNet18 as student backbone and ResNet101 as the teacher backbone. We use feature-based knowledge distillation <ref type="bibr" target="#b74">[75]</ref> for features ('Feats B ' indicate features from the backbone layers and 'Feats P ' indicate features from the pyramid layers) and localization distillation <ref type="bibr" target="#b76">[77]</ref> for bounding box generator (referred as 'Box'). We initialize our backbone(s) with coco pretrained weights. We chose an SGD optimizer with an initial learning rate of (0.01, 0.02) and train for a total of (64, 12) epochs on Cityscapes and coco datasets. We reduce the learning rate by factor of 10 after 56 epochs on Cityscapes while after 8, 11 epochs on coco dataset. We use a batch size 8 (512x1024 crop) images. For hand-tuned weights, we use the hyperparameters suggested by the respective authors. Results on the <ref type="table">Table 7</ref> indicate that adaptive distillation and multiobjective outperform Hand-tuned baselines. When 'Feats B ' were used for feature distillation along with Bbox distillation, hand-tuned baseline failed to minimize the negative effects caused by distillation on backbone features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Directions</head><p>In this paper, we explored different methods for aggregating different paths for efficient knowledge distillation from a teacher to a student. We proposed an adaptive approach, with which we intend to learn the importance of each distillation path during the training process. The effectiveness of this approach is being corroborated by our extensive empirical studies on various tasks such as classification, semantic segmentation, and object detection. Moreover, we introduce another baseline for this problem based on multiobjective optimization.</p><p>Although our approach has been examined on a single teacher with multiple distillation paths, the extension of these methods to multiple teacher can be investigated in future works. Moreover, as another future direction, a theoretical investigation of these approaches can further illuminate the effect of each path on the distillation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Experimental Details</head><p>In this section, we will provide more details regarding our approach and the empirical studies discussed in the main body. In Section 4, we present results of applying different methods for distilling the knowledge from different paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Distillation Losses</head><p>In this part, we explain each knowledge distillation paths we used in our experimental studies.</p><p>Soft Target First, we start by the soft target (ST) <ref type="bibr" target="#b21">[22]</ref>, which is the primary form of distillation used in many different applications. In this form of distillation the goal is for the student model to match the probability distribution of the teacher for each data sample. Hence, if the input data is x, by denoting the soft target of each model as:</p><formula xml:id="formula_10">qi (x|? ) = e z i (x) ? j?[C] e z j (x) ? , ?i ? [C],</formula><p>where ? is the temperature for the soft targets and zi (.)s are the logits for each class generated by the model. The total number of classes is C. Thus, the loss for this distillation path is the cross entropy between the soft targets of the teacher and the student defined as:</p><formula xml:id="formula_11">LST x; w S , w T = ?? 2 i?[C] q T i (x|? ) log q S i (x|? ),</formula><p>where q T i and q S i are the soft targets of the student and the teacher, respectively.</p><p>Attention Transfer This type of distillation is used on hint layer features and tries to minimize the distance between the feature maps of the student and teacher as defined in <ref type="bibr" target="#b33">[34]</ref>. The loss function for this distillation path can be written as:</p><formula xml:id="formula_12">LAT x; w S , w T = j?[N ] a S j a S j 2 ? a T j a T j 2 2</formula><p>where a S j = vec A S j and a T j = vec A T j are respectively the j-th pair of student and teacher attention maps in vectorized form, and p refers to norm type. The attention maps Aj ? R H j ?W j are computed by adding the transformed features across its channel dimension Dj of feature map Fj ? R H j ?W j ?D j for both student and teacher models:</p><formula xml:id="formula_13">Aj = D j d=1 (F j,d ) 2</formula><p>Neural Selective Transfer In this distillation, we match the features (in the spatial dimension) between teacher and student networks by minimizing a special case (d = 1, c = 0) of Maximum Mean Discrepancy (MMD) distance described in <ref type="bibr" target="#b23">[24]</ref> as,</p><formula xml:id="formula_14">LNST x; w S , w T = G F T j ? G w A j F S j 2</formula><p>where F T j and w A j F S j are j-th pair of teacher and adapted student feature maps normalized across the channel dimension and G is the Gram matrix computed as:</p><formula xml:id="formula_15">G(FD j ?H j W j ) = F D j ?H j W j FD j ?H j W j</formula><p>Compared to attention transfer <ref type="bibr" target="#b33">[34]</ref> which computes spatial attention (Aj ? R H j ?W j ) of the feature maps, neural selection transfer <ref type="bibr" target="#b23">[24]</ref> computes Gram Matrix (Gj ? R H j W j ?H j W j ) to capture interactions across the spatial domain.</p><p>2 -Logit This form of distillation is similar to the soft target, but in here, we simply measure the euclidean distance between teacher and student output logits (z T , z S ) instead of their cross entropy <ref type="bibr" target="#b2">[3]</ref>.</p><formula xml:id="formula_16">L 2 ?Logit x; w S , w T = z T x; w T ? z S x; w S 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Metrics</head><p>For the experimental results in addition to the main metric of the task (classification error for the classification, IoU for the semantic segmentation, and mAP for the object detection tasks), we used top1 agreement error, a metric introduced by [63] to evaluate the performance of knowledge distillation approaches. We compute top1 agreement error using prediction probabilities (p T ? R C and p S ? R C ) from teacher and student as:</p><formula xml:id="formula_17">Top1Agreement Error = (7) 1 ? 1 N N i=1 [arg max C (p T i ) = arg max C (p S i )] ? 100</formula><p>Looking into Eq. <ref type="formula">(7)</ref> it can be inferred that this metric wants to evaluate the objective of the primary distillation form as in soft target in Eq. <ref type="bibr" target="#b6">(7)</ref>. As it can be seen in the experimental results, having a better alignment with the teacher does not necessary reflects as a better generalization performance for the student. Especially, in cases when the student can have a better performance than the teacher using knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Ablations</head><p>Adaptive vs Adaptive-layerwise In adaptive and all other baselines, we add the distillation losses estimated by comparing teacher and student feature maps from different layers of their backbones as shown in <ref type="figure" target="#fig_5">Figure 3(a)</ref>. In adaptive-layerwise shown in <ref type="figure" target="#fig_5">Figure 3(b)</ref>, we consider the distillation loss from layer (i) as an independent distillation path and learn their importance by scaling their loss term with vi. We compare the performance between adaptive and adaptive-layerwise methods in <ref type="table">Table 3</ref>, as well as in <ref type="table" target="#tab_4">Table 8</ref> by varying the ? value for distillation. From both tables, it seems that by adding more paths, the agreement between the teacher and the student increases, however, this does not necessarily reflects on the performance of the student model. Also, by increasing ?, the same pattern emerges that increase in agreement is not necessarily followed by increase in the performance of the student model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptive vs Multiobjective</head><p>To further compare our proposed adaptive distillation and multiobjective optimization approaches, we investigate their learned weights and gradient similarities between distillation paths in both methods. In <ref type="figure" target="#fig_6">Figure 4</ref>, we visualize the weights learned by both methods for each distillation path during the training on CIFAR-100 dataset. We observe that in both adaptive and multiobjective methods, AT distillation gets higher importance compared ST distillation, which is mostly due to its lower scale in its loss. In adaptive distillation, as it can be seen in <ref type="figure" target="#fig_6">Figure 4(a)</ref>, both weights can increase unboundedly. However, in multiobjective optimization based on Eq. (4), we know that the weights belong to a simplex and cannot increase without a limit. <ref type="figure" target="#fig_6">Figure 4(b)</ref> shows the weights for the multiobjective optimization, where the sum of two weights is equal to 1.0 at all time. In <ref type="figure" target="#fig_6">Figure 4</ref>(c), we visualize the cosine similarity between gradients from different distillation paths (AT and ST) for both adaptive and multiobjective methods. It can be seen that for multiobjective optimization, this similarity decreases as the training goes on, while for the adaptive distillation it increases. When the similarity decreases, it means that different losses bring more diversity to the training and can be more effective for the student model. Hence, based on this observation, we need to adjust the weight of distillation losses ? during the training. Based on this observation, this weight should be increased for multiobjective optimization during the training, and should be decreased smoothly for the adaptive distillation training.</p><p>Training time and convergence comparisons An important aspect in model training using knowledge distillation is the overhead computation we are adding for this purpose. Especially, when the number of distillation paths increases this might be costly. Hence, we compare the time of training iteration for different approaches we used in our experiments with different number of distillation paths. In <ref type="table" target="#tab_5">Table 9</ref>, we compare the average training time for an iteration with batch-size of 256 images (64?64) from ImageNet-200 dataset trained on 4 V100 GPUs. Comparing the adaptive method's speed with single distillation path's or handtuned method's, it can be inferred that the computational overhead of the adaptive distillation is minimal. On the other hand, multiobjective optimization has a huge computational overhead compared to adaptive distillation, due to its gradient computation w.r.t different objectives and the quadratic optimization to find the pareto optimal gradient descent.</p><p>In addition to the time of training, we can compare the convergence rate of different algorithms during training. <ref type="figure" target="#fig_7">Figure 5</ref> demonstrates the validation top1 classification error during the training for different approaches on CIFAR-10, CIFAR-100 and ImageNet-200 datasets. It can be inferred that our Adaptive distillation method can converge to a lower validation errors, in some cases even lower than the teacher model.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Schematic of different aggregation approaches for knowledge distillation. (a) The single distillation approach. (b) Hyperparameter optimization approach to find the optimal weight for each path before the training. (c) Multiobjective optimization to find the descent direction for multiple paths at every iteration. (d) The proposed adaptive distillation to adaptively learn the weights using the main optimizer of the problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc><ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b0">1]</ref> for 'Equal' weights baseline and chose v = [1000, 0.1] for 'Hand-tuned' weights baseline based on our grid search hyperparameter tuning. Multiobjective, and Adaptive methods initializeweights as v = [v AT , v ST ] = [1, 1]  and aim to learn the best weights during training. We use ?=1.0 which is the relative importance of aggregated distillation losses compared to the main loss. A more detailed discussion on the experimental setups with additional results are deferred to Appendix A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Comparison between several combinations of existing knowledge distillation methods using Hand-tuned vs Adaptive (ours) methods on (a) CIFAR-10, (b) CIFAR-100 and (c) ImageNet-200 datasets. AT: attention transfer<ref type="bibr" target="#b33">[34]</ref>, ST: soft target<ref type="bibr" target="#b21">[22]</ref>, OFD: overhaul of feature distillation<ref type="bibr" target="#b20">[21]</ref>, FM: feature map<ref type="bibr" target="#b70">[71]</ref>, and SR: Softmax regression and representation learning<ref type="bibr" target="#b70">[71]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Aggregated loss from different layers of backbone.(b) Independent loss from each layer, aka layerwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Knowledge distillation with multiple paths. (a) Backbone features from different layers are passed through a single distillation loss function. (b) Each feature from a backbone layer is passed through a distillation loss function. Path(s) CIFAR-100 ImageNet-200 ? AT ST NST 2 -Logit top1 err (?) top1 agr (?) top1 err (?) top1 agr (?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of changes in importance for each distillation path in (a) Adaptive (e ?z i ) (b) Multiobjective during the training process on CIFAR-100 dataset (c) Gradient similarity between distillation paths AT &amp; ST during training on CIFAR-100 using cosine similarity Path(s) Avg Train ImageNet-200 AT ST NST 2 -Logit iter time (s) (?) top1 err (?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>(a) CIFAR-10 (b) CIFAR-100 (c) ImageNet-200 Training progress of different knowledge distillation methods on (a) CIFAR-10, (b) CIFAR-100 and (c) ImageNet-200 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>AT ST top1 err (?) top1 agr (?) top1 err (?) top1 agr (?) top1 err (?) top1 agr (?) top1 err (?) top1 agr (?)</figDesc><table><row><cell>Path(s)</cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell>CIFAR-100</cell><cell cols="2">ImageNet-200</cell><cell cols="2">ImageNet-1K</cell></row><row><cell>Student</cell><cell>5.19</cell><cell>-</cell><cell>24.84</cell><cell>-</cell><cell>42.34</cell><cell>-</cell><cell>30.10</cell><cell>-</cell></row><row><cell>Single</cell><cell>4.51</cell><cell>3.20</cell><cell>22.87</cell><cell>19.04</cell><cell>39.80</cell><cell>35.07</cell><cell>29.42</cell><cell>21.80</cell></row><row><cell>Single</cell><cell>5.10</cell><cell>4.06</cell><cell>22.67</cell><cell>19.71</cell><cell>40.24</cell><cell>35.38</cell><cell>29.09</cell><cell>21.66</cell></row><row><cell>Hand-tuned</cell><cell>4.75</cell><cell>3.74</cell><cell>21.47</cell><cell>17.40</cell><cell>39.00</cell><cell>32.89</cell><cell>-</cell><cell>-</cell></row><row><cell>Equal</cell><cell>5.05</cell><cell>4.23</cell><cell>21.75</cell><cell>18.53</cell><cell>38.25</cell><cell>29.83</cell><cell>28.73</cell><cell>20.52</cell></row><row><cell>Multiobjective</cell><cell>4.65</cell><cell>3.63</cell><cell>21.58</cell><cell>18.09</cell><cell>39.75</cell><cell>33.22</cell><cell>-</cell><cell>-</cell></row><row><cell>Adaptive</cell><cell>4.39</cell><cell>3.12</cell><cell>20.04</cell><cell>15.89</cell><cell>37.68</cell><cell>29.46</cell><cell>28.39</cell><cell>20.29</cell></row><row><cell>Teacher</cell><cell>4.63</cell><cell>-</cell><cell>20.10</cell><cell>-</cell><cell>40.81</cell><cell>-</cell><cell>23.45</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Validation results on CIFAR-100 and ImageNet-200 by varying ? (the weight indicating the importance of the distillation loss compared to the main empirical loss). When the number of distillation paths increases, the lower ? values are preferable.</figDesc><table><row><cell>datasets and</cell></row></table><note>? AT ST NST 2 -Logit top1 err (?) top1 agr (?) top1 err (?) top1 agr (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .Table 7 .</head><label>57</label><figDesc>AT ST NST 2 -Logit top1 err (?) top1 agr (?) top1 err (?) top1 agr (?) Validation results on CIFAR-100 and ImageNet-200 for Student ResNet10 and Teacher ResNet50. The effect of number of distillation paths can be seen in this table since layerwise approaches have more paths and need smaller ? values to improve the performance of the student model. Performance of knowledge distillation with multiple paths on Cityscapes<ref type="bibr" target="#b8">[9]</ref> and COCO<ref type="bibr" target="#b41">[42]</ref> datasets for object detection.(?) indicates higher the better. FeatsB: features from backbone layers, FeatsP: features from pyramid layers, and Box: bounding box generator.</figDesc><table><row><cell></cell><cell cols="2">Path(s)</cell><cell cols="2">CIFAR-100</cell><cell cols="2">ImageNet-200</cell></row><row><cell cols="2">? Student (ResNet-10)</cell><cell></cell><cell>27.06</cell><cell>-</cell><cell>45.75</cell><cell>-</cell></row><row><cell>Single</cell><cell>1.00</cell><cell></cell><cell>26.29</cell><cell>23.43</cell><cell>43.89</cell><cell>37.69</cell></row><row><cell>Single</cell><cell>1.00</cell><cell></cell><cell>25.31</cell><cell>22.82</cell><cell>44.23</cell><cell>38.35</cell></row><row><cell>Single</cell><cell>1.00</cell><cell></cell><cell>24.61</cell><cell>21.53</cell><cell>43.98</cell><cell>39.86</cell></row><row><cell>Single</cell><cell>1.00</cell><cell></cell><cell>24.76</cell><cell>22.16</cell><cell>44.65</cell><cell>37.90</cell></row><row><cell>Adaptive</cell><cell>1.00</cell><cell></cell><cell>23.88</cell><cell>19.78</cell><cell>41.36</cell><cell>34.76</cell></row><row><cell>Adaptive</cell><cell>0.75</cell><cell></cell><cell>23.13</cell><cell>19.64</cell><cell>43.15</cell><cell>36.26</cell></row><row><cell>Adaptive</cell><cell>0.75</cell><cell></cell><cell>22.77</cell><cell>19.27</cell><cell>40.93</cell><cell>33.25</cell></row><row><cell>Teacher (ResNet-50)</cell><cell></cell><cell></cell><cell>20.10</cell><cell>-</cell><cell>40.81</cell><cell>-</cell></row><row><cell></cell><cell>Path(s)</cell><cell cols="2">Cityscapes</cell><cell cols="2">ADE 20K</cell></row><row><cell></cell><cell cols="3">AT ST mIoU (?) mAcc (?)</cell><cell>mIoU(?)</cell><cell>mAcc (?)</cell></row><row><cell>Student</cell><cell></cell><cell>73.42</cell><cell>81.33</cell><cell>32.84</cell><cell>42.50</cell></row><row><cell>Single</cell><cell></cell><cell>75.75</cell><cell>83.59</cell><cell>34.03</cell><cell>43.69</cell></row><row><cell>Single</cell><cell></cell><cell>75.25</cell><cell>82.89</cell><cell>33.19</cell><cell>42.79</cell></row><row><cell cols="2">Hand-tuned</cell><cell>75.67</cell><cell>83.49</cell><cell>33.43</cell><cell>43.66</cell></row><row><cell>Equal</cell><cell></cell><cell>74.38</cell><cell>81.77</cell><cell>34.06</cell><cell>42.95</cell></row><row><cell cols="2">Multiobjective</cell><cell>68.99</cell><cell>77.19</cell><cell>34.01</cell><cell>43.04</cell></row><row><cell cols="2">Adaptive</cell><cell>75.43</cell><cell>82.51</cell><cell>34.73</cell><cell>44.28</cell></row><row><cell cols="2">Teacher</cell><cell>74.52</cell><cell>81.86</cell><cell>36.45</cell><cell>46.96</cell></row><row><cell cols="7">Table 6. Performance of knowledge distillation methods with multiple paths on Cityscapes [9] and ADE 20K [78] datasets for semantic</cell></row><row><cell cols="2">segmentation.(?) indicates higher the better.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Path(s)</cell><cell cols="2">Cityscapes COCO</cell><cell>Path(s)</cell><cell cols="2">Cityscapes COCO</cell></row><row><cell></cell><cell cols="2">Feats B Box mAP (?)</cell><cell cols="3">mAP (?) Feats P Box mAP (?)</cell><cell>mAP (?)</cell></row><row><cell>Student</cell><cell></cell><cell>38.1</cell><cell>36.1</cell><cell></cell><cell>38.1</cell><cell>36.1</cell></row><row><cell>Single</cell><cell></cell><cell>39.6</cell><cell>36.3</cell><cell></cell><cell>39.9</cell><cell>36.8</cell></row><row><cell>Single</cell><cell></cell><cell>39.8</cell><cell>36.2</cell><cell></cell><cell>39.8</cell><cell>36.3</cell></row><row><cell>Hand-tuned</cell><cell></cell><cell>39.7</cell><cell>36.4</cell><cell></cell><cell>40.1</cell><cell>36.7</cell></row><row><cell>Multiobjective</cell><cell></cell><cell>40.0</cell><cell>36.5</cell><cell></cell><cell>40.6</cell><cell>36.8</cell></row><row><cell>Adaptive</cell><cell></cell><cell>40.2</cell><cell>37.1</cell><cell></cell><cell>40.9</cell><cell>37.7</cell></row><row><cell>Teacher</cell><cell></cell><cell>42.9</cell><cell>42.7</cell><cell></cell><cell>42.9</cell><cell>42.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8 .</head><label>8</label><figDesc>Validation results on CIFAR-100 and ImageNet-200 by varying ? (the weight indicating the importance of the distillation loss compared to the main empirical loss).</figDesc><table><row><cell cols="2">Student (ResNet-18)</cell><cell>24.84</cell><cell>-</cell><cell>42.34</cell><cell>-</cell></row><row><cell>Adaptive</cell><cell>0.25</cell><cell>20.28</cell><cell>16.11</cell><cell>38.34</cell><cell>31.72</cell></row><row><cell>Adaptive</cell><cell>0.50</cell><cell>20.46</cell><cell>15.92</cell><cell>38.02</cell><cell>29.69</cell></row><row><cell>Adaptive</cell><cell>0.75</cell><cell>19.94</cell><cell>15.34</cell><cell>37.84</cell><cell>28.69</cell></row><row><cell>Adaptive</cell><cell>1.00</cell><cell>20.16</cell><cell>15.09</cell><cell>38.29</cell><cell>27.84</cell></row><row><cell cols="2">Adaptive-layerwise 0.25</cell><cell>20.21</cell><cell>15.28</cell><cell>38.63</cell><cell>31.58</cell></row><row><cell cols="2">Adaptive-layerwise 0.50</cell><cell>20.89</cell><cell>15.44</cell><cell>38.07</cell><cell>29.89</cell></row><row><cell cols="2">Adaptive-layerwise 0.75</cell><cell>20.55</cell><cell>15.02</cell><cell>38.80</cell><cell>28.39</cell></row><row><cell cols="2">Adaptive-layerwise 1.00</cell><cell>20.85</cell><cell>15.25</cell><cell>38.34</cell><cell>28.75</cell></row><row><cell cols="2">Teacher (ResNet-50)</cell><cell>20.10</cell><cell>-</cell><cell>40.81</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 9 .</head><label>9</label><figDesc>Average training iteration speed on ImageNet-200 dataset.</figDesc><table><row><cell>Student (ResNet-18)</cell><cell>0.1325</cell><cell>42.34</cell></row><row><cell>Single</cell><cell>0.2244</cell><cell>39.80</cell></row><row><cell>Single</cell><cell>0.2185</cell><cell>40.24</cell></row><row><cell>Single</cell><cell>0.3194</cell><cell>40.32</cell></row><row><cell>Single</cell><cell>0.2191</cell><cell>39.95</cell></row><row><cell>Hand-tuned</cell><cell>0.2243</cell><cell>39.00</cell></row><row><cell>Equal</cell><cell>0.2248</cell><cell>38.25</cell></row><row><cell>Multiobjective</cell><cell>0.5030</cell><cell>39.75</cell></row><row><cell>Adaptive</cell><cell>0.2248</cell><cell>37.68</cell></row><row><cell>Adaptive</cell><cell>0.3200</cell><cell>38.67</cell></row><row><cell>Adaptive</cell><cell>0.3272</cell><cell>37.84</cell></row><row><cell>Adaptive-layerwise</cell><cell>0.2279</cell><cell>38.32</cell></row><row><cell>Adaptive-layerwise</cell><cell>0.3210</cell><cell>39.50</cell></row><row><cell>Adaptive-layerwise</cell><cell>0.3288</cell><cell>38.07</cell></row><row><cell>Teacher (ResNet-50)</cell><cell>0.3614</cell><cell>40.81</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">-Logit] achieves the best agreement error, which is the objective of the distillation part. This suggests that shifting the focus from distillation (i.e, reducing the value of ?) to main loss could improve top1 classification error. Also, our adaptive methods with [AT, ST, NST, 2 -Logit] paths performed slightly better than [NST, 2 -Logit] and worse compared to [AT, ST] paths. This suggests that adaptive distillation can</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards understanding ensemble, knowledge distillation and self-distillation in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09816</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">N2n learning: Network to network compression via policy gradient reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beainy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blalock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J G</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03033</idno>
		<title level="m">What is the state of neural network pruning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Journal</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning efficient object detection models with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="742" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Auxnet: Auxiliary tasks enhanced semantic segmentation for automated driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sistu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rawashdeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISAPP)</title>
		<meeting>the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISAPP)</meeting>
		<imprint>
			<publisher>INSTICC, SciTePress</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="645" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">General instance distillation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7842" to="7851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple-gradient descent algorithm (mgda) for multiobjective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-A</forename><surname>D?sid?ri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comptes Rendus Mathematique</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bilevel programming for hyperparameter optimization and meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1568" to="1577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient knowledge distillation from an ensemble of teachers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fukuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3697" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Knowledge distillation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1789" to="1819" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic task prioritization for multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="282" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Federated learning with compression: Unified analysis and sharp guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Haddadpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdavi</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="2350" to="2358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances Neural Information Processing Systems (NeurIPS)</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<editor>Y. Bengio and Y. LeCun</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Knowledge adaptation for efficient semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="578" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A comprehensive overhaul of feature distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1921" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to prune filters in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="709" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Like what you like: Knowledge distill via neuron selectivity transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01219</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Speeding up convolutional neural networks with low rank expansions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>British Machine Vision Association</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multiobjective optimization approaches for bias mitigation in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kamani</surname></persName>
		</author>
		<idno>2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Haddadpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Forsati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04931</idno>
		<title level="m">Efficient fair principal component analysis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pareto efficient fairness in supervised learning: From extraction to tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Forsati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01634,2021.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to quantize deep neural networks: A competitive-collaborative approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 57th ACM/IEEE Design Automation Conference (DAC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Compression of deep convolutional neural networks for fast and low power mobile applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<editor>Y. Bengio and Y. LeCun</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="5454" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Um-adapt: Unsupervised multi-task adaptation using adversarial cross-task distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lakkakula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1436" to="1445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Knowledge distillation by on-the-fly native ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep geometric knowledge distillation with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bontonou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Hacene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8484" to="8488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic task weighting methods for multi-task networks in autonomous driving systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Leang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sistu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>B?rger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Ensemble distillation for robust model fusion in federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07242</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pareto multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-L</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Knowledge flow: Improve upon your teachers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">End-to-end multi-task learning with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1871" to="1880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unifying distillation and privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations (ICLR)</title>
		<editor>Y. Bengio and Y. LeCun</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph representation learning via multitask knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Graph Representation Learning Workshop</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-task learning with user preferences: Gradient descent with controlled ascent in pareto optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahapatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rajan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Knowledge distillation for incremental learning in semantic segmentation. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Michieli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zanuttigh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="page">103167</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miettinen</surname></persName>
		</author>
		<title level="m">Nonlinear multiobjective optimization</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Improved knowledge distillation via teacher assistant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Mirzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ghasemzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Feature-level ensemble knowledge distillation for aggregating knowledge from multiple networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A2dele: Adaptive and attentive depth distiller for efficient rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9060" to="9069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Model compression via distillation and quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Data distillation: Towards omni-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4119" to="4128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Montr?al</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">D</forename><surname>Montr?al</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>doi: 10.1007/ s11263-015-0816-y. 5</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multi-task learning as multiobjective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-L</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.02367</idno>
		<title level="m">Federated knowledge distillation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kirichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05945</idno>
		<title level="m">Does knowledge distillation really work</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multinet: Real-time joint semantic reasoning for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zoellner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1013" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Knowledge distillation and studentteacher learning for visual intelligence: A review and new outlooks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10959</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Efros. Dataset distillation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Distilling object detectors with fine-grained feature imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4933" to="4942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Tiny imagenet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning from multiple experts: Self-paced knowledge distillation for long-tailed classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="247" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Improving fast segmentation with teacher-student learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Knowledge distillation via softmax regression representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3903" to="3911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Wsod2: Learning bottom-up and top-down objectness distillation for weakly-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8292" to="8300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Better and faster: Knowledge transfer from multiple self-supervised learning tasks via graph distillation for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/158.2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI</meeting>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Improve object detection with featurebased knowledge distillation: Towards accurate and efficient detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Localization distillation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
