<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Webly Supervised Concept Expansion for General Purpose Vision Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amita</forename><surname>Kamath</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kolve</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Webly Supervised Concept Expansion for General Purpose Vision Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>General Purpose Vision systems; Webly supervised data</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>General Purpose Vision (GPV) systems are models that are designed to solve a wide array of visual tasks without requiring architectural changes. Today, GPVs primarily learn both skills and concepts from large fully supervised datasets. Scaling GPVs to tens of thousands of concepts by acquiring data to learn each concept for every skill quickly becomes prohibitive. This work presents an effective and inexpensive alternative: learn skills from supervised datasets, learn concepts from web image search, and leverage a key characteristic of GPVs: the ability to transfer visual knowledge across skills. We use a dataset of 1M+ images spanning 10k+ visual concepts to demonstrate webly-supervised concept expansion for two existing GPVs (GPV-1 and VL-T5) on 3 benchmarks: 5 Coco-based datasets (80 primary concepts), a newly curated series of 5 datasets based on the OpenImages and VisualGenome repositories (?500 concepts), and the Web-derived dataset (10k+ concepts). We also propose a new architecture, GPV-2 that supports a variety of tasks -from vision tasks like classification and localization to vision+language tasks like QA and captioning, to more niche ones like human-object interaction detection. GPV-2 benefits hugely from web data and outperforms GPV-1 and VL-T5 across these benchmarks. Our data, code, and web demo are available at https://prior.allenai.org/projects/gpv2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>General Purpose Vision systems (GPVs) <ref type="bibr">[25]</ref> are designed to support a wide range of tasks without requiring architectural changes. A task is the application of skills (e.g. localization, captioning) to concepts (e.g. monkey, brown, climbing) in order to map from the input (image, text) to a target output (text, boxes). Given the virtually unlimited number of fine-grained and topical concepts, it is not feasible to provide a GPV with annotations for all skills on all concepts, as even large pre-collected datasets cannot anticipate every need. In this work, we ask: Can a GPV leverage web image search and skill-concept transfer to massively Describe the image.</p><p>A little kid pitching a baseball at a game. <ref type="bibr">What</ref>   <ref type="figure">Fig. 1</ref>: Learning concepts from the web with GPV-2. We demonstrate weblysupervised concept expansion on two existing GPV architectures (GPV-1 and VL-T5) as well as our proposed GPV-2 architecture. In addition to outperforming previous architectures, GPV-2 expands the inputs to contain bounding boxes which enables support for niche tasks like Human-Object Interaction detection with multi-step inference without any architectural modifications.</p><p>and inexpensively expand its concept vocabulary across a variety of tasks? To answer this question, we present a large-scale webly supervised dataset for learning 10k+ concepts, a new benchmark for broader concept evaluation (?500) across 5 diverse tasks, and a new GPV architecture that improves cross-task concept transfer and outperforms existing GPVs across multiple benchmarks.</p><p>Image search engines provide remarkably good results for millions of queries by leveraging text on the accompanying web pages, visual features from images, and click data from millions of users querying and selecting relevant results each day. They often provide high-quality, decluttered, object-and action-centric images, which can be used to learn powerful visual representations for concepts. Importantly, searches scale easily and inexpensively to thousands of queries. Given the large cost of producing high-quality supervised datasets, scaling today's manually annotated datasets to support 10,000+ concepts is infeasible for many tasks. In contrast, using Bing search to create Web10k, a dataset with 1M+ images spanning 10k nouns, 300 verbs, and 150 adjectives with thousands of noun-verb and noun-adj combinations, cost us just over $150. Moreover, while existing data sources such as ImageNet-22k and YFCC100M are valuable resources, they are static snapshots of a diverse and ever-changing world. For example, these static datasets may not represent specialized categories of interest to a downstream application such as boysenberry and will definitely not contain latest concepts such as Pixel 6 or COVID-19 home test. On the other hand, modern web image search engines are designed to serve imagery on-demand and are uniquely positioned to act as a source of training data for novel and latest concepts. While search engine data provides strong supervision for classification, we demonstrate that current GPVs, GPV-1 [25] and VL-T5 <ref type="bibr" target="#b12">[14]</ref>, are able to learn concepts from web data and improve on other skills as well, such as image captioning. Importantly, we show that even models that already utilize largescale pretraining corpora such as Conceptual Captions continue to benefit from using search engine data and can be easily extended to support new concepts relevant in the present day that have little or no coverage in large static corpora.</p><p>We also propose GPV-2, a powerful GPV that can accept as input an image, a task description, and a bounding box (allowing the user to point at an object or region of interest), and output boxes and text for any bounding box or for the entire image. These diverse input and output modalities enable GPV-2 to support a large spectrum of skills ranging from vision skills like classification and localization, vision-language skills like VQA and captioning, to niche ones like classification in context and human-object interaction detection. An important design principle of GPV-2 is Language-Based Localization, whereby all tasks are based on scoring/ranking/generation using the same text decoder applied to one or more image regions. This ensures that all tasks share the same weights and representations, ranging from the input encoders all the way to the output decoders -resulting in more effective skill-concept transfer for learning from diverse tasks' datasets. We also propose a re-calibration mechanism to downweight scores of labels that are disproportionally represented in training, and demonstrate its effectiveness on out-of-domain test datasets for multiple tasks.</p><p>Benchmarking the diverse capabilities of large-vocabulary general purpose models is challenging. Most current datasets in computer vision are designed for single tasks. The recently proposed Coco-sce <ref type="bibr">[25]</ref> benchmark is designed to test the skill-concept transfer ability and overall skill competency across five vision skills. However, it is limited to evaluate these competencies on 80 primary Coco concepts. In this work, we present a new benchmark named DCE for broader concept evaluation for the same five skills but now expanding to 492 OpenImages concepts. DCE is an evaluation-only benchmark sourced from OpenImages [42], VisualGenome [40] and NoCAPS <ref type="bibr" target="#b0">[2]</ref> with new VQA annotations and has been sampled in a way that prevents over-representation of any single category while maximizing representation of infrequent categories.</p><p>We evaluate present day GPVs and GPV-2 on three benchmarks: (i) the Coco-sce and Coco benchmarks [25], (ii) the newly presented DCE benchmark; and (iii) the Web10k dataset consisting of manually verified images from Bing Image Search paired with questions and answers that covers 10,000+ concepts. Our analysis shows that all three GPVs benefit from web data. Furthermore, GPV-2 outperforms both GPV-1 and VL-T5 across these benchmarks and shows significantly large gains when using web data, particularly for captioning and classification. We also demonstrate how GPV-2 can be chained to perform niche tasks like human-object interaction detection, without any task-specific architecture modifications. Finally, we show how web data can be efficiently used to expand GPV-2's concept vocabulary to include new visual concepts that are relevant in today's world such as COVID-19 vaccination cards and N95 masks, concepts that are infrequent or non-existent in static corpora.</p><p>In summary, our main contributions include: (a) Web10k, a new web data source to learn over 10k visual concepts with an accompanying human-verified VQA benchmark; (b) demonstration that GPVs can learn concepts from Web10k and transfer this knowledge to other tasks; (c) DCE, a benchmark spanning 5 tasks and approximately 500 concepts to evaluate GPVs; and (d) GPV-2, an architecture that supports box and text modalities in both input and output, improves skill-concept transfer and outperforms existing GPVs. Our code and benchmarks are available at https://prior.allenai.org/projects/gpv2, along with a new tool to easily create a web dataset from a list of queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>General purpose models. Computer vision models have progressively become more general. Specialization first gave way to multitask models which aimed at solving multiple, albeit predefined, tasks with one architecture. A common approach for building such models [51,28] is to use task-specialized heads with a shared backbone. However, adding a new head for each new task makes scaling to a large number of tasks and reuse of previously learned skills challenging. An alternative approach is to build a general-purpose architecture without taskspecific components. This approach has become common in natural language processing via text-to-text generative models <ref type="bibr" target="#b24">[64,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr">55]</ref>, and recent work in computer vision has striven towards this kind of generality <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr">37,</ref><ref type="bibr">49]</ref>.</p><p>Examples of general-purpose computer vision models include VL-T5 <ref type="bibr" target="#b12">[14]</ref>, which adapts T5 <ref type="bibr" target="#b24">[64]</ref> to jointly train on vision+language (V+L) tasks while using a single text-generation head to produce outputs for all tasks, and GPV-1 [25], which combines a similar text-generation head with the ability to return bounding-boxes and relevance scores as output. In this work, we work with both GPV-1 and VL-T5 and extend their concept vocabulary with web data. Our proposed model, GPV-2 follows VL-T5 in its use of the T5 backbone, builds upon the vision capabilities of GPV-1, and further extends the range of tasks that can be performed by allowing a bounding-box input and introducing the ability to generate per-image-region text output. Perceiver <ref type="bibr">[31]</ref> and Perceive-rIO [30] aim to generalize the architecture beyond images and text to other modalities such as audio, video, and point cloud. However, both architectures remain to be tested for multitask learning and for learning V+L tasks such as VQA and captioning. Many other V+L models [83, <ref type="bibr" target="#b33">73,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr">47,</ref><ref type="bibr">52]</ref> can be fine-tuned on a variety of downstream tasks, but they typically use task-specific heads, while the focus of our work is on general purpose models in a multi-task setting. Web supervision. Image search engines provide highly relevant results, using a combination of text, image and user features. Researchers have used search data as a form of supervision to build computer vision models. Early works used noisy retrieved results with probabilistic Latent Semantic Analysis [20] and multiple instance learning [77] to build recognition systems. As web results improved, works used this data to build object detectors <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr">46,</ref><ref type="bibr" target="#b30">70,</ref><ref type="bibr">53,</ref><ref type="bibr">82]</ref>  <ref type="bibr" target="#b10">[12]</ref>. More recently, massive scale web data in the form of retrieved search results and the accompanying text was employed to build the powerful CLIP family of models <ref type="bibr" target="#b22">[62]</ref> that provide powerful visual representations for downstream tasks. While these works have shown that web data can be used to build single task models, we show that one can build GPVs with web data and importantly transfer this knowledge across skills. Concept transfer across skills. There has been considerable interest in transferring concept knowledge from classification to object detection, as classification labels are far cheaper to obtain than detection labels.  <ref type="bibr" target="#b23">[63]</ref> which are built from the same), obtained in large quantities on the web. Even though image-captions are noisy, the resulting detectors improve as the data is scaled up.</p><p>The V+L field has leveraged object detectors as feature inputs <ref type="bibr" target="#b1">[3,</ref><ref type="bibr">89,</ref><ref type="bibr" target="#b0">2]</ref>, which can be considered as transferring concepts from detection to downstream tasks. Another effective approach is pre-training using image-captions [50,45,47] like Conceptual Captions <ref type="bibr" target="#b27">[67]</ref>. CLIP <ref type="bibr" target="#b23">[63]</ref> is a family of powerful models that are pretrained on a massive 400M image caption paired dataset. The resulting encoders are very effective at V+L tasks <ref type="bibr" target="#b28">[68]</ref>. These methods effectively transfer visual knowledge from caption data to tasks like VQA. Recently Whitehead et al . <ref type="bibr">[81]</ref> disentangle the encoding of concepts and skills and build a model that can generalize to new skill-concept compositions and new concepts for VQA.</p><p>The focus of our work is to build a GPV that can transfer concepts across various skills, particularly from web data to vision and vision-and-language skills, and also provide a new test-only evaluation benchmark for the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The WEB10K dataset</head><p>Search engines can be leveraged to collect datasets with highly desirable characteristics: (1) Diversity -Search engines benefit from a large volume of user click data to produce high-quality results for a large vocabulary of concepts including tail concepts not frequently mentioned in annotated computer vision datasets (e.g. hyacinth); (2) Freshness -Search engines are designed to serve the freshest content on the internet, and often produce very good results for the latest queries (that may not have existed or been popular before; e.g. COVID-19 vaccination card, 2022 winter olympics mascot) which have few/no occurences in standard vision datasets that tend to be static; and (3) Concept focus -The image distribution of search engine results tends to be similar to image classification data with the image centered on the queried object with few distractions, making them ideal for learning visual concept representations.</p><p>We present Web10k, a dataset sourced from web image search data with over 10K concepts. Web10k contains queries with nouns, adjectives and verbs. Nouns. We consider single and multi-word nouns. Single-word nouns are sourced from a language corpus with a list of 40,000 concrete words <ref type="bibr" target="#b4">[6]</ref>, each with a concreteness score (defined as the degree to which a word refers to a perceptible entity). From this list, we select nouns with a concreteness score &gt; 4.0/5 and any verb or adjective with an alternate word sense as a noun (e.g. "comb") with a score &gt; 4.5/5. These thresholds avoid more abstract or non-visual words such as "humor". Multi-word nouns are sourced from Conceptual Captions (CC) <ref type="bibr" target="#b27">[67]</ref>. We identify candidates using POS tagging and select the most frequent 2,000, and an additional 282 where the second word of the multi-word noun is present in the concreteness dataset (e.g. "street artist", where "artist" is in concrete nouns). In total, we select 10,211 nouns. Sourcing nouns from a language corpus enables coverage of concepts not commonly covered in vision datasets: over 4,000 nouns in Web10k are not present in CC, e.g. "wind tunnel". Verbs. We source verbs from a combination of vision datasets with large verb vocabularies including imSitu [85], HICO <ref type="bibr" target="#b7">[9]</ref> and VRD <ref type="bibr">[48]</ref>. We remove verbs that are either polysemous (have multiple meanings e.g. "person holding breath" vs. "person holding cup") or aren't associated with an animate agent (e.g. "snowing"). This results in 298 unambiguous and visually recognizable verbs. These verbs improve model performance on action recognition (Supplementary Sec. 8).</p><p>Adjectives. We source adjectives from several datasets that have a large number of adjectives <ref type="bibr" target="#b27">[67,</ref><ref type="bibr">40,</ref><ref type="bibr">19,</ref><ref type="bibr">43,</ref><ref type="bibr">41,</ref><ref type="bibr" target="#b20">60,</ref><ref type="bibr" target="#b19">59,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr">79]</ref>. We manually filter out ones that are subjective ("beautiful"), non-visual ("loud"), or relative ("big"). This results in 144 adjectives which we group into 16 adjective types (e.g. "color", "texture"). We select noun-adj pairs and noun-verb pairs which appear at least thrice in CC: this removes nonsensical pairs, e.g. "cloudy dog". The total number of queries in Web10k is 38,072 with roughly 10k nouns, 18k noun-adj and 9k nounverb combinations. We feed each query into the Bing Search API and retrieve a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GPV-2</head><p>In this section we present GPV-2, a model combining an object detector with the T5 pre-trained language model. GPV-2 supports additional input and output modalities (and thus tasks) beyond present day GPVs (GPV-1 and VL-T5). It uses the stronger VinVL [89] object detector, uses a shared language decoder (for all tasks including localization) and employs a classification re-calibration approach, which together improve generalization to unseen concepts at test time. Model design. GPV-2 takes an image, text, and a bounding box as input. As output, it can produce text for an individual bounding box (the input box, or boxes produced by the visual model) and for the entire image (see <ref type="figure" target="#fig_2">Fig. 3</ref>). First, the input text is tokenized and embedded using T5-Base to get a sequence of text feature vectors. Then an object detection model is used to identify regions in the image and extract bounding boxes and features for those   regions (we do not use the class labels identified by the detector) via RoI pooling. We additionally use the object detector to extract features for the input bounding box, and a learned embedding is added to those features to distinguish them from the other visual features. These sets of visual features are then converted to embeddings of the same dimensionality as the text embedding using a linear layer. We primarily use the VinVL [89] object detector for our experiments. However the GPV-2 architecture allows us to easily swap in other detectors, and we use features from DETR <ref type="bibr" target="#b5">[7]</ref> for some of our experiments in Sec. 6. The resulting visual and text vectors are concatenated as a sequence and used as an input to the T5-Encoder to build joint contextualized embeddings. To generate text for the entire image we use the T5-Decoder with this contextualized embedding sequence as input, and to generate text for individual boxes we run the same T5-Decoder while using the contextualized embedding that corresponds to just that box as input. The usage of a common decoder for image-based outputs and region-based outputs enables transfer of learned concepts between skills that require processing the entire image and skills that rely primarily on the representation of a single region. Using GPV-2. GPV-2's design gives us flexibility to handle a variety of vision and vision+language tasks without needing task-specific heads. For tasks that do not have text input, we follow [25] by building appropriate text prompts for that task (e.g., "What is this object?" for classification) and selecting one at random to use as the input text. For tasks that do not have an input bounding box, we use a box around the entire image.</p><p>Decoded text from the image is used to answer questions and generate captions. For classification or limited-choice responses, answers are scored based on log-probability of generating each option, and the highest scoring answer is chosen. To localize objects, we propose Language-Based Localization (LBL) where a box is scored by first computing the log-probabilities of generating an object class or "other" from that box, and then applying a linear classifier to those scores to yield a scalar relevance score. For example, "Localize dog" is performed by computing the log-probability of "dog" and "other" for each region. Importantly, the same text decoder is used to generate image and region text, thus classification, question answering, captioning, localization, and all other tasks use the same encoders, decoder, and weights. Our experiments show that this facilitates skill-concept transfer.</p><p>Even complex tasks like human-object interaction (HOI) can be performed by chaining inference steps ( <ref type="figure">Fig. 1</ref>). HOI <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b6">8]</ref> requires localizing a person, an object and categorizing their interaction. GPV-2 performs this by first returning detections for "Locate person", then providing each person box as input with the prompt "What is this person doing?" The log-probs of generating objectinteraction phrases, such as "directing the airplane" for other boxes are used to identify the most likely interaction. Classification re-calibration. We observe that a common issue in classification is that the model becomes biased towards classes that are common in the training data. For example, we find that if the model is trained to classify Coco objects it will almost always guess the names of Coco objects in response to the prompt "What is this object?", even if no such objects exist in the image. This can be viewed as a language bias, as has been well-studied in VQA <ref type="bibr">[22,</ref><ref type="bibr" target="#b25">65]</ref>. To solve this issue we re-calibrate the models output prediction by reducing the log-probability of classes that were seen in the training data when doing answer re-ranking. The down-weighting amount is selected on the validation data. See Supplementary Sec. 2 for an analysis and example. Pre-training. Recent works have shown that pre-training V+L models on large amounts of data results in large improvements <ref type="bibr" target="#b12">[14,</ref><ref type="bibr">47,</ref><ref type="bibr">89]</ref>. We do not have the resources to fully-replicate these setups, but as a partial measure we pre-train GPV-2 for 8 epochs on the CC3M dataset <ref type="bibr" target="#b27">[67]</ref>, which shows significant gains on our benchmarks. Since GPV-2 is generative, we pre-train it by simply learning to generate the target caption rather than using span masking or other more complex objectives <ref type="bibr">[47,</ref><ref type="bibr" target="#b33">73]</ref>. While we use much less data than some V+L works, pre-training on CC3M allows us to verify that GPV-2 still benefits from web data even when exposed to a broad range of concepts during pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DCE Benchmark</head><p>The Coco benchmark focuses on 80 object categories and is insufficient for evaluating skills on a wide range of concepts. We introduce the Diverse Concept Evaluation (DCE) benchmark to evaluate GPV models on a large subset of the 600 OpenImages categories across 5 skills: classification (Cls), classification-incontext (CiC), captioning (Cap), localization (Loc), and visual question answering (VQA). See <ref type="figure" target="#fig_2">Fig. 3</ref> for the inputs and outputs for each task. We introduce CiC as a natural and unambiguous object classification task (similar to pointing at an object and asking what it is), providing a direct complement to localization. We source Cls, CiC and Loc samples from OpenImages, VQA samples from VisualGenome (VG), and use the nocaps <ref type="bibr" target="#b0">[2]</ref> benchmark for Cap evaluation. To curate the DCE benchmark, we first select a set of mutually exclusive categories from OpenImages and draw samples for each of those categories according to a sampling strategy that prevents over-representation of any category while maximizing representation of tail categories. DCE is an evaluation-only benchmark and is not accompanied by a distributionally similar training set. Category selection. OpenImages provides a total of 600 hierarchical object categories. After removing some categories due to label noise, we use the remaining 492 leaf nodes in the hierarchy as our mutually exclusive set of categories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We train models jointly on all tasks that are supported by each GPV using Coco-based datasets. In addition, each model is also trained with and without training data from Web10k. We evaluate these models on in-domain test sets for each task as well as on the Web10k and DCE test sets. We now summarize the tasks and training details. See <ref type="figure" target="#fig_2">Figure 3</ref> for the inputs/outputs for each task and Supplementary Sec. <ref type="bibr" target="#b4">6</ref>   Localization: Localization training data is built from bounding box annotations in Coco images following <ref type="bibr">[25]</ref>. We report mAP on the Coco val set (since the test servers do not support this task) and the DCE test set. VL-T5 does not support this task out-of-the-box since it does not have a means to rank its input boxes, so we do not train or evaluate it for this task. Classification: We use the classification data from [25] and report accuracy on the Coco val set and the DCE test set. Since DCE is out-of-domain we apply the re-calibration method from Sec. 4 for GPV-2. Classification-in-Context: The same as classification, except instead of cropping images the bounding box of the target object is used as an input box. Having an input box means only GPV-2 supports this task. Training details. We train GPV-2 and VL-T5 for 8 epochs with a batch size of 60 and learning rate of 3e-4 that linearly warms up from 0 for 10% of the training steps then decays to 0. We stratify the data so examples from each source are proportionally represented in each batch. Since the web data is large, we shard the data into 4 parts and use 1 shard each epoch, resulting in about a third of the data in each epoch being web data. VL-T5 is initialized with a pre-trained checkpoint <ref type="bibr" target="#b12">[14]</ref> and GPV-2 is initialized from our checkpoint after CC pre-training. We train GPV-1 to 40 epochs following [25] 3 .</p><p>Concept expansion using web data. <ref type="table" target="#tab_5">Table 2</ref> shows the performance of models when trained with and without Web10k. On DCE, which contains a more diverse set of concepts than Coco, we find that all models benefit from web data and perform better on captioning and the two classification tasks (with large gains of +7.1, +9.1, +8.6 for GPV-2). We see modest gains of +1.0 for DCE localization. VQA shows small gains, presumably because many frequent answers such as colors or numbers are common between Coco and DCE, and adding web supervision brings little benefits for such questions. Training with web data makes little difference on Coco and, unsurprisingly, leads to large gains on Web10k test, where models achieve over 40% accuracy on nouns and 60% on verbs despite the large number of concepts. Overall, these results show multi-tasking GPVs with web data improves performance significantly on concepts unseen in supervised data without compromising in-domain performance.</p><p>Of the three GPVs we test, we find GPV-2 to be the most effective across all three benchmarks. GPV-2 uses less pre-training data and a simpler and cheaper  pre-training strategy than VL-T5. However, it uses more powerful VinVL [89] features and benefits from classifier re-calibration (See Tab. 4). In contrast to VL-T5, GPV-2 can also perform CiC and localization. In contrast to GPV-1, GPV-2 has more shared features and a better pre-trained language model, which help produce large gains across the benchmarks. It also trains much faster than GPV-1 as it can use pre-computed detection features (1 day on 2 GPUs vs. &gt;3 weeks on 4 GPUs). See Supplementary Secs. 10 and 5 for more comparisons and GPV-2 efficiency metrics respectively. GPV-2 also achieves state-of-the-art on the GRIT benchmark [26] at the time of submission (Supplementary Sec. 9).</p><p>Closed world evaluation of web data. <ref type="table" target="#tab_7">Table 3</ref> shows results for GPV-2 when it is trained on the Coco-sce [25] dataset, a dataset that holds out different concepts from each Coco training supervised dataset (e.g., captions that mention the word "bed" are held out from the caption training data), and then evaluates whether models can still perform well on those unseen concepts by learning about them from the data in other tasks (e.g., captions with the word "bed" are in the captioning test set, and classification and localization training still include examples about beds). When GPV-2 is trained on Coco-sce we make two notable changes: (1) We replace VinVL features with DETR <ref type="bibr" target="#b5">[7]</ref> features trained only on the Coco-sce training categories (this avoids leaking detection information by VinVL's broad category set); and <ref type="formula">(2)</ref> We do not pre-train with CC (this avoids leaking caption information from CC's broad vocabulary). These choices severely reduce the performance of the model, but this setup serves as a closed world evaluation to determine if GPV-2 can learn skills from Coco-sce and concepts from Web10k. As seen in <ref type="table" target="#tab_7">Table 3</ref>, training with web data shows large gains across the board in this controlled experiment.</p><p>In fact, we now also see gains in the unseen categories within Coco-sce. Ablation analysis. We perform ablation studies on GPV-2. out CC pre-training, and that removing both reduces performance dramatically (&gt;30 points for captioning). This shows that the two approaches are independently effective and complementary at helping models handle new concepts. This is also true to a more modest extent for localization. Using the web data for a second round of pre-training performed better than not using it, but was significantly worse than our multi-tasking framework. Re-calibration is critical for classification, providing a gain of up to 12 points, confirming that models tend to be overly influenced by the concept distribution observed during training. Performance on Coco remains largely unchanged, which is expected as our design choices target performance on unseen concepts. Finally, VinVL significantly out-performs DETR, as expected given its much more extensive training regime.</p><p>Human object interaction. To demonstrate the flexibility of GPV-2, we also employ it for human-object interaction detection <ref type="bibr" target="#b6">[8]</ref> using the two-stage procedure described in Sec. 4. We fine-tune GPV-2 on the HICO-Det train set for 4 epochs (see Supplementary Sec. 7 for details). GPV-2 gets an AP of 20. <ref type="bibr" target="#b4">6</ref>  , they require highly specialized architectures requiring up to 5 output heads (e.g. for decoding human+object boxes, interaction score, and object and interaction categories), well crafted losses (e.g. Hungarian HOI instance matching objectives), and custom post-processing steps (e.g pairwise non-maximum suppression). GPV-2's flexibility allows us to get reasonable results by side-stepping complex model design with simple chained inference.</p><p>Qualitative results from DCE <ref type="figure">(Figure 4</ref>). Training on Web10k helps GPV-2 understand rare concepts like 'sari' or 'gondola', which it is able to use across diverse skills. See Supplementary Sec. 1 for more examples.</p><p>VQA  Novel concepts case study. A unique advantage of using web-search is the ability to easily and cheaply access new visual concepts that are too specialized or too recent to appear in statically-collected corpora. To demonstrate this we present qualitative results on an experiment to train GPV-2 to learn a number of COVID-19 related concepts. We collect 43 terms related to COVID-19 (e.g., N95 mask, face shield, etc.) and gather a 1000-image train set with a 100-image val set using the same automatic pipeline we used to gather Web10k. We fine-tune GPV-2 (after it has been trained on Coco and Web10k) on these examples mixed with a sample of 2000 examples from each Coco train set for 3 epochs. After fine-tuning, the model achieves 71% accuracy on the new val set compared to only 4% without fine-tuning (performance is initially low since these concepts are too specialized and new to appear in CC, Coco or Web10k). See some qualitative results in <ref type="figure" target="#fig_4">Figure 5</ref> that show that GPV-2 is able to use such recently-introduced concepts when applying multiple skills. Although this is a small-scale qualitative study, it shows that our approach of combining a GPV and web-search data can lead to models that not only understand a wide range of concepts and skills, but can also be efficiently adapted to new visual concepts that become common in the world or that are needed due to the specialized needs of a user. We think this is an exciting avenue for future work in GPVs.</p><note type="other">Captioning Localization Classification (cropped) Classification in Context Find balance beam</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Extensions. GPV-2 achieves transfer of concepts from web data to skills, but our results indicate that more work is needed, particularly for tasks like VQA or localization, through new architectures or training protocols. GPV-2 supports many tasks, but could be extended to handle more modalities (e.g., video) and outputs (e.g., segmentation). Recent work shows promise in this regard [30], potentially enabling transfer of web concepts to a wider range of tasks. Conclusion. As the vision community builds progressively more general models, identifying efficient ways of learning a large variety of skills and concepts is of prime importance. Our work revisits the idea of webly-supervised learning in the context of GPVs and shows that learning skills from task-datasets and concepts from the web is an efficient and inexpensive option for concept expansion. Acknowledgements. This work is partially supported by ONR award N00014-21-1-2705. Li, X., Yin, X., Li, C., Hu, X., Zhang, P., Zhang, L., Wang, L., Hu, H., <ref type="bibr">Dong,</ref> L., Wei, F., Choi, Y., Gao, J.: Oscar: Object-semantics aligned pre-training for vision-language tasks. In: ECCV (2020) 4, 5, 9 48. Liang, K., Guo, Y., Chang, H., Chen, X.: Visual relationship detection with deep structural ranking. In: AAAI (2018) 6 49. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S.C.F., Guo, B.:</p><p>Swin transformer: Hierarchical vision transformer using shifted windows. ICCV abs/2103.14030 (2021) 4 50. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In: NeurIPS (2019) 5 51. Lu, J., Goswami, V., Rohrbach, M., Parikh, D., Lee, S.: 12-in-1: Multi-task vision and language representation learning. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp. 10434-10443 (2020) 4 52. Lu, J., Goswami, V., Rohrbach, M., Parikh, D., Lee, S.: 12-in-1: Multi-task vision and language representation learning. In: CVPR (2020) 4 53. Luo, A., Li, X., Yang, F., Jiao, Z., Cheng, H. Qualitative results from GPV-2 are shown in <ref type="figure">Figure 1</ref>. Despite the presence of concepts that are not annotated in Coco (e.g, "Caterpillar", "Lifejackets", "Willow") GPV-2 is able to successfully perform classification, localization, captioning, and visual questioning answering. Visualizations of predictions from GPV-2 on randomly selected examples from the Coco, DCE, and Web10k datasets can be found in additional files in the supplementary materials. <ref type="figure" target="#fig_0">Figure 2</ref> contains an expanded version of <ref type="figure">Figure 4</ref> from the paper showing the predictions of GPV-2 when trained with and without Web10k. The model trained without web data generates Coco concepts even when they are not present in the image (e.g., writing a caption about a giraffe for a picture of a jaguar, a brown-and-white bear for a red panda, or classifying a monkey as a bear), while the model trained on web data is able to name the new concepts correctly. For localization, we observe cases where the model trained without Web10k struggles on new concepts (e.g., the without web model focuses on cups or the background for the class "coffemaker") while the model trained with Web10k can localize them accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What is happening?</head><p>A man standing next to a row of motorcycles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Describe this image.</head><p>A bunch of white plums hanging from a tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What is happening?</head><p>Three people pose in front of a statue</p><p>What is happening?</p><p>A couple of young girls riding roller skates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Caption this image.</head><p>A green caterpillar sitting on top of a green leaf.    In this section, we analyze the classification re-calibration method from Sec. 4. <ref type="table" target="#tab_3">Table 1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VQA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">WEB10K questions and statistics</head><p>In this section, we provide more detail about how we construct question-answer pairs from the web search data. For each query-image pair, we construct a question that is answered by the noun from the query. For example, the question "What entity is this?" with the answer "dog" for the query "brown dog". For queries that contain a verb, we construct two additional questions that are answered by the verb, one that specifies the noun and one that does not. For example, "What action is happening?", and "What is the dog doing?" with the answer "running", for the query "dog running". For queries that contain adjectives, we similarly construct two questions that are answered by the adjective, one that specifies the noun and one that does not. To do this, we manually map the adjectives to adjective types (e.g., "color" for "red") and specify the adjective type in the question. For example, "What is the color of this object?" and "What is the color of this dog?" with the answer "brown", for the query "brown dog". Using adjective types is important to because generic questions like "What attributes does this object have?" will have many possible correct answers. Finally, for all query-image pairs, we additionally construct a query whose answer is the entire query. During evaluation, we compute the average accuracy on questions where the is answer is a noun, verb or adjective, and report the macro-average of those results to get an overall accuracy number. The questions themselves are generated by a templating system to increase their linguistic diversity. <ref type="table" target="#tab_5">Table 2</ref> shows the templates we use. For a given query and question type we use these templates to generate a large number of possible questions, and then select one at random to use as a prompt for the model.</p><p>Additional question types are possible. For example, contrastive questions like "Is this sloth swimming or climbing?", or questions that specify hypernyms of the answer (obtained from sources such as WordNet) like "What kind of reptile is this?". We leave the generation of such questions, as well as their impact on knowledge transfer of concepts between skills, to future work. <ref type="figure">Fig. 4</ref> shows the number of categories with various frequencies of occurrence in the DCE val and test sets. Since nocaps <ref type="bibr" target="#b0">[2]</ref> annotations are hidden behind an evaluation server, we are unable to provide category counts for captioning. Note that VQA has fewer concepts for higher frequencies than localization and captioning because of a lack of a sufficient number of question-answer annotations that mention many of the OpenImages categories selected for DCE. VQA sampling strategy. Co-occurrence of concepts in questions and answers makes the sampling strategy for VQA more nuanced than the one followed for Cls, CiC, and Loc. We iterate over the categories selected for DCE and randomly sample up to 50 samples for each category. Unlike Cls/CiC and Loc, each sample in VQA may consist of multiple categories. If k samples have already been sampled for the i th category in the selected category list due to co-occurrence with previous i ? 1 categories, we only sample max(0, 50 ? k) samples for the i th category. This allows the "tail" categories from the original dataset to be maximally sampled, while "head" categories are skipped if already sufficiently represented in the annotations sampled thus far. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DCE sampling details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GPV-2 efficiency metrics</head><p>We report efficiency metrics on GPV-2 when features must be extracted from the input image from scratch using VinVL, and for when those features are assumed to have been precomputed. We report parameter count and the number of floating-point operations (FLOPs). Since the number of FLOPs depends on the length of the input, the length of the target text, and the number of regions in the image, we report the average number of FLOPs needed to process a single example on 100 random examples from the training sets for each task. We compute FLOPs using a pytorch profiler 3 while computing the loss with a single forward pass of the model. Results are shown in <ref type="table" target="#tab_7">Table 3</ref>. We find captioning is slow due to the long output sequences, classification is fast because the output text is short and there tends to be fewer objects in the cropped classification images, and detection requires generating per-box outputs so it requires the most compute. If computing the features from scratch, the computational cost is dominated by VinVL, which requires running a X152-FPN backbone and computing features for a large number of proposal regions [89].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Details</head><p>Here we give a more detailed account of how the models are trained. We train GPV-2 and VL-T5 using the Adam optimizer [38] with a batch size of 60 and learning rate of 3e-4, ? 1 of 0.9, ? 1 of 0.999, of 1e-8, and a weight decay of 1e-4. The learning rate linearly warms up from 0 over 10% the training steps and then linearly decreases back to 0. The web data is sharded into 4 parts, and a different part of used for each epoch for the first four epochs. Then the data is re-sharded into 4 new parts for the final 4 epochs. The data is stratified so that the 6 supervised datasets (VQA, Cap, Loc, CLS, CiC and the current web shard) are represented in approximately the same proportion in each batch. During training, we use the cross-entropy loss of generating the output text for all tasks besides localization. For localization, we compute relevance scores for each box following the process in Sec. 4 and then train using the Hungarianmatching loss from DETR <ref type="bibr" target="#b5">[7]</ref> with two classes (one class for relevant and one for irrelevant) following <ref type="bibr">[25]</ref>. We compute the scores on the in-domain validation sets each epoch, and use the checkpoint with the highest average score across all validation tasks. We experimented with using different learning rates for VL-T5 but found it had little impact on performance, so used the same learning rates for both models. We use the prompts created by <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Human Object Interaction experimental details</head><p>In this section, we provide more details about how GPV-2 is trained to perform human-object interaction. Both stages of the two-pass process from Sec. 4 are trained using the HOI-Det training set <ref type="bibr" target="#b6">[8]</ref>. The first pass requires the model to locate person bounding boxes in the image, GPV-2 is trained to do this by using localization examples constructed from the HOI annotations. In particular, we build examples by gathering all person-boxes in the annotations for an image and then pruning duplicate boxes by applying non-maximum suppression with a threshold of 0.7. The remaining boxes serve as ground truth for localization examples with the prompt "Locate the people". The second pass requires the model to identify object interactions given a person box. GPV-2 is trained using the same de-duplicated person boxes from the HOI annotations. For each such person box, the input to the model is the image with the prompt "What is this person doing?" and the input query box set to be the person box. Target outputs are built by gathering all HOI annotations for that input person box (annotations with person boxes that were pruned during de-duplication are mapped to the person box with the highest IoU overlap). This results in a set of object boxes labeled with HOI classes for each person box. Those object boxes are aligned with the boxes found by the object detector by finding the box with the highest IoU overlap with each ground truth object box. During training, if no box from the object detector has at least a 0.5 overlap with an object box, we manually add that object box to the regions extracted by the detector so we can still train on it. The model is trained to generate a text description of the HOI class for each box that was aligned with a ground truth box (e.g., "riding the horse" for the HOI class riding+horse), or the text "no interaction" for any box that was not aligned with a ground truth object. In practice, we only train on a randomly selected half of the "no interaction" boxes to reduce computational expense. If an object box is aligned to multiple ground truth boxes, and therefore has multiple HOI class labels, we train the model to generate all such labels with a high probability.</p><p>We train the model with the hyper-parameters specified in Sec. 6, but for 4 epochs with a batch of 48 and a learning rate of 1e-4. Since this task is intended as a demonstration, we did not spend a lot of time optimizing this process and think it could be further improved with additional effort. To evaluate the model, we first find boxes the model identifies from the prompt "Locate the people" with a score of over 0.5. Then for each such box, for each object box detected by the object detector, and for each HOI class, we score the box pair and class with the log-probability of generating the class label text from the object box when the person box is used as the input query box. In practice, for a given person box, we prune object boxes that generate the text "no interaction" with a high probability so we do not have to score a generation for every class label with that box-pair. These scores are finally used to compute the average precision metric from <ref type="bibr" target="#b6">[8]</ref>.</p><p>Finding HOIs for an image requires one forward pass with the encoder for each person box, then one forward pass with the decoder for each person box/object box pair to compute the "no interaction" probability, and then another forward pass with the decoder for each person box, non-pruned object box, and class label to get the class scores. This is made affordable by the fact the class labels are short, and we are able to label the 10k test set in about an hour using a single Quadro RTX 8000 GPU (after the VinVL image features have been precomputed). In addition to nouns, Web10k consists of compositions of nouns with verbs and adjectives. To test the learning of verbs and attributes from Web10k, we evaluate GPV-2 zero-shot on an action recognition dataset (ImSitu actions [86]) and an attribute recognition dataset (VAW <ref type="bibr" target="#b21">[61]</ref>), see <ref type="table" target="#tab_8">Table 4</ref>. For ImSitu actions we prompt the model with "What are they doing?". GPV-2 gets 34.7 top-5 accuracy compared to 58.6 from the benchmark authors [86] employing a supervised CNN+CRF approach and 68.6 from a recent supervised model <ref type="bibr" target="#b31">[71]</ref> that uses a specialized mixture-kernel attention graph neural network. For verbs present in Web10k (the Seen column), Web10k training provides a significant boost (54.4 from 33.4) showing successful transfer from web images to ImSitu images. For VAW, we prompt the model with yes/no questions (e.g., "Is this object pink?") along with the target object's bounding box to get per-box multilabel attribute results. We see no gains on VAW from Web10k, likely because the model already learns these attributes from VinVL, CC, VQA, and Captioning training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Zero-shot verb and attribute recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Performance on the GRIT benchmark</head><p>We submit GPV-2 to the Unrestricted track of the GRIT benchmark [26] and achieve state-of-the-art performance at the time of submission. We re-train GPV-2 to include RefCOCO+ <ref type="bibr">[35]</ref> in the multi-tasking framework in order to compete on the Referring Expressions Grounding task of the benchmark. See <ref type="table" target="#tab_19">Table 5</ref> for performance results of the model on the test set. The results use the acc.any.agg.&lt;task&gt; metric, which averages performance of the model on "same" and "new" source data for each task, as defined in <ref type="bibr">[26]</ref>. Note that GPV-2 is trained on more data than GPV-1, and the VinVL backbone used in GPV-2 is trained on OpenImages, which belongs to the GRIT "new" data source (as allowed by the Unrestricted track), contributing to its performance.</p><p>The GRIT benchmark website 4 contains additional information on the data and the models' ability to generalize to new data sources and concepts, robustness to image distortions, and calibration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Comparison between the GPV-2 and GPV-1 architectures when trained on the same data</head><p>We now provide an additional comparison between GPV-2 and GPV-1 in <ref type="table">Table  6</ref> using the same training data and detector backbone (frozen DETR), trained only on COCO-SCE. This shows that GPV-2 provides gains over GPV-1 on player" are of men. Coco, our main source of supervision, also suffers from these kinds of biases [90] so we do not recommend using the models in this paper in production settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Concept diversity in WEB10K. Left: Besides 10k nouns, Web10k provides dense coverage of feasible adj-noun and verb-noun combinations to enable learning of fine-grained differences in object appearance due to attributes. Right: TSNE [54] plot of Phrase-BERT [78] embeddings of Web10k nouns with bubble size indicating frequency (capped at 1000) in CC, a common large-scale pretraining dataset. Web10k nouns cover a wide range of concept groups identified using WordNet and include many concepts which are infrequent/absent in CC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Left: GPV-2 architecture. Right: I/O for 5 skills in Coco and DCE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>8 -41.7 -15.0 -17.4 --13.8 -13.3 -16.4 -11.7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative results on novel concepts: The predictions of GPV-2 after fine-tuning on COVID-related web data. The model can recognize the new concepts in new images across all skills after training on only ?20 images per concept.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>18. Fang, H., Xie, Y., Shao, D., Lu, C.: Dirv: Dense interaction region voting for endto-end human-object interaction detection. In: AAAI (2021) 13 19. Farhadi, A., Endres, I., Hoiem, D., Forsyth, D.A.: Describing objects by their attributes. 2009 IEEE Conference on Computer Vision and Pattern Recognition pp. 1778-1785 (2009) 6 20. Fergus, R., Fei-Fei, L., Perona, P., Zisserman, A.: Learning object categories from google's image search. Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1 2, 1816-1823 Vol. 2 (2005) 4 21. Golge, E., Sahin, P.D.: Conceptmap: Mining noisy web data for concept learning.In: ECCV (2014) 4 22. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In: CVPR (2017) 9, 10 23. Gu, X., Lin, T.Y., Kuo, W., Cui, Y.: Open-vocabulary object detection via vision and language knowledge distillation (2021) 5 24. Guo, S., Huang, W., Zhang, H., Zhuang, C., Dong, D., Scott, M.R., Huang, D.:Curriculumnet: Weakly supervised learning from large-scale web images. ArXiv abs/1808.01097 (2018) 4 25. Gupta, T., Kamath, A., Kembhavi, A., Hoiem, D.: Towards general purpose vision systems: An end-to-end task-agnostic vision-language architecture. In: CVPR (2022) 1, 2, 3, 4,8, 11, 12, 9  26. Gupta, T., Marten, R., Kembhavi, A., Hoiem, D.: Grit: General robust image task benchmark. arXiv preprint arXiv:2204.13653(2022)12, 11 27. Gupta, T., Schwing, A.G., Hoiem, D.: No-frills human-object interaction detection: Factorization, layout encodings, and training techniques. 2019 IEEE/CVF International Conference on Computer Vision (ICCV) pp. 9676-9684 (2019) 13 28. He, K., Gkioxari, G., Dollar, P., Girshick, R.: Mask R-CNN. In: ICCV (2017) 4 29. Hoffman, J., Guadarrama, S., Tzeng, E., Hu, R., Donahue, J., Girshick, R.B., Darrell, T., Saenko, K.: Lsda: Large scale detection through adaptation. In: NIPS (2014) 5 30. Jaegle, A., Borgeaud, S., Alayrac, J.B., Doersch, C., Ionescu, C., Ding, D., Koppula, S., Brock, A., Shelhamer, E., H'enaff, O.J., Botvinick, M.M., Zisserman, A., Vinyals, O., Carreira, J.: Perceiver io: A general architecture for structured inputs &amp; outputs. ArXiv abs/2107.14795 (2021) 4, 14 31. Jaegle, A., Gimeno, F., Brock, A., Zisserman, A., Vinyals, O., Carreira, J.: Perceiver: General perception with iterative attention. In: ICML (2021) 4 32. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q.V., Sung, Y.H., Li, Z., Duerig, T.: Scaling up visual and vision-language representation learning with noisy text supervision. In: ICML (2021) 5 33. Jin, B., Segovia, M.V.O., S?sstrunk, S.: Webly supervised semantic segmentation. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 1705-1714 (2017) 4 34. Kay, M., Matuszek, C., Munson, S.A.: Unequal representation and gender stereotypes in image search results for occupations. In: Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. pp. 3819-3828 (2015) 12 35. Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T.: ReferItGame: Referring to objects in photographs of natural scenes. In: EMNLP (2014) 11 36. Kim, B., Lee, J., Kang, J., Kim, E.S., Kim, H.J.: Hotr: End-to-end human-object interaction detection with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2021) 13 37. Kim, W., Son, B., Kim, I.: Vilt: Vision-and-language transformer without convolution or region supervision. ArXiv abs/2102.03334 (2021) 4 38. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014) 8 39. Krause, J., Sapp, B., Howard, A.G., Zhou, H., Toshev, A., Duerig, T., Philbin, J., Fei-Fei, L.: The unreasonable effectiveness of noisy data for fine-grained recognition. ArXiv abs/1511.06789 (2016) 4 40. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., Bernstein, M., Fei-Fei, L.: Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV 123, 32-73 (2017) 3, 6 41. Kumar, N., Berg, A.C., Belhumeur, P.N., Nayar, S.K.: Attribute and simile classifiers for face verification. 2009 IEEE 12th International Conference on Computer Vision pp. 365-372 (2009) 6 42. Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J.R.R., Krasin, I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M., Duerig, T., Ferrari, V.: The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale. arXiv:1811.00982 (2018) 3 43. Lampert, C.H., Nickisch, H., Harmeling, S.: Learning to detect unseen object classes by between-class attribute transfer. 2009 IEEE Conference on Computer Vision and Pattern Recognition pp. 951-958 (2009) 6 44. Li, L.J., Fei-Fei, L.: Optimol: Automatic online picture collection via incremental model learning. International Journal of Computer Vision 88, 147-168 (2007) 4 45. Li, L.H., Yatskar, M., Yin, D., Hsieh, C., Chang, K.W.: Visualbert: A simple and performant baseline for vision and language. ArXiv abs/1908.03557 (2019) 5 46. Li, Q., Wu, J., Tu, Z.: Harvesting mid-level visual concepts from large-scale internet images. 2013 IEEE Conference on Computer Vision and Pattern Recognition pp. 851-858 (2013) 4 47.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>13 1</head><label>13</label><figDesc>: Webly-supervised learning for salient object detection. Pattern Recognit. 103, 107308 (2020) 4 54. van der Maaten, L., Hinton, G.E.: Visualizing data using t-sne. Journal of Machine Learning Research 9, 2579-2605 (2008) 6 55. McCann, B., Keskar, N., Xiong, C., Socher, R.: The natural language decathlon: Multitask learning as question answering. ArXiv abs/1806.08730 (2018) 4 74. Uijlings, J.R.R., Popov, S., Ferrari, V.: Revisiting knowledge transfer for training object class detectors. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition pp. 1101-1110 (2018) 5 75. Ulutan, O., Iftekhar, A.S.M., Manjunath, B.S.: Vsgnet: Spatial attention network for detecting human object interactions using graph convolutions. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp. 13614-13623 (2020) 13 76. Vedantam, R., Zitnick, C.L., Parikh, D.: Cider: Consensus-based image description evaluation. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 4566-4575 (2015) 10 77. Vijayanarasimhan, S., Grauman, K.: Keywords to visual categories: Multipleinstance learning forweakly supervised object categorization. 2008 IEEE Conference on Computer Vision and Pattern Recognition pp. 1-8 (2008) 4 78. Wang, S., Thompson, L., Iyyer, M.: Phrase-bert: Improved phrase embeddings from bert with an application to corpus exploration. In: EMNLP (2021) 6 79. Wang, S., Joo, J., Wang, Y., Zhu, S.C.: Weakly supervised learning for attribute localization in outdoor scenes. 2013 IEEE Conference on Computer Vision and Pattern Recognition pp. 3111-3118 (2013) 6 80. Wang, X.J., Zhang, L., Li, X., Ma, W.Y.: Annotating images by mining image search results. IEEE Transactions on Pattern Analysis and Machine Intelligence 30, 1919-1932 (2008) 4 81. Whitehead, S., Wu, H., Ji, H., Feris, R.S., Saenko, K., MIT-IBM, U.: Separating skills and concepts for novel visual question answering. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp. 5628-5637 (2021) 5 82. Wu, Z., Tao, Q., Lin, G., Cai, J.: Exploring bottom-up and top-down cues with attentive learning for webly supervised object detection. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp. 12933-12942 (2020) 4 83. Xu, H., Yan, M., Li, C., Bi, B., Huang, S., Xiao, W., Huang, F.: E2e-vlp: End-toend vision-language pre-training enhanced by visual learning (2021) 4 84. Yang, J., Feng, L., Chen, W., Yan, X., Zheng, H., Luo, P., Zhang, W.: Webly supervised image classification with self-contained confidence. In: ECCV (2020) 4 85. Yatskar, M., Zettlemoyer, L., Farhadi, A.: Situation recognition: Visual semantic role labeling for image understanding. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 5534-5542 (2016) 6 86. Yatskar, M., Zettlemoyer, L., Farhadi, A.: Situation recognition: Visual semantic role labeling for image understanding. In: Conference on Computer Vision and Pattern Recognition (2016) 10 87. Zareian, A., Rosa, K.D., Hu, D.H., Chang, S.F.: Open-vocabulary object detection using captions. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp. 14388-14397 (2021) 5 88. Zhang, A., Liao, Y., Liu, S., Lu, M., Wang, Y., Gao, C., Li, X.: Mining the benefits of two-stage and one-stage hoi detection. arXiv preprint arXiv:2108.05077 (2021) 13 89. Zhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L.J., Choi, Y., Gao, J.: Vinvl: Making visual representations matter in vision-language models. ArXiv abs/2101.00529 (2021) 5, 7, 8, 9, 12 90. Zhao, J., Wang, T., Yatskar, M., Ordonez, V., Chang, K.W.: Men also like shopping: Reducing gender bias amplification using corpus-level constraints. In: EMNLP (2017). https://doi.org/10.18653/v1/D17-1323, https://aclanthology. org/D17-1323 13 91. Zheng, W., Yan, L., Gou, C., Wang, F.: Webly supervised knowledge embedding model for visual reasoning. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp. 12442-12451 (2020) 4 92. Zhong, X., Ding, C., Qu, X., Tao, D.: Polysemy deciphering network for humanobject interaction detection. In: ECCV (2020) 13 93. Zou, C., Wang, B., Hu, Y., Liu, J., Wu, Q., Zhao, Y., Li, B., Zhang, C., Zhang, C., Wei, Y., et al.: End-to-end human object interaction detection with hoi transformer. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2021) Allen Institute for Artificial Intelligence 2 University of Illinois at Urbana-ChampaignThe supplementary includes the following sections:?Sec 1: Qualitative results from GPV-2 ? Sec 2: Classification re-calibration analysis ? Sec 3: Web10k questions and statistics ? Sec 4: DCE sampling details ? Sec 5: GPV-2 efficiency metrics ? Sec 6: Experimental details ? Sec 7: Human Object Interaction experimental details ? Sec 8: Zero-shot verb and attribute recognition ? Sec 9: Performance on the GRIT benchmark ? Sec 10: Comparison between the GPV-2 and GPV-1 architectures when trained on the same data ? Sec 11: Results on all nocaps splits for DCE captioning ? Sec 12: Biases in web data 1 Qualitative results from GPV-2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 2 :</head><label>2</label><figDesc>Qualitative Examples: GPV-2 on DCE, with and without training on WEB10K. The use of Web10k allows GPV-2 to understand more concepts across all skills, especially for rare concepts such as "red panda" (captioning upper right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 3 :</head><label>3</label><figDesc>Qualitative examples of re-calibration. This figure shows two CiC examples, where the left tables show GPV-2's top 9 predictions and log-probability scores, and the right table shows how the scores and rankings change after re-calibration. The model has a strong preference for answers seen in the Coco classification data (black), resulting in the model ranking Coco classes that are vaguely visually similar to the image over the correct class (green). Re-calibration increases the relative score of the non-Coco answers (green if correct, orange otherwise) allowing the model to get these examples correct.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>shows a breakdown of how GPV-2 behaves on DCE classification with and without re-calibration. Without re-calibration GPV-2 predicts a Coco category for 56% of CiC examples and 65.7% of the CLS examples, even though only 14% of these examples belong to a Coco category, showing that the model has a strong bias towards these categories. Adding re-calibration mostly mitigates this bias and significantly boosts performance on non-Coco categories. It comes at the cost of some performance on examples that belong to Coco categories, but those examples are only a small portion of the data so performance is increased by 12 points overall. These results show re-calibration is an important component to allowing models to transfer concepts learned from non-classification data to the classification skill. Qualitative examples are shown in Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>sport is being played ? Baseball What is this ? Sports ball Locate the baseball glove.</figDesc><table><row><cell>COCO data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Learn Skills</cell><cell>Skill-Concept Transfer</cell><cell>What is this ?</cell><cell>kingfisher</cell><cell>bluejay</cell><cell>heron</cell><cell>mallard</cell></row><row><cell></cell><cell></cell><cell></cell><cell>What is the lady doing ? Harvesting</cell><cell></cell><cell>Locate the guitar / drums / mic</cell><cell></cell><cell>A person is working</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>on a sewing machine</cell></row><row><cell></cell><cell>GPV-2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Web Search data</cell><cell></cell><cell></cell><cell cols="3">Human Object Interaction Detection</cell><cell></cell><cell></cell></row><row><cell>kingfisher flying</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GPV-2</cell><cell></cell></row><row><cell>kingfisher sitting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>electric sewing machine vintage sewing machine farmer selling farmer harvesting</cell><cell>Concepts Learn</cell><cell>Inference Chained</cell><cell>Locate people</cell><cell>GPV-2</cell><cell>What is this person doing?</cell><cell>GPV-2</cell><cell>sitting on airplane</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>directing airplane</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Hoffman et al . [29] cast this problem as a domain adaptation problem, adapting classifiers to detectors. Redmon et al . [66] build a 9,000 class detector using Imagenet22k classification data [16] by jointly training for the two tasks. Uijlings et al . [74] use Multiple Instance Learning to pseudo-label data and train a large vocabulary detector. Recent works build open vocabulary detectors [87,23,32] by leveraging image caption pairs (or models like CLIP</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Left: Web10k statistics (Sec. 3). There are approximately 25 images per concept. Right: DCE val and test statistics (Sec. 5).</figDesc><table><row><cell>Type</cell><cell>Count</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Subset Skill</cell><cell cols="3">Samples Images Categories</cell></row><row><cell></cell><cell>Nouns: 10211</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Adjectives: 144</cell><cell></cell><cell>VQA</cell><cell>5169</cell><cell>2131</cell><cell>295</cell></row><row><cell>Concepts</cell><cell>Verbs: 298 Noun-adjective pairs: 18616</cell><cell>Val</cell><cell>Localization Classification</cell><cell>8756 9485</cell><cell>7588 6770</cell><cell>463 464</cell></row><row><cell></cell><cell>Noun-verb pairs: 9243</cell><cell></cell><cell>Cls-in-context</cell><cell>9485</cell><cell>6770</cell><cell>464</cell></row><row><cell></cell><cell>Total: 38072 (Nouns + Pairs)</cell><cell></cell><cell>Captioning</cell><cell>4500</cell><cell>4500</cell><cell>-</cell></row><row><cell></cell><cell>Noun images: 255073</cell><cell></cell><cell>VQA</cell><cell>5281</cell><cell>2160</cell><cell>307</cell></row><row><cell>Images</cell><cell>Noun-adjective images: 465146 Noun-verb images: 230224</cell><cell>Test</cell><cell>Localization Classification</cell><cell>10586 10888</cell><cell>9986 9161</cell><cell>476 476</cell></row><row><cell></cell><cell>Total: 950443</cell><cell></cell><cell>Cls-in-context</cell><cell>10888</cell><cell>9161</cell><cell>476</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Captioning</cell><cell>10600</cell><cell>10600</cell><cell>-</cell></row><row><cell></cell><cell>Templates: 26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>QAs</cell><cell>Noun QAs: 1900886 Adjective QAs: 930292 Verb QAs: 460448 Total: 3291626</cell><cell cols="5">Note: Since nocaps [2] annotations are hidden behind an evaluation server, we are unable to provide category counts for captioning.</cell></row><row><cell cols="7">total of 950,443 image URLs (approx. 25 per query). Importantly, this cost us</cell></row><row><cell cols="7">$154, so it is inexpensive to scale further, and such data acquisition is affordable</cell></row><row><cell cols="6">for many other research organizations. See Tab. 1 for detailed statistics.</cell><cell></cell></row><row><cell cols="7">Conversion into QA data. We convert each query-image pair into multiple</cell></row><row><cell cols="7">templated QA pairs where the answer is the noun, adjective or verb from the</cell></row><row><cell cols="7">query. For example "What is the [noun] doing?" and "What [adj type] is this</cell></row><row><cell cols="7">object?"; see Supplementary Sec. 3 for all question templates. The QA format</cell></row><row><cell cols="7">has two advantages: (1) it removes ambiguity from the task (e.g., "What color</cell></row><row><cell cols="7">is this" tells the model not to return a potentially accurate non-color attribute);</cell></row><row><cell cols="6">and (2) it bridges the domain gap to other tasks posed as questions.</cell><cell></cell></row><row><cell cols="7">Data Splits. We split image-query pairs into train (874k), val (38k) and test</cell></row><row><cell cols="7">(38k). We sample 5k and 10k pairs from the val and test sets and ask 3 crowd-</cell></row><row><cell cols="7">workers to verify that the query is present in the image. We only retain unani-</cell></row><row><cell cols="7">mously verified examples (71%) resulting in: Val -4k images (9k QAs), Test -8k</cell></row><row><cell cols="7">images (19k QAs). The Train set has about 3M QAs with no manual verification.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Concept expansion with web data. Jointly training on Web10k + Coco shows consistent gains on DCE and Web10k benchmarks without adversely affecting Coco performance for 3 different GPVs. GPV-1 20 refers to 20 epoch training. GPV-2 with web 71.4 113.0 70.9 82.3 93.2 61.1 72.5 75.9 45.4 52.2 62.0 41.7 60.0 84.3 Sampling strategy. For Cls, CiC and Loc, we randomly sample up to 25 samples from each of the selected categories. A sample for Cls/CiC is defined as any bounding box annotated with a category. For Loc, a sample is all bounding boxes in an image annotated with a category (we discard "group" annotations). For VQA, we first discard annotations exceeding 2 word answers after removing articles and tag each QA pair in VG with any of the selected categories mentioned in either the question or answer. Then, for each category, we sample up to 50 data points. Since each sample in VQA may consist of multiple categories, this strategy does result in more than 50 samples for some categories, but in practice it achieves the goal of preventing common categories from dominating the evaluation. Finally, some of the 492 categories do not have annotations in the source datasets. The final sample, image, and category counts for each skill are in Tab. 1 and category frequencies are shown in Supplementary Sec. 4. Additional VQA annotations. VQA annotations from VG only consist of one answer per question. For each selected VQA sample, we source 9 additional answers from Amazon Mechanical Turk as in standard Coco-based VQA benchmarks [22,4]. Samples where ?3 workers agreed on an answer were retained.</figDesc><table><row><cell></cell><cell cols="2">Coco</cell><cell>DCE</cell><cell cols="2">Web10k</cell></row><row><cell>Model</cell><cell cols="5">Web data VQA Cap Loc Cls CiC VQA Cap Loc Cls CiC All Nouns Verbs Adj</cell></row><row><cell>[a] GPV-1</cell><cell cols="2">no web 62.5 102.3 73.0 83.6 -</cell><cell cols="2">45.3 25.8 61.9 10.1 -11.9 2.7</cell><cell>8.5 24.5</cell></row><row><cell cols="3">[b] GPV-1 20 no web 61.2 95.7 65.3 82.3 -</cell><cell>44.3 23.1 60.3 9.3</cell><cell>-13.1 3.1</cell><cell>7.7 28.4</cell></row><row><cell cols="3">[c] GPV-1 20 with web 61.5 97.3 64.9 82.8 -</cell><cell cols="3">45.8 28.6 61.5 20.0 -54.4 32.7 51.7 78.8</cell></row><row><cell>[d] VL-T5</cell><cell>no web 69.8 100.7</cell><cell>-78.1 -</cell><cell cols="2">60.2 31.6 -10.9 -18.6 4.3</cell><cell>15.8 35.7</cell></row><row><cell>[e] VL-T5</cell><cell>with web 69.9 106.4</cell><cell>-77.3 -</cell><cell cols="3">59.9 45.0 -16.2 -61.0 38.0 59.3 85.8</cell></row><row><cell>[f] GPV-2</cell><cell cols="4">no web 71.1 112.1 70.9 82.2 93.4 60.6 65.4 74.8 36.3 43.6 22.5 3.8</cell><cell>23.6 39.9</cell></row><row><cell>[g]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>for additional experimental details. VQA: We train on the VQA v2 [22] train set and report results using the annotator-weighted metric from [22] on the VQA v2 test-dev set and DCE test set. Captioning: We train on Coco captioning and report CIDEr-D [76] on Coco test. DCE uses nocaps<ref type="bibr" target="#b0">[2]</ref> for captioning. Due to space constraints, we only report CIDEr-D on the out-of-domain split, as performance on novel concepts is our primary interest. See Supplementary Sec. 11 for results on all splits.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Concept scaling using web data: Closed world experiment. To eliminate the effect of VinVL features and CC pretraining, we restrict GPV-2 to Coco-sce trained DETR features. Training jointly with Web10k still shows massive gains on DCE and Web10k vs training with only Coco-sce. Test Sn Unsn Test Sn Unsn Test Sn Unsn Test Sn UnsnGPV-2 no web 59.6 60.1 48.5 88.4 91.7 55.5 62.2 67.2 14.0 73.1 77.2 33.9 46.9 21.1 54.9 13.6 14.0 3.3 11.6 27.1 GPV-2 with web 59.9 60.3 49.7 89.2 92.1 58.0 62.2 67.0 14.8 73.0 77.2 32.6 46.8 33.4 58.7 26.5 47.0 25.1 43.0 73.0</figDesc><table><row><cell></cell><cell>Coco-sce</cell><cell></cell><cell></cell><cell>DCE</cell><cell>Web10k</cell></row><row><cell>VQA</cell><cell>Cap</cell><cell>Loc</cell><cell>Cls</cell><cell cols="2">VQA Cap Loc Cls All Noun Verb Adj</cell></row><row><cell>Model Web data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Ablating GPV-2. The left-most columns indicate using Web10k ('Pre.' indicates pre-training with Web10k instead of multi-tasking), CC pre-training, classifier re-calibration (Cb), language-based localization (LBL) (see Sec. 4), and VinVL instead of the DETR detector from GPV-1. The first row shows results for GPV-2, and the lower rows show the differences in scores between ablations and GPV-2. Each component improves performance on DCE.</figDesc><table><row><cell>Coco</cell><cell>DCE</cell><cell></cell><cell>Web10k</cell></row><row><cell cols="2">Web CC Cb LBL Vin. VQA Cap Loc Cls CiC VQA Cap Loc</cell><cell>Cls CiC</cell><cell>All Nouns Verbs Adj</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4</head><label>4</label><figDesc>shows results on the validation sets. The model that does not use LBL scores each box using a linear classifier on top of its contextualized embedding instead. On both classification tasks and captioning, we find that web data helps with and with-</figDesc><table><row><cell>VQA</cell><cell>Captioning</cell><cell>Localization</cell><cell cols="4">Classification (cropped) Classification in Context</cell></row><row><cell>What is the type of dress women wearing?</cell><cell>What is happening?</cell><cell>Find balance beam.</cell><cell>What is this thing?</cell><cell></cell><cell></cell><cell>What is this?</cell></row><row><cell>sari scarves</cell><cell>a toddler wearing a hat riding a tricycle. a small child in a hat</cell><cell>solid lines dotted lines</cell><cell>gondola motorcycle</cell><cell></cell><cell></cell><cell>guacamole broccoli</cell></row><row><cell></cell><cell>riding a bike</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>What color is the burrito?</cell><cell>Describe this image.</cell><cell>Locate cart in the image.</cell><cell>What object is this?</cell><cell></cell><cell cols="2">What is this object?</cell></row><row><cell>brown green</cell><cell>on a table a small blueberry muffin on a yellow plate. a close up of a plate of food</cell><cell>solid lines dotted lines</cell><cell>Legend: harpsichord suitcase</cell><cell cols="2">with web</cell><cell>without web polar bear sheep</cell></row><row><cell>VQA</cell><cell>Captioning</cell><cell>Localization</cell><cell cols="2">Classification (cropped)</cell><cell cols="2">Classification in Context</cell></row><row><cell>What is he holding?</cell><cell>Describe the image.</cell><cell>Find the temperature scanner.</cell><cell>What is this?</cell><cell></cell><cell></cell><cell>What is this?</cell></row><row><cell>covid vaccination card</cell><cell>a close up of a person</cell><cell></cell><cell>nasal swab</cell><cell></cell><cell></cell><cell>pcr test</cell></row><row><cell></cell><cell>wearing a kn95 mask</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Fig. 4: Qualitative results of GPV-2 on DCE with and without WEB10K: Without web training, GPV-2 can ignore concepts rarely seen in the supervised train- ing data (e.g., 'balance beam' top middle) or predict frequently occurring concepts that do not appear in the image (e.g., 'sheep' lower right). Web training fixes these issues and allows generalization to rare concepts like 'sari' and 'harpsichord'.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Qualitative examples for GPV-2. Examples are from DCE val, except for the last image in each row, which comes from Coco val. GPV-2 is able to use concepts that do not appear in the Coco training data across all five skills.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>VQA</cell><cell></cell></row><row><cell></cell><cell cols="2">What color is the burrito?</cell><cell>Who has black ears?</cell><cell>What is the stuffed toy?</cell><cell cols="2">What is the type of dress</cell><cell>What is brown with black</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>women wearing?</cell><cell>writing?</cell></row><row><cell cols="2">What are the skiers holding?</cell><cell cols="2">What is the yellow food under</cell><cell>What is the shape of the stop</cell><cell>Where is the mirror?</cell><cell>What flag is in the background?</cell></row><row><cell>with web:</cell><cell>brown</cell><cell></cell><cell>the carrot? panda</cell><cell>sign? monkey</cell><cell>sari</cell><cell>surfboard</cell></row><row><cell>without web:</cell><cell>green</cell><cell></cell><cell>bear</cell><cell>bear</cell><cell>scarves</cell><cell>sign</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Captioning</cell><cell></cell></row><row><cell cols="7">poles Find chairs in this image. What object is this? willow What object is this? camel Caption this image. a jaguar yawning while What is this object? rice Find all instances of lifejackets. raccoon printer What is this object? What is happening? Classification (cropped image) octagon Captioning Localization Find dresses. What is this? fountain Classification in Context What is this? bee What is happening? a woodpecker that is sitting in a tree. a bird perched on top of a tree branch. a mannequin is standing in a clothing store. a woman's dress hanging on a clothes line. sitting on a tree branch. a close up of a giraffe in a tree branch Localization Find jaguars in this image. Find all instances of coffeemakers. Find balance beam. What object is this? kettle What is this object? hippopotamus What is this? sewing machine Classification (cropped image) Classification in Context What object is this? What is this object? What is this? with web: without web: Caption this image. a pineapple that is growing in a field. a close up of a plant with leaves What is happening? a toddler wearing a hat riding a tricycle. a small child in a hat riding a bike Describe this image. a black and white caterpillar on a green leaf. a close up of a zebra on a plant with web: without web: with web: solid lines without web: dotted lines with web: Fig. 1: What is happening? american Locate the pumpkins. above the sink Locate people in the image. What is this thing? sushi What object is this? motorcycle pillow What is this thing? What is this? motorcycle a red panda walking across a lush green field. a brown and white bear walking across a field. Describe this image. a small blueberry muffin on a yellow plate. a close up of a plate of food on a table Locate the mule. Locate cart in the image. What is this thing? gondola What object is this? harpsichord What is this thing? What is this? Describe this image. a close up of a llama looking at the camera. a close up of a sheep with a rock background What is happening? a close up of a person playing an accordion a close up of a person playing an instrument vase elephant dining table motorcycle suitcase without web:</cell></row><row><cell>with web:</cell><cell>harp</cell><cell></cell><cell>polar bear</cell><cell>guacamole</cell><cell>woodpecker</cell><cell>caterpillar</cell></row><row><cell>without web:</cell><cell>giraffe</cell><cell></cell><cell>sheep</cell><cell>broccoli</cell><cell>stop sign</cell><cell>cat</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 1 :</head><label>1</label><figDesc>GPV-2 accuracy on DCE classification with and without classifier re-calibration (Cb). The Acc. column shows overall accuracy, Coco Acc. shows accuracy on examples with labels in the 80 Coco categories, Other Acc. shows accuracy on other examples, and Coco Ans. shows how often the model predicts a Coco category.Task Cb Acc. Coco Acc. Other Acc. Coco Ans.</figDesc><table><row><cell cols="2">CiC -39.4</cell><cell>92.0</cell><cell>30.8</cell><cell>56.4</cell></row><row><cell>CiC</cell><cell>52.2</cell><cell>77.5</cell><cell>48.1</cell><cell>19.7</cell></row><row><cell cols="2">CLS -34.0</cell><cell>85.7</cell><cell>25.5</cell><cell>65.7</cell></row><row><cell>CLS</cell><cell>45.8</cell><cell>69.9</cell><cell>41.9</cell><cell>24.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 2 :</head><label>2</label><figDesc>Templates for generating web prompts. Templates are grouped by whether they have a noun, verb, or adjective answer. These templates are expanded by substituting the all-caps words for any one of the substitute words specified below the table, except ADJ TYPE which is replaced by the type of the adjective for questions with adjective answers. For verb and adjective questions where the object is specified, OBJ is replaced by the noun instead, and verb templates that do not contain OBJ are not used. DCE val and test set category frequencies. Bars at &gt; x indicate the number of categories with at least x samples per category for each DCE skill with publicly available annotations. DCE expands the scope of concept evaluation across skills beyond Coco's 80 concepts and maximizes representation of a large subset of mutually exclusive concepts in OpenImages while avoiding over-representation of "head" concepts (e.g. "man", "woman").</figDesc><table><row><cell cols="2">Answer Type Prompts</cell></row><row><cell></cell><cell>What is DT OBJ?</cell></row><row><cell></cell><cell>What OBJ is this?</cell></row><row><cell>Noun</cell><cell>What OBJ is that? Classify DT OBJ.</cell></row><row><cell></cell><cell>Specify DT OBJ.</cell></row><row><cell></cell><cell>Name DT OBJ.</cell></row><row><cell></cell><cell>WH ADJ TYPE is DT OBJ?</cell></row><row><cell>Adjective</cell><cell>What is the ADJ TYPE of DT OBJ?</cell></row><row><cell></cell><cell>CMD the ADJ TYPE of DT OBJ.</cell></row><row><cell></cell><cell>What is DT OBJ doing?</cell></row><row><cell></cell><cell>What action is DT OBJ taking?</cell></row><row><cell></cell><cell>What action is DT OBJ performing?</cell></row><row><cell></cell><cell>What action is DT OBJ carrying out?</cell></row><row><cell></cell><cell>What action is DT OBJ doing?</cell></row><row><cell></cell><cell>What activity is DT OBJ doing?</cell></row><row><cell></cell><cell>CMD the action being taken by DT OBJ.</cell></row><row><cell></cell><cell>CMD the activity DT OBJ is doing.</cell></row><row><cell>Verb</cell><cell>CMD what DT OBJ is doing.</cell></row><row><cell></cell><cell>What is being done?</cell></row><row><cell></cell><cell>WH action is being done?</cell></row><row><cell></cell><cell>WH activity is being done?</cell></row><row><cell></cell><cell>WH activity is this?</cell></row><row><cell></cell><cell>WH action is being taken?</cell></row><row><cell></cell><cell>CMD the activity being done.</cell></row><row><cell></cell><cell>CMD the action being done.</cell></row><row><cell></cell><cell>CMD the action being taken.</cell></row><row><cell></cell><cell>What is DT OBJ doing?</cell></row><row><cell>Entire Query</cell><cell>What is this? What is that?</cell></row><row><cell>DT ? the, this, that</cell><cell></cell></row><row><cell>OBJ ? entity, object</cell><cell></cell></row><row><cell>WH ? What, Which</cell><cell></cell></row><row><cell cols="2">CMD ? Describe, State, Specify, Name</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 3 :</head><label>3</label><figDesc>Number of parameters and FLOPs in GPV-2. Results are shown for both when the image features are pre-computed (top), and when they have to be generated from scratch (bottom).</figDesc><table><row><cell cols="2">Pre. Params VQA Cap Loc CLS CiC</cell></row><row><cell></cell><cell>224M 4.68G 6.31G 25.1G 2.63G 4.73G</cell></row><row><cell>-</cell><cell>370M 7.35T 7.38T 7.64T 6.62T 7.30T</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>25] for CLS, Loc and Cap, and from our questions template for Web10k (See Sec. 3). For CiC we use the CLS prompts. During testing, we generate text using beam search with a beam size of 20, except for classification on DCE in which case we use the ranking approach from Sec. 4.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 4 :</head><label>4</label><figDesc>Learning verbs and attributes from Web10k. We test verb and attribute learning from Web10k by evaluating GPV-2 without further finetuning on verb (imSitu) and attribute recognition (VAW) benchmarks. | 23.0 15.6 | 33.4 2.5 | 9.1 53.2 56.9 52.0 GPV-2+web 16.7 | 34.7 27.5 | 54.4 2.2 | 8.3 52.4 56.2 51.3</figDesc><table><row><cell></cell><cell cols="3">imSitu (top-1 | top-5 acc.)</cell><cell cols="3">VAW (mAP)</cell></row><row><cell>Model</cell><cell>Test</cell><cell>Seen</cell><cell cols="4">Unsn Test Seen Unsn</cell></row><row><cell cols="2">GPV-2 10.0 Supervised 43.2 | 68.6</cell><cell>-</cell><cell>-</cell><cell>68.3</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 5 :</head><label>5</label><figDesc>Performance on GRIT benchmark, unrestricted test set. GPV-2 competes on four of the seven benchmark tasks: Object Categorization (cat), Object Localization (loc), VQA (vqa) and Referring Expression Grounding (ref). It cannot compete on Segmentation (seg), Person Keypoint Detection (kp), or Surface Normal Estimation (sn). The aggregation takes the average of all seven tasks, assigning 0 to the tasks that models cannot perform. GPV-1 here has not been trained on referring expressions, or with web data. VinVL, trained on COCO, VG, 55.1 53.6 63.2 52.1 ---32.0 Objects365 and OpenImages</figDesc><table><row><cell>Model</cell><cell>Detector Backbone</cell><cell cols="2">cat loc vqa ref seg kp sn All</cell></row><row><cell>GPV-1</cell><cell>DETR, trained on COCO</cell><cell>33.2 42.7 49.8 26.8 -</cell><cell>--21.8</cell></row><row><cell>GPV-2</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Since [25] takes a long time to train when using the web data (over 3 weeks), results for GPV-1 with and without web data are reported after 20 epochs training.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* Equal contribution</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/facebookresearch/fvcore/blob/main/docs/flop_count.md</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://grit-benchmark.org/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>Amita Kamath * 1 , Christopher Clark * 1 , Tanmay Gupta * 1 , Eric Kolve 1 , Derek Hoiem 2 , and Aniruddha Kembhavi 1</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<title level="m">nocaps: novel object captioning at scale. International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kr?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mc-Candlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Language models are few-shot learners. ArXiv abs/2005.14165 (2020</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brysbaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kuperman</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-013-0403-5</idno>
		<ptr target="https://doi.org/10.3758/s13428-013-0403-56" />
		<title level="m">Concreteness ratings for 40 thousand generally known english word lemmas. Behavior research methods</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno>abs/2005.12872</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">HICO: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Describing clothing by semantic attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Webly supervised learning of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">NEIL: Extracting visual knowledge from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv abs/1909.11740</idno>
		<title level="m">Uniter: Learning universal image-text representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unifying vision-and-language tasks via text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02779</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning everything about anything: Weblysupervised visual concept learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li-Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno>ICLR abs/2010.11929</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning from noisy web data with category-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Webly supervised learning meets zeroshot learning: A hybrid approach for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Competent men and warm women: Gender stereotypes and backlash in image search results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Otterbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 chi conference on human factors in computing systems</title>
		<meeting>the 2017 chi conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sun attribute database: Discovering, annotating, and recognizing scene attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to predict visual attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<title level="m">Learning transferable visual models from natural language supervision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICML</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03649</idno>
		<title level="m">Overcoming language priors in visual question answering with adversarial regularization</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolo9000: Better, faster, stronger. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">How much can clip benefit vision-and-language tasks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno>ArXiv abs/2107.06383</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bootstrapping the performance of webly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Noiseaware fully webly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mixture-kernel graph attention network for situation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suhail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Mining cross-image semantics for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP/IJCNLP</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">3 tasks purely due to its architecture. In addition, adding web data training to GPV-2 (no other changes) provides further improvements on 2 tasks in-domain</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Direct comparison between GPV-2 and GPV-1. Performance on COCO-SCE when trained on the same data and using the same detector backbone. Model Web data VQA Cap Loc Cls</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Table 7: Full DCE Captioning results. Training on web data improves performance for all three GPVs, for all splits -even in-domain, which focuses on Coco concepts. including the &quot;isFam-ilyFriendly&quot; filter on Bing, removing inappropriate words per a popular blacklist [1], and conducting manual spot checks. However, the entire dataset has not been human-curated, so we cannot guarantee it is free from objectionable imagery</title>
	</analytic>
	<monogr>
		<title level="m">out-of-domain, and all</title>
		<imprint/>
	</monogr>
	<note>The out-of-domain results are reported in the main paper, as our focus is on learning novel concepts. It is important to be aware that search results are known to reflect human biases and stereotypes [58,34], for example, most of our images for &quot;soccer</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
