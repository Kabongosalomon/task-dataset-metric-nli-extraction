<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON AFFECTIVE COMPUTING AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE, Behzad Hasani</roleName><forename type="first">Ali</forename><surname>Mollahosseini</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Mohammad</forename><forename type="middle">H Mahoor</forename></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON AFFECTIVE COMPUTING AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated affective computing in the wild setting is a challenging problem in computer vision. Existing annotated databases of facial expressions in the wild are small and mostly cover discrete emotions (aka the categorical model). There are very limited annotated facial databases for affective computing in the continuous dimensional model (e.g., valence and arousal). To meet this need, we collected, annotated, and prepared for public distribution a new database of facial emotions in the wild (called AffectNet). AffectNet contains more than 1,000,000 facial images from the Internet by querying three major search engines using 1250 emotion related keywords in six different languages. About half of the retrieved images were manually annotated for the presence of seven discrete facial expressions and the intensity of valence and arousal. AffectNet is by far the largest database of facial expression, valence, and arousal in the wild enabling research in automated facial expression recognition in two different emotion models. Two baseline deep neural networks are used to classify images in the categorical model and predict the intensity of valence and arousal. Various evaluation metrics show that our deep neural network baselines can perform better than conventional machine learning methods and off-the-shelf facial expression recognition systems.</p><p>Index Terms-Affective computing in the wild, facial expressions, continuous dimensional space, valence, arousal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>!</head><p>? Authors are with the</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A FFECT is a psychological term used to describe the outward expression of emotion and feelings. Affective computing seeks to develop systems and devices that can recognize, interpret, and simulate human affects through various channels such as face, voice, and biological signals <ref type="bibr" target="#b0">[1]</ref>. Face and facial expressions are undoubtedly one of the most important nonverbal channels used by the human being to convey internal emotion.</p><p>There have been tremendous efforts to develop reliable automated Facial Expression Recognition (FER) systems for use in affect-aware machines and devices. Such systems can understand human emotion and interact with users more naturally. However, current systems have yet to reach the full emotional and social capabilities necessary for building rich and robust Human Machine Interaction (HMI). This is mainly due to the fact that HMI systems need to interact with humans in an uncontrolled environment (aka wild setting) where the scene lighting, camera view, image resolution, background, users head pose, gender, and ethnicity can vary significantly. More importantly, the data that drives the development of affective computing systems and particularly FER systems lack sufficient variations and annotated samples that can be used in building such systems.</p><p>There are several models in the literature to quantify affective facial behaviors: 1) categorical model, where the emotion/affect is chosen from a list of affective-related categories such as six basic emotions defined by Ekman et al. <ref type="bibr" target="#b1">[2]</ref>, 2) dimensional model, where a value is chosen over a continuous emotional scale, such as valence and arousal <ref type="bibr" target="#b2">[3]</ref> and 3) Facial Action Coding System (FACS) model, where all possible facial actions are described in terms of Action Units (AUs) <ref type="bibr" target="#b3">[4]</ref>. FACS model explains facial movements and does not describe the affective state directly. There are several methods to convert AUs to affect space (e.g., EMFACS <ref type="bibr" target="#b4">[5]</ref> states that the occurrence of AU6 and AU12 is a sign of happiness). In the categorical model, mixed emotions cannot adequately be transcribed into a limited set of words. Some researchers tried to define multiple distinct compound emotion categories (e.g., happily surprised, sadly fearful) <ref type="bibr" target="#b5">[6]</ref> to overcome this limitation. However, still the set is limited, and the intensity of the emotion cannot be defined in the categorical model. In contrast, the dimensional model of affect can distinguish between subtly different displays of affect and encode small changes in the intensity of each emotion on a continuous scale, such as valence and arousal. Valence refers to how positive or negative an event is, and arousal reflects whether an event is exciting/agitating or calm/soothing <ref type="bibr" target="#b2">[3]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> shows samples of facial expressions represented in the 2D space of valence and arousal. As it is shown, there are several different kinds of affect and small changes in the same emotion that cannot be easily mapped into a limited set of terms existing in the categorical model.</p><p>The dimensional model of affect covers both intensity and different emotion categories in the continuous domain. Nevertheless, there are relatively fewer studies on developing automated algorithms in measuring affect using the continuous dimensional model (e.g., valence and arousal). One of the main reasons is that creating a large database to cover the entire continuous space of valence and arousal is expensive and there are very limited annotated face databases in the continuous domain. This paper contributes to the field of affective computing by providing a large annotated face database of the dimensional as well as the arXiv:1708.03985v4 [cs.CV] 9 Oct 2017 The majority of the techniques for automated affective computing and FER are based on supervised machine learning methodologies. These systems require annotated image samples for training. Researchers have created databases of human actors/subjects portraying basic emotions <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b11">[11]</ref>. Most of these databases mainly contain posed expressions acquired in a controlled lab environment. However, studies show that posed expressions can be different from unposed expressions in configuration, intensity, and timing <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>. Some researchers captured unposed facial behavior while the subject is watching a short video <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref>, engaged in laboratory-based emotion inducing tasks <ref type="bibr" target="#b16">[16]</ref>, or interacted with a computer-mediated tutoring system <ref type="bibr" target="#b17">[17]</ref>. Although a large number of frames can be obtained by these approaches, the diversity of these databases is limited due to the number of subjects, head position, and environmental conditions.</p><p>Recently, databases of facial expression and affect in the wild received much attention. These databases are either captured from movies or the Internet, and annotated with categorical model <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>, dimensional model <ref type="bibr">[21]</ref>, and FACS model <ref type="bibr" target="#b22">[22]</ref>. However, they only cover one model of affect, have a limited number of subjects, or contain few samples of certain emotions such as disgust. Therefore, a large database, with a large amount of subject variations in the wild condition that covers multiple models of affect (especially the dimensional model) is a need.</p><p>To address this need, we created a database of facial Affect from the InterNet (called AffectNet) by querying different search engines (Google, Bing, and Yahoo) using 1250 emotion related tags in six different languages (English, Spanish, Portuguese, German, Arabic, and Farsi). AffectNet contains more than one million images with faces and extracted facial landmark points. Twelve human experts manually annotated 450,000 of these images in both categorical and dimensional (valence and arousal) models and tagged the images that have any occlusion on the face. <ref type="figure" target="#fig_0">Figure 1</ref> shows sample images from AffectNet and their valence and arousal annotations.</p><p>To calculate the agreement level between the human labelers, 36,000 images were annotated by two human labelers. AffectNet is by far the largest database of facial affect in still images which covers both categorical and dimensional models. The cropped region of the facial images, the facial landmark points, and the affect labels will be publicly available to the research community <ref type="bibr" target="#b0">1</ref> . Considering the lack of inthe-wild large facial expressions datasets and more specifically annotated face datasets in the continuous domain of valence and arousal, AffectNet is a great resource which will enable further progress in developing automated methods for facial behavior computing in both the categorical and continuous dimensional spaces.</p><p>The rest of this paper is organized as follows. Section 2 reviews the existing databases and state-of-the-art methods for facial expression recognition with emphasis on the dimensional model and in the wild setting databases. Section 3 explains the process of collecting AffectNet images from the Internet and annotating the categorical and dimensional models. Section 4 presents two different baselines for automatic recognition of categorical emotions and prediction of dimensional valence and arousal in the continuous space using AffecNet images. Finally Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Existing databases</head><p>Early databases of facial expressions such as JAFFE <ref type="bibr" target="#b6">[7]</ref>, Cohn-Kanade <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, MMI <ref type="bibr" target="#b10">[10]</ref>, and MultiPie <ref type="bibr" target="#b11">[11]</ref> were captured in a lab-controlled environment where the subjects portrayed different facial expressions. This approach resulted in a clean and high-quality database of posed facial expressions. However, posed expressions may differ from daily life unposed (aka spontaneous) facial expressions. Thus, capturing spontaneous expression became a trend in the affective computing community. Examples of these environments are recording the responses of participants' faces while watching a stimuli (e.g., DISFA <ref type="bibr" target="#b14">[14]</ref>, AM-FED <ref type="bibr" target="#b15">[15]</ref>) or performing laboratory-based emotion inducing tasks (e.g., Belfast <ref type="bibr" target="#b16">[16]</ref>). These databases often capture multi-modal affects such as voice, biological signals, etc. and usually a series of frames are captured that enable researchers to work on temporal and dynamic aspects of expressions. However, the diversity of these databases is limited due to the number of subjects, head pose variation, and environmental conditions.</p><p>Hence there is a demand to develop systems that are based on natural, unposed facial expressions. To address this demand, recently researchers paid attention to databases in the wild. Dhall et al. <ref type="bibr" target="#b18">[18]</ref> released Acted Facial Expressions in the Wild (AFEW) from 54 movies by a recommender system based on subtitles. The video clips were annotated with six basic expressions plus neutral. AFEW contains 330 subjects aged 1-77 years and addresses the issue of temporal facial expressions in the wild. A static subset (SFEW <ref type="bibr" target="#b27">[27]</ref>) is created by selecting some frames of AFEW. SFEW covers unconstrained facial expressions, different head poses, age range, occlusions, and close to real world illuminations. However, it contains only 700 images, and there are only 95 subjects in the database.</p><p>The Facial Expression Recognition 2013 (FER-2013) database was introduced in the ICML 2013 Challenges in Representation Learning <ref type="bibr" target="#b19">[19]</ref>. The database was created using the Google image search API that matched a set of 184 emotion-related keywords to capture the six basic expressions as well as the neutral expression. Images were resized to 48x48 pixels and converted to grayscale. Human labelers rejected incorrectly labeled images, corrected the cropping if necessary, and filtered out some duplicate images. The resulting database contains 35,887 images most of which are in the wild settings. FER-2013 is currently the biggest publicly available facial expression database in the wild settings, enabling many researchers to train machine learning methods such as Deep Neural Networks (DNNs) where large amounts of data are needed. In FER-2013, the faces are not registered, a small number of images portray disgust (547 images), and unfortunately most of facial landmark detectors fail to extract facial landmarks at this resolution and quality. In addition, only the categorical model of affect is provided with FER-2013.</p><p>The Affectiva-MIT Facial Expression Dataset (AM-FED) database <ref type="bibr" target="#b15">[15]</ref> contains 242 facial videos (160K frames) of people watching Super Bowl commercials using their webcam. The recording conditions were arbitrary with differ-ent illumination and contrast. The database was annotated frame-by-frame for the presence of 14 FACS action units, head movements, and automatically detected landmark points. AM-FED is a great resource to learn AUs in the wild. However, there is not a huge variance in head pose (limited profiles), and there are only a few subjects in the database.</p><p>The FER-Wild <ref type="bibr" target="#b20">[20]</ref> database contains 24,000 images that are obtained by querying emotion-related terms from three search engines. The OpenCV face recognition was used to detect faces in the images, and 66 landmark points were found using Active Appearance Model (AAM) <ref type="bibr" target="#b29">[28]</ref> and a face alignment algorithm via regression local binary features <ref type="bibr" target="#b30">[29]</ref>, <ref type="bibr" target="#b31">[30]</ref>. Two human labelers annotated the images into six basic expressions and neutral. Comparing with FER-2013, FER-Wild images have a higher resolution with facial landmark points necessary to register the images. However, still a few samples portray some expressions such as disgust and fear and only the categorical model of affect is provided with FER-Wild.</p><p>The EmotioNet <ref type="bibr" target="#b22">[22]</ref> consists of one million images of facial expressions downloaded from the Internet by selecting all the words derived from the word "feeling" in Word-Net <ref type="bibr" target="#b32">[31]</ref>. Face detector <ref type="bibr" target="#b33">[32]</ref> was used to detect faces in these images and the authors visually inspected the resultant images. These images were then automatically annotated with AUs and AU intensities by an approach based on Kernel Subclass Discriminant Analysis (KSDA) <ref type="bibr" target="#b34">[33]</ref>. The KSDA-based approach was trained with Gabor features centered on facial landmark with a Radial Basis Function (RBF) kernel. Images were labeled as one of the 23 (basic or compound) emotion categories defined in <ref type="bibr" target="#b5">[6]</ref> based on AUs. For example, if an image has been annotated as having AUs 1, 2, 12 and 25, it is labeled as happily surprised. A total of 100,000 images (10% of the database) were manually annotated with AUs by experienced coders. The proposed AU detection approach was trained on CK+ <ref type="bibr" target="#b8">[9]</ref>, DISFA <ref type="bibr" target="#b14">[14]</ref>, and CFEE <ref type="bibr" target="#b35">[34]</ref> databases, and the accuracy of the automated annotated AUs was reported about 80% on the manually annotated set. EmotioNet is a novel resource of FACS model in the wild with a large amount of subject variation. However, it lacks the dimensional model of affect, and the emotion categories are defined based on annotated AUs and not manually labeled.</p><p>On the other hand, some researchers developed databases of the dimensional model in the continuous domain. These databases, however, are limited since the annotation of continuous dimensions is more expensive and necessitate trained annotators. Examples of these databases are Belfast <ref type="bibr" target="#b16">[16]</ref>, RECOLA <ref type="bibr" target="#b25">[25]</ref>, Affectiva-MIT Facial Expression Dataset (AM-FED) <ref type="bibr" target="#b15">[15]</ref>, and recently published Aff-Wild Database [21] which is the only database of dimensional model in the wild.</p><p>The Belfast database <ref type="bibr" target="#b16">[16]</ref> contains recordings (5s to 60s in length) of mild to moderate emotional responses of 60 participants to a series of laboratory-based emotion inducing tasks (e.g., surprise response by setting off a loud noise when the participant is asked to find something in a black box). The recordings were labeled by information on self-report of emotion, the gender of the participant/experimenter, and the valence in the continuous domain. The arousal dimension was not annotated in Belfast database. While the portrayed emotions are natural and spontaneous, the tasks have taken place in a relatively artificial setting of a laboratory where there was a control on lighting conditions, head poses, etc.</p><p>The Database for Emotion Analysis using Physiological Signals (DEAP) <ref type="bibr" target="#b26">[26]</ref> consists of spontaneous reactions of 32 participants in response to one-minute long music video clip. The EEG, peripheral physiological signals, and frontal face videos of participants were recorded, and the participants rated each video in terms of valence, arousal, like/dislike, dominance, and familiarity. Correlations between the EEG signal frequencies and the participants ratings were investigated, and three different modalities, i.e., EEG signals, peripheral physiological signals, and multimedia features on video clips (such as lighting key, color variance, etc.) were used for binary classification of low/high arousal, valence, and liking. DEAP is a great database to study the relation of biological signals and dimensional affect, however, it has only a few subjects and the videos are captured in lab controlled settings.</p><p>The RECOLA benchmark <ref type="bibr" target="#b25">[25]</ref> contains videos of 23 dyadic teams (46 participants) that participated in a video conference completing a task which required collaboration. Different multi-modal data of the first five minutes of interaction, i.e., audio, video, ECG and EDA) were recorded continuously and synchronously. Six annotators measured arousal and valence. The participants reported their arousal and valence through the Self-Assessment Manikin (SAM) <ref type="bibr" target="#b36">[35]</ref> questionnaire before and after the task. RECOLA is a great database of the dimensional model with multiple cues and modalities, however, it contains only 46 subjects and the videos were captured in the lab controlled settings.</p><p>Audio-Visual Emotion recognition Challenge (AVEC) series of competitions <ref type="bibr" target="#b37">[36]</ref>, <ref type="bibr" target="#b38">[37]</ref>, <ref type="bibr" target="#b39">[38]</ref>, <ref type="bibr" target="#b40">[39]</ref>, <ref type="bibr" target="#b41">[40]</ref>, <ref type="bibr" target="#b42">[41]</ref> provided a benchmark of automatic audio, video and audiovisual emotion analysis in continuous affect recognition. AVEC 2011, 2012, 2013, and 2014 used videos from the SEMAINE <ref type="bibr" target="#b43">[42]</ref> database videos. Each video is annotated by a single rater for every dimension using a two-axis joystick. AVEC 2015 and 2016 used the RECOLA benchmark in their competitions. Various continuous affect recognition dimensions were explored in each challenge year such as valence, arousal, expectation, power, and dominance, where the prediction of valence and arousal are studied in all challenges.</p><p>The Aff-Wild Database [21] is by far the largest database for measuring continuous affect in the valence-arousal space "in-the-wild". More than 500 videos from YouTube were collected. Subjects in the videos displayed a number of spontaneous emotions while watching a particular video, performing an activity, and reacting to a practical joke. The videos have been annotated frame-by-frame by three human raters, utilizing a joystick-based tool to rate valence and arousal. Aff-Wild is a great database of dimensional modeling in the wild that considers the temporal changes of the affect, however, it has a small subject variance, i.e., it only contains 500 subjects. <ref type="table" target="#tab_0">Table 1</ref> summarizes the characteristics of the reviewed databases in all three models of affect, i.e., categorical model, dimensional model, and Facial Action Coding System (FACS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation Metrics</head><p>There are various evaluation metrics in the literature to measure the reliability of annotation and automated affective computing systems. Accuracy, F1-score <ref type="bibr" target="#b50">[49]</ref>, Cohens kappa <ref type="bibr" target="#b51">[50]</ref>, Krippendorfs Alpha <ref type="bibr" target="#b52">[51]</ref>, ICC <ref type="bibr" target="#b53">[52]</ref>, area under the ROC curve (AUC), and area under Precision-Recall curve (AUC-PR) <ref type="bibr" target="#b54">[53]</ref> are well-defined widely used metrics for evaluation of the categorical and FACS-based models.</p><p>Since, the dimensional model of affect is usually evaluated in a continuous domain, different evaluation metrics are necessary. In the following, we review several metrics that are used in the literature for evaluation of dimensional model.</p><p>Root Mean Square Error (RMSE) is the most common evaluation metric in a continuous domain which is defined as:</p><formula xml:id="formula_0">RM SE = 1 n n i=1 (? i ? ? i ) 2<label>(1)</label></formula><p>where? i and ? i are the prediction and the ground truth of i th sample, and n is the number of samples in the evaluation set. RMSE-based evaluation can heavily weigh the outliers <ref type="bibr" target="#b55">[54]</ref>, and it is not able to provide the covariance of prediction and ground-truth to show how they change with respect to each other. Pearsons correlation coefficient is therefore proposed in some literature <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b37">[36]</ref>, <ref type="bibr" target="#b38">[37]</ref> to overcome this limitation:</p><formula xml:id="formula_1">CC = COV {?, ?} ??? ? = E[(? ? ??)(? ? ? ? )] ??? ?<label>(2)</label></formula><p>Concordance Correlation Coefficient (CCC) is another metric <ref type="bibr" target="#b41">[40]</ref>, <ref type="bibr" target="#b42">[41]</ref> which combines the Pearsons correlation coefficient (CC) with the square difference between the means of two compared time series:</p><formula xml:id="formula_2">? c = 2???? ? ? 2 ? + ? 2 ? + (?? ? ? ? ) 2<label>(3)</label></formula><p>where ? is the Pearson correlation coefficient (CC) between two time-series (e.g., prediction and ground-truth), ? 2 ? and ? 2 ? are the variance of each time series, and ?? and ? ? are the mean value of each. Unlike CC, the predictions that are well correlated with the ground-truth but shifted in value are penalized in proportion to the deviation in CCC.</p><p>The value of valence and arousal are [-1,+1] and their signs are essential in many emotion-prediction applications. For example, if the ground-truth valence is +0.3, prediction of +0.7 is far better than prediction of -0.1, since +0.7 indicates a positive emotion similar to the ground-truth (despite both predictions have the same RMSE). Sign Agreement Metric (SAGR) is another metric that is proposed in <ref type="bibr" target="#b24">[24]</ref> to evaluate the performance of a valence and arousal prediction system. SAGR is defined as:</p><formula xml:id="formula_3">SAGR = 1 n n i=1 ?(sign(? i ), sign(? i ))<label>(4)</label></formula><p>where ? is the Kronecker delta function, defined as:</p><formula xml:id="formula_4">?(a, b) = 1, a = b 0, a = b<label>(5)</label></formula><p>The above discussed metrics are used to evaluate the categorical and dimensional baselines on AffectNet in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Existing Algorithms</head><p>Affective computing is now a well-established field, and there are many algorithms and databases for developing automated affect perception systems. Since it is not possible to include all those great works, we only give a brief overview and cover the state-of-the-art methods that are applied on the databases explained in Sec. 2.1.</p><p>Conventional algorithms of affective computing from faces use hand-crafted features such as pixel intensities <ref type="bibr" target="#b56">[55]</ref>, Gabor filters <ref type="bibr" target="#b57">[56]</ref>, Local Binary Patterns (LBP) <ref type="bibr" target="#b45">[44]</ref>, and Histogram of Oriented Gradients (HOG) <ref type="bibr" target="#b14">[14]</ref>. These handcrafted features often lack enough generalizability in the wild settings where there is a high variation in scene lighting, camera view, image resolution, background, subjects head pose and ethnicity.</p><p>An alternative approach is to use Deep Neural Networks (DNN) to learn the most appropriate feature abstractions directly from the data and handle the limitations of handcrafted features. DNNs have been a recent successful approach in visual object recognition <ref type="bibr" target="#b58">[57]</ref>, human pose estimation <ref type="bibr" target="#b59">[58]</ref>, face verification <ref type="bibr" target="#b60">[59]</ref> and many more. This success is mainly due to the availability of computing power and existing big databases that allow DNNs to extract highly discriminative features from the data samples. There have been enormous attempts on using DNNs in automated facial expression recognition and affective computing <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b44">[43]</ref>, <ref type="bibr" target="#b47">[46]</ref>, <ref type="bibr" target="#b48">[47]</ref>, <ref type="bibr" target="#b49">[48]</ref> that are especially very successful in the wild settings. <ref type="table" target="#tab_1">Table 2</ref> shows a list of the state-of-the-art algorithms and their performance on the databases listed in <ref type="table" target="#tab_0">Table 1</ref>. As shown in the table, the majority of these approaches have used DNNs to learn a better representation of affect, especially in the wild settings. Even some of the approaches, such as the winner of the AVEC 2015 challenge <ref type="bibr" target="#b47">[46]</ref>, trained a DNN with hand-crafted features and still could improve the prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AFFECTNET</head><p>AffectNet (Affect from the InterNet) is the largest database of the categorical and dimensional models of affect in the wild (as shown in <ref type="table" target="#tab_0">Table 1</ref>). The database is created by querying emotion related keywords from three search engines and annotated by expert human labelers. In this section, the process of querying the Internet, processing facial images and extracting facial landmarks, and annotating facial expression, valence, and arousal of affect are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Facial Images from the Web</head><p>Emotion-related keywords were combined with words related to gender, age, or ethnicity, to obtain nearly 362 strings in the English language such as "joyful girl", "blissful Spanish man", "furious young lady", "astonished senior". These keywords are then translated into five other languages: Spanish, Portuguese, German, Arabic and Farsi. The direct translation of queries in English to other languages did not accurately result in the intended emotions since each language and culture has differing words and expressions for different emotions. Therefore, the list of English queries was provided to native non-English speakers who were proficient in English, and they created a list of queries for each emotion in their native language and inspected the quality of the results visually. The criteria for highquality queries were those that returned a high percentage of human faces showing the intended queried emotions rather than drawings, graphics, or non-human objects. A total of 1250 search queries were compiled and used to crawl the search engines in our database. Since a high percentage of results returned by our query terms already contained neutral facial images, no individual query was performed to obtain additional neutral face.</p><p>Three search engines (Google, Bing, and Yahoo) were queried with these 1250 emotion related tags. Other search engines such as Baidu and Yandex were considered. However, they either did not produce a large number of facial images with intended expressions or they did not have available APIs for automatically querying and pulling image URLs into the database. Additionally, queries were combined with negative terms (e.g., "drawing", "cartoon", "animation", "birthday", etc.) to avoid non-human objects as much as possible. Furthermore, since the images of stock photo websites are posed unnaturally and contain watermarks mostly, a list of popular stock photo websites was compiled and the results returned from the stock photo websites were filtered out.</p><p>A total of ?1,800,000 distinct URLs returned for each query were stored in the database. The OpenCV face recognition was used to obtain bounding boxes around each face. A face alignment algorithm via regression local binary features <ref type="bibr" target="#b30">[29]</ref>, <ref type="bibr" target="#b31">[30]</ref> was used to extract 66 facial landmark points. The facial landmark localization technique was trained using the annotations provided from the 300W competition <ref type="bibr" target="#b61">[60]</ref>. More than 1M images containing at least one face with extracted facial landmark points were kept for further processing.</p><p>The average image resolution of faces in AffectNet are 425 ? 425 with STD of 349 ? 349 pixels. We used Microsoft cognitive face API to extract these facial attributes on 50,000 randomly selected images from the database. According to MS face API, 49% of the faces are men. The average estimated age of the faces is 33.01 years with the standard deviation of 16.96 years. In particular, 10.85, 3.9, 30.19, 26.86, 14.46, and 13.75 percent of the faces are in age ranges [0, 10), <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b20">20)</ref>, <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b31">30)</ref>, <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b41">40)</ref>, <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b51">50)</ref> and <ref type="bibr">[50, -)</ref>, respectively. MS face API detected forehead, mouth, and eye occlusions in 4.5, 1.08, and 0.49 percent of the images, respectively. Also, 9.63% of the faces wear glasses, 51.07 and 41.4% of the faces have eye and lip make-ups, respectively. In terms of head pose, the average estimated pitch, yaw, roll are 0.0,-0.7, and -1.19 degrees, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Annotation</head><p>Crowd-sourcing services like Amazon Mechanical Turk are fast, cheap and easy approaches for labeling large databases. The quality of labels obtained from crowd-sourcing services, however, varies considerably among the annotators. Due to these issues and the fact that annotating the valence and arousal requires a deep understanding of the concept, we avoided crowd-sourcing facilities and instead hired 12 fulltime and part-time annotators at the University of Denver to label the database. A total of 450,000 images were given to these expert annotators to label the face in the images into both discrete categorical and continuous dimensional (valence and arousal) models. Due to time and budget constraints each image was annotated by one annotator.</p><p>A software application was developed to annotate the categorical and dimensional (valence and arousal) models of affect. <ref type="figure" target="#fig_1">Figure 2</ref> shows a screen-shot of the annotation application. A comprehensive tutorial including the definition of the categorical and dimensional models of affect with some examples of each category, valence and arousal was given to the annotators. Three training sessions were provided to each annotator, in which the annotator labeled the emotion category, valence and arousal of 200 images and the results were reviewed with the annotators. Necessary feedback was given on both the categorical and dimensional labels. In addition, the annotators tagged the images that have any occlusion on the face. The occlusion criterion was defined as if any part of the face was not visible. If the person in the images wore glasses, but the eyes were visible without any shadow, it was not considered as occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Categorical Model Annotation</head><p>Eleven discrete categories were defined in the categorical model of AffectNet as: Neutral, Happy, Sad, Surprise, Fear, Anger, Disgust, Contempt, None, Uncertain, and Non-face. The None ("None of the eight emotions") category is the type of expression/emotions (such as sleepy, bored, tired, seducing, confuse, shame, focused, etc.) that could not be assigned by annotators to any of the six basic emotions, contempt or neutral. However, valence and arousal could be assigned to these images. The Non-face category was defined as images that: 1) Do not contain a face in the image; 2) Contain a watermark on the face; 3) The face detection algorithm fails and the bounding box is not around the face; 4) The face is a drawing, animation, or painted; and 5) The face is distorted beyond a natural or normal shape, even if an expression could be inferred. If the annotators were uncertain about any of the facial expressions, images were tagged as uncertain. When an image was annotated as Nonface or uncertain, valence and arousal were not assigned to the image.</p><p>The annotators were instructed to select the proper expression category of the face, where the intensity is not important as long as the face depicts the intended emotion. <ref type="table" target="#tab_2">Table 3</ref> shows the number of images in each category. <ref type="table" target="#tab_3">Table 4</ref> indicates the percentage of annotated categories for queried emotion terms. As shown, the happy emotion  had the highest hit-rate (48%), and the rest of the emotions had hit-rates less than 20%. About 15% of all query results were in the No-Face category, as many images from the web contain watermarks, drawings, etc. About 15% of all queried emotions resulted in neutral faces. Among other expressions, disgust, fear, and contempt had the lowest hitrate with only 2.7%, 4%, and 2.4% hit-rates, respectively. As one can see, the majority of the returned images from the search engines were happy or neutral faces. The authors believe that this is because people tend to publish their images with positive expressions rather than negative expressions. <ref type="figure" target="#fig_2">Figure 3</ref> shows a sample image in each category and its intended queries (in parentheses).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Dimensional (Valence &amp; Arousal) Annotation</head><p>The definition of valence and arousal dimensions was adapted from <ref type="bibr" target="#b2">[3]</ref> and was given to annotators in our tutorial as: "Valence refers to how positive or negative an event is, and arousal reflects whether an event is exciting/agitating or calm/soothing". A sample circumplex with estimated positions of several expressions, borrowed from <ref type="bibr" target="#b62">[61]</ref>, was provided in the tutorial as a reference for the annotators. The provided circumplex in the tutorial contained more than 34 complex emotions categories such as suspicious, insulted, impressed, etc., and used to train annotators. The annotators were instructed to consider the intensity of valence and arousal during the annotation. During the annotation process, the annotators were supervised closely and constant necessary feedback was provided when they were uncertain about some images. To model the dimensional affect of valence and arousal, a 2D Cartesian coordinate system was used where the x-axis and y-axis represent the valence and arousal, respectively. Similar to Russell's circumplex space model <ref type="bibr" target="#b2">[3]</ref>, our annotation software did not allow the value of valence and arousal outside of the circumplex. This allows us to convert the Cartesian coordinates to polar coordinates with 0 ? r ? 1 and 0 ? ? &lt; 360. The annotation software showed the value of valence and arousal to the annotators when they selected a point in the circumplex. This helped the annotators to pick more precise locations of valence and arousal with a higher confidence.</p><p>A predefined estimated region of valence and arousal was defined for each categorical emotion in the annotation software (e.g., for happy emotion the valence is in (0.0, 1.0], and the arousal is in [-0.2, 0.5] ). If the annotators select a value of valence and arousal outside of the selected emotion's region, the software indicates a warning message. The annotators were able to proceed, and they were instructed to do so, if they were confident about the value of valence and arousal. The images with the warning messages were marked in the database, for further review by the authors. This helped to avoid mistakes in the annotation of the dimensional model of affect.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Annotation Agreement</head><p>In order to measure the agreement between the annotators, 36,000 images were annotated by two annotators. The annotations were performed fully blind and independently, i.e., the annotators were not aware of the intended query or other annotator's response. The results showed that the annotators agreed on 60.7% of the images. <ref type="table" target="#tab_5">Table 6</ref> shows the agreement between two annotators for different categories. As it is shown, the annotators highly agreed on the Happy and No Face categories, and the highest disagreement occurred in the None category. Visually inspecting some of the images in the None category, the authors believe that the images in this category contain very subtle emotions and they can be easily confused with other categories (the last two example of <ref type="figure" target="#fig_2">Fig. 3</ref> show images in the None category). <ref type="table" target="#tab_4">Table 5</ref> shows various evaluation metrics between the two annotators in the continuous dimensional model of affect. These metrics are defined in Sec. 2.2. We calculated these metrics in two scenarios: 1) the annotators agreed on the category of the image; 2) on all images that are annotated by two annotators. As <ref type="table" target="#tab_4">Table 5</ref> shows, when the annotators agreed on the category of the image, the annotations have a high correlation and sign agreement (SAGR). According to <ref type="table" target="#tab_5">Table 6</ref>, this occurred on only 60.7% images. However, there is less correlation and SAGR on overall images, since the annotators had a different perception of emotions expressed in the images. It can also be seen that the annotators agreed on valence more than arousal. The authors believe that this is because the perception of valence (how positive or negative the emotion is) is easier and less subjective than arousal (how excited or calm the subject is) especially in still images. Comparing the metrics in the existing dimensional databases (shown in <ref type="table" target="#tab_1">Table 2</ref>) with the agreement of human labelers on AffectNet, suggest that AffectNet is a very challenging database and even human annotations have more RMSE than automated methods on existing databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BASELINE</head><p>In this section, two baselines are proposed to classify images in the categorical model and predict the value of valence and arousal in the continuous domain of dimensional model. Since deep Convolutional Neural Networks (CNNs) have been a successful approach to learn appropriate feature abstractions directly from the image and there are many samples in AffectNet necessary to train CNNs, we proposed two simple CNN baselines for both categorical and dimensional models. We also compared the proposed baselines with conventional approaches (Support Vector Machines <ref type="bibr" target="#b63">[62]</ref> and Support Vector Regressions <ref type="bibr" target="#b64">[63]</ref>) learned from hand-crafted features (HOG). In the following sections, we first introduce our training, validation and test sets, and then show the performance of each proposed baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Test, Validation, and Training Sets</head><p>Test set: The subset of the annotated images that are annotated by two annotators is reserved for the test set. To determine the value of valence and arousal in the test set, since there are two responses for one image in the continuous domain, one of the annotations is picked randomly.</p><p>To select the category of image in the categorical model, if there was a disagreement, a favor was given to the intended query, i.e., if one of the annotators labeled the image as the intended query, the image was labeled with the intended query in the test set. This happened in 29.5% of the images with disagreement between the annotators. On the rest of the images with disagreement, one of the annotations was assigned to the image randomly. Since the test set is a random sampling of all images, it is heavily imbalanced. In other words, there are more than 11,000 images with happy expression while it contains only 1,000 images with contemptuous expression. Validation set: Five hundred samples of each category is selected randomly as a validation set. The validation set is used for hyper-parameter tuning, and since it is balanced, there is no need for any skew normalization.</p><p>Training set: The rest of images are considered as training examples. The training examples, as shown in <ref type="table" target="#tab_2">Table 3</ref>, are heavily imbalanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Categorical Model Baseline</head><p>Facial expression data is usually highly skewed. This form of imbalance is commonly referred to as intrinsic variation, i.e., it is a direct result of the nature of expressions in the real world. This happens in both the categorical and dimensional models of affect. For instance, Caridakis et al. <ref type="bibr" target="#b65">[64]</ref> reported that a bias toward quadrant 1 (positive arousal, positive valence) exists in the SAL database. The problem of learning from imbalanced data sets has two challenges. First, training data with an imbalanced distribution often causes learning algorithms to perform poorly on the minority class <ref type="bibr" target="#b66">[65]</ref>. Second, the imbalance in the test/validation data distribution can affect the performance metrics dramatically. Jeni et al. <ref type="bibr" target="#b54">[53]</ref> studied the influence of skew on imbalanced validation set. The study showed that with exception of area under the ROC curve (AUC), all other studied evaluation metrics, i.e., Accuracy, F1-score, Cohens kappa <ref type="bibr" target="#b51">[50]</ref>, Krippendorfs Alpha <ref type="bibr" target="#b52">[51]</ref>, and area under Precision-Recall curve (AUC-PR) are affected by skewed distributions dramatically. While AUC is unaffected by skew, precision-recall curves suggested that AUC may mask poor performance. To avoid or minimize skew-biased estimates of performance, the study suggested to report both skew-normalized scores and the original evaluation.</p><p>We used AlexNet <ref type="bibr" target="#b58">[57]</ref> architecture as our deep CNN baseline. AlexNet consists of five convolution layers, followed by max-pooling and normalization layers, and three fully-connected layers. To train our baseline with an imbalanced training set, four approaches are studied in this paper as Imbalanced learning, Down-Sampling, Up-Sampling, and Weighted-Loss. The imbalanced learning approach was trained with the imbalanced training set without any change in the skew of the dataset. To train the down-sampling approach, we selected a maximum of 15,000 samples from each class. Since there are less than 15,000 samples for some classes such as Disgust, Contempt, and Fear, the resulting training set is semi-balanced. To train the up-sampling approach, we heavily up-sampled the under-represented classes by replicating their samples so that all classes had the same number of samples as the class with maximum samples, i.e., Happy class.</p><p>The weighted-loss approach weighted the loss function for each of the classes by their relative proportion in the training dataset. In other words, the loss function heavily penalizes the networks for misclassifying examples from under-represented classes, while penalizing networks less for misclassifying examples from well-represented classes. The entropy loss formulation for a training example (X, l) is defined as:</p><formula xml:id="formula_5">E = ? K i=1 H l,i log(p i )<label>(6)</label></formula><p>where H l,i denotes row l penalization factor of class i, K is the number of classes, andp i is the predictive softmax with values [0, 1] indicating the predicted probability of each class as:p</p><formula xml:id="formula_6">i = exp(x i ) K j=1 exp(x j )<label>(7)</label></formula><p>Equation <ref type="formula" target="#formula_5">(6)</ref> can be re-written as:</p><formula xml:id="formula_7">E = ? i H l,i log( exp(x i ) j exp(x j ) ) = ? i H l,i x i + i H l,i log( j exp(x j )) = log( j exp(x j )) i H l,i ? i H l,i x i<label>(8)</label></formula><p>The derivate with respect to the prediction x k is:</p><formula xml:id="formula_8">?E ?x k = ? ?x k [log( j exp(x j )) i H l,i ] ? ? ?x k [ i H l,i x i ] = ( i H l,i ) 1 j exp(x j ) ? ?x k j exp(x j ) ? H l,k = ( i H l,i ) exp(x k ) j exp(x j ) ? H l,k = ( i H l,i )p k ? H l,k<label>(9)</label></formula><p>When H = I, the identity, the proposed weighted-loss approach gives the traditional cross-entropy loss function. We used the implemented Infogain loss in Caffe <ref type="bibr" target="#b67">[66]</ref> for this purpose. For simplicity, we used a diagonal matrix defined as:</p><formula xml:id="formula_9">H ij = fi fmin , if i = j 0, otherwise<label>(10)</label></formula><p>where f i is the number of samples of the i th class and f min is the number of samples in the most under-represented class, i.e., Disgust class in this situation. Before training the network, the faces were cropped and resized to 256?256 pixels. No facial registration was performed at this baseline. To augment the data, five crops of 224?224 and their horizontal flips were extracted from the four corners and the center of the image at random during the training phase. The networks were trained for 20 epochs using a batch size of 256. The base learning rate was set to 0.01, and decreased step-wise by a factor of 0.1 every 10,000 iterations. We used a momentum of 0.9. <ref type="table" target="#tab_6">Table 7</ref> shows the top-1 and top-2 F1-Scores for the imbalanced learning, down-sampling, up-sampling, and weighted-loss approaches on the test set. Since the test set is imbalanced, both the skew-normalized and the original scores are reported. The skew normalization is performed by random under-sampling of the classes in the test set. This process is repeated 200 times, and the skew-normalized score is the average of the score on multiple trials. As it is shown, the weighted-loss approach performed better than other approaches in the skew-normalized fashion. The improvement is significant in under-represented classes, i.e., Contempt, Fear, and Disgust. The imbalanced approach performed worst in the Contempt and Disgust categories since there were a few training samples of these classes compared with other classes. The up-sampling approach also did not classify the Contempt and Disgust categories well, since the training samples of these classes were heavily upsampled (almost 20 times), and the network was over-fitted to these samples. Hence the network lost its generalization and performed poorly on these classes of the test set.</p><p>The confusion matrix of the weighted-loss approaches is shown in <ref type="table" target="#tab_7">Table 8</ref>. The weighted-loss approach classified the samples of Contempt and Disgust categories with an acceptable accuracy but did not perform well in Happy and Neutral. This is because the network was not penalized enough for misclassifying examples from these classes. We believe that a better formulation of the weight matrix H based on the number of samples in the mini-batches or other data-driven approaches can improve the recognition of wellrepresented classes. <ref type="table" target="#tab_8">Table 9</ref> shows accuracy, F1-score, Cohens kappa, Krippendorfs Alpha, area under the ROC curve (AUC), and area under the Precision-Recall curve (AUC-PR) on the test sets. Except for the accuracy, all the metrics are calculated in a binary-class manner where the positive class contains the samples labeled by the given category, and the negative class contains the rest. The reported result in <ref type="table" target="#tab_8">Table 9</ref> is the average of these metrics over eight classes. The accuracy is defined in a multi-class manner in which the number of correct predictions is divided by the total number of samples in the test set. The skew-normalization is performed by balancing the distribution of classes in the test set using random under-sampling and averaging over 200 trials. Since the validation set is balanced, there is no need for skewnormalization.</p><p>We compared the performance of CNN baseline with a Support Vector Machine (SVM) <ref type="bibr" target="#b63">[62]</ref>. To train SVM, the faces in the images were cropped and resized to 256?256 pixels. HOG <ref type="bibr" target="#b68">[67]</ref> features were extracted with the cell size of 8. We applied PCA retaining 95% of the variance to reduce the HOG features dimensionality from 36,864 to 6,697 features. We used a linear kernel SVM in Liblinear package <ref type="bibr" target="#b69">[68]</ref> (which is optimized for large-scale linear classification and regression). <ref type="table" target="#tab_8">Table 9</ref> shows the evaluation metrics of SVM. Reported AUC and AUCPR values for SVM are calculated using the LibLinear's resulting decision values. We calculated the scores of predictions using a posterior-probability transformation sigmoid function. Comparing the performance of SVM with the CNN baselines on AffectNet, indicates that CNN models perform better than conventional SVM and HOG features in all metrics.</p><p>We also compared the baseline with an available off-theshelf expression recognition system (Microsoft Cognitive Services emotion API <ref type="bibr" target="#b70">[69]</ref>). The MS cognitive system had an excellent performance on Neutral and Happy categories with an accuracy of 0.94 and 0.85, respectively. However, it performed poorly on other classes with an accuracy of 0.25, 0.27 and 0.04 in the Fear, Disgust and Contempt categories. <ref type="table" target="#tab_8">Table 9</ref> shows the evaluation metrics on the MS cognitive system. Comparing the performance of the MS cognitive with the simple baselines on AffectNet indicates that Affect-Net is a challenging database and a great resource to further improve the performance of facial expression recognition systems. <ref type="figure">Figure 5</ref> shows nine samples of randomly selected misclassified images of the weighted-loss approach and their corresponding ground-truth. As the figure shows, it is really difficult to assign some of the emotions to a single category. Some of the faces have partial similarities in facial features to the misclassified images, such as nose wrinkled in disgust, or eyebrows raised in surprise. This emphasizes the fact that classifying facial expressions in the wild is a challenging task and, as mentioned before, even human annotators agreed on only 60.7% of the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dimensional Model (Valence and Arousal) Baseline</head><p>Predicting dimensional model in the continuous domain is a real-valued regression problem. We used AlexNet <ref type="bibr" target="#b58">[57]</ref> architecture as our deep CNN baseline to predict the value of valence and arousal. Particularly, two separate AlexNets were trained where the last fully-connected layer was replaced with a linear regression layer containing only one neuron. The output of the neuron predicted the value of valence/arousal in continuous domain <ref type="bibr">[-1,1]</ref>. A Euclidean (L2) loss was used to measure the distance between the predicted value (? n ) and actual value of valence/arousal (y n ) as:</p><formula xml:id="formula_10">E = 1 2N N n=1 ||? n ? y n || 2 2<label>(11)</label></formula><p>The faces were cropped and resized to 256?256 pixels. The base learning rate was fixed and set to 0.001 during the training process. We used a momentum of 0.9. Training was continued until a plateau was reached in the Euclidean error of the validation set (approximately 16 epochs with a minibatch size of 256). <ref type="figure">Figure 6</ref> shows the value of training and validation losses over 16K iterations (about 16 epochs).</p><p>We also compared Support Vector Regression (SVR) <ref type="bibr" target="#b64">[63]</ref> with our DNN baseline for predicting valence and arousal in AffectNet. In our experiments, first, the faces in the images were cropped and resized to 256?256 pixels. Histogram of Oriented Gradient (HOG) <ref type="bibr" target="#b68">[67]</ref> features were extracted with the cell size of 8. Afterward, we applied PCA retaining 95%  of the variance of these features to reduce the dimensionality. Two separate SVRs were trained to predict the value of valence and arousal. Liblinear <ref type="bibr" target="#b69">[68]</ref> package was used to implement SVR baseline. <ref type="table" target="#tab_0">Table 10</ref> shows the performances of the proposed baseline and SVR on the test set. As shown, the CNN baseline can predict the value of valence and arousal better than SVR. This is because the high variety of samples in AffectNet allows the CNN to extract more discriminative features than hand-crafted HOG, and therefore it learned a better representation of dimensional affect.</p><p>The RMSE of CNN baseline (AlexNet) between the predicted valence and arousal and the ground-truth are shown in <ref type="figure">Fig. 7</ref>. As illustrated, the CNN baseline has a lower error rate in the center of circumplex. In particular, predicting low-valence mid-arousal and low-arousal mid-valence areas were more challenging. These areas correspond to the expressions of contempt, bored, and sleepy. It should be mentioned that predicting valence and arousal in the wild is a challenging task, and as discussed in Sec. 3.3, the disagreement between two human annotators has RMSE=0.367 and RMSE=0.481 for valence and arousal, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>The analysis of human facial behavior is a very complex and challenging problem. The majority of the techniques for automated facial affect analysis are mainly based on machine learning methodologies, and their performance highly depends on the amount and diversity of annotated training samples. Recently, databases of facial expression and affect in the wild received much attention. However, existing databases of facial affect in the wild only cover one model of affect, have a limited number of subjects, or contain few samples of certain emotions.</p><p>The Internet is a vast source of facial images, most of which are captured in uncontrolled conditions. These images are often taken in the wild under natural conditions. In this paper, we introduced a new publicly available database of a facial Affect from the InterNet (called AffectNet) by querying different search engines using emotion related tags in six different languages. AffectNet contains more than 1M images with faces and extracted landmark points. Twelve human experts manually annotated 450,000 of these images in both the categorical and dimensional (valence and arousal) models and tagged the images that have any occlusion on the face.</p><p>The agreement level of human labelers on a subset of AffectNet showed that expression recognition and predicting valence and arousal in the wild is a challenging task. The two annotators agreed on 60.7% of the category of facial expressions, and there was a large disagreement on the value of valence and arousal (RMSE=0.34 and 0.36) between the two annotators.</p><p>Two simple deep neural network baselines were examined to classify the facial expression images and predict the value of valence and arousal in the continuous domain of dimensional model. Evaluation metrics showed that simple deep neural network baselines trained on AffectNet can perform better than conventional machine learning methods and available off-the-shelf expression recognition systems. AffectNet is by far the largest database of facial expression, valence and arousal in the wild, enabling further progress in the automatic understanding of facial behavior in both categorical and continuous dimensional space. The interested investigators can study categorical and dimensional models in the same corpus, and possibly co-train them to improve the performance of their affective computing systems. It is highly anticipated that the availability of this database for the research community, along with the recent advances in deep neural networks, can improve the performance of automated affective computing systems in recognizing facial expressions and predicting valence and arousal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is partially supported by the NSF grants IIS-1111568 and CNS-1427872. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPUs used for this research. Mohammad H. Mahoor received the BS degree in electronics from the Abadan Institute of Technology, Iran, in 1996, the MS degree in biomedical engineering from the Sharif University of Technology, Iran, in 1998, and the Ph.D. degree in electrical and computer engineering from the University of Miami, Florida, in 2007. He is an Associate Professor of Electrical and Computer Engineering at DU. He does research in the area of computer vision and machine learning including visual object recognition, object tracking and pose estimation, motion estimation, 3D reconstruction, and humanrobot interaction (HRI) such as humanoid social robots for interaction and intervention with children with special needs (e.g., autism) and elderly with depression and dementia. He has received over $3M of research funding from state and federal agencies including the National Science Foundation. He is a Senior Member of IEEE and has published about 100 conference and journal papers.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Sample images in Valence Arousal circumplex categorical models of affect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>A screen-shot of the software application used to annotate categorical and dimensional (valence and arousal) models of affect and the osculation tag if existing. Only one detected face in each image is annotated (shown in the green bounding box).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FearFig. 3 .</head><label>3</label><figDesc>(Fear) Disgust (Disgust) Angry (Angry) Contempt (Happy) Non-face (Surprise) Uncertain (Sad) None (Fear) None (Happy) Samples of queried images from the web and their annotated tags. The queried expression is written in parentheses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>shows the histogram (number of samples in each range/area) of annotated images in a 2D Cartesian coordinate system. As illustrated, there are more samples in the center and the right middle (positive valence and small positive arousal) of the circumplex, which confirms the higher number of Neutral and Happy images in the database compared to other categories in the categorical model.<ref type="bibr" target="#b1">2</ref> 2. A numerical representation of annotated images in each range/area of valence and arousal is provided in the Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Histogram (number of frames in each range/area) of valence and arousal annotations (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Samples of miss-classified images. Their corresponding groundtruth is given in parentheses. Euclidean error of training valence and arousal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Ali</head><label></label><figDesc>Mollahosseini received the BSc degree in computer software engineering from the Iran University of Science and Technology, Iran, in 2006, and the MSc degree in computer engineering -artificial intelligence from AmirKabir University, Iran, in 2010. He is currently working toward the Ph.D. degree and is a graduate research assistant in the Department of Electrical and Computer Engineering at the University of Denver. His research interests include deep neural networks for the analysis of facial expression, developing humanoid social robots and computer vision. Behzad Hasani received the BSc degree in computer hardware engineering from Khaje Nasir Toosi University of Technology, Tehran, Iran, in 2013, and the MSc degree in computer engineering -artificial intelligence from Iran University of Science and Technology, Tehran, Iran, in 2015. He is currently pursuing his Ph.D. degree in electrical &amp; computer engineering and is a graduate research assistant in the Department of Electrical and Computer Engineering at the University of Denver. His research interests include Computer Vision, Machine Learning, and Deep Neural Networks, especially on facial expression analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>*Fig. 8 .</head><label>8</label><figDesc>A1 to A12 indicate Annotators 1 to 12 ** Zero means that there were no common images between the two annotators Sample images in Valence Arousal circumplex with their corresponding Valence and Arousal values (V: Valence, A: Arousal).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 The</head><label>1</label><figDesc>Summary and Characteristics of Reviewed Databases in Affect Recognition</figDesc><table><row><cell>Database</cell><cell>Database information</cell><cell># of Subjects</cell><cell>Condition</cell><cell>Affect Modeling</cell></row><row><cell>CK+ [9]</cell><cell>-Frontal and 30 degree images</cell><cell>-123</cell><cell>-Controlled -Posed</cell><cell>-30 AUs -7 emotion categories</cell></row><row><cell>MultiPie [11]</cell><cell>-Around 750,000 images -Under multiple viewpoints and illuminations</cell><cell>-337</cell><cell>-Controlled -Posed</cell><cell>-7 emotion categories</cell></row><row><cell>MMI [10]</cell><cell>-Subjects portrayed 79 series of facial expressions -Image sequence of frontal and side view are captured</cell><cell>-25</cell><cell>-Controlled -Posed &amp; Spontaneous</cell><cell>-31 AUs -Six basic expression</cell></row><row><cell>DISFA [14]</cell><cell>-Video of subjects while watching a four minutes video -Clip are recorded by a stereo camera</cell><cell>-27</cell><cell>-Controlled -Spontaneous</cell><cell>-12 AUs</cell></row><row><cell>SALDB [23], [24]</cell><cell>-SAL -Audiovisual (facial expression,shoulder, audiocues) -20 facial feature points, 5 shoulder points for video</cell><cell>-4</cell><cell>-Controlled -Spontaneous</cell><cell>-Valence -Quantized [23] -Continuous [24]</cell></row><row><cell>RELOCA [25]</cell><cell>-Multi-modal audio, video, ECG and EDA</cell><cell>-46</cell><cell>-Controlled -Spontaneous</cell><cell>-Valence and arousal (continuous) -Self assessment</cell></row><row><cell>AM-FED [15]</cell><cell>-242 facial videos</cell><cell>-242</cell><cell>-Spontaneous</cell><cell>-14 AUs</cell></row><row><cell>DEAP [26]</cell><cell>-40 one-minute long videos shown to subjects -EEG signals recorded</cell><cell>-32</cell><cell>-Controlled -Spontaneous</cell><cell>-Valence and arousal (continuous) -Self assessment</cell></row><row><cell>AFEW [18]</cell><cell>-Videos</cell><cell>-330</cell><cell>-Wild</cell><cell>-7 emotion categories</cell></row><row><cell>FER-2013 [19]</cell><cell>-Images queried from web</cell><cell>-?35,887</cell><cell>-Wild</cell><cell>-7 emotion categories</cell></row><row><cell></cell><cell>-Images queried from web</cell><cell></cell><cell></cell><cell>-12 AUs annotated</cell></row><row><cell>EmotioNet [22]</cell><cell>-100,000 images annotated manually</cell><cell>-?100,000</cell><cell>-Wild</cell><cell>-23 emotion categories</cell></row><row><cell></cell><cell>-900,000 images annotated automatically</cell><cell></cell><cell></cell><cell>based on AUs</cell></row><row><cell>Aff-Wild [21]</cell><cell>-500 videos from YouTube</cell><cell>-500</cell><cell>-Wild</cell><cell>-Valence and arousal (continuous)</cell></row><row><cell>FER-Wild [20]</cell><cell>-24,000 images from web</cell><cell>-?24,000</cell><cell>-Wild</cell><cell>-7 emotion categories</cell></row><row><cell>AffectNet (This work)</cell><cell>-1,000,000 images with facial landmarks -450,000 images annotated manually</cell><cell>-?450,000</cell><cell>-Wild</cell><cell>-8 emotion categories -Valence and arousal (continuous)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>State-of-the-art Algorithms and Their Performance on the Databases Listed in Table 1. Inception based Convolutional Neural Network (CNN) -Subject-independent and cross-database experiments -93.2% accuracy on CK+ -94.7% accuracy on MultiPie Shan et al. [44] MMI -Different SVM kernels trained with LBP features -Subject-independent and cross-database experiments -86.9% accuracy on MMI Zhang et al. [45] DISFA -lpnorm multi-task multiple kernel learning -learning shared kernels from a given set of base kernels -0.70 F1-score on DISFA -0.93 recognition rate on DISFA</figDesc><table><row><cell>Work</cell><cell>Database</cell><cell>Method</cell><cell>Results</cell></row><row><cell cols="3">Mollahosseini et al. [43] -Nicolaou CK+ MultiPie et al. [24] SALDB -Bidirectional LSTM -Trained on multiple engineered features extracted from audio, facial geometry , and shoulder</cell><cell>-Leave-one-sequence-out -BLSTM-NN outperform SVR -Valence (RMSE=0.15 and CC=0.796) -Arousal (RMSE=0.21 and CC=0.642)</cell></row><row><cell></cell><cell></cell><cell>-Multiple stack of bidirectional LSTM (DBLSTM-RNN)</cell><cell>-Winner of AVEC 2015 challenge</cell></row><row><cell>He et al. [46]</cell><cell>RECOLA</cell><cell>-Trained on engineered features extracted from audio (LLDs),</cell><cell>-Valence (RMSE=0.104 and CC=0.616)</cell></row><row><cell></cell><cell></cell><cell>video (LPQ-TOP), 52 ECG features, and 22 EDA features</cell><cell>-Arousal (RMSE=0.121 and CC=0.753)</cell></row><row><cell>McDuff et al. [15]</cell><cell>AM-FED</cell><cell>-HOG features extracted -SVM with RBF kernel</cell><cell>-AUC 0.90, 0.72 and 0.70 for smile, AU2 and AU4 respectively</cell></row><row><cell></cell><cell></cell><cell>-Gaussian naive Bayes classifier</cell><cell>-0.39 F1-score on Arousal</cell></row><row><cell>Koelstra et al. [26]</cell><cell>DEAP</cell><cell>-EEG, physiological signals, and multimedia features</cell><cell>-0.37 F1-score on Valence</cell></row><row><cell></cell><cell></cell><cell>-Binary classification of low/high arousal, valence, and liking</cell><cell>-0.40 F1-score on Liking</cell></row><row><cell>Fan et al. [47]</cell><cell>AFEW</cell><cell>-Trained on both video and audio. -VGG network are followed by LSTMs and combined with 3D convolution</cell><cell>-Winner of EmotiW 2016 challenge -56.16% accuracy on AFEW</cell></row><row><cell>Tang et al. [48]</cell><cell>FER-2013</cell><cell>-CNN with linear one-vs-all SVM at the top</cell><cell>-Winner of the FER challenge -71.2% accuracy on test set</cell></row><row><cell>Benitez-Quiroz et al. [22]</cell><cell>EmotioNet</cell><cell>-New face feature extraction method using Gabor filters -Subject-independent and cross-database experiments -KSDA classification</cell><cell>-?80% AU detection on EmotioNet</cell></row><row><cell>Mollahosseini et al. [20]</cell><cell>FER-Wild</cell><cell>-Trained on AlexNet -Noise estimation methods used</cell><cell>-82.12% accuracy on FER-Wild</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>Number of Annotated Images in Each Category</figDesc><table><row><cell>Expression</cell><cell>Number</cell></row><row><cell>Neutral</cell><cell>80,276</cell></row><row><cell>Happy</cell><cell>146,198</cell></row><row><cell>Sad</cell><cell>29,487</cell></row><row><cell>Surprise</cell><cell>16,288</cell></row><row><cell>Fear</cell><cell>8,191</cell></row><row><cell>Disgust</cell><cell>5,264</cell></row><row><cell>Anger</cell><cell>28,130</cell></row><row><cell>Contempt</cell><cell>5,135</cell></row><row><cell>None</cell><cell>35,322</cell></row><row><cell>Uncertain</cell><cell>13,163</cell></row><row><cell>Non-Face</cell><cell>88,895</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc>Percentage of Annotated Categories for Queried Emotion Terms (%)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Query Expression</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>HA</cell><cell>SA</cell><cell>SU</cell><cell>FE</cell><cell>DI</cell><cell>AN</cell><cell>CO</cell></row><row><cell></cell><cell>NE*</cell><cell>17.3</cell><cell>16.3</cell><cell>13.9</cell><cell>17.8</cell><cell>17.8</cell><cell>16.1</cell><cell>20.1</cell></row><row><cell>Annotated Expression</cell><cell>HA SA SU FE DI AN CO NO UN</cell><cell>48.9 2.6 2.7 0.7 0.6 2.8 1.3 5.4 1.3</cell><cell>27.2 15.7 3.1 1.2 0.7 4.5 0.9 8.7 3.1</cell><cell>30.4 4.8 16 4.2 0.7 3.8 0.4 4.8 4.3</cell><cell>28.6 5.8 4.4 4 0.9 5.6 1.1 8.1 3.1</cell><cell>33 4.5 3.6 1.5 2.7 6 1.1 8.8 4.1</cell><cell>29.5 5.4 3.4 1.4 1.1 12.2 1.2 9.3 3.7</cell><cell>30.1 4.6 4.1 1.3 1 6.1 2.4 11.2 2.7</cell></row><row><cell></cell><cell>NF</cell><cell>16.3</cell><cell>18.6</cell><cell>16.7</cell><cell>20.6</cell><cell>16.9</cell><cell>16.8</cell><cell>16.3</cell></row></table><note>* NE, HA, SA, SU, FE, DI, AN, CO, NO, UN , and NF stand for Neutral, Happy, Sad, Surprise, Fear, Anger, Disgust, Contempt, None, Uncertain, and Non-face categories, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="5">Annotators' Agreement in Dimensional Model of Affect</cell></row><row><cell></cell><cell cols="2">Same Category</cell><cell>All</cell><cell></cell></row><row><cell></cell><cell cols="2">Valence Arousal</cell><cell cols="2">Valence Arousal</cell></row><row><cell>RMSE</cell><cell>0.190</cell><cell>0.261</cell><cell>0.340</cell><cell>0.362</cell></row><row><cell>CORR</cell><cell>0.951</cell><cell>0.766</cell><cell>0.823</cell><cell>0.567</cell></row><row><cell>SAGR</cell><cell>0.906</cell><cell>0.709</cell><cell>0.815</cell><cell>0.667</cell></row><row><cell>CCC</cell><cell>0.951</cell><cell>0.746</cell><cell>0.821</cell><cell>0.551</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6 Agreement</head><label>6</label><figDesc>Between Two Annotators in Categorical Model of Affect (%)</figDesc><table><row><cell></cell><cell>Neutral</cell><cell>Happy</cell><cell>Sad</cell><cell>Surprise</cell><cell>Fear</cell><cell>Disgust</cell><cell>Anger</cell><cell>Contempt</cell><cell>None</cell><cell>Uncertain</cell><cell>Non-Face</cell></row><row><cell>Neutral</cell><cell>50.8</cell><cell>7.0</cell><cell>9.1</cell><cell>2.8</cell><cell>1.1</cell><cell>1.0</cell><cell>4.8</cell><cell>5.3</cell><cell>11.1</cell><cell>1.9</cell><cell>5.1</cell></row><row><cell>Happy</cell><cell>6.3</cell><cell>79.6</cell><cell>0.6</cell><cell>1.7</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>3.0</cell><cell>4.6</cell><cell>1.0</cell><cell>2.2</cell></row><row><cell>Sad</cell><cell>11.8</cell><cell>0.9</cell><cell>69.7</cell><cell>1.2</cell><cell>3.4</cell><cell>1.3</cell><cell>4.0</cell><cell>0.3</cell><cell>3.5</cell><cell>1.2</cell><cell>2.6</cell></row><row><cell>Surprise</cell><cell>2.0</cell><cell>3.8</cell><cell>1.6</cell><cell>66.5</cell><cell>14.0</cell><cell>0.8</cell><cell>1.9</cell><cell>0.6</cell><cell>4.2</cell><cell>1.9</cell><cell>2.7</cell></row><row><cell>Fear</cell><cell>3.1</cell><cell>1.5</cell><cell>3.8</cell><cell>15.3</cell><cell>61.1</cell><cell>2.5</cell><cell>7.2</cell><cell>0.0</cell><cell>1.9</cell><cell>0.4</cell><cell>3.3</cell></row><row><cell>Disgust</cell><cell>1.5</cell><cell>0.8</cell><cell>3.6</cell><cell>1.2</cell><cell>3.5</cell><cell>67.6</cell><cell>13.1</cell><cell>1.7</cell><cell>2.7</cell><cell>2.3</cell><cell>2.1</cell></row><row><cell>Anger</cell><cell>8.1</cell><cell>1.2</cell><cell>7.5</cell><cell>1.7</cell><cell>2.9</cell><cell>4.4</cell><cell>62.3</cell><cell>1.3</cell><cell>5.5</cell><cell>1.9</cell><cell>3.3</cell></row><row><cell>Contempt</cell><cell>10.2</cell><cell>7.5</cell><cell>2.1</cell><cell>0.5</cell><cell>0.5</cell><cell>4.4</cell><cell>2.1</cell><cell>66.9</cell><cell>3.7</cell><cell>1.5</cell><cell>0.6</cell></row><row><cell>None</cell><cell>22.6</cell><cell>12.0</cell><cell>14.5</cell><cell>8.0</cell><cell>6.0</cell><cell>2.3</cell><cell>16.9</cell><cell>1.3</cell><cell>9.6</cell><cell>4.3</cell><cell>2.6</cell></row><row><cell>Uncertain</cell><cell>13.5</cell><cell>12.1</cell><cell>7.8</cell><cell>7.3</cell><cell>4.0</cell><cell>4.5</cell><cell>6.2</cell><cell>2.6</cell><cell>12.3</cell><cell>20.6</cell><cell>8.9</cell></row><row><cell>Non-Face</cell><cell>3.7</cell><cell>3.8</cell><cell>1.7</cell><cell>1.1</cell><cell>0.9</cell><cell>0.4</cell><cell>1.7</cell><cell>0.4</cell><cell>1.2</cell><cell>1.4</cell><cell>83.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7 F1</head><label>7</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">-Scores of four different approaches of training AlexNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Imbalanced</cell><cell></cell><cell></cell><cell cols="2">Down-Sampling</cell><cell></cell><cell></cell><cell cols="2">Up-Sampling</cell><cell></cell><cell></cell><cell cols="2">Weighted-Loss</cell><cell></cell></row><row><cell></cell><cell cols="2">Top-1</cell><cell cols="2">Top-2</cell><cell cols="2">Top-1</cell><cell cols="2">Top-2</cell><cell cols="2">Top-1</cell><cell cols="2">Top-2</cell><cell cols="2">Top-1</cell><cell cols="2">Top-2</cell></row><row><cell></cell><cell cols="16">Orig* Norm* Orig Norm Orig Norm Orig Norm Orig Norm Orig Norm Orig Norm Orig Norm</cell></row><row><cell>Neutral</cell><cell>0.63</cell><cell>0.49</cell><cell>0.82</cell><cell>0.66</cell><cell>0.58</cell><cell>0.49</cell><cell>0.78</cell><cell>0.70</cell><cell>0.61</cell><cell>0.50</cell><cell>0.81</cell><cell>0.64</cell><cell>0.57</cell><cell>0.52</cell><cell>0.81</cell><cell>0.77</cell></row><row><cell>Happy</cell><cell>0.88</cell><cell>0.65</cell><cell>0.95</cell><cell>0.80</cell><cell>0.85</cell><cell>0.68</cell><cell>0.92</cell><cell>0.85</cell><cell>0.85</cell><cell>0.71</cell><cell>0.95</cell><cell>0.80</cell><cell>0.82</cell><cell>0.73</cell><cell>0.92</cell><cell>0.88</cell></row><row><cell>Sad</cell><cell>0.63</cell><cell>0.60</cell><cell>0.84</cell><cell>0.81</cell><cell>0.64</cell><cell>0.60</cell><cell>0.81</cell><cell>0.78</cell><cell>0.6</cell><cell>0.57</cell><cell>0.81</cell><cell>0.77</cell><cell>0.63</cell><cell>0.61</cell><cell>0.83</cell><cell>0.81</cell></row><row><cell>Surprise</cell><cell>0.61</cell><cell>0.64</cell><cell>0.84</cell><cell>0.86</cell><cell>0.53</cell><cell>0.63</cell><cell>0.75</cell><cell>0.83</cell><cell>0.57</cell><cell>0.66</cell><cell>0.80</cell><cell>0.81</cell><cell>0.51</cell><cell>0.63</cell><cell>0.77</cell><cell>0.86</cell></row><row><cell>Fear</cell><cell>0.52</cell><cell>0.54</cell><cell>0.78</cell><cell>0.79</cell><cell>0.54</cell><cell>0.57</cell><cell>0.80</cell><cell>0.82</cell><cell>0.56</cell><cell>0.58</cell><cell>0.75</cell><cell>0.76</cell><cell>0.56</cell><cell>0.66</cell><cell>0.79</cell><cell>0.86</cell></row><row><cell>Disgust</cell><cell>0.52</cell><cell>0.55</cell><cell>0.76</cell><cell>0.78</cell><cell>0.53</cell><cell>0.64</cell><cell>0.74</cell><cell>0.81</cell><cell>0.53</cell><cell>0.59</cell><cell>0.70</cell><cell>0.72</cell><cell>0.48</cell><cell>0.66</cell><cell>0.69</cell><cell>0.83</cell></row><row><cell>Anger</cell><cell>0.65</cell><cell>0.59</cell><cell>0.83</cell><cell>0.80</cell><cell>0.62</cell><cell>0.60</cell><cell>0.79</cell><cell>0.78</cell><cell>0.63</cell><cell>0.59</cell><cell>0.81</cell><cell>0.77</cell><cell>0.60</cell><cell>0.60</cell><cell>0.81</cell><cell>0.81</cell></row><row><cell>Contempt</cell><cell>0.08</cell><cell>0.08</cell><cell>0.49</cell><cell>0.49</cell><cell>0.22</cell><cell>0.32</cell><cell>0.60</cell><cell>0.70</cell><cell>0.15</cell><cell>0.18</cell><cell>0.42</cell><cell>0.42</cell><cell>0.27</cell><cell>0.59</cell><cell>0.58</cell><cell>0.79</cell></row></table><note>*Orig and Norm stand for Original and skew-Normalized, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 8 Confusion</head><label>8</label><figDesc>Matrix of Weighted-Loss Approach on the Test Set</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Predicted</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>NE</cell><cell>HA</cell><cell>SA</cell><cell>SU</cell><cell>FE</cell><cell>DI</cell><cell>AN</cell><cell>CO</cell></row><row><cell></cell><cell>NE</cell><cell>53.3</cell><cell>2.8</cell><cell>9.8</cell><cell>8.7</cell><cell>1.7</cell><cell>2.5</cell><cell cols="2">10.4 10.9</cell></row><row><cell></cell><cell>HA</cell><cell>4.5</cell><cell>72.8</cell><cell>1.1</cell><cell>6.0</cell><cell>0.6</cell><cell>1.7</cell><cell>1.0</cell><cell>12.2</cell></row><row><cell>Actual</cell><cell>SA SU FE DI</cell><cell>13.0 3.4 1.5 2.0</cell><cell>1.3 1.2 1.5 2.2</cell><cell>61.7 1.7 4.6 5.8</cell><cell cols="2">3.6 69.9 18.9 5.8 13.5 70.4 3.3 6.2</cell><cell cols="2">4.4 1.7 4.2 68.6 10.6 9.2 2.8 4.3</cell><cell>1.2 0.5 0.2 1.3</cell></row><row><cell></cell><cell>AN</cell><cell>6.2</cell><cell>1.2</cell><cell>5.0</cell><cell>3.2</cell><cell>5.8</cell><cell cols="2">11.1 65.8</cell><cell>1.9</cell></row><row><cell></cell><cell cols="3">CO 16.2 13.1</cell><cell>3.5</cell><cell>3.1</cell><cell>0.5</cell><cell>4.3</cell><cell>5.7</cell><cell>53.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 9 Evaluation</head><label>9</label><figDesc>Metrics and Comparison of CNN baselines, SVM and MS Cognitive on Categorical Model of Affect.</figDesc><table><row><cell></cell><cell cols="2">Imbalanced</cell><cell cols="4">CNN Baselines Down-Sampling Up-Sampling</cell><cell cols="2">Weighted-Loss</cell><cell></cell><cell>SVM</cell><cell cols="2">MS Cognitive</cell></row><row><cell></cell><cell cols="2">Orig Norm</cell><cell>Orig</cell><cell>Norm</cell><cell cols="2">Orig Norm</cell><cell>Orig</cell><cell>Norm</cell><cell cols="2">Orig Norm</cell><cell>Orig</cell><cell>Norm</cell></row><row><cell>Accuracy</cell><cell>0.72</cell><cell>0.54</cell><cell>0.68</cell><cell>0.58</cell><cell>0.68</cell><cell>0.57</cell><cell>0.64</cell><cell>0.63</cell><cell>0.60</cell><cell>0.37</cell><cell>0.68</cell><cell>0.48</cell></row><row><cell>F 1 -Score</cell><cell>0.57</cell><cell>0.52</cell><cell>0.56</cell><cell>0.57</cell><cell>0.56</cell><cell>0.55</cell><cell>0.55</cell><cell>0.62</cell><cell>0.37</cell><cell>0.31</cell><cell>0.51</cell><cell>0.45</cell></row><row><cell>Kappa</cell><cell>0.53</cell><cell>0.46</cell><cell>0.51</cell><cell>0.51</cell><cell>0.52</cell><cell>0.49</cell><cell>0.5</cell><cell>0.57</cell><cell>0.32</cell><cell>0.25</cell><cell>0.46</cell><cell>0.40</cell></row><row><cell>Alpha</cell><cell>0.52</cell><cell>0.45</cell><cell>0.51</cell><cell>0.51</cell><cell>0.51</cell><cell>0.48</cell><cell>0.5</cell><cell>0.57</cell><cell>0.31</cell><cell>0.22</cell><cell>0.46</cell><cell>0.37</cell></row><row><cell>AUC</cell><cell>0.85</cell><cell>0.80</cell><cell>0.82</cell><cell>0.85</cell><cell>0.82</cell><cell>0.84</cell><cell>0.86</cell><cell>0.86</cell><cell>0.77</cell><cell>0.70</cell><cell>0.83</cell><cell>0.77</cell></row><row><cell>AUCPR</cell><cell>0.56</cell><cell>0.55</cell><cell>0.54</cell><cell>0.57</cell><cell>0.55</cell><cell>0.56</cell><cell>0.58</cell><cell>0.64</cell><cell>0.39</cell><cell>0.37</cell><cell>0.52</cell><cell>0.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 10</head><label>10</label><figDesc></figDesc><table><row><cell cols="5">Baselines' Performances of Predicting Valence and Arousal on Test Set</cell></row><row><cell></cell><cell cols="2">CNN (AlexNet)</cell><cell>SVR</cell><cell></cell></row><row><cell></cell><cell cols="2">Valence Arousal</cell><cell cols="2">Valence Arousal</cell></row><row><cell>RMSE</cell><cell>0.394</cell><cell>0.402</cell><cell>0.494</cell><cell>0.400</cell></row><row><cell>CORR</cell><cell>0.602</cell><cell>0.539</cell><cell>0.429</cell><cell>0.360</cell></row><row><cell>SAGR</cell><cell>0.728</cell><cell>0.670</cell><cell>0.619</cell><cell>0.748</cell></row><row><cell>CCC</cell><cell>0.541</cell><cell>0.450</cell><cell>0.340</cell><cell>0.199</cell></row><row><cell cols="5">Fig. 7. RMSE of predicted valence and arousal using AlexNet and</cell></row><row><cell cols="3">Euclidean (L2) loss (Best viewed in color).</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 11</head><label>11</label><figDesc>Samples of Annotated Categories for Queried Emotion Terms</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Queried Expression</cell><cell></cell></row><row><cell></cell><cell>Happy</cell><cell>Sad</cell><cell>Surprise</cell><cell>Fear</cell><cell>Disgust</cell><cell>Anger</cell><cell>Contempt</cell></row><row><cell></cell><cell>Neutral</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Annotated Expression</cell><cell>Happy Sad</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Surprise</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Fear</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Disgust</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Anger</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Contempt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>None</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Uncertain</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Non-Face</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 12</head><label>12</label><figDesc>Samples of Annotated Images by Two Annotators (Randomly selected)</figDesc><table><row><cell>Annotator 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 13</head><label>13</label><figDesc>Agreement percentage between Two Annotators in Categorical Model of Affect (%)</figDesc><table><row><cell></cell><cell>A1*</cell><cell>A2</cell><cell>A3</cell><cell>A4</cell><cell>A5</cell><cell>A6</cell><cell>A7</cell><cell>A8</cell><cell>A9</cell><cell>A10</cell><cell>A11</cell><cell>A12</cell></row><row><cell>A1</cell><cell>0.0**</cell><cell>69</cell><cell>70</cell><cell>68</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>A2</cell><cell>69</cell><cell>0</cell><cell>64.9</cell><cell>68.3</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>64.7</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>A3</cell><cell>70</cell><cell>64.9</cell><cell>0</cell><cell>70.6</cell><cell>67.4</cell><cell>69.9</cell><cell>63</cell><cell>62.3</cell><cell>0</cell><cell>48.1</cell><cell>0</cell><cell>0</cell></row><row><cell>A4</cell><cell>68</cell><cell>68.3</cell><cell>70.6</cell><cell>0</cell><cell>70.4</cell><cell>70.8</cell><cell>64.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 14</head><label>14</label><figDesc>Number of annotated images in each range/area of valence and arousal</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 15 Evaluation</head><label>15</label><figDesc>Metrics and Comparison of CNN baselines, SVM and MS Cognitive on Categorical Model of Affect on the Validation Set.</figDesc><table><row><cell></cell><cell>Imbalanced</cell><cell cols="4">CNN Baselines Down-Sampling Up-Sampling</cell><cell cols="2">Weighted-Loss</cell><cell>SVM</cell><cell>MS Cognitive</cell></row><row><cell>Accuracy</cell><cell>0.40</cell><cell>0.50</cell><cell></cell><cell>0.47</cell><cell></cell><cell cols="2">0.58</cell><cell>0.30</cell><cell>0.37</cell></row><row><cell>F 1-Score</cell><cell>0.34</cell><cell>0.49</cell><cell></cell><cell>0.44</cell><cell></cell><cell cols="2">0.58</cell><cell>0.24</cell><cell>0.33</cell></row><row><cell>Kappa</cell><cell>0.32</cell><cell>0.42</cell><cell></cell><cell>0.38</cell><cell></cell><cell cols="2">0.51</cell><cell>0.18</cell><cell>0.27</cell></row><row><cell>Alpha</cell><cell>0.39</cell><cell>0.42</cell><cell></cell><cell>0.37</cell><cell></cell><cell cols="2">0.51</cell><cell>0.13</cell><cell>0.23</cell></row><row><cell>AUCPR</cell><cell>0.42</cell><cell>0.48</cell><cell></cell><cell>0.44</cell><cell></cell><cell cols="2">0.56</cell><cell>0.30</cell><cell>0.38</cell></row><row><cell>AUC</cell><cell>0.74</cell><cell>0.47</cell><cell></cell><cell>0.75</cell><cell></cell><cell cols="2">0.82</cell><cell>0.68</cell><cell>0.70</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE 16</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="7">Baselines' Performances of Predicting Valence and Arousal on the Validation Set</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CNN (AlexNet)</cell><cell></cell><cell>SVR</cell></row><row><cell></cell><cell></cell><cell cols="3">Valence Arousal</cell><cell cols="3">Valence Arousal</cell></row><row><cell></cell><cell></cell><cell>RMSE</cell><cell>0.37</cell><cell>0.41</cell><cell cols="2">0.55</cell><cell>0.42</cell></row><row><cell></cell><cell></cell><cell>CORR</cell><cell>0.66</cell><cell>0.54</cell><cell cols="2">0.35</cell><cell>0.31</cell></row><row><cell></cell><cell></cell><cell>SAGR</cell><cell>0.74</cell><cell>0.65</cell><cell cols="2">0.57</cell><cell>0.68</cell></row><row><cell></cell><cell></cell><cell>CCC</cell><cell>0.60</cell><cell>0.34</cell><cell cols="2">0.30</cell><cell>0.18</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Affective computing: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Affective Computing and Intelligent Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="981" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Constants across cultures in the face and emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">124</biblScope>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A circumplex model of affect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1161" to="1178" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Facial action coding system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<idno>1977. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Emfacs-7: Emotional facial action coding system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">36</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of California at San Francisco</orgName>
		</respStmt>
	</monogr>
	<note>Unpublished manuscript</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compound facial expressions of emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coding facial expressions with gabor wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kamachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gyoba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Third IEEE International Conference on</title>
		<meeting>Third IEEE International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="200" to="205" />
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recognizing action units for facial expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-I</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="115" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<biblScope unit="page">2010</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society Conference on. IEEE</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Web-based database for facial expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rademaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo, 2005. ICME 2005. IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multipie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The timing of facial motion in posed and spontaneous smiles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Wavelets, Multiresolution and Information Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="121" to="132" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spontaneous vs. posed facial behavior: automatic analysis of brow actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international conference on Multimodal interfaces</title>
		<meeting>the 8th international conference on Multimodal interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="162" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Disfa: A spontaneous facial action intensity database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="160" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Affective Computing</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Affectiva-mit facial expression dataset (am-fed): Naturalistic and spontaneous facial expressions collected</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kaliouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Senechal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The belfast induced natural emotion database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sneddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcrorie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hanratty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatically recognizing facial expression: Predicting engagement and frustration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Grafsgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Wiggins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Lester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDM</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild challenge 2013</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM on International conference on multimodal interaction</title>
		<meeting>the 15th ACM on International conference on multimodal interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Challenges in representation learning: A report on three machine learning contests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Facial expression recognition from world wild web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abdollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Facial affect &quot;in-the-wild&quot;: A survey and a new database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, Affect &quot;in-the-wild&quot; Workshop</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR16)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR16)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Audio-visual classification and fusion of spontaneous affective data in likelihood space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2010 20th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3695" to="3699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="92" to="105" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Introducing the recola multimodal corpus of remote collaborative and affective interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sonderegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition (FG)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deap: A database for emotion analysis; using physiological signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koelstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Muhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nijholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2106" to="2112" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bidirectional warping of active appearance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="875" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">face-alignment-in-3000fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://github.com/yulequan/face-alignment-in-3000fps" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kernel optimization in discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">C</forename><surname>Hamsici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="631" to="638" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Painful data: The unbc-mcmaster shoulder pain expression archive database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Prkachin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition and Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Measuring emotion: the selfassessment manikin and the semantic differential</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="49" to="59" />
		</imprint>
	</monogr>
	<note>Journal of behavior therapy and experimental psychiatry</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Avec 2011-the first international audio/visual emotion challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Affective Computing and Intelligent Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Avec 2012: the continuous audio/visual emotion challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM international conference on Multimodal interaction</title>
		<meeting>the 14th ACM international conference on Multimodal interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Avec 2013: the continuous audio/visual emotion and depression recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bilakhia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schnieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge</title>
		<meeting>the 3rd ACM international workshop on Audio/visual emotion challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Avec 2014: 3d dimensional affect and depression recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Almaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krajewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 4th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Avec 2015: The 5th international audio/visual emotion challenge and workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Avec 2016-depression, mood, and emotion recognition workshop and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.01600</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schroder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Going deeper in facial expression recognition using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local binary patterns: A comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="803" to="816" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Facial expression recognition using {l} {p}-norm mkl multiclass-svm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mavadati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Vision and Applications</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multimodal affective dimension prediction using deep bidirectional long shortterm memory recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sahli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 5th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Video-based emotion recognition using cnn-rnn and c3d hybrid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 18th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="445" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deep learning using linear support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.0239</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Beyond accuracy, f-score and roc: a family of discriminant measures for performance evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sokolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Australasian Joint Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1015" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A coefficient of agreement for nominal scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Estimating the reliability, systematic error and random error of interval data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krippendorff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Intraclass correlations: uses in assessing rater reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Shrout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">420</biblScope>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Facing imbalanced data-recommendations for the use of performance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Affective Computing and Intelligent Interaction (ACII), 2013 Humaine Association Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Oriented principal component analysis for large margin classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bermejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cabestany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1447" to="1461" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pca-based dictionary building for accurate facial expression recognition via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fatemizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1082" to="1092" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="476" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: Database and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Seeing stars of valence and arousal in blog posts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Paltoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="123" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Support vector regression machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">User and context adaptive neural networks for emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Caridakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karpouzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kollias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2553" to="2562" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Microsoft cognitive services -emotion api</title>
		<idno>12/01/2016). 11</idno>
		<ptr target="https://www.microsoft.com/cognitive-services/en-us/emotion-api" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valence</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>2,0] [0,.2] [.2,.4] [.4,.6] [.6,.8] [.8,1</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
