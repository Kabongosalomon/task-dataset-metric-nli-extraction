<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AN EMPIRICAL INVESTIGATION OF 3D ANOMALY DETECTION AND SEGMENTATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliahu</forename><surname>Horwitz</surname></persName>
							<email>eliahu.horwitz@mail.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
							<email>yedid.hoshen@mail.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AN EMPIRICAL INVESTIGATION OF 3D ANOMALY DETECTION AND SEGMENTATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly detection and segmentation in images has made tremendous progress in recent years while 3D information has often been ignored. The objective of this paper is to further understand the benefit and role of 3D as opposed to color in image anomaly detection. Our study begins by presenting a surprising finding: standard color-only anomaly segmentation methods, when applied to 3D datasets, significantly outperform all current methods. On the other hand, we observe that color-only methods are insufficient for images containing geometric anomalies where shape cannot be unambiguously inferred from 2D. This suggests that better 3D methods are needed. We investigate different representations for 3D anomaly detection and discover that handcrafted orientation-invariant representations are unreasonably effective on this task. We uncover a simple 3D-only method that outperforms all recent approaches while not using deep learning, external pretraining datasets, or color information. As the 3D-only method cannot detect color and texture anomalies, we combine it with 2D color features, granting us the best current results by a large margin (Pixel-wise RO-CAUC: 99.2%, PRO: 95.9% on MVTec 3D-AD). We conclude by discussing future challenges for 3D anomaly detection and segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image anomaly detection seeks to identify patterns in images that significantly deviate from the norm while anomaly segmentation localizes anomalies within anomalous images. These tasks have important applications (e.g. manufacturing defect detection <ref type="bibr" target="#b1">[2]</ref> and identifying new biomarkers in medical images <ref type="bibr" target="#b37">[37]</ref>.) The field has seen tremendous progress in recent years, accelerated by advancements in representation learning and the introduction of the widely used MVTec AD <ref type="bibr" target="#b1">[2]</ref> benchmark. Current state-of-the-art methods (e.g. SPADE, PatchCore <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33]</ref>) extract representations of local image regions and score the regions as normal or anomalous using the estimated probability density of the representation. These methods virtually solve the MVTec AD dataset with PatchCore achieving ROCAUC score of 99.1% for anomalous image detection and 98.1% for pixel-wise segmentation. While shape information is fundamental to computer vision, it has typically not been considered by current anomaly detection and segmentation approaches, probably due to the lack of suitable datasets. Recently, the MVTec 3D-AD <ref type="bibr" target="#b3">[4]</ref> dataset was introduced in order to encourage research into 3D anomaly detection and segmentation (AD&amp;S). To address the gap between 2D and 3D AD&amp;S, we conduct an empirical investigation on the MVTec 3D-AD which seeks the answers to several questions:</p><p>1. Are current 3D methods outperforming state-of-the-art 2D AD&amp;S methods? 2. Is 3D information potentially useful for AD&amp;S? 3. What are effective ways of representing 3D local regions for AD&amp;S? 4. Are there complementary benefits from using both 3D and color features?</p><p>Our investigation begins by examining if current AD&amp;S are indeed benefiting from 3D information. Perhaps surprisingly, we find that standard 2D methods (particularly PatchCore <ref type="bibr" target="#b32">[33]</ref>) using RGB-only information, outperform <ref type="figure">Figure 1</ref>: RGB vs. 3D: Top: Anomalous images using the RGB-only image. Bottom: The same image is viewed from different angles, enabled by the 3D information. The cookie and potato exhibit geometric anomalies while the cable gland and foam contain RGB anomalies. Cookie: The hole blends in with the rest of the chocolate chips. Potato: The dent goes undetected when viewed from above. Cable Gland: The deformed texture is hard to tell apart by examining the geometry alone. Foam: The colors can not be inferred by relying solely on 3D cues. all current methods by a wide margin. We also investigate whether 3D information is needed at all for AD&amp;S. Encouragingly, we find that several types of anomalies are not easy to detect using 2D-only methods, motivating the development of better 3D methods. We present several examples of such cases in the left half of <ref type="figure">Fig. 1</ref>-top row, the anomaly in each object cannot be detected from color information only. In the bottom row we present another view of the same objects where the anomalies are easily detected. This motivates using 3D information for AD&amp;S.</p><p>We conduct an extensive empirical investigation of representations for AD&amp;S based on 3D data. Guidance is provided by 2D AD&amp;S methods, where the best results are obtained by using simple anomaly detection methods on strong local region feature representations. Our investigation examines a large range of approaches including handcrafted descriptors and pre-trained deep features for both depth and 3D approaches. Our surprising result is that a classical 3D handcrafted point cloud descriptor outperforms all other 3D methods -including all previously presented baselines as well as a concurrent deep-learning-based point cloud approach. Furthermore, these features outperform the stateof-the-art 2D color approaches (particularly PatchCore). The main insight is that 3D surface matching approaches are highly effective for 3D AD&amp;S tasks. Finally, we note that for several categories, anomalies cannot be detected solely using geometric cues. We therefore investigate whether geometric and color features hold complimentary benefits. Our best investigated approach combines 3D and color to achieve the best recorded result on the MVTec 3D-AD dataset by a very wide margin (99.2% Pixel-wise ROCAUC, 95.9% PRO, and 86.5% Image ROCAUC).</p><p>To summarize, our main contributions in this paper are:</p><p>? Conducting a thorough analysis of the important and unexplored field of anomaly detection and segmentation for images with color and 3D information. ? Identifying significant gaps in today's current state-of-the-art 3D-based methods.</p><p>? The discovery of highly effective 3D AD&amp;S methods with connections to surface matching.</p><p>? Proposing a combination of the handcrafted 3D method with the current state-of-the-art 2D color-based method, outperforming current state-of-the-art by a wide margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Anomaly detection and segmentation. Anomaly detection methods have been researched for several decades, most approaches are based either on density estimation or out-of-domain generalization ideas. Classical approaches include: k-Nearest-Neighbors (kNN) <ref type="bibr" target="#b13">[14]</ref>, KDE <ref type="bibr" target="#b24">[25]</ref>, GMM <ref type="bibr" target="#b17">[18]</ref>, PCA <ref type="bibr" target="#b22">[23]</ref>, one class SVM (OCSVM) <ref type="bibr" target="#b39">[39]</ref>, and isolation forests <ref type="bibr" target="#b26">[27]</ref>. With the advent of deep learning, these methods were extended with deep representations including: DAGMM <ref type="bibr" target="#b48">[48]</ref> extending PCA, and DeepSVDD <ref type="bibr" target="#b33">[34]</ref> extending OCSVM. A novel line of work extends self-supervised approaches to anomaly detection, including Golan and El-Yaniv <ref type="bibr" target="#b18">[19]</ref>, and Hendrycks et al. <ref type="bibr" target="#b21">[22]</ref> that extend RotNet <ref type="bibr" target="#b16">[17]</ref> and CSI <ref type="bibr" target="#b43">[43]</ref> who extend contrastive methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b7">8]</ref>. We follow another line of works that assumes the availability of pre-trained representations, and combines them with a kNN scoring function. Such works include Perera and Patel <ref type="bibr" target="#b28">[29]</ref>, and PANDA <ref type="bibr" target="#b30">[31]</ref>. These works have been extended to anomaly segmentation including SPADE <ref type="bibr" target="#b8">[9]</ref>, PADIM <ref type="bibr" target="#b11">[12]</ref> and PatchCore <ref type="bibr" target="#b32">[33]</ref>. Very recent works have used more advanced density estimation models on the extracted representation, an example is FastFlow <ref type="bibr" target="#b45">[45]</ref>. Other approaches for anomaly segmentation include Student-Teacher autoencoder approaches <ref type="bibr" target="#b2">[3]</ref> as well as self-supervised methods that synthesize anomalies such as CutPaste <ref type="bibr" target="#b25">[26]</ref> and NSA <ref type="bibr" target="#b38">[38]</ref>.</p><p>Anomaly detection and segmentation with 3D information. In contrast to the large amount of research on 2D anomaly detection approaches, 3D anomaly detection has not been extensively researched. In medical imaging research, work was performed to adapt anomaly detection methods to voxel data. Simarro et al. <ref type="bibr" target="#b41">[41]</ref> extend f-Anogan <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b36">36]</ref> to 3D. Bengs et al. <ref type="bibr" target="#b0">[1]</ref> presented a 3D autoencoder approach for medical voxel data. Voxel data is significantly different from point cloud 3D data. Bergmann et al. <ref type="bibr" target="#b3">[4]</ref> recognized that a dataset for anomaly segmentation in 3D point cloud data is missing and introduced MVTec 3D-AD. We expect this to be a critical contribution to the development of 3D anomaly detection and segmentation. Concurrently to our work, Bergmann and Sattlegger <ref type="bibr" target="#b4">[5]</ref> introduced a 3D point cloud based approach dubbed 3D ? ST 128 for anomaly detection, we include this work in our investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AD&amp;S Formulation</head><p>The setting in this paper assumes a set of input training samples x 1 , x 2 ..x N that are all normal. At test time, we are given a test sample y. The task of anomaly detection is to learn a sample-level scoring function ? d , such that ? d (y) &gt; 0 if the sample is anomalous and ? d (y) ? 0 if the sample is normal. The task of anomaly segmentation is to learn a pixel-level scoring function ? s , which satisfies ? s (y, i) &gt; 0 if pixel i of sample y is anomalous, and ? s (y, i) ? 0 if it is normal. Note that a sample can be an RGB image, a depth map (i.e. 1-channel image), an organized or unorganized point cloud, or a pair of these as a tuple. For brevity we use the term "pixel", however, when relevant, it may stand for a point cloud point.</p><p>A common pipeline used by state-of-the-art methods (e.g. SPADE <ref type="bibr" target="#b8">[9]</ref>, PatchCore <ref type="bibr" target="#b32">[33]</ref>) consists of two stages: i) Representation extraction of local regions ii) Evaluating the normality of each local representation, for example by using the nearest-neighbor distance to a normal training dataset. The focus of this paper is on the representation stage, particularly, on finding the best representation for AD&amp;S when 3D information is available. Representation. The first stage computes a representation of each local region. Let us denote each local region of an image by index j, where the region can consist of one or more pixels. The representation function of region j of image x is denoted ?(x, j). The representation can be learned or be handcrafted. This paper will investigate many different such representations.</p><p>Anomaly scoring. Given the representations in every local region j of every training image x, we can train a model ? s (y, j ? ) which computes the likelihood of a new representation ?(y, j ? ). Although some approaches train parametric models for the density of the representations, non-parametric approaches are much simpler and require no training. Specifically, in our investigation, we use the k-Nearest-Neighbor distance of representation ?(y, j ? ) to the set of all training set representations S = {?(x, j) ?x ?j}. Despite their simplicity, such approaches are very accurate, require no training, and can be significantly sped up using coreset techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Benchmark</head><p>Our investigation uses the recently published MVTec 3D Anomaly Detection dataset (MVTec 3D-AD <ref type="bibr" target="#b3">[4]</ref>). It contains over 4000 high-resolution 3D scans of industrially manufactured products across 10 categories. Each sample is represented by an organized point cloud and a corresponding RGB image with a one-to-one mapping between the pixels in the point cloud and those in the RGB image. Currently, this is the only dataset available for 3D AD&amp;S. Five of the classes in the dataset exhibit natural variations (bagel, carrot, cookie, peach, and potato). The classes cable gland and dowel are of rigid bodies, while the classes foam, rope, and tire are "man-made" but deformable. Alongside the dataset, three baselines are introduced i) GAN-based ii) AE-based iii) a classic baseline based on per-pixel mean and standard deviation dubbed the "Variation Model (VM)". These models operate either on the depth images or in voxel space with additional variants that operate on 3D+RGB information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics</head><p>Several evaluation metrics for AD&amp;S exist. For anomaly detection on the image level, ROCAUC <ref type="bibr" target="#b6">[7]</ref> is used, in this paper, we denote it by I-ROC. Two metrics exist for anomaly segmentation, both operate on the pixel level. Pixel-wise ROCAUC is an extension of the classic ROCAUC to work on the pixel level by simply treating each pixel in the dataset as a sample and taking the ROCAUC over all pixels in the dataset, we denote it by P-ROC The PRO <ref type="bibr" target="#b1">[2]</ref> metric is defined as the average relative overlap of the binary prediction P with each ground truth connected component C k . It is measured in a similar way to the P-ROC and is simply denoted by PRO.   Where K denotes the number of ground truth components. The final metric is computed by integrating this curve up to some false positive rate and normalizing. It is common practice to report PRO on an integration limit of at most 0.3, unless otherwise stated, all PRO figures reported in this paper use this integration limit.</p><formula xml:id="formula_0">PRO = 1 K K k=1 |P ? C k | |C k | ,<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">An Empirical Investigation of 3D AD&amp;S</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Examining Current 3D vs. 2D Methods for AD&amp;S</head><p>We begin our investigation by answering the question: "How effective are current 3D methods over the state-of-the-art 2D methods?". To represent 3D methods, we test two approaches: i) Voxel GAN <ref type="bibr" target="#b3">[4]</ref>, a generative method proposed as a baseline for 3D AD&amp;S. While it has several variants, we use the best performing ones, which are "Voxel" and "Voxel + RGB". ii) 3D-ST <ref type="bibr" target="#b5">[6]</ref>, a concurrent method that uses a point cloud student-teacher model to learn meaningful 3D representations. We use PatchCore <ref type="bibr" target="#b32">[33]</ref> to represent color-based 2D AD&amp;S methods. Importantly, PatchCore uses features that were pre-trained on the ImageNet <ref type="bibr" target="#b12">[13]</ref> dataset, which have been shown to be highly effective for AD&amp;S, in contrast, 3D-ST used ModelNet10 <ref type="bibr" target="#b44">[44]</ref> for pretraining the teacher model. The results on MVTec 3D-AD are presented in Tab. 1. In accordance with previous work, we report the PRO metric with integration limits of 0.3. Surprisingly, PatchCore, which does not use 3D information, outperforms all other methods.</p><p>Conclusion. Current methods that use 3D or 3D + color information do not outperform current state-of-the-art methods that use only 2D color information but not 3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Inspecting the Utility of 3D Information for AD&amp;S</head><p>Provided the negative results of Sec. 4.1, we are faced with a second question: "Is 3D information potentially useful for AD&amp;S?". Although we do not give a definite answer to this question, we motivate the reason as to why the answer may be affirmative.</p><p>Ambiguous geometry. Frequently, the geometry of an underlying object cannot be unambiguously determined by observing the 2D RGB image exclusively. In such cases, observing the three-dimensional shape of the object may   <ref type="figure">Fig. 1</ref>.</p><p>The top row contains RGB-only images, in the bottom row, different views of the same object are displayed, revealing the actual geometry and unmasking the anomaly. In the case of the cookie, the hole blends in with the rest of the chocolate chips, making it hard to visually identify the image as anomalous. Using the 3D information to view the object from a different angle makes it easy to spot the anomaly. In the potato case, when looking at the image, it is hard to infer the geometry of the dent from shadow and texture. However, when viewing the potato from different angles, the different texture reveals the dent.</p><p>Background variation. While curated datasets often contain synthetic conditions with an object centered in the frame and a clean background, the reality is seldom as simple. Real images contain background which is highly varying but which methods may consider anomalous and therefore trigger false positive alerts. Although background segmentation is not trivial in 2D, it is far easier when the 3D structure is provided. Particularly, the MVTec 3D-AD dataset contains test images with backgrounds that deviate from those of normal data. Such an example is demonstrated in <ref type="figure">Fig. 5</ref>, the background of some objects contains "wave" like patterns in the fabric, due to the dark background, these issues go undetected.</p><p>Conclusion. Some anomalies are detectable using 3D information but are not easily detected using 2D color-only images. It is therefore expected that the addition of 3D information should improve AD&amp;S over just 2D information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">A Study of 3D-Aware Representations for AD&amp;S</head><p>Having shown that 3D is under-utilized by current methods, we now seek to answer a third question: "What are effective ways to represent 3D local regions for 3D AD&amp;S?". Using the 3D information, we distinguish among several categories.</p><p>Learning-based Representations. We begin by evaluating approaches that utilize learned features. We adapt the two most popular 2D approaches to 3D data i.e. ImageNet-pre-trained features and self-supervised methods.</p><p>Depth-only ImageNet Features. The evaluated approach applies PatchCore directly on depth images. This is motivated by the impressive results of ImageNet-pre-trained features on 2D color images (Sec. 4.1). There is also evidence <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b8">9]</ref> that ImageNet-pre-trained features are effective on image data which is significantly out-of-distribution to ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NSA.</head><p>A different class of learning-based methods approach AD&amp;S from a generative perspective. CutPaste and NSA <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">38]</ref> are recent works that try to mimic anomalies by pasting image patches at different image locations. Specifically, NSA uses Poisson blending <ref type="bibr" target="#b29">[30]</ref> to make these appear more natural.</p><p>Results. We can observe in Tab. 2 that Imagenet-pre-trained features significantly outperform NSA on depth images. It is possible that NSA performance can be improved by specific per-class augmentations, however, this requires prior knowledge of anomalies. Both approaches underperform PatchCore on 2D color images.</p><p>Handcrafted 2D Representations. A notable difference between depth and RGB information is the simplicity of depth information. Consequently, we hypothesize that a simple, handcrafted descriptor should suffice. The following representations use only the depth information (i.e. the Z channel of the point cloud). Furthermore, they do not require external data or training and are all learning-free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw Depth Values.</head><p>Here, we test perhaps the simplest possible representation, the raw depth values of a patch comprising the region.</p><p>Histogram of Oriented Gradients (HoG). Instead of using absolute depth values, HoG <ref type="bibr" target="#b10">[11]</ref> considers image gradients and uses histograms to capture the distribution of gradient orientations in the patch. This is potentially more powerful than raw values as the descriptor encodes the spatial structure of the data while being invariant to small translations. On the other hand, HoG is not invariant to global rotations, a much-desired property for 3D representations. Additionally, the small context of HoG makes it invariant to local geometric changes, however, this is counterproductive to our goal of detecting anomalies -usually manifested as local geometric changes.</p><p>Dense Scale-Invariant Feature Transform (D-SIFT). In contrast to HoG, SIFT is rotation, scale, and shift-invariant. Different from HoG, SIFT is rotated to align the most dominant direction to the base orientation. This reduces the rotation ambiguity allowing matches between rotated images. While D-SIFT was originally designed for 2D images, it has been shown suitable for certain 3D use cases and extensions have been proposed for 3D-specific tasks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">40]</ref>.</p><p>Results. It is clear from Tab. 3, that even the simplest descriptor (raw) excels over all the baselines in their "Depthonly" variant. We can see in Tab. 2 that HoG significantly boosts the pixel-level accuracy achieving better results than raw and learning-based features. These strong results are obtained despite HoG not being specifically designed for 3D information. Finally, the D-SIFT <ref type="bibr" target="#b27">[28]</ref> descriptor is able to surpass all previous results (including HoG) on all three metrics. D-SIFT outperforms even the PatchCore RGB results, which have thus far not been surpassed by depth features.</p><p>3D Orientation-Invariant Representations. As orientation-invariant 2D features on depth maps have led to the best performance, it begs the question of whether orientation-invariant 3D features can do better. We therefore investigate a representative method.</p><p>Fast Point Feature Histograms (FPFH). FPFH <ref type="bibr" target="#b34">[35]</ref> first computes the k-Nearest-Neighboring points to the region center point. It then computes a histogram-based representation as a function of the surface normals and vector distance to the nearest neighbors. The representation consists of the histograms of the surface normal based features. This method is chosen as the representative due to its time-tested excellent performance.</p><p>Conclusion. FPFH outperforms all methods that use 2D color, depth or both (Tab. 2). The results show that strong, handcrafted, orientation-invariant 3D representations are extremely effective for AD&amp;S when 3D information is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The Benefits of Combining 3D and Color</head><p>Having studied different representations for 3D information we now ask: "Are there complimentary benefits from using both 3D and color features?". To address this question we observe that in some cases, geometry alone may not suffice for detecting anomalies. Two examples are fine-textures and color-based anomalies. E.g. the "cable gland" in <ref type="figure">Fig. 1</ref>-right is slightly scraped. While this anomalous texture is clearly observed in the RGB image, it is virtually impossible to detect with the current resolution of the 3D information. This is even more apparent for the foam example, wherein the anomaly is by its nature one of color only. We can also observe in Tab. 4 that amongst methods discussed thus far, ImageNet-pre-trained RGB features achieve the top results on the cable gland class. This suggests that an exclusive focus on 3D fails to account for anomalies involving fine textures or color changes. Combining 3D and RGB representations is therefore necessary.</p><p>A Combined RGB + 3D Approach. In order to cater both to 3D geometry and color, we take a combined RGB + 3D approach. To this end, RGB representations are extracted using the 2D ImageNet-based features method discussed in Sec. 4.1 and 3D representations are extracted using FPFH as discussed in Sec. 4.3. Extending PatchCore, these two representations are concatenated and treated as our RGB + 3D representation.</p><p>Results. Compared with the previous best method of combining 3D and RGB ("Voxel GAN + RGB"), this method improves the PRO (i.e. anomaly segmentation) metric by 32% and I-ROC (i.e. anomaly detection) by 16.6%. As illustrated in Tab. 2, compared to using only 3D, the combination improves on the previously discussed FPFH by 3.5% on the PRO metric, 8.3% on I-ROC, and achieves a score of 99.2% on P-ROC, a 1.4% improvement over FPFH.</p><p>Conclusion. It is evident by the results RGB and 3D information are complementary and thus by combining their representations one is able to achieve better results. A class level breakdown for each of the investigated methods can be found in Tab. 4 and 5 as well as in the App. A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Implementation Details</head><p>Unless otherwise stated, results in the paper downsample the original point clouds and RGB images to 224. Downsampling the point clouds is done by downsampling the organized point cloud (i.e. image downsampling) with nearestneighbor interpolation, the RGB images are downsampled using bicubic interpolation. When working with unorganized point clouds, the organized point cloud is reshaped from n ? m ? 3 into n ? m ? 3. For depth images, the Z channel of the organized point cloud is used. We use 28 ? 28 = 784 patches (features) per sample, with varying feature dimensions based on the representations used. If the representation is extracted at a different resolution, average pooling is used to match 28 ? 28 = 784. For method-specific details see App. B.</p><p>(a) (b) (c) <ref type="figure">Figure 5</ref>: 3D-Aware Preprocessing: In (a), a "wave" in the fabric is an example of possible background artifacts. Although a RANSAC-based plane removal step approximates the best fitting plane, in some cases, artifacts are too far away from it to be removed (b). To further remove remaining artifacts, DB Scan connected components scanning and removal is used (c).</p><p>Following the discussion in Sec. 4.2 on the preprocessing potential, we developed a short preprocessing protocol to handle such cases. First, the background plane is estimated using RANSAC <ref type="bibr" target="#b15">[16]</ref> on the 3D locations of image boundary points. Once removed, a connected-components step ensures outliers and areas far from the plane are not kept. This process is illustrated by <ref type="figure">Fig. 5</ref>. A final preprocessing step includes changing the aspect ratio of two nonsquare classes (i.e. rope and tire). Unless otherwise stated, our results in the paper use this preprocessing pipeline. For a full discussion on the benefits it holds, see Sec. 5. For implementation details see App. C.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Having conducted an extensive empirical evaluation of 3D representations for AD&amp;S, we can answer the questions raised in the introduction. As a result of the comparison performed in Sec. 4.1 we can conclude that current 3D methods for AD&amp;S do not outperform established methods from 2D AD&amp;S. Based on the analysis performed in Sec. 4.2 we can motivate the utility of 3D information, both in regards to the core task of AD&amp;S as well as for secondary tasks such as preprocessing and refinement. The main insight drawn by the extensive evaluation performed in Sec. 4.3 is that for 3D AD&amp;S that utilizes handcrafted representations, orientation invariance is of utmost importance. As a result, currently, handcrafted features surpass learning-based ones. Last, we point to the fact that by fusing RGB with 3D, one can take the best from both worlds.</p><p>We thus conclude with a short list of mini-discussions intended to spark further research in the field of 3D AD&amp;S.</p><p>Image-level anomaly detection. While the best performing method that we investigated established a new state-ofthe-art on all metrics, the image level detection accuracy is far from perfect. The best performing method reaches an I-ROC of 86.5%, a great improvement compared to past methods, but still a relatively low score. As seen in <ref type="figure" target="#fig_1">Fig. 3</ref> and <ref type="figure" target="#fig_2">Fig. 4</ref>, it is not uncommon for methods to perform very well on the segmentation task while underperforming on the detection task. Since we use the PatchCore method as the backbone for most of our experiments, the I-ROC score is a function of the test patch most distant from all training patches. However, this assumption was made to address 2D images. It is possible that for multi-modal cases such as 3D and RGB, new assumptions and methods are needed.</p><p>Failure classes. As can be seen from Tab. 4, 5, different methods find different classes are harder than others. We highlight two classes for which the simple RGB method outperforms even the best methods. Both cable gland and foam perform noticeably poorly for all depth-based methods. Although these classes exhibit more RGB-only anomalies compared to others (see <ref type="figure">Fig. 1</ref>), the fusion of both modalities would be expected to handle them. Unfortunately, the fused feature underperformed the RGB only result. This suggests that further research into the feature fusion process is required.</p><p>Datasets. While MVTec 3D-AD is a good starting point for 3D AD&amp;S, its setting is somewhat synthetic. For the field to mature, more datasets are needed. Specifically, due to the complex geometry and interactions of real world scenes, scene level datasets (e.g. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">42]</ref>) for 3D AD&amp;S may provide a setting where 3D information provides an even greater advantage over 2D counterparts. As LiDAR sensors become cheaper and ever more present (e.g. some iPhone models are now equipped with LiDAR sensors), new opportunities for "in the wild" 3D AD&amp;S present themselves e.g. a skin lesion dataset for 3D AD&amp;S. Utilizing 3D information, such datasets could pave the way for early-detection and self-diagnosis AD&amp;S methods against terminal illnesses. Additionally, datasets captured by a variety of methods and 3D formats could improve generalization and robustness.</p><p>Methods. We have shown a surface matching descriptor achieved the state-of-the-art results on 3D AD&amp;S tasks. The main reason behind its success is the orientation invariance. While both depth and 3D methods optimize this property, explicit orientation invariance, guided by 3D information can significantly improve the RGB features. An additional exciting future research direction is the adaptation of surface matching ideas to the AD&amp;S setting. Moreover, while our study focused on handcrafted features as a simple and highly effective way to tackle this task, newer, learning-based methods from the surface matching domain are a promising future direction. Another possibility for future research is the finetuning of pre-trained features on the normal training images. Such methods (e.g. MSAD <ref type="bibr" target="#b31">[32]</ref>) achieve strong results in the 2D setting.</p><p>3D-based preprocessing. As mentioned in Sec. 4.5 and illustrated by <ref type="figure">Fig. 5</ref>, our results are all acquired by using the described preprocessing protocol. It left the RGB method mostly unaffected. More interestingly, it drastically improved results for the depth-based methods, while for 3D-based methods (i.e. FPFH) it slightly decreases results. We postulate this is due to the different ways in which depth and 3D methods handle missing information by the sensor. 3D sensing methods are prone to sampling noise and areas with missing information. In MVTec 3D-AD it is common to have very noisy backgrounds, these areas are replaced by zeros by the dataset designers. When working with point clouds, these values are all located at the origin and may be ignored by the method as they are not in the spatial context of other points. In contrast, when working with the depth image, these values are in the spatial context of other points. Removing these planes thus helps depth-based methods. We conjecture this is in part the reason for the poor results on depth images reported by the MVTec 3D-AD baselines and that this preprocessing may improve them. A quantitative comparison is available at Tab. 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We first showed that color-only approaches outperform all existing 3D methods on the MVTec 3D-AD dataset. We conducted an extensive investigation of 3D representations and found that an orientation-invariant representation achieves the best performance on 3D anomaly detection. Combinations of 3D and color features set a new state-ofthe-art. We hope that our work will serve as a strong starting point for future 3D anomaly detection and segmentation approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>This work was supported in part by Oracle Cloud credits and related resources provided by the Oracle for Research program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed P-ROC Results</head><p>Detailed </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Method Specific Implementation Details</head><p>Bellow, we present additional implementation details for each of our investigated methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 RGB-only ImageNet Features</head><p>Utilizing PatchCore, RGB images are fed into an ImageNet <ref type="bibr" target="#b12">[13]</ref> pre-trained WideResNet50 <ref type="bibr" target="#b46">[46]</ref> backbone as a feature extractor. To allow for localized segmentation, patch-level features are extracted from the aggregated outputs of blocks 2 and 3, resulting in a feature dimension of 1536.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Depth-only ImageNet Features</head><p>As in the RGB-only case, PatchCore is used with a depth map normalized according to ImageNet statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Raw Depth Values</head><p>The depth image is divided into patches of 8 ? 8 pixels, resulting in 28 ? 28 patches. The descriptor is comprised of the 8 ? 8 pixels of each patch, flattened to a 1D list of length 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 NSA</head><p>We were not able to find an official implementation of CutPaste <ref type="bibr" target="#b25">[26]</ref> and the public unofficial implementations lag behind the reported figures by up to 10%. Therefore, we compare our method to NSA <ref type="bibr" target="#b38">[38]</ref>, a follow-up to CutPaste that uses Poisson blending <ref type="bibr" target="#b29">[30]</ref> to achieve more realistic augmentations. To test NSA on the new dataset, we modified their official implementation to handle depth images. The current implementation requires the images to be represented as integers, as such, the depth images are discretized to [0, 255]. Furthermore, NSA uses extensive, class-dependent hyperparameters. MVTec 3D-AD classes were assigned hyperparameters by visually comparing them with the original classes and assigning the values of the most similar class. Depth images are used for running these experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 HoG</head><p>The depth image is used as an input. To align with the feature map resolution, 8 pixels per cell, and 1 cell per block are used. 8 bins are used to arrive at a 32 dimensional representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 D-SIFT</head><p>The depth image is used as an input. Dense SIFT is applied to all the pixels, to reduce the resolution, average pooling is applied. Following standard SIFT practice, a 128 feature dimension is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7 FPFH</head><p>To speed up calculations of FPFH, the point cloud is downsampled prior to running the algorithm. The downsampling is performed on the organized point cloud (i.e. image downsampling). It is then flattened into an unorganized point cloud. Using the implementation from the open-source library Open3D <ref type="bibr" target="#b47">[47]</ref>, a descriptor is extracted for each point. These descriptors are then reshaped back into an organized point cloud and their resolution is lowered by average pooling them. FPFH requires normals to run, we estimate the normals using Open3D. The radius for the FPFH algorithm is 0.25 and the max nn parameter is set to 100. The resulting feature is of dimension 33.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Preprocessing Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Plane Removal</head><p>By design, the objects in the dataset are centered within the image. We thus make a simplifying assumption that all the edges of the image lie on the same plane. To this end, a 10 pixels wide strip around the image boundary is taken from the organized point cloud. After removing all NaNs (i.e. noise), RANSAC <ref type="bibr" target="#b15">[16]</ref> is used to approximate the plane that best describes the boundary. The distance to this plane is calculated for each point in the point cloud, any point within 0.005 distance is removed. In practice, instead of removing the point, we zero the XYZ coordinates and RGB values for the point. This ensures the original resolution is kept. We use the Open3D <ref type="bibr" target="#b47">[47]</ref> "Segment Plane" implementations for the RANSAC step with ransac n = 50 and num i terations = 1000, for the actual plane removal we use the returned plane equation and manually zero the values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Clustering Based Outlier Removal</head><p>Although the plane removal step can identify and remove the majority of the planes, in some cases, the planes are not planer, see <ref type="figure">Fig 5.</ref> Points in those areas may therefore be flagged as anomalies. By running DB-Scan <ref type="bibr" target="#b14">[15]</ref> as a connected-components approach, each cluster is treated as a connected component. We keep the largest component and remove all points from other components (as before, we zero the XYZ coordinates and RGB values of the point).</p><p>We use the Open3D <ref type="bibr" target="#b47">[47]</ref> DB-Scan implementation with ? = 0.006 and min points = 30.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Additional Results: RGB + FPFH method is used. All anomalies are correctly segmented (marked in red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Anomaly Heatmaps (Pixel-wise): For each method, the distances heatmap is shown, PRO and P-ROC are drawn from these distances. Blue colors are further away (i.e. more anomalous) while red colors are closer (i.e. less anomalous). Ground truth segmentation in red, anomaly indicated by a red rectangle in the 2D view,"iNet" indicates ImageNet pre-trained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Most Distant Patch (I-ROC): The patch with the largest kNN distance is shown in red for each representation. Anomaly indicated by a red square in the 2D view, "iNet" indicates ImageNet pre-trained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>0.701 0.695 0.618 0.841 0.702 0.770 Depth iNet 0.686 0.532 0.769 0.853 0.857 0.511 0.573 0.620 0.758 0.590 0.675 NSA 0.841 0.494 0.776 0.913 0.636 0.616 0.795 0.597 0.856 0.438 0.696 Raw 0.627 0.506 0.599 0.654 0.573 0.531 0.531 0.611 0.412 0.678 0.573 HoG 0.487 0.588 0.690 0.546 0.643 0.593 0.516 0.584 0.506 0.429 0.559 SIFT 0.711 0.656 0.892 0.754 0.828 0.686 0.622 0.754 0.767 0.598 00.582 0.896 0.912 0.921 0.886 0.865</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>MVTec Baselines vs. 2D RGB (PRO/I-ROC): Average metrics across all classes, best performing MVTec 3D-AD baselines are shown Voxel Voxel + RGB Point Cloud RGB GAN GAN 3D ? ST 128 PatchCore 0.583/0.537 0.639/0.517 0.833/- 0.876/0.770</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary of Our Findings: Average metrics across all classes, "iNet" indicates ImageNet pre-trained, PC for Point Cloud RGB Depth Depth Depth Depth Depth PC RGB + PC iNet iNet NSA Raw HoG SIFT FPFH RGB + FPFH</figDesc><table><row><cell>PRO 0.876 0.755 0.572 0.442 0.771 0.910 0.924</cell><cell>0.959</cell></row><row><cell>I-ROC 0.770 0.675 0.696 0.573 0.559 0.727 0.782</cell><cell>0.865</cell></row><row><cell>P-ROC 0.967 0.930 0.817 0.771 0.930 0.974 0.978</cell><cell>0.992</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>MVTec Baselines vs. Raw (PRO): Average metrics across all classes, all models use the depth representation GAN AE VM Raw 0.143 0.203 0.374 0.442 reveal the true geometry where the 2D image could not. Two such examples are illustrated in the left half of</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Detailed PRO Results: Top half are current state-of-the-art, bottom half are methods investigated by us. A large number of our methods outperform all current methods by a wide margin. "iNet" indicates ImageNet pre-trained</figDesc><table><row><cell></cell><cell>Method</cell><cell>Bagel</cell><cell>Cable Gland</cell><cell cols="3">Carrot Cookie Dowel Foam Peach Potato Rope</cell><cell>Tire</cell><cell>Mean</cell></row><row><cell></cell><cell>Voxel GAN</cell><cell cols="3">0.440 0.453 0.825</cell><cell>0.755</cell><cell>0.782 0.378 0.392 0.639 0.775 0.389 0.583</cell></row><row><cell></cell><cell cols="4">+ RGB 0.664 0.620 0.766</cell><cell>0.740</cell><cell>0.783 0.332 0.582 0.790 0.633 0.483 0.639</cell></row><row><cell></cell><cell>Voxel AE</cell><cell cols="3">0.260 0.341 0.581</cell><cell>0.351</cell><cell>0.502 0.234 0.351 0.658 0.015 0.185 0.348</cell></row><row><cell>Previous Methods</cell><cell cols="4">+ RGB 0.467 0.750 0.808 Voxel VM 0.453 0.343 0.521 + RGB 0.510 0.331 0.413 Depth GAN 0.111 0.072 0.212 + RGB 0.421 0.422 0.778 Depth AE 0.147 0.069 0.293 + RGB 0.432 0.158 0.808</cell><cell>0.550 0.697 0.715 0.174 0.696 0.217 0.491</cell><cell>0.765 0.473 0.721 0.918 0.019 0.170 0.564 0.680 0.284 0.349 0.634 0.616 0.346 0.492 0.680 0.279 0.300 0.507 0.611 0.366 0.471 0.160 0.128 0.003 0.042 0.446 0.075 0.143 0.494 0.252 0.285 0.362 0.402 0.631 0.474 0.207 0.181 0.164 0.066 0.545 0.142 0.203 0.841 0.406 0.262 0.216 0.716 0.478 0.481</cell></row><row><cell></cell><cell>Depth VM</cell><cell cols="3">0.280 0.374 0.243</cell><cell>0.526</cell><cell>0.485 0.314 0.199 0.388 0.543 0.385 0.374</cell></row><row><cell></cell><cell cols="4">+ RGB 0.388 0.321 0.194</cell><cell>0.570</cell><cell>0.408 0.282 0.244 0.349 0.268 0.331 0.335</cell></row><row><cell></cell><cell>3D ? ST 128</cell><cell cols="3">0.950 0.483 0.986</cell><cell>0.921</cell><cell>0.905 0.632 0.945 0.988 0.976 0.542 0.833</cell></row><row><cell></cell><cell>RGB iNet</cell><cell cols="3">0.901 0.949 0.928</cell><cell>0.877</cell><cell>0.892 0.563 0.904 0.932 0.908 0.906 0.876</cell></row><row><cell>Our Findings</cell><cell>Depth iNet NSA Raw HoG SIFT</cell><cell cols="3">0.769 0.664 0.887 0.724 0.228 0.716 0.401 0.311 0.638 0.711 0.763 0.931 0.942 0.842 0.974</cell><cell>0.880 0.856 0.498 0.497 0.896</cell><cell>0.864 0.269 0.876 0.865 0.852 0.624 0.755 0.320 0.432 0.712 0.655 0.818 0.258 0.572 0.250 0.254 0.527 0.530 0.808 0.201 0.442 0.833 0.502 0.743 0.948 0.916 0.858 0.771 0.910 0.723 0.944 0.981 0.953 0.929 0.910</cell></row><row><cell></cell><cell>FPFH</cell><cell cols="3">0.973 0.879 0.982</cell><cell>0.906</cell><cell>0.892 0.735 0.977 0.982 0.956 0.961 0.924</cell></row><row><cell></cell><cell>RGB+FPFH</cell><cell cols="3">0.976 0.969 0.979</cell><cell>0.973</cell><cell>0.933 0.888 0.975 0.981 0.950 0.971 0.959</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Detailed I-ROCAUC Results: Top half are current state-of-the-art, bottom half are methods investigated by us. A large number of our methods outperform all current methods by a wide margin. "iNet" indicates ImageNet pre-trained</figDesc><table><row><cell>Method</cell><cell>Bagel</cell><cell>Cable Gland</cell><cell>Carrot Cookie Dowel Foam Peach Potato Rope</cell><cell>Tire</cell><cell>Mean</cell></row><row><cell>Previous Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Preprocessing results, average PRO/I-ROC across all classes are reported</figDesc><table><row><cell>RGB</cell><cell>HoG</cell><cell>SIFT</cell><cell>FPFH</cell></row><row><cell cols="4">Raw 0.876/0.788 0.625/0.558 0.869/0.723 0.930/0.764</cell></row><row><cell cols="4">Pre 0.876/0.770 0.771/0.559 0.910/0.727 0.924/0.782</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>P-ROC Results: Results not reported for previous methods, "iNet" indicates ImageNet pre-trained</figDesc><table><row><cell>Method</cell><cell>Bagel</cell><cell>Cable Gland</cell><cell cols="3">Carrot Cookie Dowel Foam Peach Potato Rope</cell><cell>Tire</cell><cell>Mean</cell></row><row><cell>RGB iNet</cell><cell cols="3">0.983 0.984 0.980</cell><cell>0.974</cell><cell cols="2">0.972 0.849 0.976 0.983 0.987 0.977 0.967</cell></row><row><cell>Depth iNet</cell><cell cols="3">0.959 0.896 0.966</cell><cell>0.969</cell><cell cols="2">0.967 0.743 0.969 0.958 0.977 0.888 0.930</cell></row><row><cell>NSA</cell><cell cols="3">0.925 0.638 0.872</cell><cell>0.908</cell><cell cols="2">0.674 0.777 0.902 0.825 0.972 0.676 0.817</cell></row><row><cell>Raw Depth</cell><cell cols="3">0.803 0.747 0.848</cell><cell>0.801</cell><cell cols="2">0.610 0.694 0.829 0.770 0.950 0.657 0.771</cell></row><row><cell>HoG</cell><cell cols="3">0.912 0.933 0.985</cell><cell>0.826</cell><cell cols="2">0.936 0.857 0.923 0.987 0.980 0.955 0.930</cell></row><row><cell>SIFT</cell><cell cols="3">0.986 0.956 0.996</cell><cell>0.952</cell><cell cols="2">0.966 0.919 0.985 0.997 0.993 0.983 0.974</cell></row><row><cell>FPFH</cell><cell cols="3">0.994 0.966 0.999</cell><cell>0.946</cell><cell cols="2">0.966 0.927 0.996 0.999 0.996 0.990 0.978</cell></row><row><cell cols="4">RGB+FPFH 0.996 0.992 0.997</cell><cell>0.994</cell><cell cols="2">0.981 0.974 0.996 0.998 0.994 0.995 0.992</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Three-dimensional deep learning with spatial erasing for unsupervised anomaly segmentation in brain mri</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bengs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Behrendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kr?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Opfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schlaefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The mvtec 3d-ad dataset for unsupervised 3d anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09045</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.11660</idno>
		<title level="m">Anomaly detection in 3d point clouds using deep geometric descriptors</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.11660</idno>
		<title level="m">Anomaly detection in 3d point clouds using deep geometric descriptors</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The use of the area under the roc curve in the evaluation of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02357</idno>
		<title level="m">Sub-image anomaly detection with deep pyramid correspondences</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE computer society conference on computer vision and pattern recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Padim: a patch distribution modeling framework for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Defard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Setkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Audigier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A geometric framework for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prerau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Portnoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stolfo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of data mining in computer security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ensemble gaussian mixture models for probability density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glodek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schwenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<title level="m">Momentum contrast for unsupervised visual representation learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC 2008-19th British Machine Vision Conference</title>
		<imprint>
			<publisher>British Machine Vision Association</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Outlier detection with kernel density functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazarevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pokrajac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Machine Learning and Data Mining in Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cutpaste: Self-supervised learning for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Isolation forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth IEEE International Conference on Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deep features for one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<title level="m">Poisson image editing. SIGGRAPH</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Panda: Adapting pretrained features for anomaly detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03844</idno>
		<title level="m">Mean-shifted contrastive loss for anomaly detection</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pemula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zepeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08265</idno>
		<title level="m">Towards total recall in industrial anomaly detection</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gornitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICML</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (fpfh) for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3212" to="3217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ROBOT.2009.51524737</idno>
		<ptr target="https://doi.org/10.1109/ROBOT.2009.51524737" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">f-anogan: Fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Self-supervised out-of-distribution detection and localization with natural synthetic anomalies (nsa)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.15222</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Support vector method for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM international conference on Multimedia</title>
		<meeting>the 15th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised 3d brain anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Simarro Viana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>De La Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vande Vyvere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Sima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08176</idno>
		<title level="m">Csi: Novelty detection via contrastive learning on distributionally shifted instances</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07677</idno>
		<title level="m">Fastflow: Unsupervised anomaly detection and localization via 2d normalizing flows</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Open3D: A modern library for 3D data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09847</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
