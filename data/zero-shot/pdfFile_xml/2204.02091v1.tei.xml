<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">P3Depth: Monocular Depth Estimation with a Piecewise Planarity Prior</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishakh</forename><surname>Patil</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Liniger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">PSI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<region>KU</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">P3Depth: Monocular Depth Estimation with a Piecewise Planarity Prior</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monocular depth estimation is vital for scene understanding and downstream tasks. We focus on the supervised setup, in which ground-truth depth is available only at training time. Based on knowledge about the high regularity of real 3D scenes, we propose a method that learns to selectively leverage information from coplanar pixels to improve the predicted depth. In particular, we introduce a piecewise planarity prior which states that for each pixel, there is a seed pixel which shares the same planar 3D surface with the former. Motivated by this prior, we design a network with two heads. The first head outputs pixel-level plane coefficients, while the second one outputs a dense offset vector field that identifies the positions of seed pixels. The plane coefficients of seed pixels are then used to predict depth at each position. The resulting prediction is adaptively fused with the initial prediction from the first head via a learned confidence to account for potential deviations from precise local planarity. The entire architecture is trained end-to-end thanks to the differentiability of the proposed modules and it learns to predict regular depth maps, with sharp edges at occlusion boundaries. An extensive evaluation of our method shows that we set the new state of the art in supervised monocular depth estimation, surpassing prior methods on NYU Depth-v2 and on the Garg split of KITTI. Our method delivers depth maps that yield plausible 3D reconstructions of the input scenes. Code is available at: https://github.com/SysCV/P3Depth</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Depth estimation is a fundamental problem in computer vision. It consists in predicting the perpendicular coordinate of the 3D point depicted at each pixel. Applications range from robotics to autonomous cars. There is experimental evidence <ref type="bibr" target="#b84">[85]</ref> that depth is the most vital vision-level cue for executing actions, together with semantic segmentation. In this work, we focus on monocular depth estimation, which involves the challenge of scale ambiguity, as the same input image can be generated by infinitely many 3D scenes.</p><p>The current trend in solving this task involves fully convolutional neural networks that output a dense depth predic- tion either with standard supervision on depth <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b71">72]</ref> or with self-supervision by using the predicted depth to reconstruct neighboring views of the scene <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b85">86]</ref>. Most supervised approaches use a pixel-level loss which treats predictions at different pixels separately. This regime ignores the high degree of regularity of real-world 3D scenes, which generally yield piecewise smooth depth maps.</p><p>A common choice for modeling this prior knowledge of the geometry of real 3D scenes are planes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. Planes are the local first-order Taylor approximation for locally differentiable depth maps and they are easy to parameterize using three independent coefficients. Once a pixel is associated with a plane, its depth can be recovered from the position of the pixel and the coefficients of the associated plane. In <ref type="bibr" target="#b83">[84]</ref>, such a plane coefficient representation is used to learn to predict planes explicitly.</p><p>We adopt the plane representation from <ref type="bibr" target="#b83">[84]</ref>, but we depart from the explicit prediction of planes and rather use this representation as an appropriate output space for defining interactions between pixels based on planarity priors. In particular, the first head of our network outputs dense plane coefficient maps which are afterwards converted to depth maps, as shown in <ref type="figure">Fig. 2</ref>. Predicting plane coefficients is motivated by the fact that two pixels p and q that belong to the same plane ideally have equal plane coefficient representations, whereas they generally have different depth. Thus, using the plane coefficient representation of q for predicting depth at the position of p results in a correct prediction if the pixels belong to the same plane.</p><p>We leverage this property by learning to identify seed pixels which share the same plane as the examined pixel, whenever such pixels exist, in order to selectively use the plane coefficients of these pixels for improving the predicted depth. This approach is motivated by a piecewise planarity prior which states that for each pixel p with an associated 3D plane, there is a seed pixel q in the neighborhood of p which is associated with the same 3D plane as p. To predict depth with this scheme, we need to identify (i) the regions where the prior is valid and (ii) the seed pixels in these regions, by predicting the offsets q ? p. We thus design a second head in the network, which outputs a dense offset vector field and a confidence map, as shown in <ref type="figure">Fig. 2</ref>. The predicted offsets are used to resample the plane coefficients from the first head and generate a second depth prediction. The depth predictions from the two heads are then fused adaptively using the confidence map as fusion weights, in order to down-weigh the offset-based prediction and rely primarily on the basic depth prediction in regions where the piecewise planarity prior is not valid, e.g. on parts of the scene with high-frequency structures. Supervision on the offsets and confidence map is applied implicitly, by supervising the fused depth prediction. Thanks to using seed pixels for prediction, our model implicitly learns to group pixels based on their membership in smooth regions of the depth map. This helps preserve sharp depth discontinuities, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Last but not least, we propose a mean plane loss which enforces first-order consistency of our predicted 3D surfaces with the ground truth and further improves performance.</p><p>We evaluate our method extensively on 6 datasets for supervised monocular depth estimation: NYU Depth-v2, KITTI, ScanNet, SUN-RGBD, DIODE Indoor, and ETH-3D. Comparisons to competing approaches demonstrate that we set a new state of the art on NYU Depth-v2 and KITTI, surpassing the former best-performing method in all commonly used evaluation metrics on NYU and on the Garg split <ref type="bibr" target="#b16">[17]</ref> of KITTI. Moreover, in a challenging zero-shot transfer setup, we outperform the prior state of the art on ScanNet, SUN-RGBD, DIODE Indoor, and ETH-3D. We conduct a thorough ablation study and show quantitatively the merit of our novel formulation for depth prediction. We also provide qualitative comparisons with the prior state of the art, which evidence the high quality of our predictions, in particular when the latter are used for 3D reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Supervised monocular depth estimation assumes that ground-truth depth maps are available for training images and requires inference on single images. A notable early approach is Make3D <ref type="bibr" target="#b59">[60]</ref>, which explicitly handcrafts a piecewise planar structure on the scene and learns the associated parameters locally using a Markov random field. The multiscale network of <ref type="bibr" target="#b9">[10]</ref> pioneered the usage of deep CNNs in depth estimation by learning an end-to-end mapping from images to depth maps. Several works have afterwards focused on this setting, proposing i.a. (i) more advanced architectures such as residual networks <ref type="bibr" target="#b32">[33]</ref>, convolutional neural fields <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b73">74]</ref>, fusion of multiple scales in the frequency domain <ref type="bibr" target="#b34">[35]</ref>, transformer-based blocks that attend to global depth statistics <ref type="bibr" target="#b0">[1]</ref> and depth merging networks for handling multiple resolutions <ref type="bibr" target="#b50">[51]</ref>, (ii) losses that are better suited for depth prediction such as reverse Huber loss <ref type="bibr" target="#b32">[33]</ref>, classification loss <ref type="bibr" target="#b2">[3]</ref>, ordinal regression loss <ref type="bibr" target="#b12">[13]</ref>, pairwise ranking loss <ref type="bibr" target="#b71">[72]</ref> and adaptive combinations of several depth-related losses <ref type="bibr" target="#b35">[36]</ref>, and (iii) joint learning of depth with normals or semantic labels <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b72">73]</ref>. The ambiguity in depth shift and focal length scale in mixed-data setups is addressed in <ref type="bibr" target="#b78">[79]</ref> by applying 3D point cloud encoders to the lifted depth map. Our method belongs to this category and casts the depth prediction to a more appropriate space for exploiting regularities of input scenes.</p><p>Other depth estimation setups include unsupervised and semi-supervised monocular depth estimation as well as stereo-based depth estimation. Unsupervised learning of depth with stereo pairs based on novel view synthesis <ref type="bibr" target="#b10">[11]</ref> that uses an image reconstruction loss in which the predicted depth is used for warping one image of the pair to the frame of the other was introduced in <ref type="bibr" target="#b16">[17]</ref> and was cast in a fully differentiable formulation in <ref type="bibr" target="#b18">[19]</ref>. Further works in this direction leverage temporal information <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b82">83]</ref>. The need for stereo pairs in this framework was lifted in <ref type="bibr" target="#b85">[86]</ref>, which operates on monocular videos. Consistency of the estimated 3D structures and of ego-motion across video frames is enforced in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49</ref>]. Depth and ego-motion are combined with optical flow and semantics in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b79">80]</ref> and with edges in <ref type="bibr" target="#b76">[77]</ref>. Robustness to occlusions across video frames is achieved in <ref type="bibr" target="#b19">[20]</ref> with a minimum reprojection loss. The optimization is facilitated with specialized losses in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b61">62]</ref>. Recent methods exploit video input at test time <ref type="bibr" target="#b69">[70]</ref>, consistency to segmentation outputs <ref type="bibr" target="#b86">[87]</ref> and scale consistency across adjacent frames <ref type="bibr" target="#b67">[68]</ref>. Unsupervised approaches generally assume more complex training data than supervised ones and suffer from scale ambiguity and violations of the Lambertian assumption. Semisupervised depth estimation is introduced in <ref type="bibr" target="#b29">[30]</ref>, which combines sparse depth measurements with an image reconstruction loss. Dataset-specific assumptions on the pres-ence and the format of depth supervision are also relaxed in <ref type="bibr" target="#b39">[40]</ref>, which utilizes multi-view image collections for generating reliable large-scale depth supervision, and in <ref type="bibr" target="#b56">[57]</ref>, where diverse datasets providing different forms of supervision for monocular depth estimation are leveraged to generalize better on unseen data. Early stereo methods rely on hand-crafted matching costs <ref type="bibr" target="#b22">[23]</ref> for estimating disparity. Initial approaches that learned the matching function include <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b65">66]</ref>, while subsequent works rely on fully convolutional architectures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b49">50]</ref>. Stereo methods also assume more complex data in the form of stereo pairs both at training and testing, which prevents their application to more general and uncontrolled monocular settings.</p><p>Geometric priors for depth have been extensively studied in the literature. In particular, the piecewise planarity prior has been traditionally used in multi-view stereo <ref type="bibr" target="#b14">[15]</ref> and 3D reconstruction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> in order to make these problems amenable to faster optimization. These approaches involve explicit depth planes and fit these planes on image superpixels or point sets from input point clouds. Superpixellevel depth planes are also leveraged in depth denoising and completion <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b66">67]</ref>. In more recent, deep learningbased approaches, the incorporation of geometric priors is performed either explicitly by segmenting planes <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b81">82]</ref> or implicitly by properly designing the loss <ref type="bibr" target="#b80">[81]</ref>. Non-local 3D context is leveraged in the virtual normal framework of <ref type="bibr" target="#b77">[78]</ref> by using supervision from virtual planes which correspond to triplets of non-collinear points of the depth map. A non-local coplanarity constraint is embedded to the network in <ref type="bibr" target="#b23">[24]</ref> via a depth-attention volume. Surface normals are used in <ref type="bibr" target="#b46">[47]</ref> to increase geometric consistency on regular structures. A representation directly associated with coefficients of 3D planes in the image space without dependence on intrinsic camera parameters is used in <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b83">84]</ref> for estimating the dominant depth planes in the scenes. The same representation with plane coefficients is employed in <ref type="bibr" target="#b33">[34]</ref> to guide the upsampling modules of the decoder part of depth networks, achieving state-of-the-art performance. We also use this representation with plane coefficients, but contrary to <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b83">84]</ref>, we learn it without requiring annotations for planes. Instead, we optimize plane coefficients together with spatial offset vectors to learn to identify coplanar pixels and use this coplanarity for predicting depth. While offset vectors are also used in <ref type="bibr" target="#b54">[55]</ref> for post-processing depth by merely resampling the prediction, we incorporate offset vectors in a single end-to-end architecture and generate the prediction via interpolation with the plane associated with the seed pixel pointed by the offset. Our approach is loosely inspired by <ref type="bibr" target="#b51">[52]</ref>, which trains offset vectors to identify instance segmentation centers from annotated images, while we focus on depth prediction and operate without supervision for plane instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>As pointed out in Sec. 1, our network estimates depth by selectively combining depth from each pixel and its corresponding seed pixel. For this formulation to work, it is vital to use a common representation which can capture pixelwise depth as well as planarity information. We achieve this by using a plane coefficient representation similar to <ref type="bibr" target="#b83">[84]</ref>. We explain this representation and derive an analytical relation between plane coefficients and depth in Sec. 3.2, which allows us to supervise the network only with depth. The main advantage of the plane coefficient representation is that the depth of a pixel in the image can be directly computed by the plane coefficients of a different pixel, under the assumption that the two pixels are on the same plane. This advantage forms the basis of our planarity prior and the selective planar depth bootstrapping using seed pixels, which we explain in Sec. 3.3. Finally, in Sec. 3.4 we present an additional patch-level mean plane loss, which is complementary to the previous components and contributes independently to learning regular depth maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Monocular depth estimation requires learning a dense mapping f ? :</p><formula xml:id="formula_0">I(u, v) ? D(u, v),</formula><p>where I is the input image with spatial dimensions H ?W , D is the corresponding depth map of the same resolution, (u, v) are pixel coordinates in the image space and ? are the parameters of the mapping f . In the supervised setup, a ground-truth depth map D * is available for each image I at training time. During training, the parameters ? are optimized such that the function f ? minimizes the difference between the predicted depth and the ground-truth depth over the training set T . This can be formalized as</p><formula xml:id="formula_1">min ? (I,D * )?T L(f ? (I), D * ),<label>(1)</label></formula><p>where L is a loss function that penalizes deviations between the prediction and the ground truth. Furthermore, given a depth map D along with the camera intrinsics, we can backproject each pixel to the 3D space. Using the pinhole camera model and given the focal lengths (f x , f y ) and the principal point</p><formula xml:id="formula_2">(u 0 , v 0 ), every pixel p = (u, v) T is mapped to a 3D point P = (X, Y, Z) T according to Z = D(u, v), X = Z(u ? u 0 ) f x , Y = Z(v ? v 0 ) f y .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Plane Coefficient Representation for Depth</head><p>Suppose that the backprojected 3D point P corresponds to a planar part of the 3D scene. The equation of the associated plane in the point-normal form can be written as n ? P + d = 0, where n = (a, b, c) T is the normal vector to</p><formula xml:id="formula_3">? ? ? ? Plane Coefficients (C) Plane Coefficients to Depth (h) Plane Coefficients to Depth (h) Offset Vector Field Refinement (I) Adaptive Fusion Plane Coefficient Resampling backproject backproject o Off. Vec. Field Ref. F D i D s D* ? MPL ?(D f , D * ) Plane Coefficient Decoder Feature Extractor ?(D i , D * ) ?(D s , D * ) D f Image (I)</formula><p>Off. Vec. Field Decoder <ref type="figure">Figure 2</ref>. Overview of our end-to-end P3Depth method. P3Depth includes two output heads. The first head outputs pixel-level plane coefficients (C), while the second head outputs a dense offset vector field (o) identifying positions of seed pixels along with a confidence map (F ). Then, the plane coefficients of seed pixels are used to predict depth at each position. The resulting prediction (Ds) is adaptively fused with the initial prediction (Di) using the confidence map to account for potential deviations from precise local planarity.</p><p>the plane and ?d is the distance of the plane from the origin. Substituting P from <ref type="formula" target="#formula_2">(2)</ref> into the point-normal equation</p><formula xml:id="formula_4">yields 1 Z = ?a f x d ? u + ?b f y d ? v + 1 d ( a f x u 0 + b f y v 0 ? c) ? .<label>(3)</label></formula><p>Thus, for image regions that depict planar 3D surfaces, the inverse depth is an affine function of pixel position, where the coefficients encode both the camera intrinsics and the 3D plane. We reformulate (3) by introducing ? = ? 2 +? 2 +? 2 and normalizing ? =? ? , ? =? ? and</p><formula xml:id="formula_5">? =? ? into Z = [(?u + ?v + ?)?] ?1 .<label>(4)</label></formula><p>We term C = (?, ?, ?, ?) T as the plane coefficients. Using this notation, (4) can be written as Z = h(C, u, v). Instead of directly predicting depth, we design our model to have a plane coefficient head, which first predicts a dense plane coefficient representation C(u, v) and then applies (4) to compute an initial depth prediction which we denote with D i . More formally, the mapping f ? from Sec. 3.1 is now a com-</p><formula xml:id="formula_6">position f ? = h ? (g ? , p), where g ? : I(u, v) ? C(u, v) maps the input image to the plane coefficient representa- tion and h : (C(u, v), u, v) ? D i (u, v) applies (4) at each pixel. Supervision is applied to the output depth D i via (1).</formula><p>Predicting the plane coefficients as an intermediate output does not give a immediate advantage compared to directly predicting the depth. However, two pixels that depict the same 3D plane have the same parameters C, but generally a different depth. This fact is the core to the next part of the network, which allows to predict depth by selectively bootstrapping the plane coefficients from a seed pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning to Identify Seed Pixels</head><p>Let us assume we have one pixel p which belongs to a planar surface in 3D. By definition, every other pixel on this planar surface has the same C values. Thus, in an ideal world the network only has to predict C at one of these pixels, q, to get all their depth values correct. This pixel can be interpreted as the seed pixel that describes the plane. However, defining this seed pixel and the region in which the depth should be bootstrapped from it is non-trivial. Thus, in this work we let the network discover this seed pixel and the respective region.</p><p>Formally, let us start by defining our piecewise planarity prior which is a relaxed version of the previous idea. Definition 1. (Piecewise planarity prior) For every pixel p with an associated 3D plane, there exists a seed pixel q in the neighborhood of p which is also associated with the same plane as p.</p><p>Note that in general, there may exist multiple seed pixels or no seed pixel for p.</p><p>Given that the prior holds, the task of depth prediction for p can also be solved by identifying q, i.e., by predicting the offset o(p) = q ? p. Thus, we design our model so that it features a second, offset head and let this offset head predict a dense offset vector field o(u, v). The two heads of the network share a common encoder and have separate decoders, as shown in <ref type="figure">Fig. 2</ref>. We use the predicted offset vector field to resample the plane coefficients via</p><formula xml:id="formula_7">C s (p) = C(p + o(p)),<label>(5)</label></formula><p>using bilinear interpolation to handle fractional offsets. The resampled plane coefficients are then used to compute a second depth prediction</p><formula xml:id="formula_8">D s (u, v) = h(C s (u, v), u, v),<label>(6)</label></formula><p>based on the seed locations. This allows the network to bootstrap the depth from the seed pixel. However, the prior is not always valid, so the initial depth prediction D i may actually be preferable compared to the seed-based prediction D s . To account for such cases, the second head additionally predicts a confidence </p><formula xml:id="formula_9">D f (u, v) = F (u, v)D s (u, v)+(1?F (u, v))D i (u, v). (7)</formula><p>We apply supervision on each of D f , D i and D s in our model, by optimizing the following loss:</p><formula xml:id="formula_10">L depth = L(D f , D * ) + ?L(D s , D * ) + ?L(D i , D * ),<label>(8)</label></formula><p>with ? and ? being hyperparameters. In this way, we encourage (i) the plane coefficient head to output a representation that is accurate across all pixels even when they have a high confidence value and (ii) the offset head to learn high confidence values for pixels for which the planarity prior holds and low confidence values for the converse.</p><p>However, there is a caveat in this formulation. In particular, the model is not supervised directly on the offsets. In fact, it could simply predict zero offsets everywhere and still produce valid predictions D s and D f , which would be identical to D i . This unwanted behavior is avoided in practice thanks to the fact that the initial predictions D i are erroneously smoothed near depth boundaries, due to the regularity of the mapping f ? for the case of neural networks. As a result, for pixels on either side of a boundary, predicting a non-zero offset that points away from the boundary yields a lower value for L depth , because such an offset uses a seed pixel for D s which is further from the boundary and suffers from smaller error owing to smoothing. Also due to regularity of the mapping that generates the offset vector field, these non-zero offsets are propagated from the boundaries to the inner parts of regions with smooth depth, helping the network to predict non-trivial offsets.</p><p>In the fully-fledged version of our method, we cascade the offset vectors multiple times before resampling the plane coefficient maps. For example, a single cascading step samples the position p + o(p) + o(p + o(p)). Our motivation for this cascaded refinement is that seed pixels within the same planar region should converge to the center of the region, which helps accumulate information from more pixels in predicting the plane coefficients of the region. At the same time, pixels without a reliable seed pixel are anyway assigned a low confidence value, so cascading the offsets does not hurt the respective depth prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Mean Plane Loss</head><p>The assumption we use for formulating our mean plane loss is that given a pixel coordinate, its neighboring pixels should lie on the same plane in the 3D space. The normal n of this plane should satisfy an overdetermined system of linear equations. However, ground-truth depth maps are usually captured by consumer-level sensors with noisy measurements and limited precision, which renders the above regime for local fitting of normals inapplicable, as finding the true optimal solution is not guaranteed.</p><p>Even though this is a valid observation, depth measurements still contain comprehensive details about the scene structure. This information can be aggregated locally to enforce first-order consistency between the predicted and the ground-truth 3D surface. Normals are one way how this aggregation across patches can be performed. For an input patch, the corresponding normal n needs to satisfy An = b, s.t. ?n? 2 = 1, where A is a data matrix build by stacking the 3D points in the patch and b is a vector of ones. Following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b53">54]</ref>, the closed-form solution of this least-squares problem is:</p><formula xml:id="formula_11">n = (A T A) ?1 A T b (A T A) ?1 A T b 2 .<label>(9)</label></formula><p>To compute the mean plane loss, we first estimate surface normals for all K non-overlapping patches in D and D * and then penalize their difference via</p><formula xml:id="formula_12">L MPL = K k=1 ?n k ? n * k ? 1 .<label>(10)</label></formula><p>For patches with depth discontinuities, even when the n * k for patch k does not correspond to a ground-truth 3D plane, the mean plane loss still provides a useful supervision signal, as it penalizes local depth profiles that are inconsistent with n * k . Also, we do not require ground-truth normals, as opposed to <ref type="bibr" target="#b53">[54]</ref>. Given (9), we can see that the loss (10) directly affects the depth of all points inside the patch via A. Finally, the complete loss is L total = L depth + L MPL .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We structure this section as follows. We first discuss our experimental setup, i.e., datasets, evaluation metrics and implementation details for our method. We then compare our method to state-of-the-art approaches, followed by a thorough ablation study of our method. Ours GT <ref type="figure">Figure 4</ref>. Qualitative results on NYU Depth-v2. We compare our method against SOTA methods using the same examples as <ref type="bibr" target="#b77">[78]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>In this section, we present the datasets used to evaluate our approach. The NYU Depth-v2 and KITTI datasets are used as the main sets for training and testing. Additionally, we use four more datasets for testing our method in a zero-shot transfer setup in order to evaluate its generalization potential. Evaluation on all six datasets is performed using standard depth evaluation metrics explained below. NYU Depth-v2 <ref type="bibr" target="#b62">[63]</ref>. The NYU Depth-v2 dataset consists of 464 indoor scenes of size 640?480. These scenes are  <ref type="table" target="#tab_0">Table 1</ref>. Evaluation metrics. We use the standard depth estimation metrics for evaluation. In particular, we use root mean square error (RMSE) and its log variant (RMSElog), Log10 error, absolute (A.Rel) and squared (S.rel) mean relative error and the percentage of inlier pixels with ?. The maximum depth for KITTI is set to 50m and 80m for the Garg and Eigen splits respectively. For NYU Depth-v2, the maximum depth is set to 10m as per the Eigen split. Zero-shot transfer is performed by using a model trained on NYU Depth-v2 without additional fine-tuning. The maximum depth for this task is set to 10m across all four test datasets. In all evaluations, the predicted depth is rescaled so that its median matches that of the ground truth, as per standard practice. Implementation details. Our network includes two heads. The first head outputs four channels, one for each plane coefficient. The second head outputs three channels: one for each coordinate of the offsets and one for confidence. These heads are fed by a ResNet101 encoder <ref type="bibr" target="#b21">[22]</ref> initialized with pre-trained ImageNet <ref type="bibr" target="#b28">[29]</ref> weights. This initialization is important to achieve competitive results as in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b77">78]</ref>. The decoders, inspired from <ref type="bibr" target="#b70">[71]</ref>, are initialized with weights drawn from a normal distribution with ?= 0.01. The plane coefficient decoder is additionally equipped with a guidance module. See for details. The offset vectors are restricted via a tanh layer to have a maximum length of ? in normalized image coordinates. We set ? to 0.1 by default and apply two steps of cascaded refinement to the offsets. The confidence map is predicted through a sigmoid layer. For all experiments, we use a batch size of 8 and employ the Adam optimizer <ref type="bibr" target="#b25">[26]</ref> with a learning rate of 10 ?4 and a weight decay of 10 ?4 . We train our network for 25 epochs, although the model starts producing decent predictions from epoch 5. The learning rate is reduced every 5 epochs by a factor of 10 using a step scheduler. The training images are resized similarly to <ref type="bibr" target="#b33">[34]</ref>. For all direct depth losses, we use the loss formulation from <ref type="bibr" target="#b9">[10]</ref>. The loss weights ? and ? are set to 0.5. In addition, the mean plane loss is applied using the final depth prediction D f . The offset head performs better with dense supervision. Hence, D s is supervised using completed D * . To complete D * , the depth inpainting method from <ref type="bibr" target="#b62">[63]</ref> is used. The inpainted ground truth is also used for computing the mean plane loss to provide stability to the SVD algorithm for least squares. We set the patch size to 32 and K in (10) is set indirectly by the patch size and the image size. The data augmentation techniques from <ref type="bibr" target="#b33">[34]</ref> are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State of the Art NYU Depth-v2:</head><p>The results on NYU Depth-v2, which is the major indoor depth benchmark, are presented in <ref type="table" target="#tab_1">Table 2</ref>. We set the new state of the art on NYU Depth-v2, outperforming prior state-of-the-art (SOTA) methods across all six standard metrics. We achieve a superior relative performance gain of 9.18% in RMSE and 3.7% in A.Rel, while also improving ? 1 by 1.1%. This improvement in performance indicates that without using ground-truth planes as supervision, P3Depth learns an implicit representation of the planes which can benefit the overall depth estimation capability of the network. Qualitative results on NYU Depth-v2 support the above findings. In <ref type="figure">Fig. 4</ref>, we show the high-quality predictions generated by our method in comparison with SOTA methods using the same examples as in <ref type="bibr" target="#b77">[78]</ref>. It can be clearly observed that the surfaces which fit our piece-wise planar assumption, such as table, cupboard, and even smaller objects, e.g. computer screens, photo frames etc. have consistent predictions with sharp details in comparison with the SOTA methods. Overall, our method generates superior visual results. In some cases, especially w.r.t. the metric scale of the scene, results from <ref type="bibr" target="#b33">[34]</ref> are comparable to ours. Our method excels especially on man-made regular structures of the indoor scenes. What is more, the predicted depth maps produce 3D reconstructions which are consistent with ground-truth point clouds and preserve the structure of the scene better than competing methods, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>.</p><p>KITTI: The results on KITTI in <ref type="table" target="#tab_2">Table 3</ref> suggest that our method is fully applicable to outdoor datasets. In particular, we surpass prior state of the art on the Garg split (with maximum range of 50m) in all metrics by a significant margin. More specifically, we improve RMSE by 14.2% and ? 1 by 1.0%. This proves that our method takes advantage of regular structures in outdoor scenes to improve depth predictions. Moreover, our method is comparable to state of the art on the Eigen split <ref type="bibr" target="#b9">[10]</ref>, where the maximum range is 80m. The reason why our ranking is slightly lower on the Eigen split is that distant parts of the scene get projected to smaller regions and thus the extent of the respective smooth pieces of the depth map is also smaller, making it more difficult to predict a correct offset. Additionally, for the results on the KITTI benchmark suite, please refer to the appendix B. Overall, the method is able to handle planar objects quite well even in variable lighting conditions as <ref type="figure">Figure 6</ref>. Qualitative results on KITTI. We present the predicted offset vector fields (middle) and the depth estimates (right).</p><p>shown in <ref type="figure">Fig. 6</ref>. Although the sign board in the bottom image is brightly lit and the car in the middle image is badly lit, the method is able to detect the regularity of the object surface and predict consistent depth. The top image of <ref type="figure">Fig. 6</ref> demonstrates a limitation of our method. In particular, the road segments on either side of the pole of the traffic sign on the right get mapped to significantly different depth values because they are disconnected and thus do not interact in terms of plane coefficients, even though they belong to the same 3D plane. Additionally, the planar specular glass surface of the car in middle image is predicted incorrectly. This is due to the shortcomings of the sensor used to measure depth. The erroneous ground truth does not allow the network to learn the depth or the offset vector field in these regions.</p><p>Zero-shot experiments: In <ref type="table" target="#tab_3">Table 4</ref>, we prove the generalization ability of our method in a zero-shot setting where the test domains have not been seen during training. We achieve the best performance on the ScanNet <ref type="bibr" target="#b7">[8]</ref> and SUN-RGBD <ref type="bibr" target="#b63">[64]</ref> indoor datasets in all metrics. On DIODE Indoor and ETH-3D, <ref type="bibr" target="#b77">[78]</ref> performs best in terms of A.Rel, but we are by far the best in terms of both RMSE and ? 1 . This comparison shows that even when our method is trained only on an indoor dataset as NYU Depth-v2, it works well on a variety of datasets with different types of scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>We study the importance of the components of our method by ablating them in <ref type="table">Table 5</ref>. We observe that in a standalone setting, directly predicting depth is better than predicting planar coefficients. However, once we insert the second head which predicts offset vectors, a substantial benefit is obtained by using the plane coefficient representation compared to directly predicting depth. This demonstrates that the network learns to make effective use of local planar information at seed pixels to improve depth, thanks to the plane coefficient representation. Moreover, adding our guidance module provides a slight improvement. The ablation also verifies the utility of cascaded refinement of off-  <ref type="table">Table 5</ref>. Ablation study of components of our method. D: directly predicting depth, C: predicting plane coefficients, "Guid.": guidance module for plane coefficient decoder, "OV": offset vectors, "Ref.": cascaded refinement of offsets, "MPL": mean plane loss, "+": offset length is restricted to ? =0.3 instead of ? =0. sets, which yields better results than simply using a higher maximum offset length. Finally, adding our mean plane loss on top of plane coefficients, offset vectors and cascaded offset refinement yields SOTA results on NYU Depth-v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a supervised method for monocular depth estimation which leverages local planar information in the 3D scene in order to predict consistent depth values across smooth parts of the scene. The method uses a plane coefficient representation for depth, which enables to share information from seed locations and improve the predicted depth. We implicitly learn to predict the offsets to these seed locations and to weigh the depth obtained from them adaptively according to accuracy. We empirically validated our method on the major indoor and outdoor benchmarks for monocular depth estimation and set the new state of the art among supervised approaches, which shows the potential of well-selected geometric priors for depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>Encoder: Following the recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34]</ref>, we use a ResNet101 <ref type="bibr" target="#b21">[22]</ref> as the encoder for the image. Each ResNet block consists of series of convolution operations with stride of 2 and pooling operations. The receptive field of the convolution is increased by decreasing resolution of the feature maps. This helps to capture more contextual information while compromising the feature map resolution. The final size of the feature map is usually 1/32 of the input image. The original ResNet is designed for the image classification task. To utilize it for a per-pixel prediction task, we remove the last 3 layers, i.e. pooling layer, fullyconnected layer and the softmax layer. The ResNet encoder can be divided into 4 different blocks. Each block generates feature maps of different resolution (scales). These feature maps from different scales can be used as skip connections, i.e. fused with decoder outputs to integrate different level of semantic information. The output of the last encoder block is fetched to both decoder heads. Both decoder heads also receive the skip connection information.</p><p>Decoder: We base our decoder on <ref type="bibr" target="#b70">[71]</ref> following <ref type="bibr" target="#b71">[72]</ref>. We replace all ReLU operations with ELU <ref type="bibr" target="#b6">[7]</ref> nonlinearities. The decoder is assembled from three modules: 1) Feature fusion modules: For each of these modules, residual convolution block is used to transform the skip connection feature map from the ResNet encoder. The output of the residual convolution block is fused with output of last feature fusion block using summation operation. Finally, the feature maps are upsampled to match the resolution of next layers input. 2) Residual convolution modules: This module is a series of two units of ELU and 3 ? 3 convolution operations to merge the output of a previous decoder feature map output with a previous feature fusion module output 3) Adaptive output module: This is applied at the last stage to get the final output. It consist of two 3 ? 3 convolution operation followed by up-sampling. Plane coefficient decoder: The last layer of this decoder head is modified to output 4-channels for each planar coefficient instead of single channel depth. Offset Vector field decoder: The last layer of this decoder head is modified to output 3-channels, i.e. two channels for the offset vector field and one for the confidence. The offset vector field is restricted by tanh layers and the confidence is generated through a sigmoid layer. Plane coefficient guidance: This module is loosely based on <ref type="bibr" target="#b33">[34]</ref>. The output of each decoder block is passed through the Plane coefficient guidance module to generate 4 channels of plane coefficients. The output size of the guidance module is up-sampled to match the input size of last decoder layer. At the end, these plane coefficients from each scale are converted into depth. All these depth maps are concatenated with feature map of the previous decoder layer passed to the last decoder layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Results</head><p>KITTI Benchmark <ref type="bibr" target="#b17">[18]</ref>: In this section we present the results of KITTI Benchmark server evaluation. Note that we train our model only on the KITTI Eigen split <ref type="bibr" target="#b9">[10]</ref> training data. It can be seen in <ref type="table" target="#tab_5">Table 6</ref> that our results are on a par with SOTA methods and superior than the baseline. However, <ref type="bibr" target="#b33">[34]</ref> performs better on this test set. In comparison with <ref type="bibr" target="#b77">[78]</ref>, we have a better absolute relative error and our performance is comparable to <ref type="bibr" target="#b77">[78]</ref> in all other metrics. The drop in overall performance is expected considering the design of our method. Our method is specially designed to identify planar regions in the scene, to improve the depth quality. So, as the depth of the scene increases, the projections of distant parts of the scene get smaller. This causes difficulties in predicting offset vector field in these regions. We have already seen that our method produced the SOTA results on the Garg split <ref type="bibr" target="#b16">[17]</ref>, in which the maximum depth value is 50m. Due to the aforementioned reason, when tested on Eigen split <ref type="bibr" target="#b9">[10]</ref> with max depth of 80m, we observe degradation in the performance. The KITTI Benchmark extends beyond that with 80m+ distances, thus affecting our results due to similar reasons. Qualitative Results: Here, we present additional qualitative results on both KITTI <ref type="bibr" target="#b17">[18]</ref> and NYU Depth-v2 <ref type="bibr" target="#b62">[63]</ref> datasets. We start with some examples from the KITTI dataset. We present some of the best cases along with the failure cases on this dataset. Additionally, we provide visualizations of the predicted depth maps and offset vector fields on NYU Depth-v2. Finally, we use the predicted depth maps to reconstruct the scenes and demonstrate quality in 3D. We observe that the predicted depth maps produce 3D reconstructions which are consistent with ground-truth point clouds and preserve the structure of the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground-truth Depth Predicted Depth <ref type="figure">Figure 8</ref>. Visualization of predictions on KITTI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground-truth Depth Predicted Depth   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Real-world 3D scenes have a high degree of regularity. We propose a method which can exploit this regularity, by implicitly learning intermediate representations that contain useful information about local planes in the scene. The proposed end-toend model predicts high-quality depth maps with sharp edges at occlusion boundaries, which yield consistent 3D reconstructions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Ground truth planes vs. predicted offset vector field. The predicted offset vector at a pixel tends to point towards a seed pixel with which it shares the same plane coefficients. The left image shows the overlayed labels of the segmented planes on an example from NYU Depth-v2 and the right image shows the respective predicted offset vector field. The bottom left legend in the right image shows the color coding for the vector field. map F (u, v) ? [0, 1], which indicates the confidence of the model in using the predicted seed pixels for estimating depth via D s . The confidence map is leveraged to compute the final depth prediction by adaptively fusing D i and D s :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Reconstruction example from NYU Depth-v2. We compare the 3D reconstruction induced by our predicted depth to results using two SOTA depth estimation methods<ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b77">78]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Plane coefficient guidance module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Visualization of some failure cases on KITTI dataset. Image Predicted off. vec. field Predicted Depth Visualization of predictions on NYU Depth-v2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 .</head><label>11</label><figDesc>Additional reconstruction examples from NYU Depth-v2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Datasets used in our experiments. (*) uses mix of RGBD sensors. Sup.: supervision.</figDesc><table><row><cell>Dataset</cell><cell cols="4"># Training # Testing Sup. Type Scene Type</cell></row><row><cell cols="2">NYU Depth-v2 [63] 24,231</cell><cell>654</cell><cell>Kinect</cell><cell>Indoor</cell></row><row><cell>KITTI [18]</cell><cell>23,488</cell><cell>697</cell><cell>LiDAR</cell><cell>Outdoor</cell></row><row><cell>ScanNet [8]</cell><cell>-</cell><cell>2167</cell><cell>Kinect</cell><cell>Indoor</cell></row><row><cell>SUN-RGBD [64]</cell><cell>-</cell><cell>5050</cell><cell>Mixed*</cell><cell>Indoor</cell></row><row><cell>DIODE Indoor [65]</cell><cell>-</cell><cell>325</cell><cell>LiDAR</cell><cell>Indoor</cell></row><row><cell>ETH-3D [61]</cell><cell>-</cell><cell>454</cell><cell>LiDAR</cell><cell>Mixed</cell></row><row><cell cols="5">split into 249 scenes for training and 215 for testing. We</cell></row><row><cell cols="5">use the official split provided by previous methods [34] for</cell></row><row><cell cols="4">training and the test set is based on [10].</cell><cell></cell></row><row><cell cols="5">KITTI [18]. KITTI is an autonomous driving dataset con-</cell></row><row><cell cols="5">sisting of 61 outdoor scenes of different types. We employ</cell></row><row><cell cols="5">the standard depth estimation split proposed by Eigen et</cell></row><row><cell>al.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>[10] and Garg et al. [17], for training and testing. We use 32 scenes for training and 29 scenes for testing. Datasets used for zero-shot testing. To test the gener- alization of our P3Depth, we evaluate it on four datasets which are not seen during training: ScanNet, SUN-RGBD, DIODE Indoor and ETH-3D. The resolution of all images is reduced to 640?480. Details are provided in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of depth estimation methods on NYU Depth-v2<ref type="bibr" target="#b62">[63]</ref> test set. Comparison is performed on the Eigen split<ref type="bibr" target="#b9">[10]</ref>. (nF) is number of frames, (*) indicates self-supervised methods and ( ? ) denotes retrained results with train set from<ref type="bibr" target="#b33">[34]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="3">A.Rel Log10 RMSE Lower is better</cell><cell cols="3">? 1 Higher is better ? 2 ? 3</cell></row><row><cell></cell><cell cols="4">Plane detection based methods</cell><cell></cell></row><row><cell>PlaneNet [43]</cell><cell cols="2">0.142 0.060</cell><cell>0.514</cell><cell cols="3">0.812 0.957 0.989</cell></row><row><cell>PlaneRCNN [42]</cell><cell cols="2">0.124 0.077</cell><cell>0.644</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Yu et al. [82]</cell><cell cols="2">0.134 0.057</cell><cell>0.503</cell><cell cols="3">0.827 0.963 0.990</cell></row><row><cell>P 2 Net (5F)* [81]</cell><cell cols="2">0.147 0.062</cell><cell>0.553</cell><cell cols="3">0.801 0.951 0.987</cell></row><row><cell cols="3">StruMonoNet [76] 0.107 0.046</cell><cell>0.392</cell><cell cols="3">0.887 0.980 0.995</cell></row><row><cell cols="5">Other monocular depth estimation methods</cell><cell></cell></row><row><cell>Saxena et al. [60]</cell><cell>0.349</cell><cell>-</cell><cell>1.214</cell><cell cols="3">0.447 0.745 0.897</cell></row><row><cell>Karsch et al. [25]</cell><cell cols="2">0.349 0.131</cell><cell>1.21</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Liu et al. [46]</cell><cell cols="2">0.335 0.127</cell><cell>1.06</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ladicky et al. [32]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.542 0.829 0.941</cell></row><row><cell>Li et al. [38]</cell><cell cols="2">0.232 0.094</cell><cell>0.821</cell><cell cols="3">0.621 0.886 0.968</cell></row><row><cell>Wang et al. [69]</cell><cell cols="2">0.220 0.094</cell><cell>0.745</cell><cell cols="3">0.605 0.890 0.970</cell></row><row><cell>Liu et al. [45]</cell><cell cols="2">0.213 0.087</cell><cell>0.759</cell><cell cols="3">0.650 0.906 0.974</cell></row><row><cell>Roy et al. [58]</cell><cell cols="2">0.187 0.078</cell><cell>0.744</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AdaBins  ? [1]</cell><cell cols="2">0.178 0.078</cell><cell>0.595</cell><cell cols="3">0.698 0.937 0.988</cell></row><row><cell>Eigen et al. [10]</cell><cell>0.158</cell><cell>-</cell><cell>0.641</cell><cell cols="3">0.769 0.950 0.988</cell></row><row><cell>Chakrabarti [4]</cell><cell>0.149</cell><cell>-</cell><cell>0.620</cell><cell cols="3">0.806 0.958 0.987</cell></row><row><cell>Li et al. [39]</cell><cell cols="2">0.143 0.063</cell><cell>0.635</cell><cell cols="3">0.788 0.958 0.991</cell></row><row><cell>Laina et al. [33]</cell><cell cols="2">0.127 0.055</cell><cell>0.573</cell><cell cols="3">0.811 0.953 0.988</cell></row><row><cell>Fu et al. [14]</cell><cell cols="2">0.115 0.051</cell><cell>0.509</cell><cell cols="3">0.828 0.965 0.992</cell></row><row><cell>Yin et al. [78]</cell><cell cols="2">0.108 0.048</cell><cell>0.416</cell><cell cols="3">0.875 0.976 0.994</cell></row><row><cell>Huynh et al. [24]</cell><cell>0.108</cell><cell>-</cell><cell>0.412</cell><cell cols="3">0.882 0.980 0.996</cell></row><row><cell>Lee et al. [34]</cell><cell cols="2">0.110 0.047</cell><cell>0.392</cell><cell cols="3">0.885 0.978 0.994</cell></row><row><cell>Long et al. [47]</cell><cell cols="2">0.101 0.044</cell><cell>0.377</cell><cell cols="3">0.890 0.982 0.996</cell></row><row><cell>Ranftl et al. [56]</cell><cell cols="2">0.110 0.045</cell><cell>0.357</cell><cell cols="3">0.904 0.988 0.998</cell></row><row><cell>Ours</cell><cell cols="2">0.104 0.043</cell><cell>0.356</cell><cell cols="3">0.898 0.981 0.996</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of depth estimation methods on KITTI<ref type="bibr" target="#b17">[18]</ref>. Comparison is performed on the Eigen test split.</figDesc><table><row><cell>Method</cell><cell cols="4">A.Rel S.Rel RMSE RMSElog Lower is better</cell><cell>?1 Higher is better ?2 ?3</cell></row><row><cell></cell><cell></cell><cell cols="3">Garg split [17] cap: 50m</cell><cell></cell></row><row><cell>Garg et al. [17]</cell><cell cols="2">0.169 1.080</cell><cell>5.104</cell><cell>0.273</cell><cell>0.740 0.904 0.962</cell></row><row><cell cols="3">Godard et al. [19] 0.108 0.657</cell><cell>3.729</cell><cell>0.194</cell><cell>0.873 0.954 0.979</cell></row><row><cell>Kuznietsov [30]</cell><cell cols="2">0.108 0.595</cell><cell>3.518</cell><cell>0.179</cell><cell>0.875 0.964 0.988</cell></row><row><cell>Gan et al. [16]</cell><cell cols="2">0.094 0.552</cell><cell>3.133</cell><cell>0.165</cell><cell>0.898 0.967 0.986</cell></row><row><cell>Fu et al. [14]</cell><cell cols="2">0.071 0.268</cell><cell>2.271</cell><cell>0.116</cell><cell>0.936 0.985 0.995</cell></row><row><cell>AdaBins [1]</cell><cell>0.058</cell><cell>0.19</cell><cell>2.36</cell><cell>0.088</cell><cell>0.964 0.995 0.999</cell></row><row><cell>Lee et al. [34]</cell><cell cols="2">0.056 0.169</cell><cell>1.925</cell><cell>0.087</cell><cell>0.964 0.994 0.999</cell></row><row><cell>Ours</cell><cell cols="2">0.055 0.130</cell><cell>1.651</cell><cell>0.081</cell><cell>0.974 0.997 0.999</cell></row><row><cell></cell><cell></cell><cell cols="3">Eigen split [10] cap: 80m</cell><cell></cell></row><row><cell cols="3">Saxena et al. [60] 0.280 3.012</cell><cell>8.734</cell><cell>0.361</cell><cell>0.601 0.820 0.926</cell></row><row><cell>Eigen et al. [10]</cell><cell cols="2">0.203 1.548</cell><cell>6.307</cell><cell>0.282</cell><cell>0.702 0.898 0.967</cell></row><row><cell>Liu et al. [44]</cell><cell cols="2">0.201 1.584</cell><cell>6.471</cell><cell>0.273</cell><cell>0.680 0.898 0.967</cell></row><row><cell cols="3">Godard et al. [19] 0.114 0.898</cell><cell>4.935</cell><cell>0.206</cell><cell>0.861 0.949 0.976</cell></row><row><cell>Kuznietsov [30]</cell><cell cols="2">0.113 0.741</cell><cell>4.621</cell><cell>0.189</cell><cell>0.862 0.960 0.986</cell></row><row><cell>Gan et al. [16]</cell><cell cols="2">0.098 0.666</cell><cell>3.933</cell><cell>0.173</cell><cell>0.890 0.964 0.985</cell></row><row><cell>Fu et al. [14]</cell><cell cols="2">0.072 0.307</cell><cell>2.727</cell><cell>0.120</cell><cell>0.932 0.984 0.994</cell></row><row><cell>Yin et al. [78]</cell><cell>0.072</cell><cell>-</cell><cell>3.258</cell><cell>0.117</cell><cell>0.938 0.990 0.998</cell></row><row><cell>Lee et al. [34]</cell><cell cols="2">0.059 0.245</cell><cell>2.756</cell><cell>0.096</cell><cell>0.956 0.993 0.998</cell></row><row><cell>AdaBins [1]</cell><cell cols="2">0.067 0.278</cell><cell>2.96</cell><cell>0.103</cell><cell>0.949 0.992 0.998</cell></row><row><cell>Ranftl et al. [56]</cell><cell>0.062</cell><cell>-</cell><cell>2.573</cell><cell>0.092</cell><cell>0.959 0.995 0.999</cell></row><row><cell>Ours</cell><cell cols="2">0.071 0.270</cell><cell>2.842</cell><cell>0.103</cell><cell>0.953 0.993 0.998</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of SOTA methods on generalized learning of metric depth. All methods are trained on NYU Depth-v2 and tested on four other datasets without fine-tuning.</figDesc><table><row><cell>Dataset</cell><cell>Metric</cell><cell>VNL [78]</cell><cell>BTS [34]</cell><cell>Ours</cell></row><row><cell></cell><cell>A.Rel ?</cell><cell>0.227</cell><cell>0.255</cell><cell>0.223</cell></row><row><cell>ScanNet [8]</cell><cell>RMSE ?</cell><cell>0.563</cell><cell>0.615</cell><cell>0.538</cell></row><row><cell></cell><cell>? 1 ?</cell><cell>0.544</cell><cell>0.472</cell><cell>0.551</cell></row><row><cell></cell><cell>A.Rel ?</cell><cell>0.317</cell><cell>0.317</cell><cell>0.307</cell></row><row><cell>SUN-RGBD [64]</cell><cell>RMSE ?</cell><cell>0.449</cell><cell>0.461</cell><cell>0.431</cell></row><row><cell></cell><cell>? 1 ?</cell><cell>0.793</cell><cell>0.794</cell><cell>0.797</cell></row><row><cell></cell><cell>A.Rel ?</cell><cell>0.291</cell><cell>0.310</cell><cell>0.373</cell></row><row><cell>Diode Indoor [65]</cell><cell>RMSE ?</cell><cell>0.890</cell><cell>0.981</cell><cell>0.784</cell></row><row><cell></cell><cell>? 1 ?</cell><cell>0.635</cell><cell>0.559</cell><cell>0.639</cell></row><row><cell></cell><cell>A.Rel ?</cell><cell>0.331</cell><cell>0.366</cell><cell>0.343</cell></row><row><cell>ETH-3D [61]</cell><cell>RMSE ?</cell><cell>1.649</cell><cell>1.840</cell><cell>1.637</cell></row><row><cell></cell><cell>? 1 ?</cell><cell>0.462</cell><cell>0.398</cell><cell>0.468</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>1.</figDesc><table><row><cell cols="5">Pred. Guid. OV Ref. MPL</cell><cell cols="2">A.Rel ? RMSE ?</cell><cell>?1 ?</cell></row><row><cell>D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.142</cell><cell>0.458</cell><cell>0.821</cell></row><row><cell>C</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.144</cell><cell>0.487</cell><cell>0.811</cell></row><row><cell>C</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>0.142</cell><cell>0.458</cell><cell>0.824</cell></row><row><cell>D</cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>0.140</cell><cell>0.453</cell><cell>0.824</cell></row><row><cell>C</cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>0.116</cell><cell>0.390</cell><cell>0.877</cell></row><row><cell>C</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>0.118</cell><cell>0.395</cell><cell>0.872</cell></row><row><cell>C</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>0.115</cell><cell>0.384</cell><cell>0.879</cell></row><row><cell>C</cell><cell>?</cell><cell>?+</cell><cell></cell><cell></cell><cell>0.116</cell><cell>0.390</cell><cell>0.879</cell></row><row><cell>D</cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell>0.134</cell><cell>0.440</cell><cell>0.839</cell></row><row><cell>C</cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell>0.113</cell><cell>0.378</cell><cell>0.884</cell></row><row><cell>C</cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>0.109</cell><cell>0.370</cell><cell>0.890</cell></row><row><cell>C</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>0.109</cell><cell>0.373</cell><cell>0.889</cell></row><row><cell>C</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>0.104</cell><cell>0.356</cell><cell>0.898</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Results of KITTI Evaluation Server.</figDesc><table><row><cell>Method</cell><cell cols="4">SILog sqErrorRel absErrorRel iRMSE</cell></row><row><cell cols="2">Official Baseline 18.19</cell><cell>7.32</cell><cell>14.24</cell><cell>18.50</cell></row><row><cell cols="2">VNL [78] 12.65</cell><cell>2.46</cell><cell>10.15</cell><cell>13.02</cell></row><row><cell cols="2">BTS [34] 11.67</cell><cell>2.21</cell><cell>9.04</cell><cell>12.23</cell></row><row><cell cols="2">Ours 12.82</cell><cell>2.53</cell><cell>9.92</cell><cell>13.71</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is funded by Toyota Motor Europe via the research project TRACE-Z?rich. We thank Guolei Sun for sharing his GPU quota.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">AdaBins: Depth estimation using adaptive bins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Shariq Farooq Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast, approximate piecewise-planar modeling based on sparse structure-from-motion and superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Bodis-Szomoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayko</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Estimating depth from monocular images as classification using deep fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhouhan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3174" to="3182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Depth from a single image by harmonizing overcomplete local network predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust piecewise-planar 3d reconstruction and completion from large-scale unstructured point data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne-Laure</forename><surname>Chauve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Pons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DeepStereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Datadriven 3d primitives for single image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>David F Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Piecewise planar and non-planar stereo for urban scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with affinity, vertical pooling, and label enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised CNN for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset. The International</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Brostow. Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PLADE-Net: Towards pixel-level accuracy for self-supervised single-view depth estimation with neural positional encoding and distilled matting loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Luis</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschm?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="328" to="341" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Guiding monocular depth estimation using depth-attention volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lam</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Nguyen-Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Heikkil?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sing Bing</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2144" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Yoshua Bengio and Yann LeCun</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Aike</forename><surname>Term?hlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust consistent video depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejian</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semisupervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevhen</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning the matching function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubor</forename><surname>Ladick?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno>abs/1502.00652</idno>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubor</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Federico Tombari, and Nassir Navab</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Kyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Wook</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il Hong</forename><surname>Suh</surname></persName>
		</author>
		<idno>abs/1907.10326</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Single-image depth estimation based on Fourier domain analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Han</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyeok</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Rae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-loss rebalancing algorithm for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Han</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">StructDepth: Leveraging the structural regularities for self-supervised indoor depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danping</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxian</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Anton Van Den Hengel, and Mingyi He. Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A two-streamed network for estimating fine-scaled depth maps from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">MegaDepth: Learning singleview depth prediction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural RGB(r)D sensing: Depth and uncertainty from a video camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">PlaneRCNN: 3d plane detection and reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">PlaneNet: Piece-wise planar reconstruction from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adaptive surface normal constraint for depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">UnDEMoN: Unsupervised deep network for depth and ego-motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>V Madhu Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swagat</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Boosting monocular depth estimation models to high-resolution via content-adaptive multi-resolution merging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Miangoleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Dille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Sylvain Paris, and Yagiz Aksoy</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Instance segmentation by jointly optimizing spatial embeddings and clustering bandwidth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Don&apos;t forget the past: Recurrent depth estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishakh</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="6813" to="6820" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">GeoNet: Geometric neural network for joint depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Predicting sharp and accurate occlusion boundaries in monocular depth estimation using displacement fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ramamonjisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="973" to="992" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Make3D: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A multi-view stereo benchmark with highresolution images and multi-camera videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sch?ps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Feature-metric loss for self-supervised learning of depth and egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">DIODE: A Dense Indoor and Outdoor DEpth Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Vasiljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruotian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Falcon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">F</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Daniele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shakhnarovich</surname></persName>
		</author>
		<idno>abs/1908.00463</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Jure?bontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2287" to="2318" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Stereoscopic inpainting: Joint color and depth completion from stereo images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglun</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Can scale-consistent monocular depth be learned in a self-supervised scale-invariant manner?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linzhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlong</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The temporal opportunist: Self-supervised multi-frame monocular depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Monocular relative depth perception with web stereo data supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Structure-guided ranking loss for single image depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">PAD-Net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Structured attention guided convolutional neural fields for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Recovering 3d planes from a single image via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengting</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Strumononet: Structure-aware monocular 3d prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenpei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Erran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page" from="2021" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">LEGO: Learning edge with geometry all at once by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Learning to recover 3D scene shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">GeoNet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">P2Net: Patch-match and plane-regularization for unsupervised indoor depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Single-image piece-wise planar 3d reconstruction via associative embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chamara</forename><surname>Saroj Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">GeoLayout: Geometry driven room layout estimation based on depth maps of planes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Does computer vision matter for action?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brady</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno>2019. 1</idno>
	</analytic>
	<monogr>
		<title level="j">Science Robotics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">The edge of depth: Explicit constraints between segmentation and depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
