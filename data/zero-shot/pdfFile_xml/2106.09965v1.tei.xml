<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HifiFace: 3D Shape and Semantic Prior Guided High Fidelity Face Swapping</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="laboratory">Media Analytics and Computing Lab</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Chu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="laboratory">Media Analytics and Computing Lab</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Tencent</roleName><forename type="first">Youtu</forename><surname>Lab</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johann</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">/</forename><surname>Hififace</surname></persName>
						</author>
						<title level="a" type="main">HifiFace: 3D Shape and Semantic Prior Guided High Fidelity Face Swapping</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Equal Contribution. Work done when Yuhan and Xu are both interns at Youtu Lab, Tencent. ? Corresponding Author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Face swapping results generated by our HifiFace. The face in the target image is replaced by the face in the source image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this work, we propose a high fidelity face swapping method, called HifiFace, which can well preserve the face shape of the source face and generate photo-realistic results. Unlike other existing face swapping works that only use face recognition model to keep the identity similarity, we propose 3D shape-aware identity to control the face shape with the geometric supervision from 3DMM and 3D face reconstruction method. Meanwhile, we introduce the Semantic Facial Fusion module to optimize the combination of encoder and decoder features and make adaptive blending, which makes the results more photo-realistic. Extensive experiments on faces in the wild demonstrate that our method can preserve better identity, especially on the face shape, and can generate more photo-realistic results than previous state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Face swapping is a task of generating images with the identity from a source face and the attributes (e.g., pose, expression, lighting, background, etc.) from a target image (as shown in <ref type="figure">Figure 1</ref>), which has attracted much interest with great potential usage in film industry <ref type="bibr" target="#b0">[Alexander et al., 2009</ref>] and computer games.</p><p>In order to generate high-fidelity face swapping results, there are several critical issues: (1) The identity of the result face including the face shape should be close to the source face.</p><p>(2) The results should be photo-realistic which are faithful to the expression and posture of the target face and consistent with the details of the target image like lighting, background, and occlusion.</p><p>To preserve the identity of the generated face, previous works <ref type="bibr" target="#b8">[Nirkin et al., 2018;</ref><ref type="bibr" target="#b8">Nirkin et al., 2019;</ref><ref type="bibr" target="#b8">Jiang et al., 2020]</ref> generated inner face region via 3DMM fitting or face landmark guided reenactment and blend it into the target image, as shown in <ref type="figure">Figure 2</ref>(a). These methods are weak in identity similarity because 3DMM can not imitate the identity details and the target landmarks contain the identity of target image. Also, the blending stage restricts the change of face shape. As shown in <ref type="figure">Figure 2</ref>(b), <ref type="bibr" target="#b8">[Liu et al., 2019;</ref> draw support from a face recognition net-work to improve the identity similarity. However, face recognition network focuses more on texture and is insensitive to the geometric structure. Thus, these methods can not preserve the exact face shape robustly.</p><p>As for generating photo-realistic results, <ref type="bibr" target="#b8">[Nirkin et al., 2018;</ref><ref type="bibr" target="#b8">Nirkin et al., 2019]</ref> used Poisson blending to fix the lighting, but it tended to cause ghosting and could not deal with complex appearance conditions. <ref type="bibr" target="#b8">[Jiang et al., 2020;</ref><ref type="bibr" target="#b7">Zhu et al., 2020;</ref><ref type="bibr" target="#b8">Li et al., 2019]</ref> designed an extra learningbased stage to optimize the lighting or occlusion problem, but they are fussy and can not solve all problems in one model.</p><p>To overcome the above defects, we propose a novel and elegant end-to-end learning framework, named HifiFace, to generate high fidelity swapped faces via 3D shape and semantic prior. Specifically, we first regress the coefficients of source and target face by a 3D face reconstruction model and recombine them as shape information. Then we concatenate it with the identity vector from a face recognition network. We explicitly use the 3D geometric structure information and use the recombined 3D face model with source's identity, target's expression, and target's posture as auxiliary supervision to enforce precise face shape transfer. With this dedicated design, our framework can achieve more similar identity performance, especially on face shape.</p><p>Furthermore, we introduce a Semantic Facial Fusion (SFF) module to make our results more photo-realistic. The attributes like lighting and background require spatial information and the high image quality results need detailed texture information. The low-level feature in the encoder contains spatial and texture information, but also contains rich identity from the target image. Hence, to better preserve the attributes without the harm of identity, our SFF module integrates the low-level encoder features and the decoder features by the learned adaptive face masks. Finally, in order to overcome the occlusion problem and achieve perfect background, we blend the output to the target by the learned face mask as well. Unlike <ref type="bibr" target="#b8">[Nirkin et al., 2019]</ref> that used the face masks of the target image for directly blending, HifiFace learns face masks at the same time under the guidance of dilated face semantic segmentation, which helps the model focus more on the facial area and make adaptive fusion around the edge. HifiFace handles image quality, occlusion, and lighting problems in one model, making the results more photo-realistic. Extensive experiments demonstrate that our results surpass other State-of-the-Art (SOTA) methods on wild face images with large facial variations.</p><p>Our contributions can be summarized as follows:</p><p>1. We propose a novel and elegant end-to-end learning framework, named HifiFace, which can well preserve the face shape of the source face and generate high fidelity face swapping results.</p><p>3D-based Methods. 3D Morphable Models (3DMM) transformed the shape and texture of the examples into a vector space representation <ref type="bibr" target="#b1">[Blanz and Vetter, 1999]</ref>. <ref type="bibr" target="#b8">[Thies et al., 2016]</ref> transferred expressions from source to target face by fitting a 3D morphable face model to both faces. <ref type="bibr" target="#b8">[Nirkin et al., 2018]</ref> transferred the expression and posture by 3DMM and trained a face segmentation network to preserve the target facial occlusions. These 3D-based methods follow a sourceoriented pipeline like <ref type="figure">Figure 2</ref>(a) which only generates the face region by 3D fitting and blends it into the target image by the mask of the target face. They suffer from unrealistic texture and lighting because the 3DMM and the renderer can not simulate complex lighting conditions. Also, the blending stage limits the face shape. In contrast, our HifiFace accurately preserves the face shape via geometric information of 3DMM and achieves realistic texture and attributes via semantic prior guided recombination of both encoder and decoder feature. GAN-based Methods. GAN has shown great ability in generating fake images since it was proposed by <ref type="bibr" target="#b5">[Goodfellow et al., 2014]</ref>. <ref type="bibr" target="#b7">[Isola et al., 2017]</ref> proposed a general imageto-image translation method, which proves the potential of conditional GAN architecture in swapping face, although it requires paired data. The GAN-based face swapping methods mainly follow source-oriented pipeline or target-oriented pipeline. <ref type="bibr" target="#b8">[Nirkin et al., 2019;</ref><ref type="bibr" target="#b8">Jiang et al., 2020]</ref> followed source-oriented pipeline in <ref type="figure">Figure 2</ref>(a) which used face landmarks to compose face reenactment. But it may bring weak identity similarity, and the blending stage limited the change of face shape. <ref type="bibr" target="#b8">[Liu et al., 2019;</ref><ref type="bibr" target="#b8">Li et al., 2019]</ref> followed targetoriented pipeline in <ref type="figure">Figure 2</ref>(b) which used a face recognition network to extract the identity and use decoder to fuse the encoder feature with identity, but they could not robustly preserve exact face shape and is weak in image quality. Instead, HifiFace in <ref type="figure">Figure 2</ref>(c) replaces the face recognition network with a 3D shape-aware identity extractor to better preserve identity including the face shape and introduces an SFF module after the decoder to further improve the realism.</p><p>Among these, FaceShifter <ref type="bibr" target="#b8">[Li et al., 2019]</ref> and Sim-Swap <ref type="bibr" target="#b3">[Chen et al., 2020]</ref> follow target-oriented pipeline and can generate high fidelity results. FaceShifter <ref type="bibr" target="#b8">[Li et al., 2019]</ref> leveraged a two-stage framework and achieved state-of-theart identity performance. But it could not perfectly preserve the lighting despite using an extra fixing stage. However, Hi-fiFace can well preserve lighting and identity in one stage. Meanwhile, HifiFace can generate photo-realistic results with higher quality than FaceShifter. <ref type="bibr" target="#b3">[Chen et al., 2020]</ref> proposed weak feature matching loss to better preserve the attributes, but it harms the identity similarity. While HifiFace can better preserve attributes and do not harm the identity.  <ref type="figure">Figure 2</ref>: The pipelines of previous works and our HifiFace. (a) Source-oriented pipeline uses 3D fitting or reenactment to generate inner face region and blend it into the target image, in which Fr means the face region of the result. (b) Target-oriented pipeline uses a face recognition network to exact identity and combines encoder feature with identity in the decoder. (c) Our pipeline consists of four parts: the Encoder part, Decoder part, 3D shape-aware identity extractor, and SFF module. The encoder extracts features from It, and the decoder fuses the encoder feature and the 3D shape-aware identity feature. Finally, the SFF module helps further improve the image quality. part, 3D shape-aware identity extractor (Sec. 3.1), and SFF module (Sec. 3.2). First, we set I t as the input of the encoder and use several res-blocks <ref type="bibr" target="#b6">[He et al., 2016]</ref> to get the attribute feature. Then, we use the 3D shape-aware identity extractor to get 3D shape-aware identity. After that, we use res-block with adaptive instance normalization <ref type="bibr" target="#b8">[Karras et al., 2019]</ref> in decoder to fuse the 3D shape-aware identity and attribute feature. Finally, we use the SFF module to get higher resolution and make the results more photo-realistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">3D Shape-Aware Identity Extractor</head><p>Most GAN-based methods only use a face recognition model to obtain identity information in the face swapping task. However, the face recognition network focuses more on texture and is insensitive to the geometric structure. To get more exact face shape features, we introduce 3DMM and use a pretrained state-of-the-art 3D face reconstruction model <ref type="bibr" target="#b3">[Deng et al., 2019]</ref> as a shape feature encoder, which represents the face shape S by an affine model:</p><formula xml:id="formula_0">S = S(?, ?) =S + B id ? + B exp ?,<label>(1)</label></formula><p>whereS is the average face shape; B id , B exp are the PCA bases of identity and expression; ? and ? are the corresponding coefficient vectors for generating a 3D face. As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>(a), we regress 3DMM coefficients c s and c t , containing identity, expression, and posture of the source and target face by the 3D face reconstruction model F 3d . Then, we generate a new 3D face model by c f use with the source's identity, target's expression and posture. Note that posture coefficients do not decide face shape but may affect the 2D landmarks locations when computing the loss. We do not use the texture and lighting coefficients because the texture reconstruction still remains unsatisfactory. Finally, we concatenate the c f use with the identity feature v id extracted by F id , a pre-trained state-of-the-art face recognition model <ref type="bibr" target="#b7">[Huang et al., 2020]</ref>, and get the final vector v sid , called 3D shape-aware identity. Thus, HifiFace achieves well identity information including geometric structure, which helps preserve the face shape of the source image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic Facial Fusion Module</head><p>Feature-Level. The low-level feature contains rich spatial information and texture details, which may significantly help generate more photo-realistic results. Here, we propose the SFF module to not only make full use of the low-level encoder and decoder features, but also overcome the contradiction in avoiding harming the identity because of the target's identity information in low-level encoder feature.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 3</ref>(b), we first predict a face mask M low when the decoder features z dec are of size 1/4 of the target. Then, we blend z dec by M low and get z f use , formulated as:</p><formula xml:id="formula_1">z f use = M low z dec + (1 ? M low ) ?(z enc ),<label>(2)</label></formula><p>where z enc means the low-level encoder feature with 1/4 size of the original and ? means a res-block <ref type="bibr" target="#b6">[He et al., 2016]</ref>.</p><p>The key design of SFF is to adjust the attention of the encoder and decoder, which helps disentangle identity and attributes. Specifically, the decoder feature in non-facial area can be damaged by the inserted source's identity information, thus we replace it with the clean low-level encoder feature to avoid potential harm. While the facial area decoder feature, which contains rich identity information of the source face, should not be disturbed by the target, therefore we preserve the decoder feature in the facial area.</p><p>After the feature-level fusion, we generate I low to compute auxiliary loss for better disentangling the identity and attributes. Then we use a 4? Upsample Module F up which contains several res-blocks to better fuse the feature maps. Based on F up , it's convenient for our HifiFace to generate even higher resolution results (e.g., 512 ? 512). Image-Level. In order to solve the occlusion problem and better preserve the background, previous works <ref type="bibr" target="#b8">[Nirkin et al., 2019;</ref><ref type="bibr" target="#b8">Natsume et al., 2018]</ref> directly used the mask of the target face. However, it brings artifacts because the face shape may change. Instead, we use SFF to learn a sightly dilated mask and embrace the change of the face shape. Specifically, we predict a 3-channel I out and 1-channel M r , and blend I out to the target image by M r , formulated as: In summary, HifiFace can generate photo-realistic results with high image quality and well preserve lighting and occlusion with the help of the SFF module. Note that these abilities still work despite the change of face shape, because the masks have been dilated and our SFF benefits from inpainting around the contour of predicted face.</p><formula xml:id="formula_2">I r = M r I out + (1 ? M r ) I t .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Function</head><p>3D Shape-Aware Identity (SID) Loss. SID loss contains shape loss and ID loss. We use 2D landmark keypoints as geometric supervision to constrain the face shape, which is widely used in 3D face reconstruction <ref type="bibr" target="#b3">[Deng et al., 2019]</ref>. First we use a mesh renderer to generate 3D face model by coefficients of source image identity and target image expression and posture. Then, we generate 3D face model of I r and I low by regressing 3DMM coefficients. Finally we project the 3D facial landmark vertices of reconstructed face shapes onto the image obtaining landmarks {q f use }, {q r } and {q low }:</p><formula xml:id="formula_3">L shape = 1 N N n=1 ||q f use n ? q r n || 1 + ||q f use n ? q low n || 1 . (4)</formula><p>Also, we use identity loss to preserve source image's identity:</p><formula xml:id="formula_4">L id =(1 ? cos(? id (I s ), ? id (I r )))+ (5) (1 ? cos(? id (I s ), ? id (I low ))),</formula><p>where ? id means the identity vectors generated by F id , and cos(, ) means the cosine similarity of two vectors. Finally, our SID loss is formulated as:</p><formula xml:id="formula_5">L sid = ? shape L shape + ? id L id ,<label>(6)</label></formula><p>where ? id = 5 and ? shape = 0.5. Realism Loss. Realism loss contains segmentation loss, reconstruction loss, cycle loss, perceptual loss, and adversarial loss. Specifically, M low and M r in the SFF module are both under the guidance of a SOTA face segmentation network HRNet <ref type="bibr">[Sun et al., 2019]</ref>. We dilated the masks of the target image to eliminate the limitation in face shape change and get M tar . The segmentation loss is formulated as:</p><formula xml:id="formula_6">L seg = ||R(M tar ) ? M low || 1 + ||M tar ? M r || 1 ,<label>(7)</label></formula><p>where R(.) means the resize operation. If I s and I t share the same identity, the predicted image should be the same as I t . So we use reconstruction loss to give pixel-wise supervision:</p><formula xml:id="formula_7">L rec = ||I r ? I t || 1 + ||I low ? R(I t )|| 1 .<label>(8)</label></formula><p>The cycle process can be conducted in the face swapping task too. Let I r as the re-target image and the original target image as the re-source image. In the cycle process, we hope to generate results with re-source image's identity and retarget image's attributes, which means it should be the same as the original target image. The cycle loss is a supplement of pixel supervision and can help generate high-fidelity results:</p><formula xml:id="formula_8">L cyc = ||I t ? G(I r , I t )|| 1 ,<label>(9)</label></formula><p>where G means the whole generator of HifiFace.</p><p>To capture fine details and further improve the realism, we follow the Learned Perceptual Image Patch Similarity (LPIPS) loss in <ref type="bibr" target="#b8">[Zhang et al., 2018]</ref> and adversarial objective in <ref type="bibr" target="#b3">[Choi et al., 2020]</ref>. Thus, our realism loss is formulated as:</p><formula xml:id="formula_9">L real = L adv + ? 0 L seg + ? 1 L rec + ? 2 L cyc + ? 3 L lpips ,<label>(10)</label></formula><p>where ? 0 = 100, ? 1 = 20, ? 2 = 1 and ? 3 = 5.</p><p>Overall Loss. Our full loss is summarized as follows:  4 Experiments Implementation Details. We choose VGGFace2 <ref type="bibr" target="#b2">[Cao et al., 2018]</ref> and Asian-Celeb <ref type="bibr">[DeepGlint, 2020]</ref> as the training set. For our model with resolution 256 (i.e., Ours-256), we remove images with either size smaller than 256 for better image quality. For each image, we align the face using 5 landmarks and crop to <ref type="bibr">256?256 [Li et al., 2019]</ref>, which contains the whole face and some background regions. For our more precise model (i.e., Ours-512), we adopt a portrait enhancement network  to improve the resolution of the training images to 512?512 as supervision, and also correspondingly add another res-block in F up of SFF compared to Ours-256. The ratio of training pairs with the same identity is 50%. ADAM [Kingma and Ba, 2014] is used with ?1 = 0; ?2 = 0.99 and learning rate = 0.0001. The model is trained with 200K steps, using 4 V100 GPUs and 32 batch size.</p><formula xml:id="formula_10">L = L sid + L real .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Qualitative Comparisons</head><p>First, we compare our method with <ref type="bibr">FSGAN [Nirkin et al., 2019]</ref>, SimSwap <ref type="bibr" target="#b3">[Chen et al., 2020]</ref> and FaceShifter <ref type="bibr" target="#b8">[Li et al., 2019]</ref> in <ref type="figure" target="#fig_2">Figure 4</ref>, <ref type="bibr">AOT [Zhu et al., 2020]</ref> and Deeper-Forensics (DF) <ref type="bibr" target="#b8">[Jiang et al., 2020]</ref> in <ref type="figure" target="#fig_3">Figure 5</ref>. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, FSGAN shares the same face shape with target faces and it can not well transfer the lighting of the target image either. SimSwap can not well preserve the identity of the source image especially for the face shape because it uses a feature matching loss and focuses more on the attributes. FaceShifter exhibits a strong identity preserva-   tion ability, but it has two limitations: (1) Attribute recovery, while our HifiFace can well preserve all the attributes like face color, expression, and occlusion.</p><p>(2) Complex framework with two stages, while HifiFace presents a more elegant end-to-end framework with even better recovered images. As shown in <ref type="figure" target="#fig_3">Figure 5(a)</ref>, AOT is specially designed to overcome the lighting problem but is weak in identity similarity and fidelity. As shown in <ref type="figure" target="#fig_3">Figure 5(b)</ref>, DF has reduced the bad cases of style mismatch, but is weak in identity similarity too. In contrast, our HifiFace not only perfectly preserves the lighting and face style, but also well captures the face shape of the source image and generates high quality swapped faces. More results can be found in our supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Comparisons</head><p>Next, we conduct quantitative comparison on FaceForensics (FF)++ <ref type="bibr" target="#b8">[Rossler et al., 2019]</ref> dataset with respect to the following metrics: ID retrieval, pose error, face shape error, and performance on face forgery detection algorithms to again demonstrate the effectiveness of our HifiFace. For FaceSwap <ref type="bibr">[FaceSwap, ]</ref> and FaceShifter, we evenly sample 10 frames from each video and compose a 10K test set. For SimSwap and our HifiFace, we generate face swapping results with the same source and target pairs above.</p><p>For ID retrieval and pose error, we follow the same setting in <ref type="bibr" target="#b8">[Li et al., 2019;</ref>. As shown in <ref type="table" target="#tab_0">Table 1</ref>, HifiFace achieves the best ID retrieval score and is comparable with others in pose preservation. For face shape error, we use another 3D face reconstruction model <ref type="bibr" target="#b8">[Sanyal et al., 2019]</ref> to regress coefficients of each test face. The error is computed by L2 distances of identity coefficients between the swapped face and its source face, and our HifiFace achieves the lowest face shape error. The parameter and speed comparisons are also shown in <ref type="table" target="#tab_0">Table 1</ref>, and our HifiFace is faster than FaceShifter, along with higher generation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Target</head><p>Ours-n3D Ours-256 Ours-512 To further illustrate the ability of HifiFace in controlling the face shape, we visualize the samplewise shape differences between HifiFace and FaceShifter <ref type="bibr" target="#b8">[Li et al., 2019]</ref> in <ref type="figure" target="#fig_4">Figure 6</ref>. The results show that, when the source and target differs much in face shape, HifiFace significantly outperforms Faceshifter with 95% samples having smaller shape errors.</p><p>Besides, we apply the models from FF++ <ref type="bibr" target="#b8">[Rossler et al., 2019]</ref> and DeepFake Detection Challenge (DFDC) <ref type="bibr">[Dolhansky et al., 2019;</ref><ref type="bibr">selimsef, 2020]</ref> to examine the realism performance of HifiFace. The test set contains 10K swapped faces and 10K real faces from FF++ for each method. As shown in <ref type="table" target="#tab_2">Table 2</ref>, HifiFace achieves the best score, indicating higher fidelity to further help improve face forgery detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of HifiFace</head><p>3D Shape-Aware Identity. To verify the effectiveness of shape supervision L shape on face shape, we train another model Ours-n3d, which replaces the shape-aware identity vector with the normal identity vector from F id . As shown in <ref type="figure">Figure 7</ref>, the results of Ours-n3d can hardly change the face shape or have obvious artifacts, while the results of Ours-256 can generate results with much more similar face shape.</p><p>Semantic Facial Fusion. To verify the necessity of the SFF module, we compare with 3 baseline models: (1) 'Bare' that removes both the feature-level and image-level fusion. (2) 'Blend' that removes the feature-level fusion. (3) 'Concat' that replaces the feature-level fusion with a concatenate. As shown in <ref type="figure">Figure 8</ref>, 'Bare' can not well preserve the background and occlusion, 'Blend' lacks legibility, and 'Concat' is weak in identity similarity, which proves that the SFF module can help preserve the attribute and improve the image quality without harming the identity.</p><p>Face Shape Preservation in Face Swapping. Face shape preservation is quite difficult for face swapping, which is not  just because of the difficulty in getting shape information, but also the challenge of inpainting when the face shape has changed. Blending is a valid way to preserve occlusion and background, but it is hard to be applied when the face shape changes. As shown in <ref type="figure">Figure 9</ref>, when the source face is fatter than the target face (row 1), it may limit the change of face shape in Blend-T. If we use Blend-DT or Blend-R, it can not well handle the occlusion. When the source face is thinner than the target (row 2), it is easy to bring artifacts around the face in Blend-T and Blend-DT and may cause a double face in Blend-R. In contrast, our HifiFace can apply the blending without the above issue, because our SFF module has the ability to inpaint the edge of the predicted mask.</p><p>To further illustrate how SFF addresses the problem, we show difference feature maps of every stage in the SFF module, named SFF-0?3, between the input of (I s ,I t ) and (I t ,I t ), where (I s ,I t ) obtains Ours-256 and (I t ,I t ) achieves target itself. In <ref type="figure">Figure 10</ref>, the bright area means where the face shape changes or contains artifacts. SFF module recombines the feature between the face region and non-face area and focuses more on the contour of the predicted mask, which brings great benefits for inpainting areas where the shape changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we propose a high fidelity face swapping method, named HifiFace, which can well preserve the face shape of the source face and generate photo-realistic results. A 3D shape-aware identity extractor is proposed to help preserve identity including face shape. An SFF module is proposed to achieve a better combination in feature-level and image-level for realistic image generation. Extensive experiments demonstrate that our method can generate higher fidelity results than previous SOTA face swapping methods both quantitatively and qualitatively. Last but not least, Hi-fiFace can also be served as a sharp spear, which contributes to the development of the face forgery detection community. a=0, b=1 a=0. <ref type="bibr">33, b=0.66 a=0.66, b=0.33 a=1, b=0 c=0.33, d=0.66 c=0.66, d=0.33 c=1, d=0</ref> c=0, d=1 a=1, b=0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Structures</head><p>Detailed structures of our HifiFace are given in <ref type="figure">Figure 12</ref>  <ref type="figure">Figure 12</ref>: Architectural details of HifiFace. <ref type="figure" target="#fig_1">Figure 13</ref>: More results on high resolution wild faces. The face in the target image is replaced by the face in the source image. <ref type="figure" target="#fig_2">Figure 14</ref>: Some results on high quality real world photos. The face in the target image is replaced by the face in the source image. This is to show that the faces generated by our method can be very naturally integrated into high-resolution shot real world scenes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Details of 3D shape-aware identity extractor and SFF module. (a) 3D shape-aware identity extractor uses F 3d (3D face reconstruction network) and F id (face recognition network) to generate shape-aware identity. (b) SFF module recombines the encoder and decoder feature by M low and makes the final blending by Mr. The Fup means the upsample Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Comparison with FSGAN, SimSwap and FaceShifter. Our results can well preserve the source face shape, target attributes and have higher image quality, even when handling occlusion cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>(a) Comparison with AOT. (b) Comparison with DF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Face shape error between Ir and Is in 200 FF++ pairs with large shape difference. Samples are sorted by shape error of HifiFace. Same column index indicates the same source/target pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Ablation study for 3D shape-aware identity extractor. Ablation study for SFF module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Comparison with results using directly mask blending. 'Blend-T', 'Blend-DT', and 'Blend-R' mean blending bare results to the target image by the mask of target, the dilated mask of target and the mask of bare results, respectively. Difference feature maps of SFF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Interpolated results with different compositions of SID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative Experiments on FaceForensics++. FPS is tested under GPU V100.</figDesc><table><row><cell cols="2">Method</cell><cell>ID?</cell><cell cols="4">Pose? Shape? MAC ? FPS?</cell></row><row><cell cols="2">FaceSwap</cell><cell>54.19</cell><cell>2.51</cell><cell>0.610</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">FaceShifter 97.38</cell><cell>2.96</cell><cell>0.511</cell><cell cols="2">121.79 22.34</cell></row><row><cell cols="2">SimSwap</cell><cell>92.83</cell><cell>1.53</cell><cell>0.540</cell><cell>55.69</cell><cell>31.17</cell></row><row><cell cols="2">Ours-256</cell><cell>98.48</cell><cell>2.63</cell><cell>0.437</cell><cell cols="2">102.39 25.29</cell></row><row><cell>(a)</cell><cell>Source</cell><cell>Target</cell><cell>AOT</cell><cell></cell><cell>Ours-256</cell><cell>Ours-512</cell></row><row><cell>(b)</cell><cell>Source</cell><cell>Target</cell><cell>DF</cell><cell></cell><cell>Ours-256</cell><cell>Ours-512</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>65.99 92.30 92.54 FaceShifter 41.62 42.92 77.18 76.50 SimSwap 76.44 72.63 78.80 78.44</figDesc><table><row><cell>Method</cell><cell cols="2">FF++</cell><cell cols="2">DFDC</cell></row><row><cell></cell><cell>AUC?</cell><cell>AP?</cell><cell>AUC?</cell><cell>AP?</cell></row><row><cell cols="2">FaceSwap 66.83 Ours-256 38.97</cell><cell>41.54</cell><cell>62.29</cell><cell>59.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results in terms of AUC and AP on FF++ and DFDC.</figDesc><table><row><cell>Sample</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>. For all residual units, we use the Leaky ReLU (LReLU) as the activation function. Resample means the Average Pooling or the Upsampling, which is used to change the size of feature maps. Res-Blocks with the Instance Normalization (IN) are used in encoder, while Res-Blocks with the Adaptive Instance Normalization (AdaIN) are used in decoder.</figDesc><table><row><cell>16?16?512</cell><cell>8?8?512</cell><cell>Res-Block</cell><cell>4?4?512</cell><cell>Conv 4?4+LReLU</cell><cell>1?1?512</cell><cell>Conv 1?1+LReLU</cell><cell>1?1?2</cell><cell>Real / Fake</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Discriminator</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. We propose a 3D shape-aware identity extractor, which can generate identity vector with exact shape information to help preserve the face shape of the source face.3. We propose a semantic facial fusion module, which can solve occlusion and lighting problems and generate results with high image quality.2 Related Work</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">ApproachLet I s be the source images and I t the target images, respectively. We aim to generate result image I r with the identity of the I s and the attributes of I t . As illustrated inFigure 2(c), our pipeline consists of four parts: the Encoder part, Decoder</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More Results</head><p>To analyse the specific impacts of the shape information from 3D face reconstruction model and the identity information from face recognition model, we adjust the composition of SID to generate interpolated results. It is formulated as:</p><p>where ? s , ? t and ? in means the 3D identity coefficients of source, target and interpolated image, ? s , ? t and ? in means source, target and interpolated image's identity vector from recognition model. As we can see in <ref type="figure">Figure 11</ref> rows 1 ? 4, we first fix c = 0 and d = 1, the face shape can still change but lake of identity detail. Then in rows 4 ? 7, we fix a = 1 and b = 0, and the identity becomes more similar. The results prove that the shape information control the basic of shape and identity, while the identity vector is helpful to the identity texture.</p><p>In the end, we download lots of wild face images from Internet and generate more face swapping results in <ref type="figure">Figure 13</ref> and <ref type="figure">Figure 14</ref> to demonstrate the strong capability of our methods. And more results can be found at https: //johann.wang/HifiFace.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Res-Block</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Creating a photoreal digital actor: The digital emily project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 Conference for Visual Media Production</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="176" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vetter ; Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08854</idno>
		<ptr target="http://trillionpairs.deepglint.com.Accessed" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops<address><addrLine>Brian Dolhansky, Russ Howes, Ben Pflaum, Nicole Baram</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Dolhansky et al., 2019. and Cristian Canton Ferrer. The deepfake detection challenge (dfdc) preview dataset</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><surname>Faceswap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faceswap</surname></persName>
		</author>
		<ptr target="https://github.com/ondyari/faceforensics/tree/master/dataset/faceswapkowalski" />
		<imprint>
			<biblScope unit="page" from="2020" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Curricularface: adaptive curriculum learning loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identity preserving generative adversarial network for cross-domain person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<idno>arXiv:1904.04514</idno>
		<ptr target="https://github.com/selimsef/dfdcdeep-fakechallenge" />
	</analytic>
	<monogr>
		<title level="m">Marc Stamminger, Christian Theobalt, and Matthias Nie?ner. Face2face: Real-time face capture and reenactment of rgb videos</title>
		<editor>Ke Sun, Yang Zhao, Borui Jiang, Tianheng Cheng, Bin Xiao, Dong Liu, Yadong Mu, Xinggang Wang</editor>
		<meeting><address><addrLine>Ba; Yuval Nirkin, Yosi Keller, and Tal Hassner; Thies, Michael Zollhofer</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Zhu et al., 2020] Hao Zhu, Chaoyou Fu, Qianyi Wu, Wayne Wu, Chen Qian, and Ran He. Aot: Appearance optimal transport based identity swapping for forgery detection. Advances in Neural Information Processing Systems</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
