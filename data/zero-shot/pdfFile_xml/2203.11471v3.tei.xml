<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Aibee Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenghai</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Beijing Technology and Business University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renliang</forename><surname>Weng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Aibee Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Aibee Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel monocular ray-based 3D (Ray3D) absolute human pose estimation with calibrated camera. Accurate and generalizable absolute 3D human pose estimation from monocular 2D pose input is an ill-posed problem. To address this challenge, we convert the input from pixel space to 3D normalized rays. This conversion makes our approach robust to camera intrinsic parameter changes. To deal with the in-the-wild camera extrinsic parameter variations, Ray3D explicitly takes the camera extrinsic parameters as an input and jointly models the distribution between the 3D pose rays and camera extrinsic parameters. This novel network design is the key to the outstanding generalizability of Ray3D approach. To have a comprehensive understanding of how the camera intrinsic and extrinsic parameter variations affect the accuracy of absolute 3D key-point localization, we conduct in-depth systematic experiments on three single person 3D benchmarks as well as one synthetic benchmark. These experiments demonstrate that our method significantly outperforms existing state-of-the-art models. Our code and the synthetic dataset are available at https://github.com/YxZhxn/Ray3D.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Accurate monocular 3D human pose estimation has found its wide applications in augmented reality <ref type="bibr" target="#b26">[26]</ref>, human-object interaction <ref type="bibr" target="#b6">[7]</ref>, and video action recognition <ref type="bibr" target="#b45">[45]</ref>. While the problem has been extensively studied in recent years, it's a well-known ill-posed problem <ref type="bibr" target="#b25">[25]</ref> with limited generalization capability. The problem becomes even more difficult when absolute 3D human pose estimation in a metric space is required, as knowing exactly where a human joint is in the World Coordinate System (WCS) is much more challenging than estimating the relative 3D offset of that joint from a reference point. While being more challenging, knowing absolute 3D poses is more desirable <ref type="figure">Figure 1</ref>. As shown in (a), if both the body size and the distance to camera are scaled up by twice, the projected 2D keypoints locations remain the same. Same phenomenon is observed in (b), where both the focal length and 3D distance are doubled. Z1 and Z2 refer to the distance from the person to the camera, H1 and H2 represent the height of camera from the ground plane. S1 and S2 are the scale of the person. f1 and f2 represent focal length of the camera.</p><formula xml:id="formula_0">Z ! S ! S " = 2S ! Z " = 2Z ! Ground plane Ground plane ! " = 2 ! (a) Z ! Z " = 2 Z ! f ! f " = 2 ! ! " = 1 (b)</formula><p>than the root-relative 3D poses in the real-world applications. For instance, unmanned store requires to detect the merchandise picked up by the customer, which relies on accurate hand localization in world coordinate system.</p><p>A key-point's 2D pixel location is jointly determined by the scale of person's body figure, camera intrinsic parameters, camera extrinsic parameters and 3D position in the world coordinate system. These factors introduce ambiguities for 3D pose estimation. For instance, as shown in <ref type="figure">Figure 1 (a)</ref>, if both the body size and the distance to camera are scaled up by twice, the projected 2D key-points locations remain the same. Similarly, if both the focal length and 3D distance are doubled, the 2D key-points keep the same, as illustrated in <ref type="figure">Figure 1 (b)</ref>. Typically, there are more than one configuration of 3D key-points that can generate the same observation of 2D key-points in the image plane. Thus, naively learning a model to map from 2D pixel locations to 3D world locations is arguably prone to failure.</p><p>To resolve these ambiguities, a number of monocular 3D human estimation approaches have been proposed <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b52">52]</ref>. These methods can be mainly categorized into two groups, i.e., lifting methods and image based methods. Lifting methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b55">55]</ref> take the 2D human poses as input and lift the 2D pose to 3D pose. A few lifting methods normalize the input according to image resolution <ref type="bibr" target="#b34">[34]</ref>, and camera principal point <ref type="bibr" target="#b4">[5]</ref>. While these normalization schemes improve the generalization ability to some extent, they fail to fully resolve the ambiguity due to variation in camera intrinsic parameters. On the other hand, image based approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b54">54]</ref> estimate the 3D root position based on the prior about the body size. In contrast, <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b52">52]</ref> rely on image-based human depth estimation for absolute root-keypoint localization. The issue with these learning-based depth estimation approaches is lack of sufficient training data with viewpoint variations. For instance, the model trained with front-view viewpoint may not generalize well to cameras with large pitch value. Moreover, they fail to fully address the aforementioned ambiguities.</p><p>To address the challenges more effectively, we propose our Ray3D method. Firstly, in order to have an intrinsicparameter-invariant representation, we convert the 2D keypoints in a pixel space to 3D rays in a normalized 3D space. With this simple design, our Ray3D approach achieves stable performance regardless of camera intrinsic parameter changes. Inspired by Videopose <ref type="bibr" target="#b34">[34]</ref> and RIE <ref type="bibr" target="#b36">[36]</ref>, we fuse 3D rays from consecutive frames by using temporal convolution in order to further resolve the ambiguity introduced by occlusion and to improve accuracy. This temporal fusion mechanism stabilizes the output and generates more accurate 3D locations. Secondly, we jointly embed the camera extrinsic parameters into the network. Camera extrinsic parameters contain essential information for accurate 3D human pose estimation. Arguably, exploiting camera extrinsic parameters is the only way to resolve the human body part size ambiguity. For instance, in <ref type="figure">Fig. 1 (a)</ref>, if the camera's height is known to be close to H 1 , we can safely eliminate incompatible hypotheses like the 3D pose with S 2 /H 2 . Therefore, it's essential to incorporate the camera extrinsic parameters into the network for accurate absolute localization. To our best knowledge, none of the existing learningbased 3D human pose estimation approaches explicitly utilize these information. In contrast, we directly take camera height and camera pitch value as input, and learn an independent camera embedding through a Multi-Layer Perceptron (MLP). This camera embedding is then concatenated with temporally fused ray features for 3D pose estimation.</p><p>To understand and diagnose the absolute 3D pose estimators, we conduct a series of comprehensive and systematic experiments. Specifically, we explicitly benchmark the robustness of the approaches against focal length, princi-pal point, camera pitch angle, camera height, camera yaw angle, body figure size variations on synthetic dataset. Furthermore, we evaluate generalization capability of these approaches on three single person benchmarks.</p><p>To summarize, the proposed method makes the following contributions,</p><p>? We convert the input space from 2D pixel space to 3D rays in a normalized coordinate system. This simple design effectively normalizes away the variations introduced by the camera intrinsic parameter changes as well as the camera pitch angle changes. ? We present a novel and simple network which learns a camera embedding using the camera extrinsic parameters, and jointly models the distribution of camera extrinsic parameters and 3D rays. ? We provide a comprehensive and systematic benchmarking of existing 3D approaches in terms of robustness against camera pose variations, as well as crossdataset generalization. ? Experiments on three real benchmark datasets and one synthetic dataset clearly demonstrates the advantages of our Ray3D approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Lifting based 3D human pose estimation</head><p>Lifting based 3D human pose estimation approaches learn a model to map from 2D human pose to 3D. While only using 2D coordinates without image features may appear sub-optimal, these methods enjoy good performancecost trade-off.</p><p>The majority of lifting based approaches work on rootrelative 3D human pose estimation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b55">55]</ref>. <ref type="bibr" target="#b28">[28]</ref> is the pioneer work that introduces the lifting design. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b53">53]</ref> exploit temporal information to improve the 3D pose estimation accuracy, especially for the occluded cases. Pose ambiguities can be partially resolved by exploiting the temporal context. Shan et al. <ref type="bibr" target="#b36">[36]</ref> propose to encode relative positional and temporal enhanced representations, and this approach achieves excellent rootrelative 3D pose accuracy. Inspired by <ref type="bibr" target="#b36">[36]</ref>, we encode relative 3D normalized rays to improve the root-relative pose model.</p><p>Only a handful of lifting approaches can be applied for absolute 3D human pose estimation. Pavllo et al. <ref type="bibr" target="#b34">[34]</ref> employ a trajectory model to estimate the 3D trajectory of the root joint. Chang et al. <ref type="bibr" target="#b4">[5]</ref> normalize the input by subtracting principal point of the camera and reconstruct up to the canonical root depth. This root depth is further multiplied with focal length to generate the final absolute depth. While these approaches achieve promising results, they fail to fully normalize the input by using camera intrinsics. Meanwhile,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D keypoints</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Plane</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalize Coordinate System (NCS)</head><p>World Coordinate System (WCS)</p><formula xml:id="formula_1">? ? 3 ( ! , " , ! , " , )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Rays</head><p>Camera Coordinate System (CCS)  <ref type="figure">Figure 2</ref>. An exposition of our Ray3D architecture. In pre-processing, we convert 2D input to ray-based 3D representation. These 3D rays are transformed to NCS, which are subsequently fed to pose estimation network and trajectory network to predict the final absolute 3D pose. With unnormalization, the 3D pose under world coordinate system is obtained.</p><formula xml:id="formula_2">1 ? 1 ? 3 1 ? ? 3 1 ? ? 3 1 ? ? 3 ? ? 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camera Embedding</head><p>camera extrinsics are simply ignored. Thus, their performance is susceptible to camera intrinsics/extrinsics variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Image based 3D human pose estimation</head><p>Image based approaches aim to improve the 3D estimation accuracy by directly utilizing image features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b40">[40]</ref><ref type="bibr" target="#b41">[41]</ref><ref type="bibr" target="#b42">[42]</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b54">54]</ref>. Rogez et al. <ref type="bibr" target="#b35">[35]</ref> frame the estimation problem as pose proposal generation, proposal scoring and proposal refinement. The 3D location of the root joint is obtained by minimizing the distance between 2D pose and projected 3D pose. Moon et al. <ref type="bibr" target="#b31">[31]</ref> devise a network to estimate the depth of root joint from cropped single person image, which inevitably loses contextual information of the subject. Alternatively, <ref type="bibr" target="#b40">[40]</ref><ref type="bibr" target="#b41">[41]</ref><ref type="bibr" target="#b42">[42]</ref><ref type="bibr" target="#b52">52]</ref> estimate the root depth from the full image up to a scale. The issue with this direction is the requirement of significant amount of training data with camera variations to make image-based depth estimation reliable. Meanwhile, camera extrinsics are not taken into account, which limits their generalizability. Additionally, <ref type="bibr" target="#b21">[21]</ref> firstly proposes to estimate perspective camera for 3D body pose regression. <ref type="bibr" target="#b21">[21]</ref> tries to estimate camera parameters along with the pose as well, which improves the generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Camera encoding</head><p>Few approaches have explicitly encoded camera extrinsics to assist the vision tasks. Nerf <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b47">47]</ref> is a popular 3D object reconstruction approach that directly takes 2D camera viewing angle into input. The 2D viewing angle (i.e., pitch and yaw) is concatenated with 3D location of object point and then processed by a multilayer perceptron network. Differently, our approach learns an camera embedding specifically for camera pitch and camera height from the ground plane. This embedding largely resolves the ambiguities in the absolute 3D human pose estimation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Intuitively, accurate monocular 3D absolute pose estimation relies on sufficient ambiguity reduction. Our method is designed to resolve ambiguities introduced by camera intrinsic parameter variation, body occlusion and camera pose variations with normalized representation of keypoints, temporal convolution and camera embedding correspondingly.</p><p>In <ref type="figure">Figure 2</ref>, we present the overview of the proposed Ray3D framework. To eliminate the impact of the intrinsic parameter variation, the 2D key-points are converted into 3D rays in Camera Coordinate System (CCS). To deal with the camera pitch angle variation, we further transform these 3D rays into (pitch) Normalized Coordinate System (NCS). Similarly, ground truth 3D poses are transformed to NCS as well. In this way, both the input and output of the model are aligned into the same coordinate system.</p><p>Temporal key-point motion information helps resolve 3D pose estimation ambiguity introduced by the occlusion <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b36">36]</ref>. Following <ref type="bibr" target="#b36">[36]</ref>, we fuse 3D rays from consecutive frames temporally and encode the relative pose rays to capture motion information. Different from <ref type="bibr" target="#b36">[36]</ref>, our Ray3D approach learns a camera embedding which can (implicitly) provide strong constraints to eliminate ambiguities in absolute 3D pose estimation. Specifically, we employ an MLP network to learn a compact embedding for camera pose representation. This camera embedding is subsequently concatenated with latent 3D ray features for the pose prediction. This novel design largely improves the model's robustness against camera pose and body scale variations.</p><p>Following <ref type="bibr" target="#b34">[34]</ref>, we decompose the problem into two sub-problems, i.e., root-relative pose estimation and root location estimation. These two sub-problems are solved by our pose network and trajectory network respectively. Specifically, the relative 3D pose generated by pose estimation network is added to the root joint coordinate predicted by trajectory network. Finally, the human pose are unnor- <ref type="figure">Figure 3</ref>. Normalized camera coordinate system is acquired by rotating the camera coordinate system along the x axis with degree of ? and translating along the z axis of the world coordinate system with distance of h. h is the height of camera in WCS. Such that the input and output of the lifting network are aligned in the same coordinate system. malized into World Coordinate System (WCS).</p><formula xml:id="formula_3">WCS Ground plane R !"# , !"# R $"# , $"# R $"! , $"! CCS X Y NCS X Y optical axis ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Input pre-processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intrinsic parameter decoupling</head><p>Lifting based 3D pose estimation approaches manage to lift predicted 2D key-points</p><formula xml:id="formula_4">{p i } J i=1 to 3D key-points {P C i } J i=1</formula><p>with deep neural network. p i = [x i , y i ] stands for the location of the i th joint of the person in the input image coordinate system and</p><formula xml:id="formula_5">P C i = [X C i , Y C i , Z C i ]</formula><p>represents the corresponding joint in CCS. J denotes the indices of joints. In order to achieve the invariance to the camera intrinsic parameter changes, we perform the following transformation to {p i } J i=1 (camera un-distortion can be added if needed):</p><formula xml:id="formula_6">x ray i = x i ? c x f x , y ray i = y i ? c y f y , z ray i = 1.<label>(1)</label></formula><p>Such that we have 3D rays {p ray</p><formula xml:id="formula_7">i } J i=1 = {[x ray i , y ray i , z ray i ]} J i=1 .</formula><p>In Eq. 1, c x and c y represent for camera center points, f x and f y denote the focal length. p ray i is a ray that points from optical center of the camera to the key-point i in the image plane.</p><p>Unlike <ref type="bibr" target="#b4">[5]</ref>, we completely eliminate the impact of focal length by explicitly normalize the ray representation with the calibrated focal length. Compared to <ref type="bibr" target="#b9">[10]</ref>, our 3D rays are converted to normalized rays in Normalized Coordinate System (NCS), which will be shortly discussed in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extrinsic parameter decoupling with the Normalized Coordinate System (NCS)</head><p>Define a key-point in the world, camera, and normalized coordinate system as P W , P C and P N respectively. With accurate calibration, one can acquire camera extrinsics including rotation matrix R W 2C , and translation vector T W 2C . And the transformation between P W and P C is as follows:  <ref type="figure">Figure 4</ref>. Overview of our lifting network. Our relative pose estimation and root joint estimation networks share the same RIE architectures. The network is equipped with positional and temporal enhanced representation. For more details, please refer to <ref type="bibr" target="#b36">[36]</ref>. MLP based camera embedding works as a plug-in to generate embedding features, and then concatenate with latent ray features for final pose prediction.</p><formula xml:id="formula_8">P C = R W 2C ? P W + T W 2C .<label>(2)</label></formula><p>In this paper, we aim to predict absolute 3D human pose in WCS. Camera pose in the 3D world can be determined by its 3D location, pitch, yaw and roll angles. Pitch, ?, describes the angle between the optical axis of the camera and the ground plane. With an assumption that camera yaw and roll values are close to 0, the camera pitch value and the camera height can uniquely specify the pose of a camera up to horizontal translation. In order to explicitly encode pitch for accurate pose estimation, we set up NCS as presented in <ref type="figure">Fig. 3</ref>. First, the CCS is rotated along the x axis of CCS to eliminate the pitch angle. Then, the coordinate system is translated along the y axis of CCS to the ground plane.</p><p>One can easily calculate the rotation matrix and translation vector between P C and P N :</p><formula xml:id="formula_9">R C2N = ? ? 1 0 0 0 cos ? sin ? 0 ? sin ? cos ? ? ? ,<label>(3)</label></formula><formula xml:id="formula_10">T C2N = 0 ?h 0 .<label>(4)</label></formula><p>According to the Eq. 2, 3 and 4, we have:</p><formula xml:id="formula_11">R W 2N = R C2N ? R W 2C ,<label>(5)</label></formula><formula xml:id="formula_12">T W 2N = R C2N ? T W 2C + T C2N ,<label>(6)</label></formula><formula xml:id="formula_13">P N = R C2N ? P C + T C2N ,<label>(7)</label></formula><formula xml:id="formula_14">P N = R W 2N ? P W + T W 2N .<label>(8)</label></formula><p>By applying the Eq. 7 to {p ray i } J i=1 and Eq. 8 to groundtruth 3D keypoints</p><formula xml:id="formula_15">{P W i } J i=1 , we can get normalized 3D rays {p ray i } J i=1 and normalized 3D ground-truth {P N i } J i=1</formula><p>. As a result, our Ray3D network is trained to lift from</p><formula xml:id="formula_16">{p ray i } J i=1 to {P N i } J i=1</formula><p>within the same coordinate system, which reduces the training difficulty and increases model robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Lifting network Absolute pose estimation</head><p>The task of estimating 3D absolute human poses is composed of two sub-problems, root location estimation (i.e., estimating the location of the center of mass of a person) and root-relative pose estimation (i.e., offset of each key point with respect to the center). We design our network to jointly learn to solve the two sub-problems with the trajectory network and the pose network, respectively (see <ref type="figure">Fig. 2</ref>). The outputs of these two networks are added to produce the absolute 3D poses.</p><p>Temporal motion information improves model's robustness against body occlusion. Inspired by <ref type="bibr" target="#b36">[36]</ref>, we adopt RIE architecture as our backbone network for both the rootrelative pose network and trajectory network. As shown in <ref type="figure">Fig. 4</ref>, RIE network is enhanced with positional and temporal information. Relative positions of input key-points are encoded as positional information within the frame, and differences between the 2D pose of current frame and the one from neighbouring frames are treated as temporal information. Such enhanced input are divided into 5 groups (torso, left arm, right arm, left leg, and right leg) for local feature learning. In addition, global feature is extracted from current frame to maintain the consistency of overall posture. Feature fusion module aggregates all these features for 3D pose estimation. Using this architecture, we replace vanilla 2D human key-points with our intrinsic-invariant normalized 3D rays as input for both pose network and trajectory network to address ambiguities. We refer the reader to <ref type="bibr" target="#b36">[36]</ref> for the details of RIE structure. Note that the contribution of this work is not on the specific network design, rather on the input representation and explicit embedding of camera extrinsic parameters. This novel design can be easily incorporated to the existing pose estimation framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camera embedding</head><p>We argue that camera extrinsic parameters are critical for absolute pose estimation in WCS, and propose to explicitly utilize extrinsic parameters by learning an independent camera embedding through a Multi-Layer Perceptron with ? and h as inputs. Specifically, the camera embedding module is constructed with two fully connected layers followed with batch normalization <ref type="bibr" target="#b16">[16]</ref>, rectified linear unit <ref type="bibr" target="#b32">[32]</ref> and Dropout <ref type="bibr" target="#b38">[38]</ref>. As shown in <ref type="figure">Fig. 4</ref>, this camera embedding is concatenated with temporally fused latent 3D ray features in both relative pose prediction and trajectory network. Therefore, both networks exploit camera extrinsic parameters for robust and accurate pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and results</head><p>The evaluation results of the proposed method with different experiment setups are reported in this section. First, datasets and evaluation metrics are introduced in Section 4.1, and details of implementation is described in Section 4.2. Section 4.3 showcases the comparison of our Ray3D and other state-of-the-arts on three public benchmarks. Then, generalization test result on a synthetic dataset is described in Section 4.4. Furthermore, the effectiveness of Ray3D's components is analyzed with ablation study in Section 4.5. Finally, the limitation of Ray3D is discussed in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation metrics</head><p>We evaluate our Ray3D on three public datasets captured with different camera poses and human poses. The camera intrinsics and extrinsics are provided by all datasets. The following datasets contain personal identifiable information about human subjects. All the subjects in these datasets have granted their permission for the dataset creation. Human3.6M (H36M) <ref type="bibr" target="#b17">[17]</ref> is a large-scale 3D human pose estimation dataset, which contains 3.6 million video frames recorded with four synchronized cameras. Following previous works <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b53">53]</ref>, five subjects (S1, S5, S6, S7, S8) and two subjects (S9, S11) with 17-key-point definition are used as training and testing data respectively for SOTA comparison. Humaneva-I [37] is a much smaller dataset compared with H36M , which is captured in a controlled indoor environment with three cameras. The proposed Ray3D representation requires well-calibrated intrinsic and extrinsic, as a result, Camera 2 and Camera 3 are removed due to bad camera calibration. MPI-INF-3DHP (3DHP) <ref type="bibr" target="#b29">[29]</ref> consists of 1.3 million video frames, which covers more diverse human motions than Hu-man3.6M. Following previous work <ref type="bibr" target="#b14">[14]</ref>, poses with 17 joints from Camera 0, 1, 2, 4, 5, 6, 7 and 8 are used for training. Sequence of TS1, TS3 and TS4 are adopted as test sets. TS2, TS5 and TS6 are excluded due to inaccurate or incomplete camera calibration.</p><p>In our experiments, we adopt following evaluation metrics: Mean Per Joint Position Error (MPJPE) in millimeters is used to evaluate root relative pose estimation results under CCS. To evaluate the performance of absolute pose under WCS, Absolute MPJPE (Abs-MPJPE) is adopted which calculates the difference between the prediction and GT pose in WCS. Mean of the Root Position Error (MRPE) proposed by <ref type="bibr" target="#b4">[5]</ref> is used to evaluate the root joint's trajectory prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>For our Ray3D approach, dimension of camera embedding is set as 64. Initial learning rate is 0.001. Adam op-  <ref type="bibr" target="#b14">[14]</ref> CVPR timizer with exponential learning rate decay factor of 0.99 is employed. Horizontal flip augmentation is adopted both in training and testing. For H36M dataset, we adopt Cascaded Pyramid Network (CPN) <ref type="bibr" target="#b7">[8]</ref> detected poses and GT 2D poses as input. As for Humaneva-I and 3DHP, only GT 2D poses are used.</p><formula xml:id="formula_17">'21 - - - - - - - - - - - - - - - 38.2 RIE (f = 9) [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on public benchmarks</head><p>In this section, we first compare our Ray3D with stateof-the-art methods under all 15 action sequences on H36M, then generalizability of these methods is evaluated by crossdataset testing. The comparing approaches include the latest PoseFormer <ref type="bibr" target="#b53">[53]</ref>, Videopose <ref type="bibr" target="#b34">[34]</ref>, PoseLifter <ref type="bibr" target="#b4">[5]</ref> and RIE <ref type="bibr" target="#b36">[36]</ref>. Note that PoseLifter was designed for absolute pose estimation. Different from Videopose and PoseLifter, both PoseFormer and RIE are incapable for absolute pose estimation. To test their capability for root-joint localization, we equip them with a trajectory model using their own network structure. For fair comparison, we carefully retrained PoseFormer, Videopose, PoseLifter and RIE with their provided source code under PyTorch <ref type="bibr" target="#b33">[33]</ref>. H36M evaluation <ref type="table" target="#tab_1">Table 1</ref> shows the performance of the methods that focus on root-relative pose prediction where ground truth 2D keypoints are taken as input. From the table, we can observe that our Ray3D obtains comparable results compared to SOTA methods. Specifically, MPJPE surpasses RIE <ref type="bibr" target="#b36">[36]</ref> by 0.4mm. <ref type="table" target="#tab_2">Table 2</ref> shows the results for absolute pose estimation in WCS using CPN <ref type="bibr" target="#b7">[8]</ref> detected 2D poses on H36M dataset. It can be seen that Ray3D outperforms all SOTA methods for both Abs-MPJPE and MRPE with clear margin. Compared with RIE, our method reduces Abs-MPJPE by 28.9mm and MRPE by 30.0mm respectively. It is worth noting that Ray3D outperforms PoseLifter by 31.3mm under Abs-MPJPE when no temporal information is used. These results demonstrate that Ray3D is effective and generates more accurate absolute 3D locations.</p><p>Among these four baseline methods, PoseLifter using single frame performs the worst. And Ray3D working with 9 frames surpasses Ray3D using single frame. This verifies the benefits of using temporal features for 3D pose estimation. Another interesting observation is that these baseline methods perform similarly in terms of MRPE. This shows the network structure design is not the critical factor for absolute pose estimation. Rather the input representation and camera embedding are the keys for accurate keypoint localization. Cross-dataset testing We train comparing models in 3DHP dataset, and evaluate them using H36M and Humaneva-I. 14-joint definition is applied for all datasets during crossdataset testing. For H36M and 3DHP, we remove mid spine, neck and chin keypoints. As for Humaneva-I, the thorax key-point is removed out of original 15 joints. As shown in <ref type="table" target="#tab_3">Table 3</ref>, none of the baselines work well in cross-scenario situations while the Ray3D shows good generalization performance in Humaneva-I and 3DHP dataset. This is because the camera intrinsic and extrinsic vary greatly across different scenes. Our Ray3D approach explicitly takes extrinsics(i.e., camera pitch and camera height) as input to learn a camera embedding, which results in improved generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on synthetic dataset</head><p>In this section, we conduct in-depth systematic experiments to benchmark 3D pose estimators' robustness against camera intrinsic, camera rotation (yaw), camera pitch, camera translation and person scale variations on a carefully cu-  rated synthetic benchmark. We use cameras from H36M to conduct camera augmentations. After the synthesis, the focal length of simulated cameras ranges from 1100 to 1180, where focal length in the training data ranges from 1143 to 1150. Camera rotation ranges from 0 to 360 degrees. Camera pitch ranges from 0 to 40 degrees. Camera translation ranges from 9 to 14 meters. The total length of human limbs ranges from 2.5 to 4.5 meters (roughly, the height of human ranges from 1 to 2 meters). Specifically, 100 virtual cameras are generated with fixed extrinsic for intrinsic generalization test, and 126 virtual cameras are generated with fixed intrinsic for extrinsic generalization test. We additionally simulated 324 cameras for training as well. Note that training and testing camera poses do not overlap. Five subjects (S1, S5, S6, S7, S8) and two subjects (S9, S11) with 14-key-point-definition are used for training and testing respectively. The detailed camera augmentation setting for training and testing are listed in the supplementary materials. To evaluate the effectiveness of camera embedding, we add a new baseline named Ray3D w/o CE where camera embedding branch is removed from Ray3D. Intrinsic generalization To verify the robustness of methods to camera intrinsic change, we change the focal length of the cameras with fixed resolution. As shown in the <ref type="figure" target="#fig_1">Fig. 5</ref> (a) and (b), focal length changes affect VideoPose, Pose-Former, RIE to varying degrees under MPJPE and MRPE metrics respectively. For instance, with only 4% variation on focal length, MRPE of the baseline approaches increase more than 50%. In contrast, both Ray3D and Ray3D w/o CE achieve stable result. This result clearly showcases the merits of our ray-based input representation. Extrinsic generalization To evaluate the impact of the change of extrinsics on generalizability, we change the rotation, pitch angles and translation of the camera pose respectively. Note that the translation is measured by the euclidean distance between camera and subject. In addition, we design a new baseline approach for root joint localization. Specifically, we estimate the height of the root joint using the mean average height from subjects of H36M, i.e., 93.95cm. Using this height assumption we may localize the root joint along its 3D ray. We term this approach as Ray Fixed Root Height (RFRH).</p><p>To measure the robustness of model against rotation variations, we rotate the camera around the scene center while keeping the camera height and camera pitch the same. As shown in <ref type="figure">Fig. 6 (a)</ref> and <ref type="figure">Fig. 7 (a)</ref>, Ray3D outperforms baseline methods by large margin on both MPJPE and MRPE, which indicates Ray3D is not only able to accurately localize the joint in WCS, it's also able to estimate the rootrelative pose robustly. Ray3D w/o CE shows better results than RIE, which indicates that normalized ray representation works better than vanilla 2D keypoints. Among the baseline approaches, the learning-based approaches achieve better result than RFRH. This demonstrates the learning based methods indeed manage to resolve the ambiguities to some extent through data-driven way. RFRH performs poorly due to the violation of root height assumption in the evaluation dataset. For instance, when the subject sits down on the floor, the root joint height can be close to 0.</p><p>To evaluate robustness against camera pitch variation, we change pitch angle of the cameras to various degrees while keeping the same distance between camera and the subject. As shown in <ref type="figure">Fig. 6 (b)</ref> and <ref type="figure">Fig. 7 (b)</ref>, Ray3D consistently outperforms baselines at all pitch angles for MRPE and MPJPE.</p><p>Similarly, to evaluate model robustness against camera translation, we generate a batch of cameras with constant pitch angle and gradually changing distance from the subject. As shown in <ref type="figure">Fig. 6 (c)</ref> and <ref type="figure">Fig. 7 (c</ref>  H36M body poses to 0.6-1.1 times as done in PoseAug <ref type="bibr" target="#b14">[14]</ref>. The experimental results are shown in <ref type="figure">Fig. 6 (d) and 7 (d)</ref>. Accuracy of all the comparing approaches degrades notably when the body size is small. For instance, for the smallest body figure, MRPE of PoseFormer, RIE, Videopose reach 4 meters, which is even higher than rule-based RFRH. MRPE of Ray3D increases to 800mm, which is still much better than baselines. Throughout these systematic experiments on the synthetic dataset, we can safely arrive following conclusions. By converting 2D keypoints to normalized rays, both our Ray3D and Ray3D w/o CE achieve stable and accurate performance regardless of camera intrinsics variations. By adding camera embedding, Ray3D outperforms Ray3D w/o CE with clear margin for most of the testing cases except for a few camera settings in <ref type="figure">Fig. 7 (c)</ref>. This verifies the effectiveness of camera embedding for both rootrelative pose estimation and root joint absolute localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>In this experiment, we further study the effectiveness of intrinsic decoupling, camera coordinate normalization and camera embedding using cross-dataset setting. We use RIE network as baseline model and add our modules gradually. All the models are trained on H36M dataset and tested on 3DHP dataset with 17-keypoint 2D poses as input. Table 4 summarizes the results. As we can see, MRPE keeps decreasing when adding proposed modules. Intrinsic decoupling improves the baseline model with a large margin. Camera normalization also drives the model to be more generalizable. With camera embedding, the trajectory estimation produces the best MRPE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Discussion</head><p>Our Ray3D approach outperforms the baseline methods significantly in terms of generalizability both on the three realistic benchmarks and on the synthetic dataset. This clearly showcases the robustness of the Ray3D approach. However, Ray3D's performance drops when the body figure size varies a lot, as shown in <ref type="figure">Fig. 7 (d)</ref>. This is mainly due to the fact that all the training body poses are adult. Meanwhile, our approach assumes the subject is on the ground plane. The model may fail if the subject is off the ground for a long time (e.g., climbing a ladder). Additionally, calibrated camera parameters need to be provided, which limits the use cases of Ray3D. Accurate 3D pose estimation might be misused for surveillance applications where skeleton configuration estimation can assist person identification. We advocate proper usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present an innovative monocular absolute 3D human pose estimation approach named Ray3D. This approach gradually resolves the inherent ambiguities through a series of novel designs: conversion from 2D keypoints to 3D normalized rays; temporal fusion of 3D rays; inclusion of camera embedding. As a result, Ray3D significantly outperforms SOTA methods on three realistic benchmarks and one synthetic benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Calculation of camera height and camera pitch</head><p>In this section, we introduce ways of calculation of camera height and pitch. We assume that the WCS is constructed with respect to the constraint that axis x and axis y of the WCS are placed on the plane ground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camera height</head><p>The 3D coordinates of the perspective camera M = [X W , Y W , Z W ] T in the WCS is calculated with its own rotation matrix R W 2C , and translation vector T W 2C as follows:</p><formula xml:id="formula_18">M = ?R ?1 W 2C ? T W 2C .<label>(9)</label></formula><p>Such that the camera height equals to Z W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camera pitch</head><p>Define the unit vector along the camera optical axis as V C = [0, 0, 1] T in the CCS. One can easily calculate its representation V W in the WCS as:</p><formula xml:id="formula_19">V W = R C2W ? V C .<label>(10)</label></formula><p>Then it is straightforward to compute camera pitch ? that defines the angle between the optical axis of the camera and the ground plane in the WCS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Synthetic dataset generation</head><p>To systematically evaluate the robustness of 3D human pose estimators against variations of camera intrinsic and extrinsic parameters, we create a synthetic dataset with intrinsic and extrinsic parameters augmentation from H36M dataset. In this section, we describe how we generate this synthetic dataset in details. First, the mathematics formulation of intrinsic/extrinsic augmentation is introduced. Then we describe the adopted augmentation parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intrinsic parameters augmentation</head><p>For camera intrinsic parameters augmentation, focal length f and principal points c are modified while keeping the projected 2D keypoints within the field of view. The augmentation is performed as follows:</p><formula xml:id="formula_20">f = f + ?f, c = c + ?c, s.t. 0 ? {x i } J i=1 ? W I , 0 ? {y i } J i=1 ? H I .<label>(11)</label></formula><p>H I and W I as the height and width of the image frame. x i and y i are the projected 2D keypoint location. Different camera intrinsics result in different 2D keypoints in the image. Without 3D ray representation, the network struggles to predict the same 3D pose facing camera intrinsic variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extrinsic parameters augmentation</head><p>For camera extrinsic parameters augmentation, we modify camera viewpoint ? (camera rotation), the relative distance between subject and the camera ? (camera translation) and pitch of the camera ? (camera pitch). The augmentation is conducted as follows with constraint that the projected 2D keypoints are in the image frame:</p><formula xml:id="formula_21">? = ? + ??, ? = ? + ??, ? = ? + ??, s.t. 0 ? {x i } J i=1 ? W I , 0 ? {y i } J i=1 ? H I .<label>(12)</label></formula><p>By changing camera rotation, camera translation and camera pitch, we may generate various located virtual cameras. Camera embedding is learned for every camera for generalisation of lifting network, which is helpful for accurate trajectory prediction. The specific augmentation parameters are detailed in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Augmentation parameters</head><p>Without loss of generality, we randomly select 1 camera whose id number is 55011271 from H36M to conduct camera augmentation. The overall summary of the dataset is shown in <ref type="table" target="#tab_6">Table 5</ref>.</p><p>To evaluate model performance against camera intrinsic variations, we generate the intrinsic testing dataset. Specifically, the focal length and the coordinate of principal points 1 of simulated cameras are augmented. Note that for training dataset and extrinsic testing set, camera intrinsics are the same as original set-up in H36M. For instance, the focal length of simulated cameras in the intrinsic testing set ranges from 1100 to 1180, compared with the cameras from training set whose focal length is in the 1143-1150 range. Similarly, for the x coordinate of principal point of simulated cameras, it has a longer range of 450 to 550 in the intrinsic testing dataset, compared with a range of 508 to 514 in the training dataset. In total, we have 100 virtual cameras generated with fixed extrinsic for intrinsic generalization test.</p><p>For extrinsic generalization test, camera rotation, camera pitch and camera translation are augmented. Specifically, camera rotation ranges from 0 to 360 degrees at 30 degree interval such that extrinsic-testing cameras evenly revolve around the subjects. Camera pitch ranges from 0 to 40 degrees, which covers both frontal-view camera and large-pitch cameras. The interval of camera pitch is 2 degrees for both training dataset and extrinsic testing dataset. Camera translation ranges from 9 to 14 meters such that the relative distance between camera and subject is changing from the near to the distant. The interval of camera translation is 0.76 meter for training and extrinsic testing cameras. In total, 126 virtual cameras are generated with fixed intrinsic for extrinsic generalization test. And 324 cameras are generated for training, such that camera embedding module learns to cope with vast range of camera pose variations. We set the augmentation parameter to the range which are common in the real-world scenarios (e.g., unmanned stores).</p><p>As for person scale generalization, the total length of augmented human limbs (bone length) ranges from 2.5 to 4.5 meters with the height of human ranging from 1 meter to 2 meters correspondingly. Note that the synthetic 3D human skeletons are only used for person scale generalization test, but excluded during model training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Supplementary experiments</head><p>In this section, we report additional evaluation results to fully analyze the proposed Ray3D on public and synthetic datasets. <ref type="table">Table 6</ref> shows the performance of the methods that focus on root-relative pose estimation where detected 2D keypoints are taken as input. When the number of video frames taken as input are similar, we can observe that our Ray3D obtains comparable results compared to SOTA methods under MPJPE metric in Camera Coordinate System (CCS). MPJPE of Ray3D surpasses Pose-Former <ref type="bibr" target="#b53">[53]</ref> and Videopose <ref type="bibr" target="#b34">[34]</ref> by 2.3mm and 0.9mm respectively, but Ray3D performs worse than RIE [36] by 1.1mm. We argue that Ray3D is designed for absolute 3D pose estimation in World Coordinate System (WCS), such performance of root-relative pose estimation in CCS is acceptable. <ref type="table">Table 7</ref> shows the results for absolute pose estimation in WCS using GT 2D poses on H36M dataset. It can be seen that Ray3D outperforms all SOTA methods for both Abs-MPJPE and MRPE with clear margin. Compared with RIE, our method reduces Abs-MPJPE by 9.6mm and MRPE by 4.2mm respectively. These results demonstrate that Ray3D is effective and generates more accurate absolute 3D locations. 3DHP evaluation <ref type="table" target="#tab_8">Table 8</ref> shows the results for absolute pose estimation in WCS using GT 2D poses on 3DHP dataset. One can observe that Ray3D outperforms all SOTA methods for both Abs-MPJPE and MRPE with clear margin. For instance, compared with PoseFormer <ref type="bibr" target="#b53">[53]</ref>, our method reduces Abs-MPJPE by 44.4mm and MRPE by 51.7mm respectively. Cross-dataset testing We train comparing models on H36M dataset, and evaluate them using H36M, Humaneva-I and 3DHP. 14-joint definition is applied for all datasets during cross-dataset testing. For H36M and 3DHP, we remove mid spine, neck and chin keypoints. As for Humaneva-I, the thorax key-point is removed out of original 15 joints. As shown in <ref type="table">Table 9</ref>, none of the baselines work well in cross-scenario situations while the Ray3D shows good generalization performance in H36M, Humaneva-I and 3DHP dataset. For instance, PoseFormer <ref type="bibr" target="#b53">[53]</ref> is able to predict better root-relative pose than Ray3D, but it struggles to predict precise root joint. And PoseLifter <ref type="bibr" target="#b4">[5]</ref> fails to generalize to cross datasets, achieving inferior MRPE performance. Evaluation with noisy cameras To test the robustness of Ray3D when taking noisy cameras parameters as input, we add gaussian noise to intrinsic parameters (i.e., focal length and center points) and extrinsic parameters (i.e., rotation and translation) of H36M's cameras respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Evaluation on public benchmarks H36M evaluation</head><p>The results of using noisy focal length and center points as input are shown in <ref type="figure" target="#fig_3">Fig. 8</ref> and <ref type="figure" target="#fig_4">Fig. 9</ref>. As for the intrinsic parameters, Videopose <ref type="bibr" target="#b34">[34]</ref>, PoseFormer <ref type="bibr" target="#b53">[53]</ref> and RIE <ref type="bibr" target="#b36">[36]</ref> do not use focal length and center points as input, noisy intrinsic parameters has no impact on these methods. While Ray3D and Poselifter <ref type="bibr" target="#b4">[5]</ref> explicitly decouple the intrinsic parameters from the input. Noisy intrinsic parameters cause inaccurate decoupling, which results in slight performance changes.</p><p>As for the extrinsic parameters, Videopose <ref type="bibr" target="#b34">[34]</ref>, Pose-Former <ref type="bibr" target="#b53">[53]</ref> and RIE <ref type="bibr" target="#b36">[36]</ref> use extrinsic parameters to convert final estimation from CCS to WCS, noisy extrinsic parameters cause performance degradation. Ray3D uses the well calibrated camera extrinsic parameters as an input, especially the camera height and camera pitch, which makes Ray3D sensitive to camera pitch change. As shown in the <ref type="figure" target="#fig_5">Fig. 10</ref>, after we added gaussian noise to camera pitch, the MPJPE increases from 81.2mm to 110.6mm. <ref type="figure" target="#fig_6">Fig. 11</ref> shows the performance with noisy camera yaw, Ray3D does not decrease significantly. As shown in <ref type="figure" target="#fig_7">Fig. 12</ref>, all methods <ref type="table">Table 6</ref>. Quantitative evaluation results under MPJPE on H36M using detected keypoints as input. (f = 9) means this approach utilizes 9 consecutive frames for pose estimation, and (f = 1) means the approach does not make use of temporal information. * means this approach using 2D keypoints detected by CPN. Best results are shown in bold.   have the same performance drop when provided with noisy camera translation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Evaluation on synthetic benchmarks</head><p>Integrate Videopose with Ray3D We integrate proposed Ray3D techniques with another baseline method Videopose <ref type="bibr" target="#b34">[34]</ref>. The model is trained and evaluated on the pro-  <ref type="table">Table 9</ref>. Cross dataset evaluation. We adopt a 14-joint skeleton training on H36M, testing on H36M, Humaneva-I and 3DHP datasets. MPJPE, Abs-MPJPE and MRPE are adopted. (f = 9) means this approach utilizes 9 consecutive frames for pose estimation, and (f = 1) means the approach does not make use of temporal information. The unit of all numbers is mm. The best results are in bold.  posed synthetic dataset. As shown in the <ref type="figure" target="#fig_9">Fig. 13</ref>, Videopose <ref type="bibr" target="#b34">[34]</ref> integrated with 3D ray representation and camera embedding techniques performs better than vanilla method under Abs-MPJPE metric. Same performance gain can be observed in the <ref type="figure">Fig. 14</ref> under MRPE metric, which showcases that Ray3D incorporated to the different existing frameworks bring consistent improvement.</p><p>Intrinsic generalization As shown in the <ref type="figure" target="#fig_1">Fig. 15 (a)</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative results in WCS</head><p>In this section, we provide qualitative results generated by Ray3D and other state-of-the-arts on H36M and 3DHP datasets. Specifically, we visualize 3D keypoints in WCS generated by corresponding methods. H36M <ref type="figure">Fig. 16</ref> shows qualitative comparison of Ray3D with VideoPose, RIE and PoseFormer on H36M. We train all four models on H36M with 17-joint definition. From the visualization, we can observe that Ray3D has superior ability to generate more precise location of root joint with comparable root-relative pose estimation results. <ref type="figure">Fig. 17</ref> presents two examples of inferior estimation of Ray3D compared to baseline, yet the error is close among these methods. 3DHP In <ref type="figure" target="#fig_3">Fig. 18</ref>, we present the qualitative comparison of Ray3D with VideoPose, RIE and PoseFormer on 3DHP as well. The models are trained with 14-joint definition. Our Ray3D shows better performance than other state-of-thearts clearly. Cross-dataset In <ref type="figure" target="#fig_4">Fig. 19</ref>, we compare generalization of Ray3D with other state-of-the-arts on H36M. We train all four models on 3DHP and test them on H36M with 14joint definition. One can clearly observe that Ray3D generates more accurate estimation results than other approaches, benefiting from our normalized ray representation and camera embedding design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-truth</head><p>VideoPose RIE PoseFormer Ray3D </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>performance under MPJPE and MRPE in case of focal length changes are plotted in (a) and (b) respectively. The x-axis represents the focal length of the virtual camera in pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figures (a), (b), (c) and (d) showcase the performance using MPJPE metric in case of rotation, camera pitch, translation and body scale variations correspondingly. The x-axis denotes the degree of camera rotation, the degree of camera pitch, euclidean distance between camera and subject in meters and the total length of human limbs in meters respectively.Figures (a), (b), (c) and (d) showcase the performance using MRPE metric in case of rotation, camera pitch, translation and body scale variations correspondingly. The x-axis denotes the degree of camera rotation, the degree of camera pitch, euclidean distance between camera and subject in meters and the total length of human limbs in meters respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Performance under MPJPE and MRPE with noisy focal length are plotted in (a) and (b) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>Performance under MPJPE and MRPE with noisy center points are plotted in (a) and (b) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 .</head><label>10</label><figDesc>Performance under MPJPE and MRPE with noisy camera pitch are plotted in (a) and (b) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 .</head><label>11</label><figDesc>Performance under MPJPE and MRPE with noisy camera yaw are plotted in (a) and (b) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 .</head><label>12</label><figDesc>Performance under MPJPE and MRPE with noisy translation are plotted in (a) and (b) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 .</head><label>13</label><figDesc>Figures (a), (b), (c) and (d) showcase the performance using Abs-MPJPE metric in case of rotation, camera pitch, translation and body scale variations correspondingly. The x-axis denotes the degree of camera rotation, the degree of camera pitch, euclidean distance between camera and subject in meters and the total length of human limbs in meters respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 .Figure 15 .</head><label>1415</label><figDesc>Figures (a), (b), (c) and (d) showcase the performance using MRPE metric in case of rotation, camera pitch, translation and body scale variations correspondingly. The x-axis denotes the degree of camera rotation, the degree of camera pitch, euclidean distance between camera and subject in meters and the total length of human limbs in meters respectively. Performance under MPJPE and MRPE in case of principal point changes are plotted in (a) and (b) respectively. The x-axis represents x-coordinate of 2D principal point of the virtual camera in pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 .Figure 17 .Figure 18 .Figure 19 .</head><label>16171819</label><figDesc>Qualitative comparison of Ray3D with VideoPose, RIE and PoseFormer on H36M. All four models are trained on H36M. First column shows 2D ground-truth poses. Black color denotes left part of person limbs, red color denotes right part of person limbs. 3D estimation results predicted by VideoPose, RIE, PoseFormer and Ray3D are shown in the second, third, forth and fifth column respectively. Dashed lines denote 3D ground-truth poses. Solid lines represent the poses estimated by corresponding approaches. Green and black color lines denotes left part of person limbs, blue and red lines denote right part of person limbs. 17-joint skeleton is visualised. Visualization of inferior performance of Ray3D, compared with other state-of-the-arts on H36M. All four models are trained on H36M. First column shows 2D ground-truth poses. Black color denotes left part of person limbs, red color denotes right part of person limbs. 3D estimation results predicted by VideoPose, RIE, PoseFormer and Ray3D are shown in the second, third, forth and fifth column respectively. Dashed lines denote 3D ground-truth poses. Solid lines represent the poses estimated by corresponding approaches. Green and black color lines denotes left part of person limbs, blue and red lines denote right part of person limbs. 17-joint skeleton is visualised. Qualitative comparison of Ray3D with VideoPose, RIE and PoseFormer on 3DHP. All four models are trained on 3DHP. First column shows 2D ground-truth poses. Black color denotes left part of person limbs, red color denotes right part of person limbs. 3D estimation results predicted by VideoPose, RIE, PoseFormer and Ray3D are shown in the second, third, forth and fifth column respectively. Dashed lines denote 3D ground-truth poses. Solid lines represent the poses estimated by corresponding approaches. Green and black color lines denotes left part of person limbs, blue and red lines denote right part of person limbs. 14-joint skeleton is visualised. Visualization of generalization of Ray3D, compared with other state-of-the-arts on H36M. All four models are trained on 3DHP. First column shows 2D ground-truth poses. Black color denotes left part of person limbs, red color denotes right part of person limbs. 3D estimation results predicted by VideoPose, RIE, PoseFormer and Ray3D are shown in the second, third, forth and fifth column respectively. Dashed lines denote 3D ground-truth poses. Solid lines represent the poses estimated by corresponding approaches. Green and black color lines denotes left part of person limbs, blue and red lines denote right part of person limbs. 14-joint skeleton is visualised.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluation results under MPJPE on H36M using GT keypoints as input. (f = 9) means this approach utilizes 9 consecutive frames for pose estimation, and (f = 1) means the approach does not make use of temporal information. Best results are shown in bold.</figDesc><table><row><cell>MPJPE</cell><cell></cell><cell>Dir.</cell><cell>Disc.</cell><cell>Eat.</cell><cell>Greet</cell><cell>Phone</cell><cell>Photo</cell><cell>Pose</cell><cell>Purch.</cell><cell>Sit</cell><cell>SitD.</cell><cell>Somke</cell><cell>Wait</cell><cell>WalkD.</cell><cell>Walk</cell><cell>WalkT.</cell><cell>Average</cell></row><row><cell>Hossain et al. [15]</cell><cell>ECCV'18</cell><cell>35.2</cell><cell>40.8</cell><cell>37.2</cell><cell>37.4</cell><cell>43.2</cell><cell>44.0</cell><cell>38.9</cell><cell>35.6</cell><cell>42.3</cell><cell>44.6</cell><cell>39.7</cell><cell>39.7</cell><cell>40.2</cell><cell>32.8</cell><cell>35.5</cell><cell>39.2</cell></row><row><cell>Liu et al. (f = 243) [27].</cell><cell>CVPR'20</cell><cell>34.5</cell><cell>37.1</cell><cell>33.6</cell><cell>34.2</cell><cell>32.9</cell><cell>37.1</cell><cell>39.6</cell><cell>35.8</cell><cell>40.7</cell><cell>41.4</cell><cell>33.0</cell><cell>33.8</cell><cell>33.0</cell><cell>26.6</cell><cell>26.9</cell><cell>34.7</cell></row><row><cell>Videopose. (f = 9) [34]</cell><cell>CVPR'19</cell><cell>37.0</cell><cell>40.7</cell><cell>35.2</cell><cell>37.4</cell><cell>38.4</cell><cell>44.2</cell><cell>42.3</cell><cell>37.1</cell><cell>46.5</cell><cell>48.8</cell><cell>38.9</cell><cell>40.1</cell><cell>38.5</cell><cell>29.9</cell><cell>32.6</cell><cell>39.2</cell></row><row><cell>PoseFormer (f = 9) [53]</cell><cell>ICCV'21</cell><cell>49.2</cell><cell>49.7</cell><cell>38.7</cell><cell>42.7</cell><cell>40.0</cell><cell>40.9</cell><cell>50.7</cell><cell>42.2</cell><cell>47.0</cell><cell>46.1</cell><cell>43.4</cell><cell>46.7</cell><cell>39.8</cell><cell>36.4</cell><cell>38.0</cell><cell>43.5</cell></row><row><cell>PoseAug (f = 1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Quantitative evaluation results under Abs-MPJPE and MRPE on H36M using CPN detected keypoints as 2D input. Best results are shown in bold.</figDesc><table><row><cell>36]</cell><cell>ACMMM'21</cell><cell>34.8</cell><cell>38.2</cell><cell>31.1</cell><cell>34.4</cell><cell>35.4</cell><cell>37.2</cell><cell>38.3</cell><cell>32.8</cell><cell>39.5</cell><cell>41.3</cell><cell>34.9</cell><cell>35.6</cell><cell>32.9</cell><cell>27.1</cell><cell>28.0</cell><cell>34.8</cell></row><row><cell>Ray3D (f = 9)</cell><cell></cell><cell>31.2</cell><cell>35.7</cell><cell>31.4</cell><cell>33.6</cell><cell>35.0</cell><cell>37.5</cell><cell>37.2</cell><cell>30.9</cell><cell>42.5</cell><cell>41.3</cell><cell>34.6</cell><cell>36.5</cell><cell>32.0</cell><cell>27.7</cell><cell>28.9</cell><cell>34.4</cell></row><row><cell>Abs-MPJPE</cell><cell></cell><cell>Dir.</cell><cell>Disc.</cell><cell>Eat.</cell><cell>Greet</cell><cell>Phone</cell><cell>Photo</cell><cell>Pose</cell><cell>Purch</cell><cell>Sit</cell><cell>SitD.</cell><cell>Somke</cell><cell>Wait</cell><cell>WalkD.</cell><cell>Walk</cell><cell>WalkT.</cell><cell>Average</cell></row><row><cell>Videopose (f = 9) [34]</cell><cell>CVPR'19</cell><cell>128.9</cell><cell>125.4</cell><cell>124.4</cell><cell>138.2</cell><cell>108.2</cell><cell>155.5</cell><cell>116.6</cell><cell>101.1</cell><cell>135.8</cell><cell>287.6</cell><cell>128.6</cell><cell>130.9</cell><cell>122.1</cell><cell>101.6</cell><cell>110.7</cell><cell>134.4</cell></row><row><cell>PoseLifter (f = 1) [5]</cell><cell>ICCV'19</cell><cell>140.9</cell><cell>113.2</cell><cell>139.9</cell><cell>148.2</cell><cell>122.0</cell><cell>155.3</cell><cell>121.5</cell><cell>121.1</cell><cell>170.0</cell><cell>267.6</cell><cell>139.2</cell><cell>142.9</cell><cell>146.4</cell><cell>132.1</cell><cell>135.2</cell><cell>146.4</cell></row><row><cell>PoseFormer (f = 9) [53]</cell><cell>ICCV'21</cell><cell>112.6</cell><cell>137.1</cell><cell>117.6</cell><cell>145.8</cell><cell>113.0</cell><cell>166.0</cell><cell>125.5</cell><cell>113.8</cell><cell>128.8</cell><cell>245.7</cell><cell>122.7</cell><cell>144.8</cell><cell>125.0</cell><cell>118.9</cell><cell>129.3</cell><cell>136.5</cell></row><row><cell>RIE (f = 9) [36]</cell><cell>ACMMM'21</cell><cell>143.2</cell><cell>133.2</cell><cell>143.9</cell><cell>142.7</cell><cell>110.9</cell><cell>151.4</cell><cell>125.9</cell><cell>98.4</cell><cell>136.4</cell><cell>273.4</cell><cell>127.5</cell><cell>138.9</cell><cell>126.8</cell><cell>107.3</cell><cell>116.0</cell><cell>138.4</cell></row><row><cell>Ray3D (f = 1)</cell><cell></cell><cell>80.1</cell><cell>100.8</cell><cell>123.8</cell><cell>125.5</cell><cell>110.7</cell><cell>111.8</cell><cell>96.1</cell><cell>99.3</cell><cell>129.4</cell><cell>176.3</cell><cell>106.8</cell><cell>129.2</cell><cell>120.4</cell><cell>109.1</cell><cell>106.6.</cell><cell>115.1</cell></row><row><cell>Ray3D (f = 9)</cell><cell></cell><cell>92.9</cell><cell>97.4</cell><cell>139.8</cell><cell>118.6</cell><cell>113.8</cell><cell>105.9</cell><cell>84.5</cell><cell>74.9</cell><cell>148.6</cell><cell>165.7</cell><cell>116.6</cell><cell>113.9</cell><cell>98.2</cell><cell>83.6</cell><cell>87.9</cell><cell>109.5</cell></row><row><cell>MRPE</cell><cell></cell><cell>Dir.</cell><cell>Disc.</cell><cell>Eat.</cell><cell>Greet</cell><cell>Phone</cell><cell>Photo</cell><cell>Pose</cell><cell>Purch.</cell><cell>Sit</cell><cell>SitD.</cell><cell>Somke</cell><cell>Wait</cell><cell>WalkD.</cell><cell>Walk</cell><cell>WalkT.</cell><cell>Average</cell></row><row><cell>Videopose. (f = 9) [34]</cell><cell>CVPR'19</cell><cell>124.2</cell><cell>115.9</cell><cell>111.0</cell><cell>127.3</cell><cell>97.6</cell><cell>141.9</cell><cell>105.7</cell><cell>96.4</cell><cell>122.0</cell><cell>276.5</cell><cell>119.6</cell><cell>123.3</cell><cell>111.3</cell><cell>94.0</cell><cell>101.6</cell><cell>124.6</cell></row><row><cell>PoseLifter (f = 1) [5].</cell><cell>ICCV'19</cell><cell>134.7</cell><cell>102.3</cell><cell>126.9</cell><cell>135.7</cell><cell>109.9</cell><cell>138.5</cell><cell>110.7</cell><cell>110.9</cell><cell>170.0</cell><cell>252.4</cell><cell>128.4</cell><cell>133.9</cell><cell>139.4</cell><cell>121.6</cell><cell>124.4</cell><cell>135.1</cell></row><row><cell>PoseFormer (f = 9) [53]</cell><cell>ICCV'21</cell><cell>104.7</cell><cell>134.7</cell><cell>103.9</cell><cell>137.4</cell><cell>99.6</cell><cell>154.6</cell><cell>119.8</cell><cell>108.9</cell><cell>108.2</cell><cell>233.7</cell><cell>111.1</cell><cell>141.1</cell><cell>116.2</cell><cell>117.9</cell><cell>123.8</cell><cell>127.7</cell></row><row><cell>RIE (f = 9) [36]</cell><cell>ACMMM'21</cell><cell>139.1</cell><cell>124.5</cell><cell>129.9</cell><cell>133.1</cell><cell>99.2</cell><cell>141.4</cell><cell>116.3</cell><cell>93.5</cell><cell>124.0</cell><cell>265.9</cell><cell>118.4</cell><cell>131.3</cell><cell>117.1</cell><cell>100.4</cell><cell>109.2</cell><cell>129.6</cell></row><row><cell>Ray3D (f = 1)</cell><cell></cell><cell>67.3</cell><cell>91.7</cell><cell>113.6</cell><cell>111.8</cell><cell>104.5</cell><cell>96.3</cell><cell>85.8</cell><cell>94.6</cell><cell>124.4</cell><cell>161.7</cell><cell>97.6</cell><cell>119.5</cell><cell>110.9</cell><cell>100.9</cell><cell>94.8</cell><cell>105.0</cell></row><row><cell>Ray3D (f = 9)</cell><cell></cell><cell>83.7</cell><cell>86.8</cell><cell>128.9</cell><cell>104.8</cell><cell>109.3</cell><cell>91.6</cell><cell>75.0</cell><cell>65.2</cell><cell>143.9</cell><cell>150.5</cell><cell>108.6</cell><cell>105.7</cell><cell>88.4</cell><cell>73.9</cell><cell>77.8</cell><cell>99.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Cross dataset evaluation. We adopt a 14-joint skeleton training on 3DHP, testing on H36M, Humaneva-I and 3DHP datasets. MPJPE, Abs-MPJPE and MRPE are reported. The unit of all numbers is mm. The best results are in bold.</figDesc><table><row><cell cols="2">method \datasets</cell><cell></cell><cell>H36M</cell><cell></cell><cell></cell><cell>HumanEva-I</cell><cell></cell><cell></cell><cell>3DHP</cell></row><row><cell></cell><cell></cell><cell cols="9">MPJPE Abs-MPJPE MRPE MPJPE Abs-MPJPE MRPE MPJPE Abs-MPJPE MRPE</cell></row><row><cell cols="2">Videopose (f = 9) [34]</cell><cell>81.2</cell><cell>1680.3</cell><cell cols="2">1686.6 86.2</cell><cell>1387.4</cell><cell cols="2">1387.1 58.2</cell><cell>149.1</cell><cell>143.0</cell></row><row><cell cols="3">PoseFormer (f = 9) [53] 97.8</cell><cell>1824.0</cell><cell cols="2">1818.9 104.7</cell><cell>1470.4</cell><cell cols="2">1452.0 47.3</cell><cell>207.5</cell><cell>211.3</cell></row><row><cell cols="2">PoseLifter (f = 1) [5]</cell><cell>92.9</cell><cell>573.3</cell><cell>570.5</cell><cell>240.2</cell><cell>1263.0</cell><cell cols="2">1129.3 76.7</cell><cell>147.8</cell><cell>133.6</cell></row><row><cell cols="2">RIE (f = 9) [36]</cell><cell>91.2</cell><cell>1679.9</cell><cell cols="2">1673.0 92.0</cell><cell>1375.8</cell><cell cols="2">1369.5 50.9</cell><cell>135.6</cell><cell>132.4</cell></row><row><cell cols="2">CDG (f = 1) [44]</cell><cell>95.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>90.3</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Ray3D (f = 9)</cell><cell>84.4</cell><cell>243.9</cell><cell>246.7</cell><cell>83.9</cell><cell>477.8</cell><cell>468.6</cell><cell>46.6</cell><cell>103.3</cell><cell>95.3</cell></row><row><cell>44 46 48 50 52 54 56 58 MPJPE</cell><cell cols="2">1100 1120 1140 1160 1180 Focal length in pixels (a) PoseFormer RIE Videopose Ray3D Ray3D_w/o_CE MRPE 150 200 250 300 350</cell><cell cols="2">1100 1120 1140 1160 1180 Focal length in pixels (b) PoseFormer RIE Videopose Ray3D Ray3D_w/o_CE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on 3DHP dataset with models trained with H36M dataset. IND denotes intrinsic decoupling, Nor. represents camera normalization. CE is camera embedding. We use RIE as base model. Best results are shown in bold.</figDesc><table><row><cell>Method</cell><cell>IND Nor. CE MRPE</cell></row><row><cell>RIE</cell><cell>1079.2</cell></row><row><cell>RIE w IND</cell><cell>448.9</cell></row><row><cell>Ray3D w/o CE</cell><cell>311.6</cell></row><row><cell>Ray3D</cell><cell>307.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Technical summary of synthetic dataset constructed based on H36M.</figDesc><table><row><cell>Dataset</cell><cell>Num. of camera pose</cell><cell>focal length/pixel</cell><cell>x-coordinate of principal point/pixel</cell><cell>camera rotation/degree</cell><cell>camera pitch/degree</cell><cell>camera translation/meter</cell><cell>subjects</cell></row><row><cell>training</cell><cell>324</cell><cell>[1143:1150]</cell><cell>[508:514]</cell><cell>[60:300:120]</cell><cell>[2:38:2]</cell><cell>[9.05:11.70:0.76]</cell><cell>S1, S5, S6, S7, S8</cell></row><row><cell>extrinsic testing</cell><cell>126</cell><cell>[1143:1150]</cell><cell>[508:514]</cell><cell>[0:360:30]</cell><cell>[1:37:2]</cell><cell>[9.43:13.19:0.76]</cell><cell>S9, S11</cell></row><row><cell>intrinsic testing</cell><cell>100</cell><cell>[1100:1180]</cell><cell>[450:550]</cell><cell>0</cell><cell>12</cell><cell>4.5</cell><cell>S9, S11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Quantitative evaluation results under MPJPE, Abs-MPJPE and MRPE on 3DHP using GT as 2D input. (f = 9) means this approach utilizes 9 consecutive frames for pose estimation, and (f = 1) means the approach does not make use of temporal information. Best results are shown in bold.</figDesc><table><row><cell>method \metric</cell><cell cols="3">MPJPE Abs-MPJPE MRPE</cell></row><row><cell>Videopose (f = 9) [34]</cell><cell>52.5</cell><cell>148.4</cell><cell>145.8</cell></row><row><cell cols="2">PoseFormer (f = 9) [53] 40.8</cell><cell>147.8</cell><cell>147.5</cell></row><row><cell>PoseLifter (f = 1) [5]</cell><cell>78.2</cell><cell>143.6</cell><cell>129.1</cell></row><row><cell>RIE (f = 9) [36]</cell><cell>47.4</cell><cell>140.8</cell><cell>141.0</cell></row><row><cell>Ray3D (f = 1)</cell><cell>48.4</cell><cell>118.2</cell><cell>114.0</cell></row><row><cell>Ray3D (f = 9)</cell><cell>46.0</cell><cell>103.4</cell><cell>95.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We set the same value for x coordinate and y coordinate of principal point for simplicity.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to reconstruct people in clothing from a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><forename type="middle">A</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><forename type="middle">Lal</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1175" to="1186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keep it SMPL: automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9909</biblScope>
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Absposelifter: Absolute 3d human pose lifting network from a single noisy 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anpei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanbo</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<idno>2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Holistic++ scene understanding: Single-view 3d holistic scene parsing and human pose estimation with human-object interaction and physical commonsense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8647" to="8656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wending</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="723" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Camera distortion-aware 3d human pose estimation in video with optimization-based meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyel</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yooshin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemyung</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="11169" to="11178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-person 3d human pose estimation from monocular images</title>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="405" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11213</biblScope>
			<biblScope unit="page" from="679" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Poseaug: A differentiable pose augmentation framework for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Chris Manafas, and Georgios Tzimiropoulos. 3d human body reconstruction from a single image via volumetric regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11132</biblScope>
			<biblScope unit="page" from="64" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bcnet: Learning body and cloth shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">12365</biblScope>
			<biblScope unit="page" from="18" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">VIBE: video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5252" to="5262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spec: Seeing people in the wild with an estimated camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chun-Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>Tesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="11035" to="11045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4501" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4704" to="4713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Determination of 3d human body postures from a single view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Hsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zen</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="148" to="168" />
		</imprint>
	</monogr>
	<note>Computer Vision Graph Image Process</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Augmented reality with human body interaction based on monocular 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung</forename><surname>Huei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Wen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Concepts for Intelligent Vision Systems -12th International Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6474</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sen-Ching</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayan</forename><forename type="middle">K</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Asari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2659" to="2668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Nerf: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="405" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3d multiperson pose estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala</editor>
		<imprint>
			<date type="published" when="2019-03" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lcr-net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?gory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1216" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving robustness and accuracy via relative information encoding in 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haopeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="4" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tom?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Absolute human pose estimation with depth prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M?rton</forename><surname>V?ges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Lorincz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-person absolute 3d human pose estimation with weak depth supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M?rton</forename><surname>V?ges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Lorincz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12396</biblScope>
			<biblScope unit="page" from="258" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">HMOR: hierarchical multi-person ordinal relations for monocular multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12348</biblScope>
			<biblScope unit="page" from="242" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Motion guided 3d pose estimation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="764" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Predicting camera viewpoint improves cross-dataset generalization for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 Workshops</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Proceedings</note>
	<note>Part</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7444" to="7452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Chirality nets for human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8163" to="8173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">pixelnerf: Neural radiance fields from one or few images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vickie</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="4578" to="4587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes -the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep network for the integrated 3d sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8420" to="8429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pymaf: 3d human pose and shape regression with pyramidal mesh alignment feedback loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yating</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11446" to="11456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Objectoccluded human shape and pose estimation from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7374" to="7383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">SMAP: single-shot multiperson absolute 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12360</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">3d human pose estimation with spatial and temporal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mat?as</forename><surname>Mendieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taojiannan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deephuman: 3d human reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
