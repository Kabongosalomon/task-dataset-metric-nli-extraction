<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D-FCT: Simultaneous 3D object detection and tracking using feature correlation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Sharma</surname></persName>
							<email>naman.sharma@seagate.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hocksoon Lim Seagate Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seagate</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hocksoon Lim Seagate Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">3D-FCT: Simultaneous 3D object detection and tracking using feature correlation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D object detection using LiDAR data remains a key task for applications like autonomous driving and robotics. Unlike in the case of 2D images, LiDAR data is almost always collected over a period of time. However, most work in this area has focused on performing detection independent of the temporal domain. In this paper we present 3D-FCT, a Siamese network architecture that utilizes temporal information to simultaneously perform the related tasks of 3D object detection and tracking. The network is trained to predict the movement of an object based on the correlation features of extracted keypoints across time. Calculating correlation across keypoints only allows for real-time object detection. We further extend the multi-task objective to include a tracking regression loss. Finally, we produce high accuracy detections by linking short-term object tracklets into long term tracks based on the predicted tracks. Our proposed method is evaluated on the KITTI tracking dataset where it is shown to provide an improvement of 5.57% mAP over a state-of-the-art approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D object detection is a fundamental task required for a multitude of robot applications such as autonomous driving, object manipulation and augmented reality. LiDAR sensors are commonly utilized in autonomous driving scenarios which capture 3D information about the environment, and the aim of the 3D object detection task is to utilize this information to provide semantically labeled 3D oriented bounding boxes for all objects in the environment. A related task is 3D object tracking, which requires ID assignment for each detected object across time.</p><p>Most 3D tracking algorithms <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref> consider object detection to be a prior step independent of tracking. In this paradigm, object detection is performed frame-byframe. This runs contrary to how 3D data is collected using sensors (e.g. RGB camera, LiDAR) in settings like <ref type="bibr">Figure 1</ref>. Example sequence of frames in the KITTI <ref type="bibr" target="#b8">[9]</ref> Tracking dataset. Objects in the same frame take the same color. A few cars, a pedestrian and a cyclist are visible. autonomous driving. Information about the detections in past frames can be an indicator of possible objects in future frames. However, almost all previous work in 3D object detection <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref> has focused on frame-by-frame detection. In our literature search, we find only Huang et al. <ref type="bibr" target="#b15">[16]</ref> have tackled this issue previously. Ngiam et al. <ref type="bibr" target="#b23">[24]</ref> and Hu et al. <ref type="bibr" target="#b14">[15]</ref> also consider multiple 3D frames as input, but both use relatively simple techniques of reusing seed points or concatenating input over multiple frames.</p><p>In this paper, we investigate a new method that makes use of temporal information across multiple LiDAR point clouds to simultaneously perform 3D detection and tracking. An example of the high correlation between object positions and object appearance in shown in <ref type="figure">Figure 1</ref>, which visualizes 3 consecutive point clouds where objects in the same frame are colored similarly. Our objective is to continuously infer a 'tracklet' over multiple frames by correlating the object features for all detected Regions of Interest (RoIs). To this effect we propose to extend the PV-RCNN architecture proposed by Shi et al. <ref type="bibr" target="#b29">[30]</ref> with a tracking formulation which utilizes cross correlation to regress tracking values. We train an end-to-end architecture on a combined detection and tracking loss. We refer to this novel architecture as 3D-FCT for 3D Feature Correlation and Tracking. The input to 3D-FCT consists of pairs of LiDAR point clouds which are first passed through a detection trunk (e.g. PV-RCNN) to produce object features. These features are shared for both the detection and the tracking tasks. A cross-correlation is calculated between the features from the two adjacent point clouds. This correlation information is then used to predict the displacement of each object by regressing the box transformations across point clouds. Finally, we infer long-term tubes for objects using the shortterm tracklets predicted by the network.</p><p>Compared to previous methods which concatenate input point clouds, our approach is more memory and compute efficient since the detection trunk is shared between the pairs of inputs as a Siamese network. Huang et al. <ref type="bibr" target="#b15">[16]</ref> propose an LSTM-based architecture which also reduces memory and compute footprint. However, their architecture depends on the ability of the LSTM network to extract useful features from the entire point cloud as its hidden state for the next point cloud. 3D-FCT takes inspiration from the D2T architecture <ref type="bibr" target="#b7">[8]</ref> in the 2D domain to utilize past features from each RoI separately. An evaluation on the KITTI <ref type="bibr" target="#b8">[9]</ref> Tracking Dataset shows that our approach is able to achieve better performance across all three classes as compared to frame-by-frame approaches, with an increase of 5.57% mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D Object Detection. Previous work on 3D object detection can be broadly divided into three categories: grid based methods, point based methods and representation learning. Grid based methods, pioneered by MV3D <ref type="bibr" target="#b2">[3]</ref>, dissect the 3D point cloud into a regular grid to apply a 2D or 3D CNN. The introduction of 3D sparse convolution <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> allowed for efficient processing of voxels, leading to further development of 3D CNN architectures in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref>. Point based methods use PointNet <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref> and its set abstraction operation to directly convert point clouds into feature vectors. F-PointNet <ref type="bibr" target="#b24">[25]</ref> and F-ConvNet <ref type="bibr" target="#b36">[37]</ref> utilize 2D image bounding boxes to generate 3D proposals to be passed to PointNet. PointRCNN <ref type="bibr" target="#b30">[31]</ref> provides 3D proposals directly from point clouds. Finally, representation learning involves learning a set of features from the 3D point cloud before performing the object detection task. PointNet, proposed by Qi et al. <ref type="bibr" target="#b25">[26]</ref>, remains a key feature in representation learning, allowing flexible receptive fields of different search radii. The PV-RCNN <ref type="bibr" target="#b29">[30]</ref> model utilizes both voxel based feature learning and PointNet based feature learning to provide high quality detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatio-temporal Methods.</head><p>A number of papers in the past few years have focused on using temporal information to improve 2D object detection in videos. This has been possible in large part because of video datasets like VidOR <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34]</ref> which label object categories and relations over an extended sequence of image frames. Several methods utilize this temporal information only in the post-processing stage of the pipeline <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref>, by constructing tubelets that are re-scored based on the object classes and 2D location. Feichtenhofer et al. <ref type="bibr" target="#b7">[8]</ref> utilize a Siamese R-FCN detector to simultaneously detect and track objects in a video. FGFA <ref type="bibr" target="#b39">[40]</ref> considers temporal information at the feature level, by utilizing optical flow from past and future frames to provide local aggregation. The MEGA <ref type="bibr" target="#b3">[4]</ref> network provides both local and global aggregation by allowing a large amount of information to be saved from past and future frames.</p><p>By contrast, using temporal information in LiDAR point clouds is largely an unexplored area of research. A direct approach followed by <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref> involves converting the point clouds into bird's-eye view projections and then utilize 2D CNN architectures along with ConvGRU or ConvLSTM. Luo et al. <ref type="bibr" target="#b21">[22]</ref> build a combined architecture for the related tasks of detection, tracking, motion forecasting and motion planning by concatenating multiple point clouds into a single input. Another related approach taken by Choy et al. <ref type="bibr" target="#b5">[6]</ref> builds a sparse 4D convolution with non-conventional kernels, with the 4th dimension being time. The approaches above can be considered as early fusion with the time dimension being added as an input. A middle fusion approach is taken with PointRNN <ref type="bibr" target="#b6">[7]</ref> which aggregates the past state and the current input based on point coordinates. Similarly, Huang et al. <ref type="bibr" target="#b15">[16]</ref> use LSTM networks to preserve a hidden state from the previous inputs. Our approach is different as we do not use a neural network to pull features from the entire point cloud into a state vector. Instead, we correlate features of individual RoI across point clouds in time.</p><p>3D Object Tracking. Previous effort has also focused on the related task of object tracking in 3D <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>. However, these works assume 3D object detection as a pre-existing input and do not necessarily improve it further. Another approach taken in FlowNet3D <ref type="bibr" target="#b19">[20]</ref> is to estimate per-point translation vectors to provide a 3D scene flow. Scene flow can provide information about the direction and magnitude of movement of objects, and hence might be useful for improving object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section we first provide a high level overview of 3D-FCT (Sect. 3.1) which outputs tracklets given two consecutive point clouds as input. In Sect. 3.2 we formulate the 3D object detection and tracking task using the PV-RCNN detector <ref type="bibr" target="#b29">[30]</ref> as a baseline. The tracking objective is formalized as a cross-cloud bounding box regression task (Sect. 3.3) which is achieved using a correlation technique detailed in Sect. 3.4.</p><p>The inter-cloud tracklets are combined into long-term tubes using the procedure described in Sect. 3.5. We finally apply 3D-FCT on the KITTI tracking dataset <ref type="bibr" target="#b8">[9]</ref> in Sect. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>We illustrate the overall architecture of the proposed 3D-FCT network in <ref type="figure" target="#fig_0">Fig. 2</ref>. We use the PV-RCNN <ref type="bibr" target="#b29">[30]</ref> 3D object detection network which combines voxel based sparse 3D convolutions as well as the keypoint based representation learning to predict oriented bounding boxes and object classes. We extend this network for multi cloud detection and tracking. The PV-RCNN module as shown in <ref type="figure" target="#fig_0">Fig. 2</ref> takes as input a point cloud and passes it through layers of 3D sparse convolution. In a parallel track, PV-RCNN selects keypoints using furthest point sampling and uses Point-Net <ref type="bibr" target="#b25">[26]</ref> to convert the point cloud within a radius of the keypoint into a feature vector. In this manner, PV-RCNN is able to provide keypoint features across multiple layers (shown as different colors in <ref type="figure" target="#fig_0">Fig. 2</ref>). Using these keypoint and BEV features, a RPN is used to propose candidate regions in each cloud based on pre-defined anchor boxes. An RoI pooling layer finally aggregates position-sensitive features and scores to classify objects and refine the box targets (regression).</p><p>The above architecture is extended by constructing a Siamese network <ref type="bibr" target="#b0">[1]</ref> which accepts a set of point clouds as an input and each point cloud is passed through a PV-RCNN with shared weights. Further, a regressor is introduced which takes as input the keypoint features from both point clouds as well as a correlation map to predict the box transformations from one point cloud to the next. To achieve coherence across point clouds, features are pooled at the same proposal region as the point cloud at time t. The trunk of the network responsible for the tracking task is trained by introducing a new tracking regression loss into the multi-task objective of PV-RCNN. The tracking loss is defined as the smooth L1 norm between the predicted and ground-truth cross-cloud bounding box movement.</p><p>The formulation of tracking as the bounding box movement is common in 2D <ref type="bibr" target="#b13">[14]</ref>, however, it does not inherently utilize translational equivariance. A correlation approach counters that shortcoming since correlation is robust to translations. A correlation map searches for feature in the search space that matches the template feature. In a single object tracking scenario, correlation maps can be utilized to calculate the displacement of an object by taking a maximum over the space.</p><p>Keypoint features extracted from PV-RCNN concatenate information from varying depths of the 3D convolution network allowing them to extract both low and high level features. Our aim is to develop a multi-object correlation tracker by computing correlation maps of the features of all keypoints in a point cloud. Our architecture can be trained end-to-end by using point clouds as an input to produce object detections and tracks. The next section describes how this end-to-end learning is formulated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">3D object detection and tracking</head><p>The 3D-FCT architecture takes as input a point cloud P t ? R m?d at time t. This point cloud is first divided into small voxels of size L ? W ? H with non-empty voxels taking the mean value of all point-wise features for the points that fall in the voxel. As in PV-RCNN <ref type="bibr" target="#b29">[30]</ref>, we utilize a series of 3 ? 3 ? 3 3D sparse convolution layers to downsample the input by 1?, 2?, 4? and 8? respectively. 3D bounding box proposals are then calculated using an anchor-based approach from a BEV feature map of dimension L 8 ? W 8 after stacking the 3D features along the Z axis. As a result we have a bank of</p><formula xml:id="formula_0">D cls = 2 ? (C + 1) ? L 8 ? W 8</formula><p>position-sensitive score maps for two anchors at 0 ? , 90 ? for C classes (and background) at every pixel of the BEV map. Using a second regression branch we also have a bank of</p><formula xml:id="formula_1">D reg = 7 ? L 8 ? W 8 class-agnostic bounding box predic- tions of a 3D box b = (x, y, z, l, w, h, r z ).</formula><p>In a parallel trunk, PV-RCNN <ref type="bibr" target="#b29">[30]</ref> selects n keypoints K = {p 1 , . . . , p n } from the point cloud P t , where n = 2, 048 for the KITTI dataset. A voxel set abstraction module subsequently extracts keypoint specific features from each level of the 3D voxel CNN. These features are concatenated together to provide a multi-scale semantic feature</p><formula xml:id="formula_2">f i = f (pv1) i , f (pv2) i , f (pv3) i , f (pv4) i for each keypoint p i .</formula><p>We now consider a pair of point clouds P t , P t+? collected at time t and t + ? as inputs to the network. An inter-cloud bounding box regression layer is introduced that performs position sensitive RoI pooling on a concatenation of the multi-scale semantic features {f t , f t+? } as well as the correlation between them (described in more detail in Sect. 3.4). The regression layer is trained to predict the</p><formula xml:id="formula_3">transformation ? t+? = (? t+? x , ? t+? y , ? t+? z , ? t+? rz ) of the RoIs from t to t + ? .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multitask detection and tracking objective</head><p>PV-RCNN <ref type="bibr" target="#b29">[30]</ref> is trained end-to-end on a combination of classification loss L cls and regression loss L reg . A tracking loss L tra is added to this overall loss function to allow the tracking regressor to be trained. Given N RoIs with softmax probabilities</p><formula xml:id="formula_4">{p i } N i=1 , regression offsets {b i } N i=1</formula><p>and cross cloud RoI tracking transformations {? t+? i } Ntra i=1 , the overall loss function for a single iteration can be written as:</p><formula xml:id="formula_5">L({p i }, {b i }, {? t+? i }) = 1 N N i=1 L cls (p i,c * ) +? reg 1 N f g N i=1 [c * i &gt; 0]L reg (b i , b * i ) (1) +? tra 1 N tra Ntra i=1 L tra (? t+? i , ? * ,t+? i )</formula><p>The classification loss is calculated as the cross entropy loss, L cls (p i,c * ) = ? log (p i,c * ) where p i,c * is the predicted softmax score for the ground-truth label c * . Hence, the first term in eq. 1 acts for all N RoIs. The regression loss (second term in eq. 1) acts only for N f g foreground RoIs. The indicator function [c * i &gt; 0] is 1 for RoIs with c * i = 0 and 0 for RoIs with c * i = 0. L reg is calculated as the smooth L1 loss defined in <ref type="bibr" target="#b9">[10]</ref> between the predicted bounding boxes b i and the ground truth bounding boxes b * i . Assignment of RoIs to ground truth is based on a minimum intersection-over-union (IoU) of 0.5. The third term in eq. 1 corresponds to the tracking loss, and is active only for foreground RoIs for which the ground truth object is present in both point clouds P t and P t+? . Therefore, we have N tra ? N f g ? N . The tradeoff parameters ? reg and ? tra were both set to 1 for this work.</p><p>The ground truth labels for track regression are calculated based on the ground truth bounding boxes in both point clouds. The bounding box for a single object in point cloud P t can be defined as b t = (x t , y t , z t , l, w, h, r t z ) denoting the 3D coordinates of the center, length, width, height and yaw angle respectively. Consider the corresponding bounding box b t+? for the same object in time t + ? . The target for the tracking regression ? * ,t+? = (? * ,t+? x , ? * ,t+? y , ? * ,t+? z , ? * ,t+? rz ) is then defined as:</p><formula xml:id="formula_6">? * ,t+? x = x t+? ? x t l ? * ,t+? y = y t+? ? y t w (2) ? * ,t+? z = z t+? ? z t h ? * ,t+? rz = r t+? z ? r t z 2?<label>(3)</label></formula><p>Note that the length, width and height of an object does not change over time in 3D object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Keypoint correlation for object tracking</head><p>Defining correlation in 3D space is challenging because of the sparsity of 3D data. The simplest method would be to define correlation over the voxelized 3D space. This technique is already quite memory intensive in 2D space, in 3D space it would be prohibitive. Furthermore, unlike We see that the correlation between keypoints of objects 1 to 5 (correspondingly labelled in (a)) is fairly accurate. Moreover, the correlation for keypoints belonging to the road surface is identically maximal for the same point in P t . This can be attributed to the fact that a local feature descriptor for road surface is similar regardless of global location. This also highlights the potential use of the proposed architecture in segmentation applications. Best viewed in color.</p><p>in the case of single object tracking, multi object tracking requires construction of correlation maps over multiple templates. Considering all possible circular shifts in 3D voxel space would produce feature maps of very large sizes. Hence, we utilize the multi-scale semantic keypoint features {f t , f t+? } provided by PV-RCNN. The correlation layer in <ref type="figure" target="#fig_0">Fig. 2</ref> performs keypoint-wise feature comparison between keypoints in two point clouds:</p><formula xml:id="formula_7">f t,t+? corr (i, j) = f t (i), f t+? (j) ?i, j ? {1, . . . , n} (4)</formula><p>Thus the correlation layer produces a feature map f t,t+? corr ? R n?n . Since the correlation feature is calculated from the keypoint features, it takes into account features from various semantic scales of the 3D sparse convolutional network.</p><p>The correlation features are concatenated with the keypoint features as {f t,t+? corr , f t , f t+? } before being passed through a fully connected layer to produce the final features f comb ? R n?n d , with n d = 256 for this work. These features are weighted based on the keypoint weighting module of PV-RCNN before they are pooled by the RoI pooling layer. <ref type="figure" target="#fig_1">Fig. 3</ref> shows an example of the correlation map generated by the correlation of keypoint features. In <ref type="figure" target="#fig_1">Fig 3(b)</ref>, the keypoint of the point cloud at time t are visualized in a bird eye view (BEV) image, with the color of the points mapped to their respective (x, y, z) location. <ref type="figure" target="#fig_1">Fig 3(c)</ref> correspondingly visualizes the keypoints of cloud at time t + ? . Here however the points are colored to match the keypoint in cloud t that maximizes the correlation. We observe that for most objects (highlighted in both <ref type="figure" target="#fig_1">Fig. 3(a)&amp;(c)</ref>) the correlation is fairly accurate, maintaining the correspondence across time. We also observe that the road plane is mapped to a single point that maximizes the correlation. This is because localized features of a road surface are expected to be very close regardless of position, and the correlation function does not consider global positions. However, it is intersting to see that patches of grass are accurately correlated across the two point clouds, which highlights the possible future use case scenario of this work for point cloud segmentation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Linking tracklets to object tubes</head><p>The high dimensionality of point clouds put a constraint on the number of point clouds that can be simultaneously processed by a single network based on the limitations of the memory available in modern GPUs. This is exacerbated by the fact that 3D object detection is commonly applied in autonomous robotics applications where processing ability may be limited. Hence, a method is required to combine the cross cloud tracklets that are predicted by our method into long-term tracks for individual objects. To this end, we utilize a location based technique, similar to those used in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref> in 2D, to link detections into long term tracks.</p><p>Consider the detections for a point cloud at time t,</p><formula xml:id="formula_8">B t i,c = {x t i , y t i , z t i , l i , w i , h i , r t z,i , p t i,c }, where B t i,c is a bounding box indexed by i, centered at (x t i , y t i , z t i )</formula><p>with dimensions (l i , w i , h i ) and a softmax probability of p t i,c for class c. In addition, we have predictions for the position of object B t i,c at time t + ? , i.e. the tracklets T t,t+?</p><formula xml:id="formula_9">i = {x t i + ? t,t+? x , y t i + ? t,t+? y , z t i + ? t,t+? z , l i , w i , h i , r t z,i + ? t,t+? rz }.</formula><p>Using these predictions we can define a class specific linking score allowing us to combine detections across time</p><formula xml:id="formula_10">s c (B t i,c , B t+? j,c , T t,t+? ) = p t i,c + p t+? j,c + ?(B t+? j , T t,t+? )<label>(5)</label></formula><p>where the pairwise score is</p><formula xml:id="formula_11">?(B t+? j , T t,t+? )) = 1, if IoU (B t+? j , T t,t+? ) &gt; ?) 0, otherwise<label>(6)</label></formula><p>where ? evaluates to 1 if 3D volumetric IoU between the expected bounding box and the predicted bounding box at time t + ? is greater than the threshold ?.</p><p>A set of graphs are constructed where bounding boxes B t0 is connected to B t0+? with edges weighted by s c <ref type="figure">(B t0 , B t0+?</ref> , T t0,t0+? ), ?t 0 ? (1, . . . , T ? ? ). Given such a set of graphs, our aim is to select the sequence of boxes that maximize the sum of the weights along the sequence. This can be done using a dynamic programming algorithm that maintains the maximum scoring sequence seen so far at each box. Once such a maximizing sequence B seq = {B ts is , . . . , B te ie } and its respective score sequence S seq = {s ts is , . . . , s te ie } have been found, the scores within the sequence are updated using a function S seq = F (S seq ). For this work, we consider F to be the maximum function. The aim is to boost the scores for positive boxes on which the detector fails to achieve sufficiently high scores.</p><p>The boxes in B seq are removed from the graph along with any boxes in B t , t ? [t s , t e ] with an IoU with B t seq greater than a set threshold. This process is repeated until no more links can be formed. This is summarized in <ref type="figure" target="#fig_3">Fig. 4</ref> which shows the BEV of 4 consecutive point clouds in time superimposed on each other. In <ref type="figure" target="#fig_3">Fig. 4(b)</ref>, 3 long term tracks are formed based on eq. 5. Such an approach prevents us from assuming that the object stays in view for the entirety of the point cloud sequence, which is seldom the case in 3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Evaluation</head><p>Our 3D-FCT architecture is evaluated on the KITTI <ref type="bibr" target="#b8">[9]</ref> tracking dataset which is used for multi object tracking evaluation for autonomous vehicle applications. The dataset consists of three classes: cars, pedestrians and cyclists, with 21 training sequences and 29 test sequences. Each sequence consists of consecutive point clouds collected over a period of time. Since the test dataset does not provide ground truth information, the training sequences are divided into a training (5027 point clouds) and validation (2981 point clouds) set, respectively. In this, we follow <ref type="bibr" target="#b34">[35]</ref> who split the dataset to approximately balance the relative number of occurrence of each class equally across the two sets 1 . Similar to the standard practice, as presented in <ref type="bibr" target="#b32">[33]</ref> and used by the KITTI 3D object detection challenge, we utilize the 3D average precision (AP) using 40 recall positions to evaluate our results. In addition, a true positive requires a minimum 3D overlap of 70% for cars and 50% for pedestrians and cyclists.</p><p>For all evaluations, we begin with a model pretrained on the KITTI <ref type="bibr" target="#b8">[9]</ref> 3D object detection dataset before training it on the tracking dataset. Also, we do not follow the data augmentation technique of oversampling ground truth objects as described in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref> and common practice with most 3D object detection neural network implementations. This augmentation technique places ground truth objects at random locations in the point cloud, greatly increasing the number of objects available for training and also solving the class imbalance. However, our implementation requires the existence of short term tracklets for each object between consecutive frames which makes the use of this augmentation technique challenging. Extension of this technique for tracking formulations is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training</head><p>PV-RCNN. Our PV-RCNN backbone is trained exactly as originally proposed in <ref type="bibr" target="#b29">[30]</ref>. The 3D voxel CNN consists of 4 level with feature dimensions 16, 32, 64, 64 respectively. The radii levels used in the voxel set abstraction module are also preserved from <ref type="bibr" target="#b29">[30]</ref> as (0.4m, 0.8m), (0.8m, 1.2m), (1.2m, 2.4m), (2.4m, 4.8m) for each layer respectively, with the neighborhood radii for the set abstraction of for raw points being (0.4m, 0.8m). The voxel size was set to be (0.1m, 0.1m, 0.15m). The RoI grid pooling operation samples 6 ? 6 ? 6 grid points in each 3D proposal. The number of keypoints is fixed to n = 2048. The generated proposals are suppressed to 512 for training and 100 for inference using NMS.</p><p>Tracking. For training the tracking arm of 3D-FCT, we start with a pretrained PV-RCNN model and fine tune it based on the added tracking loss from eq. 1. For all of our evaluations we utilize a value of ? = 1, which means we use directly consecutive point clouds as input. This is primarily because LiDAR point clouds are collected at a much lower frequency (?10 clouds per second) than RGB images (30/60 FPS). As a result, objects can move by significant amounts from 1 point cloud to the next. The feature convolution operation in eq. 4 is performed over all n = 2048 keypoints. We recognize that this may not be necessary and a more efficient strategy would be to only perform correlation with keypoints within a distance of the current keypoint. However, we leave an analysis on the benefits of this approach to future work. The multi-scale semantic features retrieved from the voxel set abstraction module is of length 128. Hence, after concatenating the correlation features f corr , the sematic features f and the global locations of the keypoints, the overall size of the features for a single iteration is 2048 ? 2310. This size is reduced to 2048 ? 256 using a single fully connected layer. After the RoI pooling operation, the features are passed through a fully connected network of 4 layers of 256 neurons each for the regression task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>We perform a series of experiments to evaluate the performance of our proposed network against alternative approaches. We also perform an ablation study to identify the effect of each sub-component of our network. These results are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Our first baseline is the PV-RCNN <ref type="bibr" target="#b29">[30]</ref> model that was trained on the KITTI tracking dataset, which achieves 65.08% mAP for the medium difficult category. Given that we do not utilize the data augmentation technique mentioned in <ref type="bibr" target="#b37">[38]</ref>, the AP for the 'Car' category is much higher than that for the 'Pedestrian' category, reflecting the imbalance inherent in the data. Our second baseline aims to measure the benefit of adding the tracking loss on the overall AP for 3D object detection. Adding this loss based on the track regression features provides a mAP of 66.78% (an increase of 1.7% over the baseline). We believe that the tracking loss improves the mAP since it reaffirms the important objects in the training data and acts as a regularizer pushing the features of the same object closer together.</p><p>Next, we evaluate the complete proposed architecture combining the tracking loss as well as the use of the regressed track predictions to link tracklets into long term tracks (3D-FCT). This leads to significant increase in performance with a mAP of 68.78% for the medium difficulty objects, an increase of 3.7% from the baseline. We see especially large improvements for the pedestrian and cyclist categories (+6.05% and +5.03% respectively). The proposed 3D-FCT architecture achieves this improvement by detecting objects that could have potentially been missed by the baseline due to occlusion, irregular pose or the fact that objects farther away from the sensor have less descriptive points. This is also why we see larger performance improvements for cyclists and pedestrians, which are smaller than cars, described by fewer points, and have higher variability in poses.</p><p>We also report the performance of other single cloud detectors evaluated on the KITTI tracking dataset. The Point-Pillars <ref type="bibr" target="#b18">[19]</ref> model is able to achieve only 43.4% mAP for the medium difficulty category. We observe that PointPillars is not very robust to class imbalance in the dataset and achieves only 23.12% AP for the pedestrian class. SEC-OND <ref type="bibr" target="#b37">[38]</ref> performs better than both PointPillars <ref type="bibr" target="#b18">[19]</ref> and PointRCNN <ref type="bibr" target="#b29">[30]</ref>, achieving 61.69% mAP. The proposed model is able to outperform both the PV-RCNN baseline as well as other single cloud detectors on the KITTI tracking evaluation set.</p><p>In <ref type="table">Table 2</ref>, we perform an ablation study to identify the effect of each component of the proposed architecture. The table shows the average precision for each class for the moderate difficulty objects from the validation KITTI tracking set. Note that if linking tracklets into long term tracks is done without track regression (based on IoU (B t , B t+? ) rather than IoU (T t,t+? , B t+? )), a sharp drop in performance for the car category is observed. This drop in performance is largely due to the increase in false positives that are predicted by the neural network. Objects can move by significant proportions of their length in the course of a single time step. Hence, it is imperative to combine the track regression along with the tracklet linking to achieve best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Our proposed 3D-FCT architecture aims to utilize temporal information to perform simultaneous 3D object detection and tracking. We leverage the correlation of keypoints in the feature space to predict the tracking regression of objects from time t to time t + ? . These predicted track regression values allow the prediction of long term tracks by linking inter-cloud tracklets across time. Experiments on the KITTI tracking dataset demonstrate that our proposed method is able to improve the mAP by 5.57% (averaged across the difficulty categories). In the future, we would also like to extend the architecture to produce 3D object segmentation maps, as the keypoint correlation information can be used directly for that purpose. Moreover, we would like to study the effect of computing the correlation between keypoints within a certain distance from each other, as op-posed to calculating them for all possible pairs.</p><p>Memory Efficiency: Although the 3D-FCT architecture in <ref type="figure" target="#fig_0">Fig. 2</ref> depicts two point clouds as an input to the network, both halves of the siamese network are independent of each other. Therefore, a more efficient implementation would only accept point cloud P t+? as an input, and utilize the keypoint features f t saved from the previous iteration to predict the track regressions. Such an implementation would be more memory efficient that other approaches that concatenate point clouds to process them together <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref>. We also note that since the correlation is performed on extracted keypoints only, the correlation workload is lightweight in comparison to performing the same over all possible points.</p><p>Online capabilities: Online application is only limited by the linking on object tracklets across time. The formulation defined in Sec 3.5 combines tracklets based on both past and future point clouds. To allow for online track formation, this can be changed to consider only causal effects at time t 0 ? (t ? ?, t) where ? is the effective window of time over which objects are tracked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented a novel 3D-FCT architecture to perform simultaneous 3D object detection and tracking on Li-DAR points clouds. Our proposed method allows for endto-end training in a memory efficient manner. Our approach has been shown to provide improvements in average precision on the KITTI tracking dataset over other baseline approaches. Jointly performing object detection and tracking has been shown to be effective in 2D in research, and we demonstrate that this approach is useful in 3D domain as well. We hope that this work can encourage other research in the same area.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the 3D-FCT temporal detection architecture. A pair of point clouds are processed by a Siamese PV-RCNN network to provide keypoint features. These keypoint features are compared using a correlation operation before being used to regress cross-cloud bounding box targets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Correlation features for two point clouds from the validation dataset (Seq 2, clouds 134 &amp; 135). (a) RGB image at time t for comparison. (b) BEV view of keypoints from point cloud P t , with RGB color mapped from the (x, y, z) location. (c) BEV view of keypoints from point cloud P t+? with color mapped to the keypoint color in (b) that maximizes the correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>(a) Four consecutive point cloud BEVs superimposed along with the predicted boxes B t and the expected boxes in the next frame T t,t+? . (b) Illustration of linking tracklets into long term tracks based on IoU (B t+? , T t,t+? ). Object A is removed from the predicted objects as it does not have IoU &gt; 0 with any past/future point clouds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison on KITTI tracking validation set. The average precision (in %) for each class and each difficulty category is shown, along with the mean average precision over all classes. We use PV-RCNN<ref type="bibr" target="#b29">[30]</ref> as a backbone for this architecture.</figDesc><table><row><cell></cell><cell></cell><cell>Car</cell><cell></cell><cell></cell><cell>Pedestrian</cell><cell></cell><cell></cell><cell>Cyclist</cell><cell></cell><cell>mAP</cell></row><row><cell>Methods</cell><cell cols="3">Easy Medium Hard</cell><cell cols="3">Easy Medium Hard</cell><cell cols="3">Easy Medium Hard</cell><cell>Easy Medium Hard</cell></row><row><cell>PointPillar [19]</cell><cell>75.28</cell><cell>51.54</cell><cell cols="2">49.64 22.08</cell><cell>23.12</cell><cell cols="2">23.07 68.27</cell><cell>55.55</cell><cell cols="2">54.81 55.21</cell><cell>43.40</cell><cell>42.51</cell></row><row><cell>SECOND [38]</cell><cell>85.33</cell><cell>69.04</cell><cell cols="2">67.17 46.73</cell><cell>47.08</cell><cell cols="2">46.87 78.89</cell><cell>68.96</cell><cell cols="2">68.26 70.32</cell><cell>61.69</cell><cell>60.77</cell></row><row><cell>PointRCNN [31]</cell><cell>83.75</cell><cell>65.03</cell><cell cols="2">65.08 45.14</cell><cell>44.43</cell><cell cols="2">42.44 55.30</cell><cell>43.22</cell><cell cols="2">41.27 61.40</cell><cell>50.89</cell><cell>49.60</cell></row><row><cell>PV-RCNN [30]</cell><cell>84.54</cell><cell>72.07</cell><cell cols="2">70.57 48.99</cell><cell>52.35</cell><cell cols="2">52.60 82.63</cell><cell>70.83</cell><cell cols="2">70.36 72.05</cell><cell>65.08</cell><cell>62.75</cell></row><row><cell cols="2">PV-RCNN + T loss 86.11</cell><cell>72.79</cell><cell cols="2">71.28 49.05</cell><cell>53.32</cell><cell cols="2">52.11 83.66</cell><cell>74.22</cell><cell cols="2">74.31 72.94</cell><cell>66.78</cell><cell>65.90</cell></row><row><cell>3D-FCT (Ours)</cell><cell>89.56</cell><cell>72.07</cell><cell cols="2">72.72 55.52</cell><cell>58.40</cell><cell cols="2">58.94 89.15</cell><cell>75.86</cell><cell cols="2">77.49 78.08</cell><cell>68.78</cell><cell>69.72</cell></row><row><cell>Improvement</cell><cell>+5.02</cell><cell>+0.00</cell><cell cols="2">+2.15 +6.53</cell><cell>+6.05</cell><cell cols="2">+6.34 +6.52</cell><cell>+5.03</cell><cell cols="2">+7.13 +6.03</cell><cell>+3.70</cell><cell>+6.97</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Sequences 0,<ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20</ref> were used for training, while the remaining were chosen for validation.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;Siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>S?ckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Series in Machine Perception and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="25" to="44" />
			<date type="published" when="1994-01" />
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Qi</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Kaichun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Memory enhanced global-local aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving 3d object detection through progressive population based augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoqi</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congcong</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="279" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">4d spatio-temporal ConvNets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pointrnn: Point recurrent neural network for moving point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with sub-manifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Seq-nms for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooya</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">Le</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to track at 100 FPS with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="749" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What you see is what you get: Exploiting visibility for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ziglar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An LSTM approach to temporal 3d object detection in Li-DAR point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="266" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object detection in videos with tubelet proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Eagermot: 3d multi-object tracking via sensor fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljo?a</forename><surname>O?ep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">FlowNet3d: Learning scene flow in 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Track to reconstruct and reconstruct to track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1803" to="1810" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d object detection for autonomous driving using temporal lidar data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Mccrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avideh</forename><surname>Zakhor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Starnet: Targeted computation for object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouais</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Frustum PointNets for 3d object detection from RGB-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Point-net++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Vip-deeplab: Learning visual perception with depth-aware video panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Yolo4d: A spatiotemporal approach for real-time multi-object detection and classification from lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><forename type="middle">El</forename><surname>Sallab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Sobh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Zidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Zahran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherif</forename><surname>Abdelkarim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2018 Workshop MLITS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Annotating objects and relations in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xindi</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 on International Conference on Multimedia Retrieval</title>
		<meeting>the 2019 on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Pointvoxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PointR-CNN: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection: From single to multiclass recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Lopez Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">YFCC100m: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MOTS: Multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berin</forename><surname>Balachandar Gnana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Voxel-fpn: multiscale voxel feature aggregation in 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayan</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Frustum ConvNet: Sliding frustums to aggregate local point-wise features for amodal 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SECOND: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">LiDAR-based online 3d video object detection with graph-based message passing and spatiotemporal transformer attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
