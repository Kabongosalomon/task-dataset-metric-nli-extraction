<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Self-Ensemble for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Bousselham</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Oregon Health and Science University Portland</orgName>
								<address>
									<region>OR</region>
									<country>USA Xubo Song</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Thibault</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Oregon Health and Science University Portland</orgName>
								<address>
									<region>OR</region>
									<country>USA Xubo Song</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Pagano</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Oregon Health and Science University Portland</orgName>
								<address>
									<region>OR</region>
									<country>USA Xubo Song</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Archana</forename><surname>Machireddy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Oregon Health and Science University Portland</orgName>
								<address>
									<region>OR</region>
									<country>USA Xubo Song</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Gray</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Oregon Health and Science University Portland</orgName>
								<address>
									<region>OR</region>
									<country>USA Xubo Song</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><forename type="middle">Hwan</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Oregon Health and Science University Portland</orgName>
								<address>
									<region>OR</region>
									<country>USA Xubo Song</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Self-Ensemble for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ensemble of predictions is known to perform better than individual predictions taken separately. However, for tasks that require heavy computational resources, e.g. semantic segmentation, creating an ensemble of learners that needs to be trained separately is hardly tractable. In this work, we propose to leverage the performance boost offered by ensemble methods to enhance the semantic segmentation, while avoiding the traditional heavy training cost of the ensemble. Our self-ensemble approach takes advantage of the multi-scale features set produced by feature pyramid network methods to feed independent decoders, thus creating an ensemble within a single model. Similar to the ensemble, the final prediction is the aggregation of the prediction made by each learner. In contrast to previous works, our model can be trained end-to-end, alleviating the traditional cumbersome multi-stage training of ensembles. Our selfensemble approach outperforms the current state-of-theart on the benchmark datasets Pascal Context and COCO-Stuff-10K for semantic segmentation and is competitive on ADE20K and Cityscapes. Code is publicly available at github.com/WalBouss/SenFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is the task of assigning each pixel of an image with a semantic category, and therefore is close to the task of image classification. Its many applications include robotics, autonomous cars, medical application, augmented reality and more. Most segmentation methods follow an Encoder-Decoder scheme. The encoder extracts the relevant features of the image to characterize each pixel, a process usually involving down-sampling the feature maps to increase the receptive field of the model. The decoder up-samples the feature maps to both recover the spatial in-formation and produce a per-pixel classification. In <ref type="bibr" target="#b34">[35]</ref> the authors extended this procedure to fully convolutional network (FCN), which paved the way for later work to achieve impressive results on various segmentation datasets and has since dominated the field of semantic segmentation, let it be for medical <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b39">40]</ref>, self-driving cars <ref type="bibr" target="#b41">[42]</ref> or robotics applications <ref type="bibr" target="#b20">[21]</ref>. Follow up work mainly focused on enhancing FCN to mitigate the inherent locality of the convolution operation. Some examples are the atrous convolution that introduces holes in convolution kernel <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, the pyramid pooling module (PPM) that aggregates context information using different kernel pooling layers, and <ref type="bibr" target="#b54">[55]</ref> that combine the PPM and the feature pyramid network (FPN) <ref type="bibr" target="#b30">[31]</ref> to capture context information at different resolutions.</p><p>The starting observation of this paper was that the combined use of a backbone and an FPN-like method <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52]</ref> allows extracting multiple features sets at different scales for a single image with a unique forward pass. Furthermore, in <ref type="bibr" target="#b30">[31]</ref> the authors show that these features are both semantically and spatially strong at each level of the feature pyramid. Consequently, one has access to multiple features representations of the same image that carry different contextual information at different scale and are loosely correlated (as shown in <ref type="bibr" target="#b51">[52]</ref>). This raises questions about the optimal way to use these multi-scales features. A canonical use is UperNet <ref type="bibr" target="#b54">[55]</ref>, which concatenates the multi-scale feature maps before feeding them to a decoder. However, this paper argues that the "features fusing" strategy consisting of merging the different sets of features maps and letting the model decide which one is important is sub-optimal and often computationally expensive. Indeed, in UperNet, the four pyramid levels are concatenated and merged by a convolution, which by its own involves 155G FLOPs, thus making the "features fusing" strategy FLOPs intensive. Moreover, we hypothesize that a single decoder cannot fully take advantage of the multi-scale features that contain different views of the same objects of interest. Hence, the model may focus on one view and overlook valuable features. This "multi-view" hypothesis is indeed supported by a recent study: in <ref type="bibr" target="#b0">[1]</ref> the authors argue that in vision datasets, objects can be recognized using multiple views and show that in the context of image classification, for a given weight initialization, a model will learn to focus on particular views while discarding others.</p><p>To overcome the limitations of "features fusion" strategies, we propose and study a different approach to exploit the FPN multi-scale features. Our approach feeds independent decoders with features coming from different levels of the feature pyramid, and then combine the segmentation maps together, hence avoiding expensive features fusion operations. Since the inputs to the learners (i.e., decoders) come from different levels of the feature pyramid that differ in scale and contain different spatial and semantic information and that the learners are independent, our method can be interpreted as a form of self-ensemble segmentation. Usually, the learners of an ensemble must be trained independently. In this work, we show that, in the context of semantic segmentation, this condition can be relaxed and imposed solely to the decoders. Our experiments show that -all else being equal -this strategy improves UperNet performance. However, increasing the number of decoders/learners inevitably increases parameters number. Overall, our observations on self-ensemble performance effectiveness but parameter burden, lead us to design a transformer-based model: SenFormer (Self-ensemble segmentation transFormer). Our motivation for using transformer-based learners is that besides transformers' ability to capture long-range dependencies, it has been observed <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref> that recursively applying the same transformer block to the same input features can produce similar -if not better -results than using different blocks while reducing the number of parameters and overfitting. Ultimately, our method has fewer parameters and FLOPs than UperNet and performs better.</p><p>Overall, our SenFormer approach achieves excellent results on various benchmark datasets. Specifically, it outperforms similar architectures <ref type="bibr" target="#b54">[55]</ref> that use "feature fusion" strategy, suggesting that our self-ensemble approach effectively leverages the expressive power of ensemble methods. In particular, SenFormer achieves 51.5 mIoU on the benchmark dataset COCO-Stuff-10K <ref type="bibr" target="#b1">[2]</ref> and 64.0 mIoU on Pascal-Context <ref type="bibr" target="#b36">[37]</ref>, outperforming the previous state-ofthe-art by a large margin of 6 mIoU and 3.0 mIoU respectively. SenFormer is also on par with state-of-the-art methods on Ade20K <ref type="bibr" target="#b59">[60]</ref> and Cityscapes <ref type="bibr" target="#b8">[9]</ref>. To summarize, our contributions are two fold:</p><p>? We propose an innovative way to leverage the multiscale features produced by the FPN to form an ensem-ble of learners inside a single model.</p><p>? We develop a light-weight transformer-based decoder that is used as a learner in our self-ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Semantic Segmentation. Since fully convolutional networks (FCN) have been introduced in the seminal work <ref type="bibr" target="#b34">[35]</ref>, it has dominated the field of semantic segmentation. However, due to the inherent locality of the convolution operation, architectures solely based on convolutions struggle to capture long-range dependencies, making it difficult to deal with large and occulted objects. Follow up methods to alleviate the locality issue include but are not limited to atrous convolution <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, pyramid pooling module <ref type="bibr" target="#b57">[58]</ref> or the use of FPN <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>More recently, motivated by the stupendous success of transformer-based architecture for image classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b45">46]</ref>, multiple works proposed to leverage the selfattention operation to improve segmentation performance of FCN scheme or even completely replace it. Transformerbased architectures can be used as a drop-in replacement of traditional CNN backbones to enhance the extracted features supplied to the decoder <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>. It has been observed that transformer backbones that produce a hierarchical feature representation <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b49">50]</ref> are the most suited for segmentation tasks. Additionally, following the original Encoder-Decoder Transformer <ref type="bibr" target="#b46">[47]</ref> used in NLP, recent works proposed to formulate the problem of semantic segmentation as a sequence-to-sequence problem <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b58">59]</ref>, freeing the architecture from any inductive prior biases.</p><p>Alternatively, motivated by the success of DETR <ref type="bibr" target="#b2">[3]</ref> which used transformer for object detection, MaX-DeepLab <ref type="bibr" target="#b48">[49]</ref> and MaskFormer <ref type="bibr" target="#b6">[7]</ref> treated semantic (and panoptic) segmentation no longer as a per-pixel classification but as a mask classification problem. These architectures first generate a set of candidate masks that are then classified. Shifting the semantic segmentation paradigm from a per-pixel classification to a mask classification problem helps bridge the gap between detection/panoptic segmentation methods and semantic segmentation. It also involves the computation of an assignment score between each generated mask and every class, therefore transferring a part of the training burden to the loss calculation. Since our investigation focuses on the efficacy of features fusion and self-ensemble, we will limit our comparisons to per-pixel classificationbased architectures.</p><p>Efficient Ensemble. A major limiting factor for building an ensemble of deep learning models is the computational cost during training and testing. Diverse methods were proposed to tackle this issue. By repeatedly applying dropout at inference on an already trained model, Monte Carlo Dropout <ref type="bibr" target="#b14">[15]</ref> allows getting many predictions from a single model, ultimately improving its accuracy. BatchEnsemble <ref type="bibr" target="#b52">[53]</ref> significantly lower ensemble cost by defining each learner's weights to be the Hadamard product between a shared matrix and a rank-one matrix per learner. Snapshot <ref type="bibr" target="#b21">[22]</ref> train a single model to converge to several local minima by leveraging cyclic learning rate scheduling. Other methods for classification include MIMO <ref type="bibr" target="#b17">[18]</ref>, hyper-batch ensemble <ref type="bibr" target="#b53">[54]</ref>, late-phase weights <ref type="bibr" target="#b47">[48]</ref> or FGE <ref type="bibr" target="#b15">[16]</ref>. For segmentation, <ref type="bibr" target="#b44">[45]</ref> improves the widely used multi-scale inference by learning relative attention between the scales during training and is used at test-time to greatly improve the performance. However, these methods still require several forward passes of the same image, let it be for training or testing. Perhaps most related to our work is TreeNet <ref type="bibr" target="#b28">[29]</ref>, which uses multiple classifier branches that share their early layers. Nevertheless, besides being for classification, unlike to our work, all the learners receive the same input, limiting the depth of the shared part. Moreover, in SenFormer, the parameter cost of the ensemble is further reduced through weight sharing within a learner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first present the general framework of our method based on self-ensemble as shown in figure 1. Then we detail the different merging strategies. Finally, we describe learners' architecture and the different weight sharing strategies.</p><p>Following notations in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b54">55]</ref>, we denote C i ? R di? H 2 i ? W 2 i the output of the i-th stage of the bottom-up network (i.e. backbone) which has stride of 2 i pixels with respect to the input image, where H ? W is the spatial di-mension of the input image and d i the number of channels. Similarly, we denote P i ? R d? H 2 i ? W 2 i the output of the ith stage of the top-down network (i.e. output of the FPN), where d is the numbers of channels in all the feature maps of the FPN. We denote N the number of class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Self-Ensemble</head><p>In this paper, we approach the problem of semantic segmentation as that of a per-pixel classification. Therefore, learners predictions and the merging strategies will be described for an arbitrary pixel and can easily generalize to the whole segmentation map.</p><p>An ensemble traditionally consists of M independently trained models called learners. For a given pixel, let denote X i ? R N the random variable parameterized by the output of the i-th learner for that particular pixel, which can be decomposed in:</p><formula xml:id="formula_0">X i = Y + i (1)</formula><p>where Y is the target and i is the prediction error of the i-th learner.</p><p>The most straightforward way to merge different learners' predictions is by averaging them. It is well known <ref type="bibr" target="#b38">[39]</ref> [61] <ref type="bibr" target="#b26">[27]</ref> that the ensemble performance is usually better than the individual learners.</p><p>Classical statistics suggest that when the predictions are roughly independent, the last term in equation 2 is close to zero and therefore averaging greatly reduces the noise.</p><formula xml:id="formula_1">V ar( 1 M M i=1 i ) = 1 M 2 M i=1 V ar( i ) + 2 M 2 i&lt;j Cov( i , j ).<label>(2)</label></formula><p>On another note, a recent study suggests that this hypothesis might not hold in the context of deep learning. In <ref type="bibr" target="#b0">[1]</ref>, Allenzhu et. al, acknowledge that for the task of image classification, the different learners learn to detect different views/features of the object of interest depending on their weight initialization. However, there are some images taken from a particular angle where the learned features may be missing. Hence, when the ensemble is large enough, all possible views are captured, thus increasing the model's accuracy. Note, however, that it is not clear in <ref type="bibr" target="#b0">[1]</ref> if this result also holds for semantic segmentation. Either way, a key requirement is that the learners' predictions must be independent, let it be for the variance reduction or the multi-view hypothesis.</p><p>Total independence of the predictions implies tediously training multiple independent models. In this paper, we aim at relaxing the independence hypothesis to reduce the training cost, while maintaining the performance benefits of ensemble. To do so, the learners/decoders share the same backbone but receive input features coming from different levels of the feature pyramid, i.e., {P 2 , P 3 , P 4 , P 5 }, as shown in figure 1.</p><p>Nevertheless, it is observed that if one trains the different learners of an ensemble altogether (i.e., applying the loss on the merged prediction), the performance boost offered by the ensemble disappears <ref type="bibr" target="#b0">[1]</ref>. However, we show in our experiments that it is not the case in our setting. We hypothesize that it is because each learner is independently initialized (as in ensemble) and receives different inputs, therefore alleviating the need for separate training. In this manner, several segmentation predictions can be obtained with only a single forward pass of the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Merging strategies</head><p>We describe the different methods considered to merge the different learner predictions (during inference).</p><p>Averaging. It is the most commonly used method for prediction merging as no additional trainable parameters are required. The merged prediction X avg of M learners is obtained by:</p><formula xml:id="formula_2">X avg = 1 M i X i (3)</formula><p>Product. The predicted probability for each pixel is multiplied rather than average. That way, more weight is given to learners with high confidence.The merged prediction X prod of M is given by:</p><formula xml:id="formula_3">X prod = M i=1 X i (4)</formula><p>Majority vote. Each learner assigns a vote to the class with the largest confidence. The merged prediction X maj is obtained by:</p><formula xml:id="formula_4">X maj = 1 M i ? c (X i ) where c = argmax 1?j?N X j i (5) where ?j ? N, ? c (X i ) j = X j i if j = c 0</formula><p>else. Hierarchical Attention. We borrow the "attention module" from <ref type="bibr" target="#b44">[45]</ref> that is used to learn a relative attention mask between adjacent scales. The module consists of</p><formula xml:id="formula_5">(3 ? 3 conv) ? (BatchN orm) ? (ReLU ) ? (3 ? 3 conv) ? (BatchN orm) ? (ReLU ) ? (1?1 conv) ? (Sigmoid),</formula><p>where the last convolution output a single (attention) map. In the original paper, the module is fed with the same input features maps of the decoder. Another variant would be to use the segmentation logits (decoder's output) instead. In our experiment, we tried both and found the latter to work better with SenFormer. Since SenFormer has four learners, we need 3 "attention modules" to predict the relative attention maps.</p><p>Explicit Attention. We used the same "attention module" as for Hierarchical Attention <ref type="bibr" target="#b44">[45]</ref>, but trained it to predict a dense mask for each scale rather than a relative mask.</p><p>Surprisingly, our experiments found the simple "averaging" strategy to perform better than others, except for the "hierarchical attention" <ref type="table" target="#tab_0">(Table 10</ref>). However, given the performance boost of the "attention module" is limited, it does not justify the overhead complexity. Therefore, SenFormer uses the "averaging" as the default merging strategy since it yields high performance without requiring additional parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learner architectures</head><p>Hereafter, we described the architecture of a single learner/decoder.</p><p>As depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, the i th decoder branch takes as input the features coming from the corresponding level of the FPN (with stride s i ) P i ? R d? H 2 i ? W 2 i , as well as a set of N learnable embeddings termed as class embeddings,</p><formula xml:id="formula_6">cls i = [cls 1 i , . . . , cls N i ] ? R N ?d ,</formula><p>where N is the number of class. In this respect, there is one learnable class embedding cls k i per segmentation class and per level in the feature pyramid.</p><p>Each decoder is a transformer composed of L layers whose architecture is inspired by the traditional transformer <ref type="bibr" target="#b46">[47]</ref>. Note however that a "pre-norm" strategy is used in place of "post-norm" for the placement of Layer Normalization (LN), i.e., the skip connections inside each transformer block are not affected by the LN <ref type="bibr" target="#b37">[38]</ref> (see ablation study in the Annex).</p><p>In a nutshell, a single Transformer Decoder block consists of three successive operations: Cross-Attention, Self-Attention and Multi-Layer Perceptron layers. In the Cross-Attention operation the feature map P i is used as key and value while the class embedding cls i is used a query. The Self-Attention and MLP are applied only to the class embeddings.</p><p>Finally, each decoder/learner is composed of L layers of decoder block and its prediction is obtained via a dot product between the class embeddings cls i and the corresponding feature pyramid feature P i -see the Annex for more details. However, using multiple decoders greatly increases the number of parameters. To mitigate this, we explore several weight-sharing strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Weight sharing</head><p>Weight sharing is a commonly used technique to reduce the number of parameters <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>, while also regularizing the optimization by reducing the degree of freedom which mitigates overfitting. However, in the context of Ensemble, special care regarding the kind of weight sharing used must be given.</p><p>Two types of weight sharing can be used: inter-learner and intra-learner sharing. The former involves sharing parameters between the different learners, while latter within the learner. <ref type="figure" target="#fig_1">Figure 2</ref> depicting the different sharing methods can be found in Annex.</p><p>Repeated block. A given learner is composed of a single decoder block recursively used L times. It is a form of "intra-learner sharing" since no parameters are shared between the different learners.</p><p>Decoder sharing. The different learners share the same decoder but have their own class embedding. It is a form of "inter-learner sharing".</p><p>Class embeddings sharing. The same learnable class embeddings cls is used for all the learners. It is also a form of "inter-learner sharing". <ref type="table" target="#tab_1">Table 2</ref> shows that any "inter-learner sharing" strategy significantly degrades the segmentation performance, confirming the importance of keeping the different learners as independent as possible. Conversely, the "repeated block" strategy performs better than when no sharing is used, while significantly reducing the number of parameters. Hence, SenFormer uses the "repeated block" as the default weight sharing policy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. We evaluate our model performance using four semantic segmentation benchmark datasets, ADE20K <ref type="bibr" target="#b59">[60]</ref>, Pascal Context <ref type="bibr" target="#b36">[37]</ref>, COCO-Stuff-10K <ref type="bibr" target="#b1">[2]</ref> and Cityscapes <ref type="bibr" target="#b8">[9]</ref>. We use ADE20K, which is a challenging scene parsing dataset consisting of 20,210 training images and 2,000 validation images and covers 150 fine-grained labeled classes, for the ablation studies. Please see the Annex for detailed descriptions of all used datasets.</p><p>Evaluation metric. We report the mean Intersection over Union (mIoU), a standard metric for semantic segmentation.</p><p>Baseline model. To demonstrate that the performance improvement of our method is genuinely a result of selfensemble instead of feature fusion, we introduce a simple decoder baseline module that borrows the features fusion strategy from UperNet <ref type="bibr" target="#b54">[55]</ref>, but uses our transformer decoder-see <ref type="figure" target="#fig_3">Figure 3</ref>. This way, the FeatureFusionBaseline and SenFormer only differ by the multi-scale fusion strategy. Following <ref type="bibr" target="#b54">[55]</ref>, the baseline multi-scale fusion strategy is as follow: we first resize (through bilinear interpolation) all the features {P 2 , P 3 , P 4 , P 5 } to match P 2 dimension (i.e. 1/4 of the input image) and concatenate them. We then apply a 3?3 convolution followed by a batch normalization layer and a ReLU activation. Note that this baseline is only used for ablation purposes and SenFormer is thereafter also compared to state of the art methods in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation and training details</head><p>Backbones. Since SenFormer uses the FPN to build a multi-scale set of features, it is compatible with any backbone architecture. In our experiments we use both convolutional ResNet50 and ResNet101 <ref type="bibr" target="#b18">[19]</ref> and the different size of the transformer-based backbones Swin-Transformers <ref type="bibr" target="#b33">[34]</ref>.</p><p>FPN. The channel dimension of the feature pyramid d is set to 512. For small backbones, we find it beneficial in the FPN to replace the traditional 3 ? 3 convolution by a Window-based Transformer Block from <ref type="bibr" target="#b33">[34]</ref>. Since it in- troduces a marginal computation overhead, we applied it to all backbones. Please see the appendix for detailed ablation of the FPN.</p><p>Decoders. Each learner is independently supervised with a cross-entropy loss. In addition, we apply a standard cross-entropy loss on the final ensemble prediction.</p><p>Training setting. We use mmsegmentation <ref type="bibr" target="#b7">[8]</ref> library as codebase and follow the standard training practice for each dataset. Moreover, we apply common data augmentation for semantic segmentation, which include left-right flipping, standard random color jittering, random resize with ratio 0.5 ? 2 and random cropping.</p><p>For the optimizer, we use AdamW <ref type="bibr" target="#b35">[36]</ref>. As common practice for segmentation, we use "poly" learning rate scheduler. Following <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b54">55]</ref>, we set the initial learning rate to 10 ?4 and weight decay to 10 ?4 for ResNet backbones, an initial learning rate of 6 ? 10 ?6 and a weight decay of 10 ?2 for Swin-Transformer. We also use gradient clipping of 3 to help stabilizing the training, and for ResNet backbones a learning rate multiplier of 0.1 is applied.</p><p>During training, the input images are cropped to a size of 512 ? 512 for ADE20K <ref type="bibr" target="#b59">[60]</ref> and COCO-Stuff-10K <ref type="bibr" target="#b1">[2]</ref>, 512 ? 512 for Pascal Context <ref type="bibr" target="#b36">[37]</ref> and 512 ? 1024 for Cityscapes <ref type="bibr" target="#b8">[9]</ref>, unless stated otherwise. All the models are trained on 8 V100 GPUs with a batch size of 8 for Cityscapes and 16 for the others (see Annex for details). The segmentation performance is reported using singlescale inference. Finally, all the backbones are pretrained on ImageNet-1K <ref type="bibr" target="#b40">[41]</ref> unless stated otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Self-ensemble vs Features Fusion</head><p>Features at different levels of the pyramid carry different scale of contextual information, and our experiments support that self-ensemble is capable of capturing and integrating such information.</p><p>Ensemble effect. We first analyze the output produced by each decoder and assess their performance. <ref type="table" target="#tab_0">Table 1</ref> outlines the mIoU scores of independent prediction of each de- coder as well as for the ensemble. Notably, the ensemble mIoU score is +3.5 better than the mean score of the learners taken separately with 1 4 5 i=2 mIoU(d i ) = 41.15. More surprisingly, even though d 5 taken separately performs significantly worse than the others -due to its low-resolution inputs -it positively contributes to the ensemble, consistent with traditional ensemble methods where even weak learners can be combined to enhance the overall prediction.</p><p>Does the performance boost really comes from selfensemble? To rule out the performance gain brought by the use of transformer-based decoders rather than convolution, we compare SenFormer and the FeaturesFusionBaseline, since they only differ in the multi-scale fusion strategy (features fusion vs. self-ensemble). In <ref type="table">Table 4</ref>, we observe that SenFormer is +2 mIoU better than the baseline. Conversely, we applied the self-ensemble method to Uper-Net <ref type="bibr" target="#b54">[55]</ref> by using the same convolution-based decoder at each level of the feature pyramid rather than merging the features. Likewise, the self-ensemble version (SenUperNet) performs better than the vanilla UperNet, suggesting that our self-ensemble approach is the main driver for improvement.</p><p>SenFormer vs UperNet. We compare SenFormer with UperNet architecture for a variety of CNN-and transformer-based backbones. As we can see from <ref type="table" target="#tab_3">Table 5</ref>, when using the same standard Swin-Transformer backbone, SenFormer consistently outperforms UperNet regardless of the backbone size. The performance gap is even larger when using convolutional backbones (+3 mIoU), suggesting that our transformer-based decoder successfully captures the long-range dependencies missed by the CNN-based backbones.</p><p>Thanks to its weight sharing strategy, SenFormer has fewer parameters than UperNet. Furthermore, since SenFormer avoids the computationally expensive features merging operation, it also has substantially fewer FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to state-of-the-art</head><p>In this section we further compare SenFormer to stateof-the-art methods on ADE20K and additional benchmark datasets.</p><p>ADE20K. In <ref type="table">Table 6</ref> we compare SenFormer to a variety of FCN-and transformer-based decoders using both CNN-and transformer-based backbones. When using standard ResNet backbones, SenFormer outperforms all other methods. The same can be said for per-pixel classificationbased models when using transformer-based backbones, where SenFormer even outperforms recently introduced transformer-based decoders like SETR <ref type="bibr" target="#b58">[59]</ref>, Segmenter <ref type="bibr" target="#b42">[43]</ref> and SegFormer <ref type="bibr" target="#b55">[56]</ref>. Note however that MaskFormer <ref type="bibr" target="#b6">[7]</ref> is doing better than SenFormer when using transformerbased backbones. Indeed, MaskFormer introduces a new approach for semantic segmentation that is based on mask classification (rather than traditional per-pixel classification) and that greatly improves segmentation performances.</p><p>In fact, MaskFormer <ref type="bibr" target="#b6">[7]</ref> significantly outperforms PerPixel-Baseline+ <ref type="bibr" target="#b6">[7]</ref> while sharing the same architecture and only differing by the problem formulation (per-pixel vs mask classification). We plan to formulate SenFormer as mask classification in our future work, as it has significant potential to improve segmentation. Pascal Context. In <ref type="table" target="#tab_4">Table 7</ref> we compare SenFormer to current state-of-the-art methods on Pascal Context test dataset, which is obtained by CAA <ref type="bibr" target="#b23">[24]</ref> using EfficientNet-B7(EN-B7) as backbone, with a mIoU of 60.5. Sen-Former outperforms previous FCN methods when using standard ResNet backbones, as well as recent transformerbased methods. SenFormer outperforms the current stateof-the-art (CAA) when using the same ResNet-101 backbone, showing the benefit of our approach. Moreover, we reach a score of 64.0 mIoU when using Swin-L as backbone. Overall, our approach shows a significant improvement of +3.5 mIoU over the previous state-of-the-art.</p><p>COCO-Stuff-10K. <ref type="table" target="#tab_5">Table 8</ref> compares SenFormer to state-of-the-art methods on COCO-Stuff-10K test dataset, which is obtained by CAA <ref type="bibr" target="#b23">[24]</ref> using EfficientNet-B7(EN-B7) as backbone, with a mIoU of 45.4. When using standard ResNet backbones, SenFormer outperforms previous FCN methods, as well as the transformer-based method MaskFormer. Moreover, we obtain 51.5 mIoU when using Swin-L as backbone, establishing a new SOTA by a substantial margin of +6 mIoU over previous methods on COCO-Stuff-10K.</p><p>Cityscapes. <ref type="table" target="#tab_6">Table 9</ref> compare SenFormer to state-ofthe-art methods on Cityscapes validation dataset. We observe that SenFormer performs on par with the best FCN and transformer-based methods. We hypothesis that since Cityscapes dataset has only 19 classes, the object classification aspect of the segmentation is easier and therefore SenFormer cannot benefit as much from its class embeddings, as it does with datasets where the number of classes </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation studies</head><p>For ablation studies, we solely use ResNet-50 pretrained on ImageNet-1K <ref type="bibr" target="#b40">[41]</ref> as backbone and trained all models for 100k iterations.</p><p>Number of blocks. <ref type="table">Table 3</ref> show the results of Sen-Former trained with varying number of decoder block per learner. The mIoU improves with more decoder blocks added. Note that using a single decoder block leads to significantly poorer performance, suggesting that all the information in P i can not be transfered to the class embeddings cls i in one operation. We choose to use 6 decoder blocks per learner as it offers a good complexity/performance tradeoff.</p><p>Weight sharing. <ref type="table" target="#tab_1">Table 2</ref> compares different weight sharing approaches for SenFormer. First, we observe that policies that involve sharing weights between the learners ("decoder sharing" and "cls embeddings") lead to a significant drop in performance, even when the number of parameters is not reduced. For example, "cls embedding" is 1.2mIoU lower than the base setting while having 89M more parameters, confirming that independence between learners is a key component in SenFormer. Furthermore, recursively applying the same decoder block leads to non trivial performance boost of +1mIoU, while having the same parameter number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Variance reduction. A common explanation for the better performance of the ensemble over its composing elements is that by averaging the variance over the merged prediction is reduced. To test this assumption, for each input image in the Ade20K validation set, we computed for the ensemble and for each learner the variance over the segmentation map prediction for each pixel (i.e., the variance along the channel axis). We then averaged over the entire validation set. As shown in <ref type="table" target="#tab_0">Table 11</ref>, the ensemble variance is not significantly smaller than the variance of the individual learners. Consequently, the variance reduction interpretation may not apply in the context of self-ensemble, and more broadly for deep learning models <ref type="bibr" target="#b0">[1]</ref>.</p><p>Multi-view approach. A more recent explanation for the success of Ensemble is that the different learners capture multi-views present in the data <ref type="bibr" target="#b0">[1]</ref>. However, since the mutli-scale inputs of the learners come from the same backbone, it is very unlikely that they focus on different views of the objects of interest. We rather hypothesize that in Sen-Former the boost in performance does not emerge from the different random initialization of the learners that will learn to focus on specific views of the input image, but rather from the different scale information captured by the FPN. Consequently, using more than one learner per level in the feature pyramid will not yield better results. It is indeed confirmed by results in <ref type="table" target="#tab_0">Table 12</ref> where SenFormer performances do not improve with additional learners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>This paper introduces our self-ensemble approach for semantic segmentation, a simple methodology that benefits from ensemble learning while avoiding the inconvenience and cost of training multiple times the same model. We leveraged the multi-scale feature set produced by FPN-like methods to build an ensemble of decoders within a single model, where learners in the ensemble are fed with fea-tures coming from different levels of the feature pyramid. We also developed a transformer-based architecture for the learner/decoders.</p><p>Our approach outperforms current state-of-the-art on Pascal Context and COCO-Stuff-10K datasets and is competitive on Ade20K and Cityscapes datasets for semantic segmentation. It is more efficient in terms of FLOPs and limit the number of parameter thanks to weight sharing. It is part of our future work to investigate "mask classification" formulation of semantic segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>We first provide more information about the datasets used to evaluate SenFormer performances (Section A). In an attempt to gain more insights about our self-ensemble approach, additional experiments and discussions on Sen-Former are presented. Eventually, we provide qualitative segmentation results of SenFormer. COCO-Stuff-10K [2] is a subset of the COCO dataset <ref type="bibr" target="#b31">[32]</ref> for semantic segmentation. It consists of 9k images for training and 1k images for testing, covering 171 semanticlevel categories. For training, all SenFormer models were trained for 80k iterations, with a batch size of 16 and a crop size of 512 ? 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets Descriptions</head><p>Pascal Context <ref type="bibr" target="#b36">[37]</ref> training set contains 4, 996 images covering 59 classes and the testing set contains 5, 104 images. The data come from the PASCAL VOC 2010 contest <ref type="bibr" target="#b12">[13]</ref>, where annotations for the whole scene have been added. All SenFormer models were trained for 40k iterations, with a batch size of 16 and a crop size of 480 ? 480.</p><p>Cityscapes <ref type="bibr" target="#b8">[9]</ref> is a high-resolution dataset of 5, 000 street-view images with 19 semantic classes. Conventionally, the dataset is split into a training set of 2, 975 images and a validation set of 500 images. All SenFormer models were trained for 100k iterations, with a batch size of 8 and a crop size of 512 ? 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detailed Learners Architecture</head><p>Hereafter, we detail the architecture of the learners. Each decoder is a transformer composed of L layers. In a nutshell, a single Transformer Decoder block consists of three successive operations: Cross-Attention (CA), Self-Attention and Multi-Layer Perceptron layers.</p><p>First, the multi-scales features {P 2 , P 3 , P 4 , P 5 } are reshaped into a set of tokens {z 2 , z 3 , z 4 , z 5 } where z i ? R ni?d , n i = HW 2 i is the number of token and d is the numbers of channels in all the feature maps of the FPN.</p><p>In the CA layer, the FPN's token features z i 's are linearly transformed through matrix multiplication to acts as the keys and values, and the class embedding cls i 's as the queries. For i ? {2, 3, 4, 5} the CA operation is as follows:</p><formula xml:id="formula_7">K = z i W K , V = z i W V , Q = LN (cls i )W Q ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_8">W K , W V , W Q ? R D?D CA(Q i , P i ) = cls i + sof tmax( QK T ? D )V</formula><p>where softmax denotes the softmax function applied along the last dimension and LN denotes the "Layer Normalization" operation. This operation aims at distilling the knowledge gained by the backbone contained in the features P i into the class embeddings cls i . This way, the model will retain the information of what is it for a feature token z i to represent a specific class. Indeed, since the class embedding vector is used to get the final prediction via a dot product with the features tokens, during the back-propagation the class embedding tensor cls k i (that represents the k th class) will be encouraged to become similar to the tokens of z i that correspond to the k th class, according to the ground truth segmentation maps.</p><p>Then, the Self-Attention layer enables sharing the information acquired during the Cross-Attention across the class embedding vectors. Eventually, an Multi-Layer Perceptron layer is used to propagate the information across the chan- Overall, each decoder is composed of L layers of decoder blocks, and its prediction is obtained via a dot product between the class embeddings cls i and the corresponding feature pyramid feature P i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Feature Pyramid Network</head><p>The set of features extracted by the backbone {C 2 , C 3 , C 4 , C 5 }, is enhanced by the FPN <ref type="bibr" target="#b30">[31]</ref> to obtain a feature pyramid that has strong spatial and semantics at all scales. To do so, {C 2 , C 3 , C 4 , C 5 } undergo a linear projection to set the channel dimension of each scale to a fixed size denoted as d(fixed to 512). Consecutive levels are then upsampled to the same size and merged by element-wise addition. Eventually, the merged features are processed by a 3 ? 3 convolution to alleviate the aliasing effect of the upsampling. In sum, the output set of features of the FPN {P 2 , P 3 , P 4 , P 5 } is obtained by:</p><formula xml:id="formula_9">P i = Conv 1?1 (C i ) , i ? {2, 3, 4, 5} (7) P 5 = P 5 P i = Conv 3?3 P i + Upsample(P i+1 ) , i ? {2, 3, 4}</formula><p>with Conv ??? being a convolution with ? ? ? kernel and Upsample being the nearest neighbor upsampling operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.1 Feature Pyramid Network Transformer</head><p>We empirically found that while introducing marginal changes to the implementation and a minimal computational cost, replacing the 3 ? 3 convolution by a transformer block in the FPN increases the segmentation performance for SenFormer, see <ref type="table" target="#tab_0">Table 13</ref>.a. In practice, we use the window-based transformer block (denoted as WTB) of <ref type="bibr" target="#b33">[34]</ref> to limit the memory footprint overhead. We name this enhanced FPN version as Feature Pyramid Network Transformer-enhanced (FPNT), where </p><formula xml:id="formula_10">P i = Conv 1?1 (C i ) , i ? {2, 3, 4, 5} (8) P 5 = P 5 P i = WTB P i + Upsample(P i+1 ) , i ? {2, 3, 4}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Experiments</head><p>Weight sharing. In <ref type="table" target="#tab_0">Table 14</ref> we compare on multiple datasets SenFormer performance when no weight sharing is used (none) and the default weight sharing setting, where the same decoder block is recursively used L times (repeated). Despite having significantly fewer parameters, our weight sharing strategy has similar performance to when no sharing is used.</p><p>Multi-scales feature generation. In <ref type="table" target="#tab_0">Table 13</ref>.a we demonstrate the benefit of the feature pyramid network transformer (FPNT) over the traditional FPN for Sen-Former. We observe that FPNT is 0.9 mIoU better than the FPN baseline (with ResNet50 as backbone). Also, not using any FPN-like method significantly reduces the performance, which can be explained by the fact that each learner must receive semantically and spatially strong features.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional discussions</head><p>This section further discusses the impact of the learners' architectural choices on their predictions. In particular, we study the behavior of the class embeddings after convergence. Furthermore, we show how the "pre-norm" strategy may help the class embeddings act as a memory bank.</p><p>Class embeddings acting like a memory bank. In Sen-Former, for a given learner, each class embedding vector represents a unique class and is used to produce the learner's prediction for that given class. Hence, we expect the class embedding to retain specific information about that class. Accordingly, at the end of the training, the different learners should converge to different vectors (as they represent different classes). <ref type="figure" target="#fig_4">Figure 4</ref> shows the distribution density of the cosine similarity between the different class embeddings of Sen-Former trained on ADE20K with ResNet-50 as backbone; i.e. for the i-th learner the distribution of the following set cos(cls k i , cls l i ), (k, l) ? {1, .., N c ? 1} ? {k, .., N c } .</p><p>As expected, although ADE20K has a large number of classes, the density curves are close to the origin, suggesting that the different class embeddings converge to different vectors. Hence, for a given learner i, the class embeddings cls i = [cls 1 i , . . . , cls Nc i ] of SenFormer effectively acts as a memory bank that is used by the decoder to assess how likely a given feature token of P i represents a certain class.</p><p>Pre-Norm vs Post-Norm. Early versions of transformers <ref type="bibr" target="#b46">[47]</ref> as well as recent applications to object detection and segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref> applied layer normalization after the skip connection (post-norm), while recent implementations tend toward using pre-normalization setting. <ref type="table" target="#tab_0">Table  13</ref>.b shows that a pre-normalization strategy performs best for our architecture. We believe that, by leaving the skip connection pathway unaltered, the pre-norm setting ease the information flow from the ground truth supervision to the input class embeddings of the learner, therefore fostering the "memory bank" mechanism described above. It may also explain why <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref> do not benefit from the use of prenorm. Indeed, in DETR and MaskFormer, each query embedding vector (the equivalent of our class embedding) does not correspond to a unique class, but is rather dynamically routed to a class by using a bipartite matching, therefore the learned embeddings do not act as a "memory bank".  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualization</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>SenFormer architecture. (Left): The features extracted by the backbone {C2, C3, C4, C5} are enhanced in a feature pyramid to produce spatially and semantically strong features maps at every level of the pyramid {P2, P3, P4, P5}. Each set of features is decoded by a different learner in the ensemble and the learners' predictions are merged. (Right): architecture of the transformer block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Weight sharing policies. Different color indicates different set of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>ADE20K [ 60 ]</head><label>60</label><figDesc>is a scene parsing dataset built from ADE20K-Full dataset, where 150 classes were selected to constitute SceneParse150 challenge. It consists of 20, 210 training images, 2, 000 validation images, and covers 150 fine-grained labeled classes. Models are trained for 160k iterations, with a batch size of 16 and a crop size of 640?640 pixels when using Swin-B and Swin-L as backbone; otherwise a crop size of 512 ? 512 is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>FeaturesFusionBaseline architecture. nel dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Distribution of the cosine similarity between the different class embeddings for each learner in SenFormer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 ,</head><label>5</label><figDesc>we visualize sample segmentation predictions of SenFormer with Swin-L backbone on ADE20K validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of SenFormer segmentation predictions on ADE20K validation with Swin-L backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performances by using different learner combinations, where / indicates whether the learner is used for the prediction.</figDesc><table><row><cell>d 2 d 3 d 4</cell><cell>d 5 mIoU</cell></row><row><cell></cell><cell>42.90</cell></row><row><cell></cell><cell>42.08</cell></row><row><cell></cell><cell>41.40</cell></row><row><cell></cell><cell>38.23</cell></row><row><cell></cell><cell>44.12</cell></row><row><cell></cell><cell>44.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance comparisons on ADE20K validation of different weight sharing settings for SenFormer. ? indicates Sen-Former's default setting.weight sharing # blocks mIoU #params.</figDesc><table><row><cell>setting</cell><cell></cell><cell></cell><cell></cell><cell>FLOPs</cell></row><row><cell>decoder shared</cell><cell>6</cell><cell>42.69</cell><cell>68M</cell><cell>179G</cell></row><row><cell>cls embeddings</cell><cell>6</cell><cell>42.91</cell><cell>67M</cell><cell>179G</cell></row><row><cell>repeated ?</cell><cell>6</cell><cell>44.38</cell><cell>55M</cell><cell>179G</cell></row><row><cell>none</cell><cell>1</cell><cell>43.12</cell><cell>55M</cell><cell>111G</cell></row><row><cell>none</cell><cell>6</cell><cell>44.68</cell><cell>144M</cell><cell>179G</cell></row><row><cell>UperNet</cell><cell></cell><cell>42.05</cell><cell>67M</cell><cell>238G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Experiments with different number of decoder blocks. Comparisons of the features fusion and self-ensemble strategies. ? indicates self-ensemble.</figDesc><table><row><cell cols="4"># block mIoU #params. FLOPs</cell></row><row><cell>1</cell><cell>42.44</cell><cell>55M</cell><cell>111G</cell></row><row><cell>3</cell><cell>43.25</cell><cell>55M</cell><cell>139M</cell></row><row><cell>6</cell><cell>43.6</cell><cell>55M</cell><cell>179G</cell></row><row><cell>9</cell><cell>43.7</cell><cell>55M</cell><cell>220G</cell></row><row><cell>method</cell><cell></cell><cell cols="3">mIoU #params. FLOPs</cell></row><row><cell>UperNet</cell><cell></cell><cell>42.02</cell><cell>67M</cell><cell>238G</cell></row><row><cell cols="2">SenUperNet ?</cell><cell>42.8</cell><cell>70M</cell><cell>135G</cell></row><row><cell cols="3">FeaturesFusionBaseline 43.1</cell><cell>52M</cell><cell>307G</cell></row><row><cell>SenFormer ?</cell><cell></cell><cell>44.3</cell><cell>55M</cell><cell>179G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Self-ensemble SenFormer vs features fusion UperNet on ADE20K validation. Backbones pre-trained on ImageNet-22K are marked with ? .</figDesc><table><row><cell></cell><cell>method</cell><cell cols="2">backbone</cell><cell>crop size</cell><cell>#params. FLOPs mIoU</cell></row><row><cell>CNN</cell><cell cols="3">UperNet SenFormer ResNet-50 ResNet-50</cell><cell cols="2">512?512 67M 512?512 55M</cell><cell>238G 179G</cell><cell>42.05 44.38</cell></row><row><cell></cell><cell>UperNet</cell><cell cols="4">ResNet-101 512?512 86M</cell><cell>257G</cell><cell>43.82</cell></row><row><cell></cell><cell cols="5">SenFormer ResNet-101 512?512 79M</cell><cell>199G</cell><cell>46.93</cell></row><row><cell>Transformer</cell><cell cols="3">UperNet SenFormer Swin-T Swin-T UperNet Swin-S SenFormer Swin-S UperNet Swin-B  ?</cell><cell cols="2">512?512 60M 512?512 59M 512?512 81M 512?512 81M 640?640 121M</cell><cell>236G 179G 259G 202G 471G</cell><cell>44.41 46.0 47.72 49.2 50.04</cell></row><row><cell></cell><cell cols="3">SenFormer Swin-B  ?</cell><cell cols="2">640?640 120M</cell><cell>371G</cell><cell>52.21</cell></row><row><cell></cell><cell>UperNet</cell><cell cols="2">Swin-L  ?</cell><cell cols="2">640?640 234M</cell><cell>647G</cell><cell>52.05</cell></row><row><cell></cell><cell cols="3">SenFormer Swin-L  ?</cell><cell cols="2">640?640 233M</cell><cell>546G</cell><cell>53.08</cell></row><row><cell cols="4">Table 6. Benchmark on ADE20K validation set.</cell><cell></cell></row><row><cell>method</cell><cell cols="3">backbone mIoU +MS</cell><cell></cell></row><row><cell>DeepLabV3+ [6]</cell><cell>R50</cell><cell>44.0</cell><cell>44.9</cell><cell></cell></row><row><cell>PerPixelBaseline+ [7]</cell><cell>R50</cell><cell>41.9</cell><cell>42.9</cell><cell></cell></row><row><cell>MaskFormer [7]</cell><cell>R50</cell><cell>44.5</cell><cell>46.7</cell><cell></cell></row><row><cell>SenFormer</cell><cell>R50</cell><cell>44.4</cell><cell>45.2</cell><cell></cell></row><row><cell>OCRNet [57]</cell><cell>R101</cell><cell>-</cell><cell>45.3</cell><cell></cell></row><row><cell>DeepLabV3+ [6]</cell><cell>R101</cell><cell>45.5</cell><cell>46.4</cell><cell></cell></row><row><cell>MaskFormer [7]</cell><cell>R101</cell><cell>45.5</cell><cell>47.2</cell><cell></cell></row><row><cell>SenFormer</cell><cell>R101</cell><cell>46.9</cell><cell>47.9</cell><cell></cell></row><row><cell>SETR-L MLA [59]</cell><cell>ViT-L</cell><cell>-</cell><cell>50.3</cell><cell></cell></row><row><cell>Segmenter [43]</cell><cell>ViT-L</cell><cell cols="2">50.71 52.25</cell><cell></cell></row><row><cell>Segmenter-Mask [43]</cell><cell>ViT-L</cell><cell cols="2">51.82 53.63</cell><cell></cell></row><row><cell>SegFormer [56]</cell><cell>MiT-B5</cell><cell>51.0</cell><cell>51.8</cell><cell></cell></row><row><cell>UperNet [34]</cell><cell>Swin-L</cell><cell cols="2">52.05 53.5</cell><cell></cell></row><row><cell>SenFormer</cell><cell>Swin-L</cell><cell cols="2">53.08 54.2</cell><cell></cell></row><row><cell>MaskFormer [7]</cell><cell>Swin-L</cell><cell>54.1</cell><cell>55.6</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Benchmark on Pascal Context test. ? /blue indicates previous/new SOTA.</figDesc><table><row><cell>method</cell><cell cols="3">backbone mIoU +MS</cell></row><row><cell>DANet [14]</cell><cell>R50</cell><cell>-</cell><cell>50.5</cell></row><row><cell>EMANet [30]</cell><cell>R50</cell><cell>-</cell><cell>50.5</cell></row><row><cell>CAA [24]</cell><cell>R50</cell><cell>50.23</cell><cell>-</cell></row><row><cell>SenFormer</cell><cell>R50</cell><cell cols="2">53.18 54.3</cell></row><row><cell>DANet [14]</cell><cell>R101</cell><cell>-</cell><cell>52.6</cell></row><row><cell>EMANet [30]</cell><cell>R101</cell><cell>-</cell><cell>53.1</cell></row><row><cell>DeepLabV3+ [6]</cell><cell>R101</cell><cell cols="2">53.2 54.67</cell></row><row><cell>OCRNet [57]</cell><cell>R101</cell><cell>-</cell><cell>54.8</cell></row><row><cell>CAA [24]</cell><cell>R101</cell><cell>-</cell><cell>55.0</cell></row><row><cell>SenFormer</cell><cell>R101</cell><cell>54.6</cell><cell>56.6</cell></row><row><cell>OCRNet [57]</cell><cell>HRNet</cell><cell>-</cell><cell>56.2</cell></row><row><cell>CAA [24]</cell><cell>EN-B7</cell><cell cols="2">58.40 60.5  ?</cell></row><row><cell>SETR-L MLA [59]</cell><cell>ViT-L</cell><cell>54.9</cell><cell>55.8</cell></row><row><cell>Segmenter [43]</cell><cell>ViT-L</cell><cell>58.1</cell><cell>59.0</cell></row><row><cell>SenFormer</cell><cell>Swin-L</cell><cell>63.1</cell><cell>64.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>Benchmark on COCO-Stuff-10K test. ? /blue indicates previous/new SOTA.</figDesc><table><row><cell>method</cell><cell cols="3">backbone mIoU +MS</cell></row><row><cell>EMANet [30]</cell><cell>R50</cell><cell>-</cell><cell>37.6</cell></row><row><cell>PerPixelBaseline+ [7]</cell><cell>R50</cell><cell>34.2</cell><cell>35.8</cell></row><row><cell>MaskFormer [7]</cell><cell>R50</cell><cell>37.1</cell><cell>38.9</cell></row><row><cell>SenFormer</cell><cell>R50</cell><cell>40.0</cell><cell>41.3</cell></row><row><cell>DANet [14]</cell><cell>R101</cell><cell>-</cell><cell>39.7</cell></row><row><cell>EMANet [30]</cell><cell>R101</cell><cell>-</cell><cell>39.9</cell></row><row><cell>OCRNet [57]</cell><cell>R101</cell><cell>-</cell><cell>39.5</cell></row><row><cell>CAA [24]</cell><cell>R101</cell><cell>-</cell><cell>41.2</cell></row><row><cell>MaskFormer [7]</cell><cell>R101</cell><cell>38.1</cell><cell>39.8</cell></row><row><cell>SenFormer</cell><cell>R101</cell><cell>41.0</cell><cell>42.1</cell></row><row><cell>OCRNet [57]</cell><cell>HRNet</cell><cell>-</cell><cell>40.5</cell></row><row><cell>CAA [24]</cell><cell>EN-B7</cell><cell>-</cell><cell>45.4  ?</cell></row><row><cell>SenFormer</cell><cell>Swin-L</cell><cell>49.8</cell><cell>51.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .</head><label>9</label><figDesc>Benchmark on Cityscapes val.</figDesc><table><row><cell>method</cell><cell cols="3">backbone mIoU +MS</cell></row><row><cell>MaskFormer [7]</cell><cell>R50</cell><cell>78.5</cell><cell>-</cell></row><row><cell>SenFormer</cell><cell>R50</cell><cell>78.8</cell><cell>80.1</cell></row><row><cell>DeepLabV3+ [6]</cell><cell>R50</cell><cell cols="2">78.97 80.46</cell></row><row><cell>MaskFormer [7]</cell><cell>R101</cell><cell>79.7</cell><cell>81.4</cell></row><row><cell>SenFormer</cell><cell>R101</cell><cell>79.9</cell><cell>81.4</cell></row><row><cell>OCRNet [57]</cell><cell>R101</cell><cell>-</cell><cell>82.0</cell></row><row><cell>DeepLabV3+ [6]</cell><cell>R101</cell><cell cols="2">80.9 82.03</cell></row><row><cell>SETR-L PUP [59]</cell><cell>ViT-L</cell><cell>-</cell><cell>82.2</cell></row><row><cell>Segmenter [43]</cell><cell>ViT-L</cell><cell>-</cell><cell>80.7</cell></row><row><cell>Segmenter-Mask [43]</cell><cell>ViT-L</cell><cell>79.1</cell><cell>81.3</cell></row><row><cell>SenFormer</cell><cell>Swin-L</cell><cell>82.8</cell><cell>84.0</cell></row><row><cell>SegFormer [56]</cell><cell>MiT-B5</cell><cell>82.4</cell><cell>84.0</cell></row><row><cell>is larger.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 .</head><label>10</label><figDesc>Merging strategies.</figDesc><table><row><cell></cell><cell cols="2">Table 11.</cell><cell>Ensemble</cell></row><row><cell></cell><cell cols="3">and learners variance on</cell></row><row><cell></cell><cell cols="3">Ade20K validation.</cell></row><row><cell cols="2">merging strategy mIoU</cell><cell>Output</cell><cell>var. (10 ?3 )</cell></row><row><cell>averaging product majority vote hierarchical att. explicit att.</cell><cell>44.4 40.28 39.89 44.5 39.7</cell><cell>ensemble d 2 d 3 d 4 d 5</cell><cell>56.6 56.0 56.8 56.3 55.2</cell></row><row><cell cols="4">Table 12. Effect of increasing the number of learners.</cell></row><row><cell cols="2"># learners per scale # learner total</cell><cell>mIoU</cell><cell></cell></row><row><cell>1</cell><cell>4</cell><cell>44.3</cell><cell></cell></row><row><cell>2</cell><cell>8</cell><cell>44.2</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 13 .</head><label>13</label><figDesc>Ablation studies related to SenFormer architectural choices. Models are trained on ADE20K validation for 100k iterations with a ResNet-50 backbone pretrained on ImageNet-1K<ref type="bibr" target="#b40">[41]</ref> </figDesc><table><row><cell cols="2">(a) FPN vs FPNT.</cell><cell cols="2">(b) Normalization strategy.</cell></row><row><cell cols="2">method mIoU</cell><cell>method</cell><cell>mIoU</cell></row><row><cell>none</cell><cell>38.15</cell><cell></cell><cell></cell></row><row><cell>FPN</cell><cell>42.7</cell><cell cols="2">Post-Norm 42.63</cell></row><row><cell>FPNT</cell><cell>43.6</cell><cell>Pre-Norm</cell><cell>43.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 14 .</head><label>14</label><figDesc>Comparison of the "repeated" sharing strategy vs no weight sharing (none) on multiple benchmark datasets.</figDesc><table><row><cell>backbone</cell><cell>sharing</cell><cell cols="5">params. ADE20k Pascal COCO Cityscapes</cell></row><row><cell>ResNet-50</cell><cell>none repeated</cell><cell>144M 55M</cell><cell>44.6 44.3</cell><cell>53.2 53.2</cell><cell>39.0 40.0</cell><cell>78.8 78.8</cell></row><row><cell>ResNet-101</cell><cell>none repeated</cell><cell>163M 79M</cell><cell>46.5 46.9</cell><cell>55.1 54.6</cell><cell>39.6 41.0</cell><cell>80.3 79.9</cell></row><row><cell>Swin-L</cell><cell>none repeated</cell><cell>314M 233M</cell><cell>53.1 53.1</cell><cell>62.4 63.1</cell><cell>49.1 49.8</cell><cell>82.2 82.8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Romain Fabre for insightful discussion without which this paper would not be possible. This work was partially supported by the National Institutes of Health (NIH), National Cancer Institute (NCI) Human Tumor Atlas Network (HTAN) Research Center (U2C CA233280), and and a NIH/NCI Cancer Systems Biology Consortium Center (U54 CA209988).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards understanding ensemble, knowledge distillation and self-distillation in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cocostuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<ptr target="https://github.com/open-mmlab/mmsegmentation,2020.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Universal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Xcit: Cross-covariance image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Jegou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Loss surfaces, mode connectivity, and fast ensembling of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew G</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7036" to="7045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Training independent subnetworks for robust prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marton</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><forename type="middle">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Mingbo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno>ICLR, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning techniques for medical image segmentation: Achievements and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Mohammad Hesam Hesamian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangjian</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="582" to="596" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Virtual-toreal: Learning to control in visual semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang-Wei</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yu-Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Yang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Yun</forename><surname>Shann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsiang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsuan-Kung</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Hsi-Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
		</author>
		<editor>Chih-Chieh Tu, Yueh-Chuan Chang, Tsu-Ching Hsiao, Hsin-Wei Hsiao, Sih-Pin Lai, and Chun-Yi Lee.</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00109</idno>
		<title level="m">Snapshot ensembles: Train 1, get m for free</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fapn: Feature-aligned pyramid network for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="864" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Channelized axial attention for semantic segmentation -considering channel relation within spatial attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangjian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Perceiver: General perception with iterative attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Machine learning: a review of classification and combining techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sotiris B Kotsiantis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panayiotis</forename><forename type="middle">E</forename><surname>Zaharakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pintelas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="159" to="190" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Why m heads are better than one: Training a diverse ensemble of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Expectation-maximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="9167" to="9176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation. CoRR, abs/1411</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4038</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Transformers without tears: Improving the normalization of self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Toan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salazar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05895</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Popular ensemble methods: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Maclin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="169" to="198" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deep semantic segmentation for automated driving: Taxonomy, roadmap and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mennatullah</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Elkerdawy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Yogamani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10781" to="10790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural networks with late-phase weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seijin</forename><surname>Johannes Von Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sacramento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Meulemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benjamin F Grewe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Crossformer: A versatile vision transformer hinging on cross-scale attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binbin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Scale-equalizing pyramid convolution for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Batchensemble: an alternative approach to efficient ensemble and lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeming</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Hyperparameter ensembles for robustness and uncertainty quantification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6514" to="6527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Segmentation transformer: Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Ensembling neural networks: many could be better than all. Artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="239" to="263" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
