<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">No Language Left Behind: Scaling Human-Centered Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nllb</forename><surname>Team</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Juss?</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>?elebi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maha</forename><surname>Elbayad</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Heffernan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elahe</forename><surname>Kalbassi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janice</forename><surname>Lam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Licht</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Maillard</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skyler</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Youngblood</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bapi</forename><surname>Akula</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Mejia Gonzalez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prangthip</forename><surname>Hansanti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hoffman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semarley</forename><surname>Jarrett</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Kaushik</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Sadagopan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shannon</forename><surname>Rowe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chau</forename><surname>Spruit</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrews</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Necip</forename><surname>Fazil</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><forename type="middle">?</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
							<email>angelafan@fb.com.</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Mourachko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Ropers</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safiyyah</forename><surname>Saleem</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meta</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">No Language Left Behind: Scaling Human-Centered Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Contents</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind?</p><p>In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb. * . Equal contribution, alphabetical order ?. Research and engineering leadership, equal contribution, alphabetical order ?. Corresponding Author.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In Jack <ref type="bibr" target="#b158">Vance (1977)</ref>'s sci-fi novel The Eyes of the Overworld, its protagonist, Cugel, encounters a wizard who compels him into a task. To assist him, the wizard grants Cugel a magical device: In order to facilitate your speech, I endow you with this instrument which relates all possible vocables to every conceivable system of meaning.</p><p>Fast-forward half a century later, we now know that Cugel's magical device is really Machine Translation. Conceived as computational systems that translate texts from one language to another, machine translation has been around since the 1940s, but its recent migration from statistical <ref type="bibr">(Brown et al., 1993;</ref><ref type="bibr">Koehn, 2009;</ref><ref type="bibr" target="#b94">Lopez, 2008)</ref> to neural systems has pushed the technology to new frontiers <ref type="bibr" target="#b31">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b35">Cho et al., 2014;</ref><ref type="bibr">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b173">Wu et al., 2016)</ref>. This shift has not only advanced translation quality at breakneck speed, but it has also furthered the expansion of machine translation into new applications. Today, machine translation impacts how people all over the world communicate, work, travel, learn, access information, and more <ref type="bibr">(Khoong and Rodriguez, 2022;</ref><ref type="bibr">Koehn and Germann, 2014;</ref><ref type="bibr" target="#b95">Lee, 2020)</ref>.</p><p>While machine translation continues to grow, the fruits it bears are unevenly distributed <ref type="bibr" target="#b151">(Fan et al., 2020)</ref>. In fact, the vast majority of improvements made in machine translation in the last decades have been for high-resource languages, or languages that have large quantities of training data available digitally. For instance, those who communicate in English, French, German or Russian-languages which have long enjoyed institutional investments and data availability-stand to gain substantially more from the maturation of machine translation than those who speak Catalan, Assamese, Ligurian, or Kinyarwanda.</p><p>Many languages of this latter group attract less attention and resources, even though most languages spoken globally today are Low-Resource languages <ref type="bibr">(Joshi et al., 2020)</ref>. Many of these languages escape researchers' gaze for a confluence of reasons, including constraints conjured up by past investments (or lack thereof), research norms, organizational priorities, and Western-centrism to name a few. Without an effort to course correct, much of the internet could continue to be inaccessible to speakers of these languages. Research indicates that while only 25.9 percent of internet users speak English, 63.7 percent of all websites are in English (the next on the list is Russian at 6.8 percent; <ref type="bibr" target="#b140">Richter, 2022)</ref>. For many low-resource language communities, The Polyglot Internet <ref type="bibr">(Zuckerman, 2008)</ref>, an instrumental medium that could propel education access and social mobility, remains out of reach because the web has long prioritized content tailored to high-resource language speakers.</p><p>Expanding machine translation to more low-resource languages is further curtailed by technical challenges <ref type="bibr" target="#b75">(Haddow et al., 2022)</ref>. Compared to their high-resource counterparts, training data for low-resource languages are expensive and logistically challenging to procure <ref type="bibr">(Kuwanto et al., 2021;</ref><ref type="bibr">Nekoto et al., 2020;</ref><ref type="bibr" target="#b116">Orife et al., 2020)</ref>. Without sufficient training data, standard techniques may not stand the test of emerging demands. These hurdles have become ever more pronounced as the popularity of data-hungry techniques such as large-scale pre-training and model scaling have become mainstream <ref type="bibr" target="#b40">(Conneau and Lample, 2019;</ref><ref type="bibr" target="#b50">Kenton and Toutanova, 2019;</ref><ref type="bibr" target="#b127">Radford et al., 2019)</ref>.</p><p>To overcome these barriers, much existing work on low-resource translation has focused on leveraging multilingual systems, or models capable of handling multiple languages. These models have the advantage of crosslingual transfer <ref type="bibr">(Nguyen and Chiang, 2017</ref>; Zoph et al., (1) We strive to understand the low-resource translation problem from the perspective of native speakers. <ref type="formula" target="#formula_1">(2)</ref> We study how to automatically create training data to move low-resource languages towards high-resource. (3) We utilize this data to create state-of-the-art translation models. (4) We evaluate every language we aim to translate. 2016), allowing related languages to learn from one another <ref type="bibr" target="#b18">(Arivazhagan et al., 2019;</ref><ref type="bibr" target="#b151">Fan et al., 2020;</ref><ref type="bibr" target="#b174">Zhang et al., 2020)</ref>. While multilingual models have demonstrated promising performance improvement compared to bilingual models <ref type="bibr" target="#b154">(Tran et al., 2021)</ref>, enabling the representation of hundreds of languages while retaining strong translation quality remains an open area of research. Another strategy aimed at mitigating the low-resource challenge is to acquire more language data. Some of these attempts have focused on collecting human translations, while others have leveraged large-scale data mining and monolingual data pipelines to consolidate data found across the web <ref type="bibr" target="#b34">(Ba??n et al., 2020;</ref><ref type="bibr">Karakanta et al., 2018;</ref><ref type="bibr" target="#b132">Ramesh et al., 2022;</ref><ref type="bibr">Schwenk et al., 2021b)</ref>. The latter techniques are often plagued by noise and biases, making it difficult to validate the quality of the created datasets <ref type="bibr">(Kreutzer et al., 2022)</ref>. Finally, developing translation models for low-resource languages requires the existence of high-quality, human-translated evaluation benchmarks. Datasets such as <ref type="bibr">Flores-101 (Goyal et al., 2022)</ref> work towards this, but coverage is capped at 100 languages.</p><p>In this article, we ask: What does it take to double the language coverage of most existing translation models while ensuring high quality and safe translations? More concretely, how do we use a human-centric approach <ref type="bibr">(Robertson et al., 2021)</ref> to create fluent, meaning-preserving translations for over 200 languages, many of which belong to a class of low-resource languages that remain underserved by existing translation technologies? And how can we do so while minimizing potential harm from catastrophic and toxic translations hallucinated by neural MT models -infrequent occurrences that nevertheless have an out-sized adverse impact on the human user?</p><p>We take on this challenge in the No Language Left Behind (NLLB) effort. We begin by creating Flores-200, a many-to-many multilingual dataset that allows us to measure translation quality through any of the 40,602 total translation directions. We developed a distillation-based sentence encoding technique, LASER3 <ref type="bibr" target="#b83">(Heffernan et al., 2022)</ref>, that helped us mine web data to create parallel datasets for low-resource languages. Using both mined data and a set of human-translated seed data, we trained multilingual Mixtures-of-Experts models with state of the art performance. Despite doubling the number of languages, our final model performs 40% better than the previous state of the art on Flores-101. To detect and prevent potentially harmful translations that are hallucinated by the translation models, we created a dataset of toxic words for all 200 languages by combining automatic and human evaluations. We proposed and conducted human evaluations on many languages our models cover, in addition to the common automatic metrics, to gain qualitative insight into the impact of the translation. Finally, beyond creating these models, we also reflect on the creation process, analyzing the risks and benefits of our research from a societal standpoint. We open source all the benchmarks, data, scripts, and models described in this effort to support further research. 1 In addition, we focus on the practical applicability of our work for low-resource speaking communities. We deploy our techniques to provide translation support to Wikipedia editors, enabling them to create new articles more efficiently for languages that are not supported by other translation systems. The rest of the article is structured as follows, with <ref type="figure">Figure 2</ref> as an overview: Section 2 describes the open challenges in low-resource translation and analyzes the widespread use of translation systems. Section 3 presents the languages we focus on and how we arrived at this set of languages. Section 4 summarizes the creation process of Flores-200 and NLLB-Seed + NLLB-MD, our translation seed datasets, with quality analysis. Section 5 overviews the creation of monolingual and mined bilingual data, which enables the creation of models for hundreds of languages. Section 6 details various modeling techniques developed to improve the performance of low-resource languages. Section 7 traces the automatic and human evaluation of our translations, including the detection of catastrophic and toxic translations. We integrate the aforementioned datasets and techniques into NLLB-200, a model that currently supports 202 languages, and analyze its quality and performance in Section 8. We conclude in Section 9, where we reflect on the social impact of our research and lay out future possibilities and challenges. It is our hope that our contribution would guide future researchers who, like us, are eager to see Cugel's magical device -machine translation covering all languages -transform from a conceptual chimera into a reality.</p><p>To make our work available to the community, we open source the following:</p><p>? Training and generation scripts to reproduce our models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Human-Centered Low-Resource Language Translation</head><p>To situate our goal of providing high-quality translation for hundreds of languages, we first explore the importance of this research to those who matter the most to us: low-resource language communities. Inspired by Value Sensitive Design <ref type="bibr" target="#b61">(Friedman and Hendry, 2019;</ref><ref type="bibr"></ref> Van Der Hoven and Manders-Huits, 2020), we attribute community-level interests and values as the cornerstone of our research. Adopting this framework propels us to start with people and prioritize how they interact with technology, with direct emphasis on ethical and social considerations <ref type="bibr">(Mukhija et al., 2021)</ref>. To understand how low-resource language speakers perceive machine translation, we conducted an interview study with 44 low-resource language speakers. As stakeholders likely to be impacted by No Language Left Behind (NLLB), their contributions helped us envision the promises many believe machine translation could deliver to their communities. Punctuating their careful optimism were concrete suggestions on ways to maximize social gains while minimizing risks. Moreover, many interviewees painted illustrative pictures of the cultural and political environments their languages live in, the ways in which language and social experiences intertwine, and how NLLB could potentially shake up the cultural status quo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Exploratory Interview Study Research Design</head><p>We designed a semi-structured interview protocol aimed at exploring the needs and concerns of low-resource language speakers vis-?-vis machine translation. Although low-resource languages could be deemed low-resource for a variety of reasons, including being underresearched, digitized, or taught <ref type="bibr" target="#b38">(Cieri et al., 2016;</ref><ref type="bibr" target="#b101">Magueresse et al., 2020)</ref>, for the purpose of the study, we define low-resource as languages which had less than 1 million sentences of publicly available example translations at the time of the study. The interviews captured a broad array of attitudes and understandings, including the usage and application of lowresource languages, perceived value of translation technology, and how translation systems ought to be developed.</p><p>Overall, our recruitment effort led us to 44 native speakers of low-resource languages from diverse backgrounds, with ages ranging from 23 to 58. Covering a total of 36 languages, the distribution is as follows: 5 languages are spoken predominantly in North America, 8 in South America, 4 in Europe, 12 in Africa, and 7 in Asia. Although our sample has breadth in terms of race, education, and location, the majority of our participants are immigrants living in the U.S. and Europe, and about a third of them (n = 17) identify as tech workers. All interviews were conducted remotely via video conferencing software. On average, the interviews lasted 1.5 hours. Two-third of the interviews were recorded and transcribed. For unrecorded interviews, two researchers took extensive notes throughout. Bringing all 44 interviews together, responses were then systematically coded to allow major themes and ideas to emerge.</p><p>We acknowledge that sampling low-resource language speakers from diasporic contexts comes with its limitations. For one, as immigrants, their perspectives may not consummately capture the sentiments of their communities back home. That said, some scholars have argued that in technologically underdeveloped nations, where many low-resource language communities reside, people tend to view technology more optimistically and aspirationally than those who live in places with higher levels of technological development <ref type="bibr">(Kapania et al., 2022;</ref><ref type="bibr">Kozyreva et al., 2021;</ref><ref type="bibr">Sambasivan, 2021;</ref><ref type="bibr">Sambasivan et al., 2021)</ref>. Thus, being exposed to critical technological discourses (especially in recent times) could in fact make many of our interviewees more cognizant of the risks behind technological advancement, affording them a more balanced outlook. Moreover, immigration scholars remind us that global movement today is a transnational process, where those in receiving societies maintain cultural ties with those who remain in sending societies via a variety of communicative and media platforms <ref type="bibr" target="#b32">(Baldassar et al., 2016;</ref><ref type="bibr">Levitt and Jaworsky, 2007;</ref><ref type="bibr">Levitt and Lamba-Nieves, 2011</ref>). Because we found strong evidence of such processes in our interviews, we trust that our participants are in a unique position to speak both critically and knowledgeably about the sociological underpinnings of their languages.</p><p>Over-sampling tech workers may introduce another form of selection bias. More specifically, research suggests that tech workers, given their insider status, are likely to espouse techno-optimism -a positive outlook with respect to technological development <ref type="bibr" target="#b107">(McLennan, 2016)</ref>. While such an effect cannot be downplayed, tech workers' personal affinity with technological practices could in fact imbue in them a critical reflexivity we were eager to tap into. As projected, while many participants speculated on the benefits of our research, they were equally keen on underscoring the potential risks such an intervention might impose on Zooming into individual communities themselves, we see similar forms of divide. For instance, most interviewees agree that those with technological know-how would benefit more from machine translation than those who do not. One interview hints that younger individuals in their communities are more well-suited to exploit the utility of machine translation than their older counterparts. Citing the recent COVID-19 pandemic as an example, she noted that in places where science-backed information was sparse due to the lack of trust-worthy formal institutions, seniors of these communities were dependent on their more tech-savvy network and family members to acquire timely, translated health information derived from international organizations. In the same vein, those with higher levels of technology know-how would also be better able to repel misinformation, fake news, or online scams that could arise from the expansion of translation technologies into low-resource languages.</p><p>Taken collectively, it is important to note that low-resource language communities are not a monolithic group; they each navigate unique sociopolitical and cultural contexts. In speaking to their constituents, we learn that realizing quality translation, while important for several reasons, remains one solution to a massive puzzle that is fair language representation and equitable knowledge access. That said, by offering up one solution, we hope to galvanize other actors into action. As one low-resource language speaker opined, incorporating more low-resource languages in machine translation helps de-prioritized languages gain digital visibility on a global scale, which could compel local institutions to take native languages more seriously and invest more resources into preserving or teaching them. This perspective underscores both the symbolic and material benefits machine translation could bring. The positive encouragements from low-resource language speakers throughout the course of the study remind us that by taking a human-centric approach and focusing on languages that have historically been left behind, we can help communities maintain a connection to their native languages-a quintessential part of many people's culture and identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">No Language Left Behind: Guiding Principles</head><p>Combining insights drawn from interviews with low-resource language speakers and good practices distilled from literature on responsible <ref type="bibr">AI (Arrieta et al., 2020;</ref><ref type="bibr" target="#b121">Bender et al., 2021;</ref><ref type="bibr">Blodgett et al., 2022;</ref><ref type="bibr" target="#b121">Paullada et al., 2021;</ref><ref type="bibr">Sambasivan and Holbrook, 2018)</ref>, we introduce four key guiding principles underlying our research:</p><p>1. Prioritize the needs of underserved communities. As aforementioned, we put the needs of low-resource language communities at the front and center of our effort.</p><p>Recognizing that machine translation is a value-laden technological artifact that has historically de-prioritized certain populations, we use this effort to redistribute power and resources to underserved communities. By elevating the needs of low-resource language communities, we hope that our contribution is part of a collective effort that propels digital representation into a more equitable era.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Sharing through open-sourcing. Low-resource language speakers across the board remind us that transparency ought to be a key emphasis when developing NLLB. With the dual intent to foster transparency and avoid a duplication of effort, we decided early on that we were going to open source NLLB. This way, the research community at large could directly benefit from our contribution. Creating NLLB with open-sourcing in mind also motivates us to be intentional and deliberative in our approach throughout the developmental process. We hope that the impact of our work could be amplified as other scientists and practitioners build on this effort to advance the field of machine translation as a whole.</p><p>3. Being interdisciplinary in our approach. As cogently put by a low-resource language speaker, machine translation is not just a coding problem, for at its very core, it is a human matter. To avoid the 'alignment problem' <ref type="bibr" target="#b37">(Christian, 2020</ref>) and allow our system to perform in a way that is both value-sensitive and socially responsible, our research effort is taken on by an interdisciplinary team with scholars from a wide array of humanities (i.e., Philosophy, Ethics), social scientific (i.e., Sociology, Linguistics), and technical (i.e., Computer Science, Statistics) backgrounds. Bolstering the diversity of our team not only expands our methodological and analytic toolkit, it also affords us a chance to leverage different skills to tackle disparate aspects of the challenge.</p><p>4. Being reflexive in our efforts. Finally, reflexivity motivates us to critically examine our own judgments, practices, and belief systems throughout NLLB's creation process to ensure that we mitigate biases commonly found in the development of artificial intelligence systems. Concretely, we offer up detailed documentation of how we arrived at various decisions below to allow different stakeholders to comb through our intentions and motivations. We acknowledge that with ambitious efforts like these, trade-offs have to be made and perfection remains elusive. As such, it is our hope that our current effort would invite critical examinations of existing practices, which would then allow us to make more informed decisions in future iterations of NLLB. Now that we have described our motivation and values, we move on to the next part of the story-overcoming the technical challenges involved in realizing machine translation for 200 languages, from language identification to training data, models, and evaluation. As is the case with any cutting edge interventions, big problems require novel adaptions. Below, we describe the journey we took to materialize the technical dimensions of NLLB, detailing ethical and social considerations along the way. First, let's meet our language candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Languages</head><p>Broadly accessible machine translation systems support around 130 languages; our goal is to bring this number up to 200. In deciding what languages to offer, we first parsed through the 101 languages covered in Flores-101, a dataset for translation evaluation covering predominantly low-resource languages. From there, we generated a preliminary list of over 250 possible language candidates, eventually trimming it down to around 210 for final expansion from 101 to 200+ languages. The creation process of the preliminary list is as follows. First, we considered all languages with a Wikipedia presence. As noted in the section above, Wikipedia is a key site of knowledge dissemination for many speaking low-resource languages, making it a pertinent place to start. Currently, Wikipedia supports over 300 languages, extending mindfully its content beyond English <ref type="bibr">(Johnson and Lescak, 2022)</ref>, and new languages can be added  as part of a community request process. 3 Next, we solicited lists of languages spoken in various regions by native speakers, focusing particularly on African languages-a category of languages that have historically been underrepresented in translation efforts <ref type="bibr">(Nekoto et al., 2020)</ref>. We then examined language coverage in multiple existing datasets in the natural language processing community, paying focused attention on training datasets without accompanying evaluation datasets. Finally, we considered the adoption and usage of each language by looking at the approximate number of native speakers and other community-level variables relevant to our work.</p><p>Next, for each of the language candidates, we partnered with linguists from various specialized language service providers to understand if each of these languages has a standardized written form. We did this because having a reliable, high-quality evaluation dataset is critical to accelerated experimental progress. However, prioritizing languages with fairly standardized written forms has notable downsides (see Appendix A). For one, many languages have natural variations and are being written in different standards or scripts in different regions. For instance, languages such as Fulah include several distinct varieties and languages such as Kashmiri and Central Kanuri contain multiple scripts in common use. Systematically documenting these dimensions helped us assess how we could best support multiple variants of different languages (such as languages with multiple writing systems or natural variation).</p><p>In tandem with these considerations, deciding which languages to include in the final list ultimately came down to assessing the potential impact we might have on the respective low-resource language communities. For instance, we exclude languages with extremely low number of native speakers. Without a concerted plan to thoroughly understand the needs of these communities and potential risks we could cause, we do not feel comfortable including their languages in our effort. Keeping in line with our guiding principles, many of the languages that made the final cut have a presence on Wikipedia and are from historically underrepresented regions. Last but not least, it is worth noting that in this work, we exclude many languages that do not have written standards or are predominantly oral. It is our hope that future research could direct more attention at languages with different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NLLB Seed</head><p>Public Bitext Monolingual Data  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FLORES-200</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Creating Professionally Translated Datasets: FLORES-200 and NLLB-Seed</head><p>Low-resource translation faces several challenges, first and foremost that of data availability. In this section, we describe three components devised to overcome this problem, shown in <ref type="figure" target="#fig_1">Figure 3</ref>. First, we describe the creation of Flores-200, a high quality, many-to-many benchmark dataset that doubles the language coverage of a previous effort known as Flores-101. Then, we trace the development process of professionally-translated seed bitext data in 39 low-resource languages, giving us the ability to train any models that require parallel data. Finally, we describe NLLB-MD, a dataset in multiple different domains to evaluate generalizable translation capability. These resources enable the evaluation and creation of models for languages that previously had marginal support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">FLORES-200</head><p>A major area of focus in machine translation research has been on the development of highquality evaluation datasets, or benchmarks that can be reliably used to assess progress in the field. The ability to evaluate allows us to compare different approaches and understand what requires further research and development. The creation of benchmark datasets at the yearly Workshop on Machine Translation <ref type="bibr" target="#b10">(Akhbardeh et al., 2021)</ref> led to rapid progress on translation directions such as English to German and English to French. We are also seeing recent work on creating low-resource translation datasets as illustrated by the SALT <ref type="bibr" target="#b9">(Akera et al., 2022;</ref> and the AmericasNLI <ref type="bibr">(Ebrahimi et al., 2022)</ref> datasets. Beyond the field of translation, evaluation benchmarks such as SQuAD <ref type="bibr">(Rajpurkar Figure 4</ref>: FLORES-200 Translation Workflow: We created a complex, multi-step process to ensure quality. First, professional translators and reviewers aligned on language standards. Next, translators translated the full set of Flores-200 sentences, followed by automated checks. Subsequently, the group of independent reviewers reviewed the quality, and based on their assessment, we sent some translations out for post-editing. If the quality assessment indicated that the quality is above 90 percent, the language is considered ready for inclusion in <ref type="bibr">Flores-200. et al., 2016)</ref>, GLUE <ref type="bibr" target="#b162">(Wang et al., 2018)</ref>, and even the Penn Treebank language modeling benchmark <ref type="bibr" target="#b110">(Mikolov and Zweig, 2012)</ref> propelled significant research advances. The creation of Flores-200 seeks to double the existing language coverage of Flores-101. This raises significant challenges due to the even more low-resource nature of the languages we have introduced in this effort. More specifically, these languages may require ever increasingly specialized professional translators, have less standardization, and the verifying process to ensure translation quality becomes more complex. Below, following a brief summary of the characteristics of Flores-101, we describe in detail how we overcome these new challenges in the creation of Flores-200, paying particular attention to the adapted protocol and quality assurance mechanisms. Then, we present an analysis on the overall quality of our evaluation benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Benchmark Creation for Low-Resource Languages</head><p>Preliminaries. As a significant extension of Flores-101, Flores-200 consists of 3001 sentences sampled from English-language Wikimedia projects for 204 total languages. Approximately one third of sentences are collected from each of these sources: Wikinews, Wikijunior, and Wikivoyage. The content is professionally translated into 200+ languages to create Flores-200. As we translate the same set of content into all languages, Flores-200 is a many-to-many multilingual benchmark. We refer the reader to <ref type="bibr">Goyal et al. (2022)</ref> for greater detail.</p><p>Finding Professional Translators and Translation Reviewers. Flores-200 is created with professional human translators who translate the FLORES source dataset into the target languages and a separate group of independent translation reviewers who perform quality assessments of the human translations and provide translation feedback to the translators. Both translators and reviewers undergo vetting processes, handled by language service providers (LSPs). Translators are required to be native speakers and educated in the target language and have a high level fluency in English. Translators are required to have at least two to three years of translation experience in the relevant language pair if they have an academic degree in translation or linguistics and three to five years of translation experience if they do not have any relevant academic qualification. Translators also undergo a translation test every 18 months to assess their translation quality. Further, Flores-200 reviewers are also required to be native speakers of the target language. Reviewers typically have a translation degree, at least five years of experience working as a translator, translation review experience, and where possible are accredited by a relevant translation board.</p><p>We note that these are stringent standards, and extensions of Flores-200 to even more low-resource languages in the future may be difficult. Already for many languages, finding a reviewer that meets the criteria above is very challenging. In these cases, we modified the qualification process to accept applications from reviewers with more general language degrees such as Linguistics or African Language Studies, or no degree provided they have had extensive commercial translation experience (e.g. &gt;10 years). To cover even more low-resource languages in the future, we believe that there are several ways to work with experienced and skilled translators while maintaining high quality standards. For instance, one of such solutions is to translate from non-English source languages. We pilot this process and describe it in greater detail in Section 4.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flores-200 Translation Workflow.</head><p>The Flores-200 data creation workflow incorporates the original Flores-101 processes along with a few new initial phases as shared in detail below.</p><p>? Alignment Phase: We have introduced an initial alignment phase to the workflow for the translators and reviewers before translating Flores-200. There are several steps incorporated in alignment between the translation and quality assurance agenciesaligning on resourcing and target regions, linguistic topics between the translators and reviewers per language through a new alignment template, and query logs between the linguists on both sides. The alignment template helped linguists identify approaches on the language script, standardization, spelling, borrowed terms, neologisms, informative content style, and resources for glossaries, and sample content in the target language. This has been especially helpful for languages with less established standards for translation.</p><p>? Translation Phase: Translation then begins with an initial translation phase, where the same 200 sentences are translated by all participating translators for each language. The initial translation data contains an even split across the three sources -Wikinews, Wikijunior, and Wikivoyage, with the segments corresponding to the same articles for context and continuity. The initial translations are then sent to the QA LSP team for review. The main focus of the initial translation and QA steps is to understand and align on the translation approach between the translators and reviewers. The report contains sentence-level feedback (identified error category, error severity level and comments where possible) and high-level feedback on locale expectations, use of specified script, use of borrowings and/or neologisms, named entities, and overall style and register.</p><p>? Iteration: Translation LSP teams may respond to the initial QA reports with arbitration. Adjustments are then made to all alignment materials where needed and the translation approach is updated and re-aligned on. The full translation of all 3000 sentences then begins (see <ref type="bibr">Goyal et al. (2022)</ref> for details).</p><p>? Completion: When full translation is completed, the QA LSP team performs a final QA review and assesses a 20% sample data. Optional arbitration, rework and QA spot checks may follow if the final quality score of the translation dataset is below 90%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Benchmark Creation for Non-English Directions</head><p>The standard Flores-200 workflow focuses on translation only from English. While this standardizes the process for all languages, it has clear limitations. For example, there are many qualified translators who may not speak English, but are able to translate between several non-English languages. Further, several languages may be easier to translate from a non-English source. Instead, we focus on adaptation and transliteration and design customized QA workflows to support this.</p><p>Translation of Arabic Languoids. We apply this workflow to create datasets for various variants of Arabic, expanding our language coverage beyond Modern Standard Arabic to regional variants such as Moroccan Arabic. To create Flores-200 for Arabic variants, LSP teams analyzed the linguistic characteristics of each Arabic languoid and how much they differed from Modern Standard Arabic on various linguistic aspects such as vocabulary differences, grammatical and structural differences, influence from other regional languages and informative content style. Based on these analyses, Arabic languoids were either translated directly from English or adapted from the Modern Standard Arabic dataset with the English source provided as context. <ref type="bibr">7</ref> For each languoid that implemented adaptation, LSP teams also created a termlist consisting of terms from Modern Standard Arabic and an equivalent term in the target Arabic languoid to ensure consistent adaptation. Two tiers of quality assessment were created for adaptation from Modern Standard Arabic. One tier encompassed a partial QA review where the reviewer assessed a 10% sample data and reviewed the termlist. This process was applied to languoids that were assessed to have mainly vocabulary differences, some structural differences and some influence from other regional languages. Another tier required the reviewer to only assess the termlist as the languoids mainly differed from Modern Standard Arabic minimally and on vocabulary usage. The 90% quality threshold is applied as usual.</p><p>Script Transliteration. There were four languages (ace_Arab, bjn_Arab, min_Arab, taq_Tfng) that were transliterated from their Latin script counterparts. The translation LSP performs transliteration into the appropriate scripts. The QA LSP reviews a 20% sample of the transliterated text with the English source and Latin script data provided for context. In the QA report, transliteration errors are flagged only by severity level; there are no error categories for transliteration errors. Two or more errors found in one segment would be flagged with a major severity level. Anything fewer would be flagged as minor. The quality threshold for transliteration is 95%. 7. acm_Arab, acq_Arab, aeb_Arab, and ars_Arab were adapted.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Flores-200 at a glance</head><p>Overview. Flores-200 consists of translations from 842 distinct web articles, totaling 3001 sentences. These sentences are divided into three splits: dev, devtest, and test. We release the full text of the dev and devtest splits, and keep the test set hidden through an evaluation server. On average, sentences are approximately 21 words long. We summarize this information in <ref type="table" target="#tab_6">Table 2</ref> (left).</p><p>Quality. To consider a language ready for inclusion in Flores-200 requires a final human quality assurance evaluation. We display the quality scores of our languages in <ref type="figure" target="#fig_2">Figure 5</ref> with several example languages labeled. Mistranslation and unnatural translation errors were still the most common errors found while assessing the quality of the human translations. These were mainly due to influences from other non-English languages that may be prominently used in the target communities, leading to excessive borrowings of vocabulary and grammar, literal translations due to infrequent usage of the target language in a formal, informative content style and the lower levels of standardization. There has also been an increasing trend in spelling inconsistencies in the human translations due to lower levels of standardization leading to inconsistent or even subjective or preferential approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Challenges in Creating Datasets for Very Low-Resource Languages.</head><p>Overall, compared to Flores-101, our new translation workflow substantially streamlines the translation effort. For example, the number of languages requiring re-translation (see <ref type="table" target="#tab_6">Table 2</ref>, right) is only 10, down from 45 in Flores-101. However, despite these improvements, we continued to experience similar challenges as in Flores-101 -but at even greater scale due to the increasing low-resource nature of the languages. For example, low-resource languages are not as often worked with in the localization or translation fields. As a result, there are lower levels of industry-wide standardization, leading to a more challenging path to navigate <ref type="bibr">(Skadi?? et al., 2014a)</ref>. This led to longer turnaround times, and often required finding new translators and reviewers several times. These challenges were especially felt during some of the more difficult languages such as Sicilian and Buginese, which have taken significantly longer periods of time to complete (287 days).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">NLLB Seed Dataset</head><p>Machine learning is notoriously data-hungry, leading to many areas of research aimed at reducing the amount of required supervision. Recent advances in zero-shot learning <ref type="bibr">Gu et al., 2019;</ref><ref type="bibr">Johnson et al., 2017;</ref><ref type="bibr" target="#b174">Zhang et al., 2020)</ref> and self-supervised learning <ref type="bibr">(Bapna et al., 2022;</ref>, for instance, seek to reduce this reliance. However, generation tasks such as translation likely are unable to reach the desired quality levels without some starter data. For instance, it is challenging to produce a good translation without seeing a minimum number of sentences in a new language. Similarly, it may be difficult to classify which language a sentence is in without seeing reliable examples of text in different languages. To this end, we create NLLB-Seed, a set of professionally-translated sentences in the Wikipedia domain. NLLB-Seed consists of around six thousand sentences in 39 languages. <ref type="bibr">8</ref> Such a dataset has numerous potential uses. Critically, NLLB-Seed contains data that is definitely in the specified language, as it is fully professionally translated by humans. NLLB-Seed's target-side data in various languages can be utilized for language identification models that classify which language an arbitrary piece of input text is in. The dataset can also be used for its aligned bitext, for example to train translation models. Another option is to utilize NLLB-Seed to do domain finetuning, such as adapting general-purpose translation models to the Wikipedia domain. contiguous sentences, ensuring no more than one triplet per article was used (similar to .</p><p>We note that like Flores-200, NLLB-Seed's source data is English-centric and sampled from English Wikipedia. 10 This has an important effect: the content reflects what Wikipedia editors find is relevant for English Wikipedia, and likely does not cover diverse content from different cultures. Further, the target text in NLLB-Seed is ultimately translated by humans, and thus potentially contains effects of translationese (often defined as awkward, unnatural, or overly literal translations) <ref type="bibr" target="#b161">(Volansky et al., 2015)</ref>.</p><p>Translation Workflow. Script, specification, spelling and translation approaches were first established and aligned on from Flores-200. Translators referenced these linguistic alignments while working on Seed Data Translations. The datasets were translated directly from English for 39 languages while two Arabic script languages (Acehnese and Banjar) and Tamasheq in Tifinagh script were transliterated from their respective Latin script datasets that were first translated from English. 11 Following the translation or transliteration phase was a linguistic quality assessment phase in which the completed datasets were checked against the linguistic alignments from FLORES along with automatic quality control checks. The datasets were then finalized and completed.</p><p>We note that NLLB-Seed has a key distinction compared to evaluation benchmarks such as Flores-200. Critically, NLLB-Seed is meant to be used for training rather than model evaluation. Due to this difference, NLLB-Seed does not go through the human quality assurance process present in Flores-200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">NLLB Multi-Domain Dataset</head><p>Avoiding overfitting and achieving strong out-of-domain performance remains a major challenge in neural machine translation <ref type="bibr">(Koehn and Knowles, 2017)</ref>. While both Flores-200 and NLLB-Seed cover a large number of topics, we want to ensure that models perform well on text coming from different domains. Additionally, since potential users might be interested in tuning general translation models for specific applications, we want to investigate how effectively our system can be fine-tuned on a dataset covering a new domain. More specifically, we want to answer the following two questions: (1) How well do models generalize to non-Wikimedia domains? (2) Does fine-tuning on high quality in-domain parallel text lead to good performance? In order to investigate these questions, we create the NLLB-MD parallel dataset, covering six directions and made up of 3,000 professionally-translated sentences in each of four different domains.</p><p>Language Selection. NLLB-MD covers the following six languages: Central Aymara (ayr_Latn), Bhojpuri (bho_Deva), Dyula (dyu_Latn), Friulian (fur_Latn), <ref type="bibr">Russian (rus_Cyrl)</ref> and <ref type="bibr">Wolof (wol_Latn)</ref>. Along with five low-resource languages, we also chose to include one high-resource language to enable comparisons with other models and datasets. We chose low-resource languages related to other high-resource ones (e.g., fur_Latn is related to ita_Latn), so as to enable future studies investigating language transfer. 10. Note: There is no overlap between the sentences in Flores-200 and NLLB-Seed 11. We had a specific process for Ligurian: half the data for Ligurian were first translated from English to Italian, then translated from Italian to Ligurian while the other half was translated directly from English. As we are lucky to have Ligurian native speaker, we developed this process to improve quality. Domain Selection. We collected 3,000 English sentences in each of four different domains, and sent them to professional translators to be translated into each of NLLB-MD's six target languages. The translation workflow used is analogous to the one followed for NLLB-Seed. The domains included are:</p><p>? News: We translate the English side of the WMT21 English-German development set, containing a sample of newspapers from 2020 <ref type="bibr" target="#b10">(Akhbardeh et al., 2021)</ref>.</p><p>? Scripted formal speech: We translate text extracted from a series of scripted English-language talks covering a variety of topics.</p><p>? Unscripted informal speech: We extract 3,000 utterances from the multi-session chat dataset of <ref type="bibr" target="#b177">Xu et al. (2022)</ref>, which contains on average 23 words per turn.</p><p>? Health: We translated one World Health Organisation report <ref type="bibr" target="#b53">(Donaldson and Rutter, 2017)</ref> and combined it with sentences translated from the English portion of the TAUS Corona Crisis Report. 12</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Conclusion</head><p>To summarize, Flores-200, which enables reliable evaluation of over 200 languages, is critical for ensuring the quality of the results our systems generate. NLLB-Seed plays an important role for training both sentence encoders (see Section 5) and translation models (see Section 6.5). Finally, we utilize NLLB-MD to measure the generalizability of our translation models across multiple domains (see Section 8.3). Now that we have described the creation of three human-translated datasets and their uses, we visit how we acquired training data for our effort in the subsequent section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Automatically Creating Translation Training Data for Hundreds of Languages</head><p>The current techniques used for training translation models are difficult to extend to lowresource settings -that is, when data for a language is limited in both aligned textual data (bitext, or pairs of translated sentences) and single language data (monolingual, or data in one language only). In fact, many low-resource languages are supported only through small targeted bitext datasets such as the Christian <ref type="bibr">Bible (McCarthy et al., 2020)</ref>, which are extremely limited in domain diversity. In this section, we detail how we built a large scale dataset that covers hundreds of languages and discuss the challenges we faced with noisy data at web-scale. For context, publicly available bitext data is often scarce <ref type="bibr">(Gowda et al., 2021)</ref>. Our approach centers around extending existing datasets by collecting non-aligned monolingual data and using large-scale data mining <ref type="bibr">(Schwenk et al., 2021b)</ref> to identify sentences that have a high probability of being translations of each other in different languages. To enable this for hundreds of languages, we first develop language identification systems (LID, Section 5.1) that label which language a given piece of text is written in. Subsequently, we curate available monolingual data, apply sentence splitting and LID along with various filtering  <ref type="figure">Figure 6</ref>: Automatic Dataset Creation Contributions of No Language Left Behind: As highlighted, we create language identification and a monolingual data cleaning process, then describe the training of LASER3 to produce large-scale mined bitext for hundreds of languages. mechanisms (Section 5.2), and then move ahead with mining aligned pairs <ref type="bibr">(Section 5.3</ref>). An overview of this process is presented in <ref type="figure" target="#fig_3">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Language Identification</head><p>Language identification (LID) is the task of predicting the primary language for a span of texts. It is widely used in commercial applications (such as the detect language feature embedded in some web browsers) and is of particular importance in natural language processing research. The rise of large-scale pretraining, particularly the increasing focus on multilingual models, is strongly dependent on the existence and identification of monolingual data at scale. Advances in cross-lingual representation learning <ref type="bibr" target="#b40">(Conneau and Lample, 2019;</ref><ref type="bibr" target="#b165">Wang et al., 2020b)</ref> such as large-scale bitext mining <ref type="bibr" target="#b34">(Ba??n et al., 2020;</ref><ref type="bibr" target="#b132">Ramesh et al., 2022;</ref><ref type="bibr">Schwenk et al., 2021b)</ref>, unsupervised machine translation <ref type="bibr">Yang et al., 2018)</ref> and back-translation at scale <ref type="bibr">(Edunov et al., 2018)</ref> require large quantities of clean monolingual data. These disparate approaches, including our focus on large-scale data mining of aligned sentences, involve taking large quantities of input text often drawn from web corpora such as CommonCrawl 13 and labeling them with corresponding languages.</p><p>There are a few well-known challenges associated with large-scale and accurate language identification using web data <ref type="bibr" target="#b142">(Caswell et al., 2020)</ref>: (1) Domain mismatch could occur due to the scarcity of text reliably labeled by language. For example, the Christian Bible has been translated into a wide array of languages. However, an LID system trained on this corpus would not reliably classify sentences from non-Biblical domains. Properly extending training data is not trivial: while the web contains data in thousands of languages <ref type="bibr" target="#b125">(Prasad et al., 2018;</ref><ref type="bibr">Scannell, 2007)</ref>, most of it is unlabeled. Filling in this gap is Wikipedia, which is frequently used for training language identification <ref type="bibr" target="#b152">(Thoma, 2018</ref>) on a broader scale beyond the Christian Bible (although such relatively clean formal text is not representative of the web at large); (2) Severe class imbalance could exist because many of the low-resource languages of interest to us have low presence on the web. For classifiers to work, they must have an extremely low false positive rate. Otherwise, low-resource languages are prone to misidentification; (3) Efficiency to run over large web collections remains low. Even though classification is massively parallelizable, running it on all texts makes speed critical. In this section, we describe our approach to language identification and how we strike a necessary balance between predictive performance and scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Related Work</head><p>There is extensive literature dedicated to the development of LID systems. <ref type="bibr">Jauhiainen et al. (2019)</ref> give a recent and comprehensive overview of the features and algorithms used in the literature. While LID could be seen as a solved problem in some domains <ref type="bibr" target="#b108">(McNamee, 2005)</ref>, it remains an open challenge for web data <ref type="bibr">(Abadji et al., 2022;</ref><ref type="bibr" target="#b142">Caswell et al., 2020;</ref><ref type="bibr">Zampieri et al., 2015b)</ref>. Specifically, issues coalesce around (1) scaling successful approaches to more languages <ref type="bibr">(Jauhiainen et al., 2017)</ref>; (2) incidents where there is significant domain mismatch <ref type="bibr" target="#b170">(Widdows and Brew, 2021)</ref> in the cases of short tweets or multiple languages <ref type="bibr" target="#b56">(Duvenhage, 2019)</ref>; and (3) distinguishing similar languages <ref type="bibr">(Goutte et al., 2016)</ref>.</p><p>Scaling LID to More Languages. Devoted attention to advance LID techniques have led to a noticeable increase in both language coverage and accuracy over time. CLD3 14 and fasttext <ref type="bibr">(Grave et al., 2018)</ref> are two readily available models offering high detection performance for 107 and 187 languages respectively. By using numerous public datasets, Dunn 14. https://github.com/google/cld3 (2020) and Brown (2014) report even higher coverage, supporting 464 and 1366 languages respectively. That said, developments around low-resource languages face slow advancement due to the emphasis on religious texts and constraints brought about by software localization. <ref type="bibr" target="#b142">Caswell et al. (2020)</ref> scale up to 1,629 languages using wordlists and self-supervision to bootstrap training data found on the web. These approaches using found data suffer from domain imbalance: because the available text domains vary by language, the classifier conflates domain with language. In contrast, we curate Flores-200 to use as development set, so that our LID system performance is tuned over a uniform domain mix. One could of course use the Christian Bible as a uniform domain. However, we believe Flores-200 is closer to web content. Domain Mismatch. Because the web covers a very broad set of domains and reliably labeled text is scarce, there is almost always a domain mismatch between training data and the web text being classified. <ref type="bibr" target="#b170">Widdows and Brew (2021)</ref> describe a new feature based on the rank of words within frequency tables that enhances robustness of LID systems to domain mismatches. They train their classifier on Wikipedia and report results on a Twitter test set, unfortunately covering only 22 languages. Short text is tackled in <ref type="bibr" target="#b56">Duvenhage (2019)</ref> for South African Languages with a stacked classifier. Neural network-based strategies are also derived in <ref type="bibr" target="#b17">Ansari et al. (2021)</ref>; <ref type="bibr">Shekhar et al. (2020)</ref> to handle text written in a mix of English and Indian languages (code mixing). <ref type="bibr" target="#b142">Caswell et al. (2020)</ref> thoroughly analyze and classify failure modes of language identification on web corpora. They suggest using a series of filters along with a new unsupervised learning approach to drastically improve precision at limited cost on recall. These filters are costly to devise and tune for all languages however. Some of them were successfully put into practice in <ref type="bibr">Abadji et al. (2022)</ref> to release a cleaner version of the OSCAR dataset. Our approach combines a data-driven fasttext <ref type="bibr">(Grave et al., 2018)</ref> model trained on Flores-200 with a small set of handwritten rules to address human feedback on classification errors.</p><p>Handling Similar Languages. Distinguishing between similar languages has been an active research topic, for instance, with the shared task on Discriminating between Similar Languages within the VarDial workshop <ref type="bibr">(Goutte et al., 2016)</ref>. Several common machine learning algorithms along with standard neural networks are compared in <ref type="bibr" target="#b71">Haas and Derczynski (2021)</ref> for Nordic languages. <ref type="bibr" target="#b56">Duvenhage (2019)</ref>; <ref type="bibr" target="#b68">Goutte et al. (2014)</ref>; <ref type="bibr">Zampieri et al. (2015a)</ref> explore various hierarchical approaches that first predict the language group of input text, then apply a more specialized classifier to distinguish between languages within that group. In this work, we collaborate in close partnership with linguists to understand which languages can be easily confused and analyze the model performance while employing a flat classification strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Models</head><p>We utilize fasttext to train language identification models <ref type="bibr">(Bojanowski et al., 2017;</ref><ref type="bibr">Joulin et al., 2017)</ref>. fasttext is widely used for text classification tasks due to its simplicity and speed, while achieving good quality. We embed character-level n-grams from the input text, then leverage a multi-class linear classifier on top. The lightweight nature of fasttext enables our LID models to handle web-scale data. Additionally, a linear model has the benefit of being easily explainable, allowing us to trace any classification error back to its root cause. This is instrumental in addressing common pitfalls that arise when detecting language on web corpora <ref type="bibr" target="#b142">(Caswell et al., 2020)</ref>.</p><p>Classifier Design. We experimented with two different designs. (1) A combination of multiple binary classifiers where the final decision is obtained by selecting the language having the highest score after a threshold is applied. We apply threshold optimization so that when the confidence of a classifier is low, the corresponding language is not considered for the final decision. If none of the classifiers surpass its threshold, the sentence is filtered out.</p><p>(2) A multiclass classifier using softmax over all possible languages. In this case, the threshold optimization is done after the softmax.</p><p>Our experiments motivated us to focus on the second approach, which offers several advantages. First, changing the threshold for one language does not impact the performance of the other , while this is not true in the first setting. Second, we found that this approach generalizes better to out of domain data which is our primary use case (Wikipedia ? Web data). Finally, a single classifier has the added benefit of being computationally simpler, thus streamlining the language identification process.</p><p>Training Data and Handling Massive Class Imbalance. We use publicly available datasets to train our LID system, partially covering our of interest. We supplement these with NLLB-Seed (see Section 4.2) for any missing language. However, the amount of data available for each language is far from uniform, and massive class imbalance in the raw training data exists <ref type="bibr" target="#b142">(Caswell et al., 2020;</ref><ref type="bibr" target="#b55">Dunn, 2020)</ref>. For example, English alone represents 10.1% of our training data, while Minangkabau (Latin script) represents only 0.06%. Following <ref type="bibr" target="#b18">Arivazhagan et al. (2019)</ref>, we experimented with multiple settings of temperature upsampling for under represented , where sentences from a language l representing p l percent of the dataset are sampled proportionally to p 1 T l . Optimal performance was obtained at 1 T = 0.3.</p><p>Training Parameters. Our best model was trained with softmax loss over two epochs with a learning rate of 0.8 and embeddings with 256 dimensions. We discarded words with less than a thousand occurrences after upsampling and picked a minimum and maximum character n-gram length of two and five respectively, which were assigned a slot in buckets of size 1,000,000. All hyperparameters were tuned on Flores-200 dev.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Improving LID with Linguistic Analysis</head><p>Language identification is a challenging task where numerous failure modes exist, often exacerbated by the gap between the clean data that LID models are trained on and the noisy data that LID models are applied to. LID models that are trained in a supervised manner on fluently written sentences may have difficulty identifying grammatically incorrect and incomplete strings extracted from the web. Furthermore, models can easily learn spurious correlations that are not meaningful for the task itself. In light of these challenges, we collaborated closely with a team of linguists throughout different stages of LID development to identify proper areas of focus, mitigate issues, and explore solutions.</p><p>LID Inspection Interface. We leveraged the linearity of fasttext to build an easyto-use interface for linguists to peek into its inner workings. The tool enabled linguists to <ref type="figure">Figure 8</ref>: LID Inspection Interface, used on an example sentence from the English Wikipedia containing a short passage in French. The top 2 labels with highest probability are displayed, along with their score. N-grams that contributed the most (either positively or negatively) to the predictions are highlighted (in green and red respectively).</p><p>analyze model errors and discern model patterns. As illustrated in <ref type="figure">Figure 8</ref>, we visualize how much each n-gram contributed to the final prediction. In one of the applications, the tool led linguists to notice the similarity in phonotactics between Standard Malay and Indonesian, which are one of the most frequently confused language pairs, and to find out through linguistic research that in spite of obvious differences, a certain degree of mutual intelligibility exists between the two.</p><p>Filtering Training Data. To mitigate the learning of spurious correlations due to noisy training samples while modeling hundreds of languages, we worked in collaboration with linguists to develop several filters, illustrated in <ref type="table" target="#tab_9">Table 3</ref> and described below. All are subsequently applied on our raw training dataset.</p><p>? Character Distribution Filtering: The public datasets we used for training were mostly built from webpages. Through investigation by linguists, numerous occurrences of mislabeled sentences were found, likely caused by short passages in a different language within a page, such as Indonesian sites that display a collection of Javanese poems. We also noticed random creative use of unexpected scripts, typically used for decoration or emphasis as pointed out in <ref type="bibr" target="#b142">Caswell et al. (2020)</ref>. <ref type="table" target="#tab_9">Table 3</ref> gives a few examples. To address this problem, we searched for distribution shifts in characters, either by computing character histograms or by looking at the language's expected script unicode range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Character Histograms:</head><p>We computed the character distributions of each language on our development set and defined an arbitrary accepted character set for each of them by considering all characters falling within the first 95 th percentile. We consequently filtered out any sentence from our training set that was composed of less than 80% of such accepted characters.</p><p>Script Detection: For languages whose script spans thousands of characters, the character histogram method mentioned above was not as effective since the character distribution trends were less prominent. As an alternative, linguists provided Unicode ranges to define accepted character sets. Any sentence containing less than 50% of characters from that set was eventually discarded. For example, the sentences shown in <ref type="table" target="#tab_9">Table 3</ref> for Japanese and Chinese do not contain the right Jpan and Hans scripts.  ? English-specific Filtering: Linguists also pointed out that many mislabeled training samples were actually plain English sentences. This can be explained by the massive prevalence of English on the web, even on pages primarily written in other languages. We built a simple, dedicated binary fasttext classifier to filter these samples out of our training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Results</head><p>This section presents a comparison of our approach to existing publicly available models on both Flores-200 and annotated noisy web data, followed by an error analysis.</p><p>Evaluation on the Flores-200 Benchmark. We analyze the performance of our LID models on the Flores-200 dataset from Section 4 and compare to other open-source models. We utilize Flores-200 for evaluation as the target-side text is human-verified as being in the right language. Utilizing standard public datasets for evaluation is less reliable given they often contain untrustworthy language labels and are quite noisy <ref type="bibr">(Kreutzer et al., 2022)</ref>.</p><p>We compare our LID model with three publicly available models: CLD3, 15 LangId 16 and LangDetect. 17 <ref type="table" target="#tab_11">Table 4</ref> reports performance of our final LID model on the set of various language intersections covered by all four models. Micro F1 scores and False Positive Rates across all languages found in Flores-200 are displayed in <ref type="table" target="#tab_12">Table 5</ref>. Given the different scopes of languages supported, we report on 3 cascading intersections with Flores-200: (1) the 51 languages also supported by LangId, LangDetect and CLD3, (2) the 78 languages also supported by LangId and CLD3 and (3) the 95 languages also supported by CLD3. We report metrics of all models across all intersections to reflect the impact of false positives on unseen languages.</p><p>Our model is capable of handling the 200 languages of Flores-200 (compared to the 107 languages supported by CLD3) while achieving significantly higher performance than all three of LangId, LangDetect and CLD3. Furthermore, the gain in F1 score is accompanied by a noticeable improvement in False Positive Rate, suggesting a much stronger fit for extracting low-resource languages from web corpora <ref type="bibr" target="#b142">(Caswell et al., 2020)</ref>.   Indeed, various sources of noise such as language mixing, creative use of various scripts, and leetspeak are widespread online. Extracting sentences from internet pages is also prone to unexpected artifacts introduced after parsing. There is no readily available evaluation set from the web domain on which to properly assess and tune performance, let alone iterate on design choices when modeling. This motivated us to audit the performance of our system with human annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Evaluation on</head><p>To this end, we select 74 low-resource languages on which our preliminary LID model yield low F1 scores. After a first run of language identification on web data, we randomly selected several thousand sentences across various languages for which prediction scores fell between 50% and 90%. That hard threshold was chosen upon manual inspection, noticing that many classification errors were found within that range. Human annotators were were tasked with inspecting our random sentences and assessing whether each was indeed in the predicted language.</p><p>Based on these annotations, we built a challenge set for language identification to benchmark our final LID model. As shown on <ref type="table" target="#tab_14">Table 6</ref>, we achieve lower performance than on the Flores-200 dataset, hinting at a non-negligible domain mismatch. We also compare performance against CLD3. As suggested in <ref type="bibr" target="#b142">Caswell et al. (2020)</ref>, we report False Positive Rates (FPR) on top of F1 scores, to get a better picture of how well our model would fare   No one was inside the apartment Na obiara nni odan no mu There's a tiny cave near the top that must be passed through Obodan ketewa bi bEn soro ho a EsE sE wofa mu For a few pennies some children will tell you the story SE woma mmofra bi sika ketewa bi a, wobEka anansesEm yi akyerE wo on web corpora with extreme class imbalance. Despite the extended language coverage, our approach yields both a higher F1 score at a lower FPR, suggesting a good fit for the downstream pipeline of Section 5.2. We share performance of our model and CLD3 on all the selected languages in <ref type="table" target="#tab_12">Table 50</ref> in the appendix. It should be noted that since the sentences to annotate were chosen based on a previous model, this challenge set is biased by that underlying intermediate model.</p><p>Analysis on Challenging Language Pairs. <ref type="figure" target="#fig_4">Figure 9</ref> brings to light a small group of confusable language pairs found to be the most difficult for our LID system: Akan/Twi, Dyula/Bambara, Faroese/Icelandic, Western Persian/Dari, and Bosnian/Croatian. We worked closely with linguists to analyze them. Upon inspection, we found that these language pairs correspond to highly similar languages, displaying major vocabulary and grammar overlap. For example, Asante Twi (aka_Latn) and Akuapem Twi (twi_Latn) are two mutually intelligible languoids of the Akan language continuum, which share common words, phrases, or even identical sentence translations. Examples can be found in <ref type="table" target="#tab_15">Table 7</ref>. This suggests that from a linguistic point of view, the LID confusion found in these similar languages is to be expected and is not a symptom of a deeper modeling issue. In practice, this means prediction performance might be underestimated for some languages and calls for collecting and accepting multiple language labels in future work.</p><p>Impact of Sentence Length. We noticed that predictions tend to be more robust for long sentences. <ref type="figure" target="#fig_0">Figure 10</ref> gives an overview of the difference in performance as a function of input length. This is consistent with an observation by <ref type="bibr">Jauhiainen et al. (2017)</ref> and could be further investigated. A potential mitigation strategy would be to tune our models on a more balanced development set with respect to length. Shorter test sentences could be synthetically created from our current development samples. In our current approach, we mitigate this issue by applying length filters in the downstream monolingual pipeline described in Section 5.2. We synthetically create test samples of a specific length without cutting words, except for languages with continuous scripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Gathering and Cleaning Monolingual Data at Scale</head><p>Monolingual data is a valuable resource which can be used for a variety of downstream tasks such as bitext mining, backtranslation, and language model training. Additionally, data quality can have a large impact on the performance of such tasks. In order to maximize the potential benefits of leveraging these data sources, we aim to produce high quality and clean monolingual data. As discussed earlier in this section, such data can be scarce, particularly in the low-resource setting. We therefore decided to extend the work done in CCNet, <ref type="bibr">CCMatrix (Schwenk et al., 2021b;</ref>, and others like OSCAR <ref type="bibr" target="#b117">(Ortiz Su?rez et al., 2019)</ref>. In this section, we describe our end-to-end process for both curating and cleaning monolingual data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Description of our Monolingual Pipeline</head><p>Data Sources. We begin with web data as our starting point, provided by CommonCrawl (CC) <ref type="bibr">18</ref> and ParaCrawl <ref type="bibr" target="#b34">(Ba??n et al., 2020)</ref>. This data has been preprocessed to remove all markup and (approximately) normalize encoding to UTF-8. HTML stripping converts block tags to newlines while inline tags are removed. The resulting lines can contain many sentences or simply a short snippet of text; we refer to them as "paragraphs".</p><p>Applying Language Identification. To convert the raw web text in paragraph form to sentences, we apply language identification in a hierarchical fashion. First, we apply LID to each web paragraph. Subsequently, we use the predicted language to choose a sentence splitter for the language. <ref type="bibr">19</ref> The raw paragraphs sometimes contain a mix of different languages or might include code switching. To avoid having a mix of languages, once we have split the documents in sentences, we re-run LID to identify the language of each sentence. If the sentence-level LID does not match the paragraph-level LID, we discard the sentence to be sure we keep high-confidence sentences in the targeted language. We also discard sentences if they do not use the expected script for the target language.</p><p>Note that many sentences are extremely noisy. In particular, they often contain long URLs or lists of hashtags. These confuse the LID and script identification process as they are in Latin script and not always in the same language as the original sentence. To identify the actual language of the sentence, we truncate the URLs and hashtags before running the language identification.</p><p>Given the domain mismatch (see Section 5.1), our development set could not be utilized to tune the detection thresholds of our fasttext classifier to a desired performance level. Instead, we relied on the distribution of model scores on the monolingual data of the ParaCrawl dataset <ref type="bibr" target="#b34">(Ba??n et al., 2020)</ref> across all predicted languages. We chose that dataset based on the assumption that its language distribution would realistically match that of CommonCrawl, despite the inevitable bias induced by the LID model used in the creation of ParaCrawl itself. The vast majority of languages fell into one of three score distribution patterns, as illustrated in <ref type="figure" target="#fig_0">Figure 11</ref> with Kimbundu, Igbo and Yoruba.</p><p>1. Left-skewed distribution, where scores rarely go below the 0.5 mark 2. Extremely left-skewed, where almost all scores landed above 0.9 3. Right-skewed, where scores very rarely go beyond the 0.5 mark This analysis motivated our choice of 0.5 as a default threshold in the first two cases, except for high-resource languages, where we could afford a more stringent value of 0.9 and still collect enough monolingual data in our downstream pipeline depicted in Section 5.2. In the last case, we picked values corresponding to the peak of each distribution (ranging from 0.2 to 0.4), in the hope to collect a sizable amount of data from our pipeline.</p><p>Heuristics for Data Cleaning. We subsequently apply a few heuristics to remove sentences that do not match reasonable quality criteria: minimum and maximum length, space/punctuation/number/emoji ratios, and maximum number of repeated characters. For example, if a sentence contains over 20% punctuation, it likely is not a well-formed sentence. As our model encoders (see Section 5.3) were not trained on substantial content with emojis, we prefer to strip emojis from all text to avoid losing sentences that could match if they did not have the special characters. Some sentences were dominated by hashtags and URLs (e.g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Reason  tweet-like sentences). In these cases, we remove these parts of the sentences and apply the heuristics on the truncated sentence instead.</p><p>We note that these ratios should differ between languages, and are not universally applicable. In certain languages, concepts may take many more words to convey, meaning that setting length-related thresholds is problematic. Similarly, other languages may utilize more punctuation or have shorter words. Thus, we do not set extremely stringent filters, and we examine the amount of text filtered across all 200 languages for each of the filters. <ref type="table" target="#tab_17">Table 8</ref> provides some typical examples of filtered sentences.</p><p>Deduplication. The margin-based criterion of our mining approach requires unique sentences (see Section 5.3), but the sentence splitting and cleaning process might generate a lot of duplicate content, so we run a global deduplication process over all sentences of the same language.</p><p>Language Model Filtering. As we are interested in keeping high quality sentences in our datasets to later train our final multilingual translation models, when possible, we also run a Language Model (LM) filtering. In practice, it is difficult to train high-quality language models for low-resource languages, so we focus on applying language model filters on a few high-resource languages only. Because we do most of the mining where one side of the pair is English, we believe that if we have high-quality content in the English corpus, the mining alignment process will also output high-quality content on the other side of the pair. For English, we use the KenLM (Heafield, 2011) model from CCNet .</p><p>Computational Challenges. We processed around 37.7 Petabytes of data through the whole pipeline. This was a challenge for data management and disk usage. In particular, we had to make hard decisions when filtering high-resource languages to artificially keep only around 30% of data from the most voluminous languages. This threshold was identified as the limit under which the LM score would identify sentences of low quality. Processing was distributed over many machines and several months to be able to get to the final monolingual dataset. We realize that processing such volume of data is not always possible and are open-sourcing much of our results and code to make it easier for everyone to benefit from our effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Monolingual Data at a Glance</head><p>We started with 107.9 billion paragraphs from the web (97.4% high-resource, 2.6% lowresource) and discarded 2.8 billion sentences (85.3% high, 14.7% low) because of LID/Script mismatch or low LID score (see Section 5.1) to produce 43.7 billion monolingual sentences  <ref type="table">Table 9</ref>: Key Statistics for the processing of low-and high-resource monolingual data.</p><p>(90.8% high, 9.2% low) to feed to the mining. 21.5 billion sentences are in English. See <ref type="table">Table 9</ref> for details on data volume. Note that we have such a big drop in number of sentences because we drastically filtered high-resource languages to keep the top 30% of sentences based on LM score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Mining Bitexts for Low-Resource Languages</head><p>Machine translation, like many machine learning applications, is heavily data-driven. Previous works have clearly established that translation quality generally increases with the amount of available high-quality training data <ref type="bibr">(Koehn and Knowles, 2017)</ref>. Existing parallel corpora for low-resource languages are often opportunistically drawn from known collections of multilingual content, such as the Christian Bible or publications of multinational organizations. These are often limited in quantity and domain. In this section, we describe how we automatically create translation training datasets for low-resource languages through bitext mining. We mainly focus on bitexts paired with English, but we are also interested in mining through other language pairs as it was shown that these can improve the overall performance of a multilingual translation system <ref type="bibr" target="#b151">(Fan et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What is Bitext Mining?</head><p>The underlying idea of our bitext mining approach is to first learn a multilingual sentence embedding space and to use a similarity measure in that space to decide whether two sentences are parallel or not. This comparison can be done for all possible pairs in two collections of monolingual texts, termed global mining in <ref type="bibr">Schwenk et al. (2021a)</ref>. Another common alternative approach is referred to as hierarchical or local mining and comprises of first performing a selection of potential document pairs, and then limiting the mining to sentences within each document pair. The European ParaCrawl project is a typical example for this approach <ref type="bibr" target="#b34">(Ba??n et al., 2020)</ref>. Earlier work along these lines handled both web sites with parallel text <ref type="bibr" target="#b139">(Resnik and Smith, 2003)</ref> and comparable data <ref type="bibr" target="#b62">(Fung and Cheung, 2004)</ref>. In this work we follow the approach from WikiMatrix <ref type="bibr">(Schwenk et al., 2021a)</ref> and <ref type="bibr">CCMatrix (Schwenk et al., 2021b)</ref>, which both used global mining. As our mining approach requires a multilingual embedding space, we faced several challenges when scaling this representation to the 200 languages of the No Language Left Behind effort. For example, how do we make sure that all languages are well-learned? And how should we account for large imbalances of available training data? Training a massively multilingual sentence encoder from scratch each time a new set of languages is added would be computationally very expensive. Furthermore, such an approach has the drawback that the learned embedding spaces from each new model are not (naturally) mutually compatible. This can make mining intractable as for each new encoder, the entirety of available monolingual data needs to be re-embedded, and for English alone, this means tens of billions of sentences and large compute resources. In order to overcome these issues, one approach is to train smaller mutually compatible sentence encoders using the teacher-student distillation technique proposed by <ref type="bibr" target="#b135">Reimers and Gurevych (2020)</ref>. Several extensions of this underlying idea were proposed by <ref type="bibr" target="#b83">Heffernan et al. (2022)</ref> that we adopt (see Section 5.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Related work</head><p>Mining Methodology. In order to find aligned texts, early approaches focused on information beyond the text itself. One notable example is the STRAND algorithm which looked for articles with a similar document structure to find translated web pages <ref type="bibr" target="#b138">(Resnik, 1999;</ref><ref type="bibr" target="#b139">Resnik and Smith, 2003)</ref>. Work subsequently used text in the pages as the basis for alignment using techniques such as crosslingual document retrieval <ref type="bibr">(Munteanu and Marcu, 2005;</ref><ref type="bibr" target="#b156">Utiyama and Isahara, 2003)</ref>, bag of words or language models (Buck and Koehn, 2016), Jaccard similarity <ref type="bibr" target="#b24">(Azpeitia et al., 2017</ref><ref type="bibr" target="#b25">(Azpeitia et al., , 2018</ref><ref type="bibr">Azpeitia, 2016), or translation (Abdul-Rauf and</ref><ref type="bibr" target="#b4">Schwenk, 2009;</ref><ref type="bibr">Bouamor and Sajjad, 2018)</ref>. More recent approaches have begun to leverage advancements in representation learning by encoding texts into an embedding space, and then using a distance-based method to determine if a pair of texts in different languages have a similar meaning. Works such as <ref type="bibr">Espa?a-Bonet et al. (2017);</ref><ref type="bibr">Guo et al. (2018)</ref>; <ref type="bibr" target="#b79">Hassan et al. (2018);</ref><ref type="bibr" target="#b98">Yang et al. (2019)</ref> used bilingual embeddings, but this has the limitation of not being able to directly mine across many languages. In order to address this, learning a massively multilingual embedding space allows for any pair of languages to be encoded and mined <ref type="bibr">(Artetxe and Schwenk, 2019a,b;</ref><ref type="bibr">Feng et al., 2020;</ref><ref type="bibr">Kvapil?kov? et al., 2020;</ref><ref type="bibr" target="#b41">Schwenk, 2018)</ref>. When attempting to mine at scale, a few approaches have been applied to large quantities of language pairs. One such example is the ParaCrawl project 20 which mined data for all official EU languages. A similar approach is that of the ccAligned project <ref type="bibr">(El-Kishky et al., 2020)</ref>. <ref type="bibr">More recently, Schwenk et al. (2021b)</ref> presented CCMatrix which successfully mined billions of sentences from the web using the LASER multilingual embedding space <ref type="bibr" target="#b22">(Artetxe and Schwenk, 2019b)</ref>. The Samanantar project focused on providing a large mined corpus for eleven Indian languages <ref type="bibr" target="#b132">(Ramesh et al., 2022)</ref>.</p><p>Multilingual Sentence Representation Learning. There are a wide range of works covering the learning of multilingual representations such as mBERT <ref type="bibr" target="#b50">(Devlin et al., 2019)</ref>, XLM <ref type="bibr" target="#b40">(Conneau and Lample, 2019)</ref>, and XLM-R . However, when applying these approaches to obtain representations at the sentence level, they can often suffer from the lack of an explicit sentence-based criterion during training, resulting in poor performance on tasks such as bitext retrieval . In order to overcome this issue, methods have been explored with an explicit sentence objective such as SentenceBERT (SBERT) <ref type="bibr" target="#b134">(Reimers and Gurevych, 2019)</ref>. SBERT is based on a siamese network, subsequently fine-tuned on NLI data <ref type="bibr">(Bowman et al., 2015)</ref>, and produces sentence embeddings by classifying pairs of sentences. Similar to this siamese network, LaBSE also uses a dual-20. https://paracrawl.eu/ <ref type="figure" target="#fig_0">Figure 12</ref>: Architecture of the LASER3 Teacher-Student Approach. We refer the reader to <ref type="bibr" target="#b83">Heffernan et al. (2022)</ref> for more details.</p><p>encoder approach with BERT. However, pre-training for the BERT encoders is done using both a masked language modelling (MLM) and translation language modelling (TLM) objective . Sentence embeddings are then produced by passing bilingual translation pairs through the dual-encoder setup and then applying an additive margin softmax loss <ref type="bibr" target="#b98">(Yang et al., 2019)</ref>. Another popular multilingual embedding model is LASER <ref type="bibr" target="#b22">(Artetxe and Schwenk, 2019b)</ref>. Unlike the previous BERT-based approaches, there is no pre-training, and sentence embeddings are produced by using an encoder-decoder architecture, and then max-pooling over the encoder outputs.</p><p>When attempting to learn a multilingual embedding space, one of the limitations of many existing approaches is that each time a model is to be expanded to include a new set of languages, the entire model must be retrained from scratch, which is very costly. In order to address this limitation, <ref type="bibr" target="#b165">Wang et al. (2020b)</ref> provide a technique which is able to extend mBERT to low-resource languages by first increasing the size of the existing vocabulary, and then continuing self-supervised training using low-resource monolingual data. Another approach introduced by Reimers and Gurevych (2020) uses a teacher-student approach. In this distillation setup, English-paired bitexts are used to both learn the English embedding space of a monolingual teacher (SBERT), while also using the non-English side to learn a new language. <ref type="bibr" target="#b83">Heffernan et al. (2022)</ref> further built upon this approach by experimenting with different architectures for teacher and student (e.g., BiLSTM and Transformer), and also applying such distilled sentence representations to the task of bitext mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Student-Teacher Mining Approach</head><p>The overall approach focuses on starting with a massively multilingual sentence encoder teacher model and adapting it to several different low-resource student models. This enables us to add low-resource languages without needing to compete with high-resource languages for capacity, to avoid retraining the full model from scratch, and to maintain compatibility of the multilingual embedding spaces for subsequent mining. <ref type="figure" target="#fig_0">Figure 12</ref> summarizes the overall architecture of the teacher-student approach. The teacher, LASER2, is an improved version of the open-source LASER encoder. <ref type="bibr">21</ref> The original training procedure <ref type="bibr" target="#b22">(Artetxe and Schwenk, 2019b)</ref> was changed as follows: the use of SentencePiece tokenization and upsampling of low-resource languages. The architecture of the 5-layer BiLSTM encoder and the max pooling method to obtain sentence embeddings were left unchanged. Training was performed on the same 93 languages with public resources obtained from OPUS <ref type="bibr" target="#b153">(Tiedemann, 2012)</ref>. The reader is referred to <ref type="bibr" target="#b22">Artetxe and Schwenk (2019b)</ref> for details on the original LASER training procedure. Training of the students follows the approach described in greater detail in <ref type="bibr" target="#b83">Heffernan et al. (2022)</ref>:</p><p>? students are specialized for one language or several similar languages;</p><p>? students are randomly initialized since we want to handle low-resource language for which we don't have a pre-trained LM;</p><p>? students may have a dedicated SentencePiece vocabulary different from the teacher, to better accommodate scripts and tokens in the student languages (see Section 5.3.3)</p><p>? students learn to minimize the cosine loss with the teacher, since we also use cosine distance for bitext mining (see <ref type="figure" target="#fig_0">Figure 12</ref>);</p><p>? students can have an MLM loss to leverage student language monolingual data (see <ref type="figure" target="#fig_0">Figure 12</ref> and Section 5.3.3).</p><p>Training Parameters. Our student encoders used a 12-layer transformer, hidden size of 1024, with 4 attention heads, totalling around 250M parameters. All students were trained with available bitexts in their respective language, complemented by two million sentences of English/English and English/Spanish. The motivation is to anchor the students to the English embedding space, make it more robust by including English/Spanish bitexts from CCMatrix, and jointly learn new languages. This technique is particularly useful when only limited amounts of bitexts are available to train the students. Teacher-student training was performed on 16 GPUs, ADAM optimizer, a learning rate of 0.0005, and a batch size of 10,000. We trained student encoders for 148 languages and named these models LASER3.</p><p>Proxy Metric for New Encoders. Mined bitexts will be subsequently utilized to improve translation quality for 200 languages. Consequently, our primary metric is neural machine translation (NMT) quality. However, mining and NMT training are computationally expensive, and it is intractable to systematically perform this evaluation for many different sentence encoder variants. As an evaluation proxy, we use a mining-based multilingual similarity search error rate, referred to here as xsim. As opposed to cosine accuracy which aligns embeddings based on the highest cosine score, xsim aligns source and target embeddings based on the highest margin score, which was shown to be beneficial in mining <ref type="bibr" target="#b21">(Artetxe and Schwenk, 2019a)</ref>. The margin-based score is defined as:   (see Section 4.2). Therefore, we chose to train individual sentence encoders specific to each of these languages and complemented the training data with 2M bitexts in a similar higher-resource European languages. Typical examples are Sicilian paired with Italian or Silesian paired with Polish. Detailed information for all 10 languages are given in <ref type="table" target="#tab_3">Table 10</ref>. We observe that pairing the minority language with another similar major language yields encoders with very low xsim error rates for most of the languages, and we were able to mine large amounts of bitexts yielding good NMT performance. Three languages reach a BLEU score superior to 20 (fur_Latn, lmo_Latn and srd_Latn), and two languages superior to 30 (ltz_Latn and ydd_Hebr).</p><formula xml:id="formula_0">score(x, y) = margin ? ? cos(x, y), z?N N k (x) cos(x, z) 2k + v?N N k (y) cos(y, v) 2k</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Creole Languages.</head><p>We applied a similar strategy for Creole languages. Linguistically speaking, Creole languages are a possible outcome of a contact situation between languages over a fairly brief period of time <ref type="bibr">(Lent et al., 2022)</ref>. When training the sentence encoders, we paired each Creole language with a "similar" high-resource language (see <ref type="table" target="#tab_3">Table 11</ref>). The sentence encoders have a very low xsim error rate for all languages except Sango (sag_Latn) for which we were not able to identify a similar language with sufficient resources.   Let us compare two extreme settings: Papiamento (pap_Latn) and Kabuverdianu (kea_Latn). For the former, we were able to crawl 28M sentences of monolingual data, while we have less than 300k for the latter. Both have less than ten thousand sentences of existing bitexts which is largely insufficient to train an NMT system. We were able to mine more than 7M bitexts for Papiamento which yielded an impressive BLEU score of 40.9, while we only achieve BLEU 4.9 for Kabuverdianu. This highlights that the amount of available monolingual data is crucial to make bitext mining successful.</p><p>Berber Languages. We considered several languages of the Berber family, namely Kabyle (kab_Latn), Tamashek (taq) and Central Atlas Tamazight (tzm). We consider Tamazight written in the Tifinagh script and Tamashek both in Latin and Tifinagh script. All are very low-resource languages with barely 10,000 sentences of available bitext, and 72,000 for Kabyle. It was also very challenging to collect monolingual data for these Berber languages. These statistics are given in <ref type="table" target="#tab_3">Table 12.</ref> We were able to mine bitexts for Kabyle and reach a modest BLEU score of 6.2. xsim error rates for Tamashek are above 20% and insufficient amount of monolingual data make it impossible to mine bitext of good quality for Tamashek. For Tamazight, a very small amount of bitexts could be mined. Tamashek and Tamazight are typical examples of very low-resource languages for which it seems to be very hard to collect written material to support training of machine translation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Malayo-Polynesian Languages.</head><p>Let us consider a larger language family: the Malayo-Polynesian family. We discuss here 13 languages from this family, and use a single encoder for ten of them. The languages Fijian, Maori, and Samoan are handled by a separate encoder. The result overview is given in <ref type="table" target="#tab_3">Table 13</ref>. We observe very low xsim error rates for most of the languages, although several languages have less than hundred thousand sentences of bitexts. Training all languages together in one specific encoder for this family seems to be very beneficial for these very low-resource languages. In addition, we were able to collect several million sentences of monolingual text for most of the languages. This gives us optimal conditions for mining and we achieve substantial improvements in the BLEU score compared to training a bilingual NMT system on the public bitexts only. We have two languages which improved by more than twenty  BLEU points: Javanese (jav_Latn): 11.1 ? 31.2 and Sundanese (sun_Latn): 1.5 ? 28.5. Even high-resource languages like Tagalog (tgl_Latn) and Standard Malay (zsm_Latn) see significant improvements of the BLEU score. We did not mine new bitexts for Indonesian since CCMatrix already provides 70 million parallel sentences, but we included it to help learning the sentence encoder for those languages. We also observe very good performance for Maori: the BLEU score improves from 5.6 to 20.5, while mined bitexts did not improve NMT performance for Fijian.</p><p>African Languages. Among the 200 languages of the NLLB project, 55 are spoken in the African continent, which is more than a quarter of all languages we handle. Except for seven languages -Modern Standard Arabic, Afrikaans, Southern Sotho, Swahili, Tswana, Xhosa and Zulu -all are low-resource languages, i.e. with less than one million publicly available sentence pairs. Twelve of them even have less than hundred thousand sentence pairs, which we named very low-resource languages. In addition, we struggled to curate meaningful amounts of monolingual data. Given these facts, training sentence encoders for African languages and mining high quality bitexts turned out to be a major challenge -even in the broader community. In fact, collecting resources, training NMT systems, and performing evaluations for African languages is the focus of several works <ref type="bibr" target="#b2">(Abbott and Martinus, 2019;</ref><ref type="bibr" target="#b28">Azunre et al., 2021c;</ref><ref type="bibr" target="#b44">Dabre and Sukhoo, 2022;</ref><ref type="bibr">Emezue and Dossou, 2020;</ref><ref type="bibr" target="#b74">Hacheme, 2021;</ref><ref type="bibr">Nekoto et al., 2020;</ref><ref type="bibr">Siminyu et al., 2021)</ref>. A detailed description and analysis of our effort is reported in <ref type="bibr" target="#b83">Heffernan et al. (2022)</ref>.</p><p>The average BLEU score over 44 languages increased from 11.0 to 14.8 with help of the mined bitexts. We also deployed a new training procedure which combines supervised training, i.e. minimizing the cosine loss between the teacher and student embedding, and unsupervised masked LM training (see left part of <ref type="figure" target="#fig_0">Figure 12</ref>). This enabled us to benefit from monolingual data during encoder training. This new approach yielded improved encoders for difficult languages such as Wolof <ref type="bibr" target="#b83">(Heffernan et al., 2022)</ref>.  Handling Specific Scripts. In massively multilingual systems, a common approach is to utilize the same SentencePiece vocabulary. In our initial experimentation, we re-used the LASER2 teacher 50k vocabulary for all student encoders. This has some advantages as shared vocabulary could ease generalization when the same tokens appear in multiple languages. However, despite upsampling, low-resource languages could be poorly represented in a joint vocabulary. We next explore utilizing specific vocabularies for small subsets of languages. <ref type="table" target="#tab_3">Table 14</ref> summarizes the results for different vocabulary strategies. Amharic was part of LASER2 but the xsim error rate is rather high, and LASER2 generalizes badly to Tigrinya. We first explore training an encoder for three Semitic languages: Amharic, Tigrinya, and Maltese. This yields a significant improvement: xsim=0.2% and 1.19% respectively, highlighting the usefulness of teacher-student training and specific encoders for a small set of similar languages. We then trained an encoder for Amharic and Tigrinya only, paired with English as in all our experiments, and a specific 8k SentencePiece vocabulary to better support the Ge'ez script. This brought xsim down to 0.1% and 0.89%, respectively, even though we use less training data. Our best model is on par with LaBSE (which includes only Amharic), and significantly outperforms it for Tigrinya.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Mining at a Glance</head><p>Overall, we mined 148 bitexts paired with English which totals to 761 million sentence pairs with an alignment score of at least 1.06. As mentioned above, bitexts aligned with English for the remaining languages were taken from CCMatrix <ref type="bibr">(Schwenk et al., 2021b)</ref>. We also provide 1465 non-English bitext pairs. This corresponds to 302 million sentence pairs. This includes all language pairs among African, Indic and Malayo-Polynesian families, respectively, as well as alignments of all African languages with French.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">Limitations of Large-Scale Mining</head><p>We note that for some languages, we are only able to create a small amount of bitext through data mining. The limiting factor is predominantly the lack of monolingual data. Many low-resource languages have limited presence on the web, and the data that we curate can be heavily filtered at many stages: language identification, aggressive cleaning of monolingual data, or even poorly aligned bitext. Even with our best efforts, those challenges compound, and they can affect certain languages far more than others. Further, note that even in our mined bitext, we still end up including some content that is already available from other sources -a good example is the Christian Bible. An important final consideration is the web already has machine translated content. For example, many websites may use translation to 'internationalize' their content. A positive is that a large majority of the languages we focus on are not contained in most existing commercial translation services (see <ref type="table" target="#tab_3">Table 1</ref>), however as we mine against higher-resource languages, it is likely our mined datasets contain translated content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.6">Ethical Considerations for Mining Research</head><p>To close this section, we would like to reflect on the issue of data ownership. While we performed our due diligence on deployments of all licensed datasets, the ownership of lowresource language data remains an open debate. In our interview study, many low-resource stakeholders express that sharing language access might in fact be a necessary trade-off for technological advancement. Blocking such access meant blocking any future benefits that could positively impact low-resource language communities. However, we stress that access and ownership are two disparate concepts. Even though we deploy many low-resource language datasets, ownership ultimately belongs to the speakers of these languages. 23</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Conclusion</head><p>We describe our methodology for automatically creating aligned translation training data for low-resource languages. We face significant challenges, as bitext data to train sentence encoders such as LASER and monolingual data to mine in is extremely scarce. To offset these, we develop (1) a high-quality language identification system for over 200 languages that outperforms publicly available LIDs in both Flores-200 and web domains, (2) a detailed, documented monolingual dataset curation and cleaning pipeline, and (3) a teacher-student based multilingual sentence encoder training methodology that enables transfer to extremely low-resource languages with minimal supervised bitext. These contributions combine to create over 1.1 billion new sentence pairs of training data for 148 languages.</p><p>Democratizing Large-Scale Dataset Creation. We are releasing all the code that was used to train the LID model, and run the monolingual sentence splitting and filtering. We are also releasing the mining code, under an open source license, with tools to run mining with our open-sourced LASER encoders. We have built a new mining library: stopes 24 that can run locally or scale over any cluster of your choice. This library is shipped to use the LASER encoders described in this article, but also any encoder available on Huggingface sentence transformers. 25 It also enables users to run the end-to-end mining pipeline with simple configurable tooling. Our hope is that this will allow more people to create high quality bitext datasets in their language of choice, and extend the research on language mining and translation. The encoders can be found in LASER repository, and the teacher-student code in the nllb branch of the fairseq repository. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Modeling</head><p>Existing research in massively multilingual machine translation has been predominantly restricted to about 100 languages, which represent only a fraction of globally written languages <ref type="bibr" target="#b18">(Arivazhagan et al., 2019;</ref><ref type="bibr" target="#b151">Fan et al., 2020;</ref><ref type="bibr" target="#b174">Zhang et al., 2020)</ref>. Some works have extended beyond <ref type="bibr" target="#b104">(Mueller et al., 2020;</ref><ref type="bibr">Siddhant et al., 2022)</ref>, but critically lack reliable performance evaluation. Despite much research, the translation quality for languages with low volumes of data is typically poor. Further, adding extremely low-resource languages beyond 100 is very challenging because there is often very little existing good quality available bitext, and even large scale bitext mining <ref type="bibr">(Schwenk et al., 2021b)</ref> can struggle to create sufficiently large datasets (see Section 5.3). Data often varies widely in available volume across languages, creating imbalance between different language directions. This complicates massively multilingual training, as some language directions have begun overfitting while others have not yet even converged.</p><p>In this section, we investigate how to most effectively scale multilingual machine translation to hundreds of languages. We develop several novel techniques that tackle the major challenges of low-resource translation, such as training models with sufficient capacity to represent 200+ languages while adjusting to variable data capacity per language pair. We present several sets of experiments that help us identify the most performant model architecture and training strategy that can scale to hundreds of languages (see Section 8.1). These involve (1) conditional compute models to minimize interference between unrelated language directions, (2) studying the best self-supervision strategies on monolingual data, and (3) data augmentation or backtranslation for the lowest volume and low accuracy language directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Preliminaries</head><p>We first describe the multilingual machine translation task setup including tokenization, model architecture, and the ablation dataset we use in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Task Setup</head><p>We model multilingual neural machine translation as a sequence-to-sequence task, where we condition on an input sequence in the source language with an encoder and generate the output sequence in the expected target language with a decoder <ref type="bibr" target="#b31">(Bahdanau et al., 2015)</ref>. We train to maximize the probability of the translation in the target language T , given the source sentence S, the source language s , and the target language t , i.e., P (T |S, s , t ). In this section, we discuss details of (1) tokenization of the text sequences in the source and target languages, (2) model architecture with the input and output specifically designed for multilingual machine translation, and (3) the multilingual machine translation dataset we use for ablation experimentation of our ideas at a smaller, but representative scale.</p><p>Segmentation with SentencePiece. To tokenize our text sequences, we train a single SentencePiece (SPM) (Kudo and Richardson, 2018) model for all languages. We sample a total of 100M sentences from primary bitext data. For more details on the primary bitext data, see Section 8.1.2. To ensure low-resource languages are well-represented in the vocabulary, we downsample high-resource and upsample low-resource languages with a sampling temperature of 5 <ref type="bibr" target="#b18">(Arivazhagan et al., 2019)</ref>. Vocabulary size is a critical hyperparameter in multilingual translation models involving low-resource languages <ref type="bibr" target="#b115">(Oladipo et al., 2022;</ref><ref type="bibr" target="#b129">Rajab, 2022)</ref>. The vocabulary size of our trained SPM model is 256,000. Such a large vocabulary size ensures adequate representation across the wide spectrum of languages we support. See Section 8.1.1 for more details.</p><p>Model Architecture. Our sequence-to-sequence multilingual machine translation model is based on the Transformer encoder-decoder architecture <ref type="bibr" target="#b159">(Vaswani et al., 2017)</ref>. The encoder transforms the source token sequence into a sequence of token embeddings. The decoder attends to the encoder output and autoregressively generates the target sentence token by token. More precisely, the encoder takes the sequence of tokens W = (w 1 , . . . , w S ) and the source language s , and produces a sequence of embeddings H = (h 1 , . . . , h S ), which are then provided to the decoder with the target language t to produce the target tokens V = (v 1 , . . . , v T ) sequentially, that is,</p><formula xml:id="formula_1">H = encoder(W, s ),<label>(2)</label></formula><formula xml:id="formula_2">?i ? [1, . . . , T ], v i+1 = decoder(H, t , v 1 , . . . , v i ).<label>(3)</label></formula><p>Note that we prefix the source sequence with the source language, as opposed to the target language as previously done in several works <ref type="bibr" target="#b18">(Arivazhagan et al., 2019;</ref><ref type="bibr">Johnson et al., 2017)</ref>. This is primarily because we prioritize optimizing zero-shot performance of our model on any pair of 200 languages at a minor cost to supervised performance. Empirically, we find zero-shot performance to be negatively affected when conditioning the encoder on the target language. When the source is conditioned on only the source language, the encoder generalizes better to pairs of source and target languages not encountered during training <ref type="bibr" target="#b151">(Fan et al., 2020)</ref>.</p><p>Both the encoder and decoder are stacks of Transformer layers. Each Transformer layer takes a sequence of embeddings as input and outputs a sequence of embeddings. In the encoder, Transformer layers are composed of two sub-layers, a self-attention and a feedforward layer. These are applied sequentially and are both preceded by a LayerNorm <ref type="bibr" target="#b29">(Ba et al., 2016)</ref> and followed by a residual connection <ref type="bibr" target="#b81">(He et al., 2015)</ref>:</p><formula xml:id="formula_3">Z = X + self-attention(norm (X)),<label>(4)</label></formula><formula xml:id="formula_4">Y = Z + feed-forward(norm (Z)).<label>(5)</label></formula><p>We apply LayerNorm at the beginning of each sub-layer (Pre-LN), as opposed to applying LayerNorm after the residual connection at the end of each sub-layer <ref type="bibr">(Post-LN)</ref>. This is because Pre-LN is more stable in practice compared to <ref type="bibr">Post-LN (Xiong et al., 2020)</ref>. The self-attention layer is an attention layer that updates each element of the sequence by looking at the other elements, while the feed-forward layer (FFN) passes each element of the sequence independently through a 2-layer MLP. In the decoder, there is an additional third sub-layer, between the self-attention and the feed-forward, which computes attention over the output of the encoder. We refer the reader to <ref type="bibr" target="#b159">Vaswani et al. (2017)</ref> for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Ablation Dataset</head><p>We construct a multilingual machine translation benchmark such that it is representative of our final benchmark on 200+ languages. We choose a representative sub-sample of 53 out of 202 languages and a total of 110 translation directions (see <ref type="table" target="#tab_3">Table 51</ref> in the appendix). These consist of 45 directions out of English (aggregated as eng_Latn-xx), 45 directions into English (aggregated as xx-eng_Latn) and 20 non-English directions (aggregated as xx-yy). In terms of resource level, there are 40 high-resource and 70 low-resource directions (see <ref type="table" target="#tab_3">Table 1</ref> and  <ref type="figure" target="#fig_0">Figure 15</ref> shows the data distribution over language pairs sorted by the example count per pair. We call this dataset our ablation dataset and use this throughout all experiments in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Conditional Compute for Massively Multilingual Machine Translation</head><p>A massively multilingual translation model is trained on several translation directions at once, utilizing the same shared model capacity. This can lead to beneficial crosslingual transfer between related languages at the risk of increasing interference between unrelated languages <ref type="bibr" target="#b151">Fan et al., 2020)</ref>. Sparsely Gated Mixture of Experts (MoE) models are a type of conditional compute models <ref type="bibr" target="#b11">(Almahairi et al., 2016;</ref><ref type="bibr">Bengio et al., 2013)</ref> that activate a subset of model parameters per input, as opposed to dense models that activate all model parameters per input. MoE models unlock significant representational capacity while maintaining the same inference and training efficiencies in terms of FLOPs as compared to the core dense architecture. In this section, we study how we can use Sparsely Gated Mixture of Experts models <ref type="bibr" target="#b92">Hwang et al., 2022;</ref><ref type="bibr">Lepikhin et al., 2020;</ref><ref type="bibr">Lewis et al., 2021;</ref><ref type="bibr" target="#b159">Shazeer et al., 2017;</ref><ref type="bibr">Zoph et al., 2022)</ref> to achieve a more optimal trade-off between crosslingual transfer and interference and improve performance for low-resource languages.</p><p>Sparsely Gated Mixture of Experts. As illustrated in <ref type="figure" target="#fig_0">Figure 16</ref>, we replace the FFN sublayer in dense models with an MoE sublayer once every f MoE layers in both the encoder and decoder. The MoE sublayer consists of E feed-forward networks (FFN), denoted with (FFN 1 , FFN 2 , . . . , FFN E ), each with input and output projections W o . A gating network, consisting of a softmax-normalized linear layer with weights W g , is attached to each MoE sublayer to decide how to route tokens to experts. Given an input token x t the output of the MoE sublayer is evaluated as:</p><formula xml:id="formula_5">FFN e (x t ) = W (e) o ReLU(W (e) i ? x t ), (?e ? {1, . . . , E}) (6) G t = softmax(W g ? x t ), G t = Top-k-Gating(G t ),<label>(7)</label></formula><p>MoE</p><formula xml:id="formula_6">(x t ) = E e=1 G te ? FFN e (x t ),<label>(8)</label></formula><p>with G t ? R E the routing vector computed by the gating network, i.e., for each expert, G t,e is the contribution of the e th expert (FFN e ) in the MoE output. We follow the Top-k-Gating algorithm of <ref type="bibr">Lepikhin et al. (2020)</ref> and dispatch each token to at most k = 2 experts. We always choose the top 2 scoring experts per token, and do not add randomization to the choice of the second expert.</p><p>26. For this ablation dataset, we only include English-centric data to manage experimental iteration speed.</p><p>In Section 8, we include thousands of non-English training directions. The Transformer encoder-decoder model, supplemented with MoE layers and their respective gating networks, learns to route input tokens to the corresponding top-2 experts by optimizing a linearly weighted combination of label-smoothed cross entropy <ref type="bibr">(Szegedy et al., 2015)</ref> and an auxiliary load balancing loss <ref type="bibr" target="#b159">(Shazeer et al., 2017)</ref>. This additional loss term (LB) pushes the tokens to be uniformly distributed across experts and is evaluated as:</p><formula xml:id="formula_7">LB = E ? E e=1 f e p e , p e = 1 T ? T t=1 G te ,<label>(9)</label></formula><p>where f e is the fraction of tokens routed to the e th expert, as their first choice, through Top-k-Gating, and p e is the average routing probability to that expert over the T tokens in the mini-batch. We refer the reader to <ref type="bibr">Lepikhin et al. (2020)</ref> for more on the optimization of MoE models.</p><p>In the rest of this section, we first detail how we train vanilla Sparsely Gated MoE models for multilingual machine translation on our benchmark and show how they compare to dense models. We then discuss why these vanilla Sparsely Gated MoE models are suboptimal for low-resource language pairs (Section 6.2.1). We propose in Section 6.2.2 a series of architectural changes that significantly improve the performance on low-resource language pairs with MoE models. Finally, we devise and study a simple but effective curriculum  learning strategy (Section 6.2.3) as another approach to get improvement on low-resource pairs with these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Vanilla Sparsely Gated MoE and its drawbacks for Low-Resource Languages</head><p>The motivation behind sparsely activating expert subnetworks in an MoE model is to allow different parameters to model different aspects of the input space. We hypothesize that the added expert capacity should help higher resource language pairs that might otherwise be constrained to share the same dense model capacity with many other language pairs. We also hypothesize that with a massive number of translation directions, the added expert capacity would reduce interference, thus benefiting tasks of all resource levels. To verify this claim, and to understand the limits of vanilla MoE models, we compare in the following set of experiments the performance of MoE models to that of their dense counterparts on our ablation dataset.  <ref type="bibr">Lepikhin et al. (2020)</ref>. All models are trained for 100k updates with an effective batch size of 1M tokens per update. For dense models, the objective function is label-smoothed cross-entropy ( = 0.1) <ref type="bibr">(Szegedy et al., 2015)</ref>, and for MoE models, the objective function is a weighted sum of label-smoothed cross-entropy and the load balancing loss (Equation <ref type="formula" target="#formula_7">(9)</ref>) with weights 1.0 and 0.01, respectively. We optimize with Adam (Kingma and Ba, 2015) using (? 1 , ? 2 , ) = (0.9, 0.98, 10 ?6 ). We linearly increase the learning rate up to 0.004 through 8000 warmup updates, then follow the inverse square root learning rate schedule. For Top-2-Gating, we set the expert capacity to 2 ? T /E, i.e., we enforce that each expert processes, at most, 2 ? T /E tokens, where T is the number of tokens in the mini-batch and E is the number of experts. During generation, we set the capacity to T so that all tokens can be routed to whichever expert they choose. We use the chrF ++ metric to compare the model performance (see Section 7.1). Adding overall dropout (sweeping over p drop ?{0.1, 0.2, 0.3}) significantly improves the performance of MoE-64 in both the 615M and 1.3B variants -For the 615M compute equivalent variant, Moe-64 with p drop =0.1 outperforms dense 615M with dropout by +1.5 to +1.7 chrF ++ points across all subsets of pairs. Importantly, when increasing the dropout from 0.0 to 0.1 for MoE 64, we see that the relative decline of -0.1 chrF ++ , changes into a relative improvement of +0.9 chrF ++ for very low resource pairs translating out of English. For the 1.3B compute equivalent variant, we see +0.5 to +0.6 chrF ++ improvement in the performance of high resource and low resource language pairs translating out of English, but no gains on translation into English or non-English pairs. This indicates that once we scale the computational cost per update, we see milder improvements on high-resource language pairs as well as low-resource pairs. We hypothesize two potential reasons for this: (1) we use a temperature of 1.0 for sampling, i.e., we do not upsample datasets from low-resource pairs. This preserved imbalance drives the 1.3B dense model to allocate capacity proportional to the resource level of each language pair. As a result, high-resource pairs will likely have enough capacity in the 1.3B dense model (given the size and nature of our ablation dataset) and will not benefit as much from the additional capacity of MoE models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. In</head><p>(2) As we increase computational cost per update, the propensity for low or very low-resource pairs to overfit increases thus causing performance to deteriorate.</p><p>To further understand the training regimes of MoE models, we look at their learning curves in <ref type="figure" target="#fig_0">Figure 17</ref>. We observe in the case of eng_Latn-kon_Latn, a very low-resource pair, that the model continues to face significant overfitting when trained for 100k updates. This is unsurprising, as iterating over a small training set with large capacity causes overfitting. Training for more steps is important for high-resource pairs, but we want to avoid negatively affecting low-resource pairs in the process. In the next two sections, we discuss two strategies that address this issue and improve model performance on low-resource pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Regularizing Massively Multilingual Mixtures of Experts</head><p>Although overall dropout is sufficient to regularize dense models, MoE models with overall dropout still significantly overfit on low-resource pairs as seen in <ref type="figure" target="#fig_0">Figure 17</ref>. To remedy this, we design and test different regularization strategies specific to MoE architectures. We describe each of these strategies and report results on our ablation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MoE Expert Output Masking (EOM).</head><p>MoE models enable specialized expert capacity to be activated based on the input token. However, with increased capacity, the learned tokenexpert assignment can cause the models to overfit, especially on low-resource translation directions. In this proposed regularization strategy, we mask the expert output for a random fraction (p eom ) of the input tokens. For input tokens with dropped expert outputs, the first and/or second expert is effectively skipped. As illustrated in <ref type="figure" target="#fig_0">Figure 18b</ref>, we mask both experts for the first token (x 1 in red), we did not mask any of the expert outputs for the second token (x 2 in blue), and the last scenario is that of the last token (x 3 in green), where only one expert is masked. Note that although this masking will zero out some combination weights G t,e in Equation <ref type="formula" target="#formula_2">(13)</ref>, it will not affect the weights used in the load balancing loss in Equation <ref type="formula" target="#formula_7">(9)</ref>. We compare EOM to Gating Dropout <ref type="bibr" target="#b92">(Liu et al., 2022)</ref>, a strategy for reducing crossmachine communication in MoE layers which also has a regularizing effect. Gating Dropout skips the All-to-All communication between GPUs with probability p gd , routing tokens to the local experts instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final Output Masking (FOM).</head><p>A simpler alternative to EOM would be to mask the combined expert output for a random fraction of tokens, i.e., after the last stage in <ref type="figure" target="#fig_0">Figure 18b</ref>. We denote with p fom the fraction of tokens masked with this regularization method. Note that this type of masking is more generic as it can be applied to dense models as well -in testing it here, we validate the advantages of using an MoE-specific masking i.e. MoE Expert Output Masking.  <ref type="figure" target="#fig_0">Figure 19</ref>, we augment MoE layers with a binary gate that decides the weights associated with two branches of the computational graph: (1) a shared dense FFN sublayer (FFN shared ) and (2) an MoE layer with its own E expert FFN sublayers. For an input token x t , the output of CMR is evaluated as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional MoE Routing (CMR</head><formula xml:id="formula_8">g(x t ) = sigmoid(W CMR ? x t ),<label>(10)</label></formula><formula xml:id="formula_9">CMR(x t ) = (1 ? g(x t )) ? FFN shared (x t ) + g(x t ) ? MoE(x t ),<label>(11)</label></formula><p>where W CMR are the weights of the CMR's binary gate. Unlike Zhang et al. (2021)'s CLSR-Gate, our CMR branches are FFN sublayers (dense or sparsely gated MoE) and not linear projections. Furthermore, our CMR does not have language-specfic parameters, but learned routing to experts using an MoE layer.</p><p>The CMR gate weights are learned by optimizing translation accuracy under a budget constraint p. For a mini-batch with T tokens, this amounts to adding the following auxiliary loss term (L CMR ) to the loss function: In , the budget p controls language-specific capacity -here we use this budget constraint to limit the capacity induced by MoE layers; At p=0, the model is dense, practically pushing all tokens through FFN shared , and at p=1, the model is free to always route tokens through the high-capacity MoE layer. This provides a regularizing effect that might help low-resource language pairs that are more likely to overfit with the large capacity of MoE layers. Initial experiments on other benchmarks show that the value of p = 0.8 achieves the desirable trade-off of improving the performance on low-resource pairs without hindering the performance on high-resource ones. An important ingredient in empirically helping CMR layers is zeroing out a fraction of the gates g (Equation <ref type="formula" target="#formula_9">(11)</ref>) in the mini-batch; we denote this fraction with p cmr . This means that we force p cmr % tokens in the mini-batch to only take the route of FFN shared . We note that CMR is also related to <ref type="bibr" target="#b130">Rajbhandari et al. (2022)</ref>, since both approaches combine the outputs of dense and MoE layers. The main difference is the absence of the auxiliary budget constraint and zeroing out a fraction of the gates g. From our initial experiments, we find that the addition of the auxiliary budget constraint L CMR and p cmr are important for improving accuracy. ? For EOM, we sweep over the values of (p drop , p eom ) ? {0.1, 0.2, 0.3} 2 and choose the best out of 9 variants based on the average chrF ++ score on the validation set.</p><formula xml:id="formula_10">L CMR = 1 T ? T t=1 |g(x t ) ? p| .<label>(12)</label></formula><p>? For FOM, we set p drop =0.3 and sweep over p fom ? {0.1, 0.2, 0.3} and choose the best out of the variants based on the average chrF ++ score on the validation set.</p><p>? For CMR, we train the baseline augmented with a shared FFN sublayer with the same dimensions as the rest of the model. We sweep over the values of (p drop , p cmr ) ? {0.1, 0.2, 0.3} 2 and choose the best out of 9 variants based on the average chrF ++ score on the validation set.  ? For Gating Dropout <ref type="bibr" target="#b92">(Liu et al., 2022)</ref>, we sweep over the values of (p drop , p gd ) ? {0.1, 0.2, 0.3} 2 and choose the best out of 9 variants based on the average chrF ++ score on the validation set.</p><p>For each model, we report chrF ++ averages on the validation set in 3 groups of directions: eng_Latn-xx, xx-eng_Latn and xx-yy, broken down w.r.t. to resource levels: high, low and very low (v.low) for eng_Latn-xx and xx-eng_Latn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>In terms of alleviating the overfitting issue, the last column of <ref type="figure" target="#fig_0">Figure 17</ref> shows that EOM leads to better regularization and less overfitting on low-resource tasks compared to overall dropout. In terms of translation quality, and as shown in <ref type="table" target="#tab_3">Table 16</ref>, we observe gains of +0.4 chrF ++ across all pairs into English and +0.6 chrF ++ across non-English pairs for MoE EOM compared to vanilla MoE with overall dropout. Gains are larger on low and very low-resource languages -for out of English, there are improvements of +0.6 and 0.9 chrF ++ with EOM.</p><p>The comparison between EOM and FOM proves that masking before combining the expert outputs is more beneficial than simply masking tokens in the final output. Our hypothesis is that this gain in performance stems from EOM strengthening the residual connection surrounding the MoE layer and reducing co-adaptation between selected top-2 experts, as well as co-adaptation between experts and the subsequent layers of the model.</p><p>We find that Gating Dropout performs better than vanilla MoE with overall dropout, but is outperformed by both EOM and CMR. For CMR, we see +0.8 chrF ++ across all pairs into English and +0.7 chrF ++ across non-English pairs. Similarly, the improvements are larger for low and very low-resource languages, with +0.7 and +1.0 chrF ++ respectively.</p><p>These results demonstrate that both EOM and CMR strategies help improve on top of vanilla MoE with overall dropout. CMR is computationally more expensive by 18-25% at training time because of the additional shared FFN layer at the level of each MoE layer in the model. Given the additional computational overhead, we use the simpler MoE EOM strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Curriculum Learning</head><p>To reduce overfitting on low-resource language pairs further, we next explore alternative means of adding additional regularization. We try a straightforward curriculum of introducing these low-resource language pairs in phases during model training. The language pairs that empirically overfit within K updates are introduced K updates before the end of training. This allows language pairs that tend to overfit with too many training updates to avoid overfitting, while allowing language pairs that benefit from additional training updates to continue training. Prior work explored, for instance, curriculum design for multilingual models based on data types <ref type="bibr">(Kuwanto et al., 2021)</ref>.</p><p>Experimental Setup. We train MoE-64 models with f MoE =2 for a total of T updates (see Section 6.2.1 for details of the base architecture). To derive the phases of the curriculum, we first train a regular model, i.e., without curriculum, then we partition language pairs into n buckets {b 0 , b 1 , . . . , b n?1 } based on when they start to overfit. In the phased curriculum training, we introduce each bucket b i after exactly T ? k i updates, where k i is the median number of updates after which all directions in bucket b i start to overfit. Based on observed overfitting patterns, we introduce pairs during training in n = 3 phases -we set T = 100k,</p><formula xml:id="formula_11">k 0 = 100k, k 1 = 40k, k 2 = 20k, so b 0 is introduced first, b 1 is introduced at step T ? 40k, and b 2 is introduced at step T ? 20k.</formula><p>We compare to the baseline of an MoE model with overall dropout 0.3 without curriculum learning, i.e. introducing all pairs at the start of training and training for 100k updates. <ref type="table" target="#tab_3">Table 17</ref>, for vanilla MoE, when translating out of English (eng_Latnxx), there is an average improvement of +0.6 chrF ++ on low-resource directions (low) and a +0.8 chrF ++ improvement on very low-resource (v.low) directions. There is, however, no significant improvement on high-resource directions (high) or translation into English (xx-eng_Latn), most likely because there is no overfitting on these directions in the baseline. For MoE EOM, training with a curriculum actually hurts performance across eng_Latn-xx, xx-eng_Latn and xx-yy. Our analysis indicates that overfitting on the ablation dataset is already reduced by EOM, which has a higher fraction of language pairs with their best checkpoint by validation perplexity at ? 70000 steps. We hypothesize that adding a curriculum on top of EOM is not needed for the ablation dataset. However, results in <ref type="table" target="#tab_6">Table 29</ref> show that with the full dataset and larger model, combining curriculum learning and EOM improves performance, especially on low and very low-resource language pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.4">Analysis of Multilingual Sparsely Gated MoE Models.</head><p>MoE theoretically enables models to specialize expert capacity for different tasks, but what do these models actually learn? We now take a closer look at the routing of tokens to experts in MoE layers at different points of the encoder-decoder architecture. We take an MoE model (E=64, p drop =0.3, p eom =0.2) trained on our ablation dataset and do a forward pass on Flores-200 dev set data in teacher-forcing mode, i.e., we feed the true target prefix to predict the next target token. For each task (language pair), we log the routing decisions prior to Top-k-Gating, and depending on whether it is an encoder layer or a decoder layer, we average the routing vectors across multiple language pairs to estimate language-level routing vectors.  </p><formula xml:id="formula_12">G &lt;lang&gt; = 1 |T &lt;lang&gt; | xt?T &lt;lang&gt; G t ,<label>(13)</label></formula><p>where T &lt;lang&gt; is the set of all tokens in &lt;lang&gt;, source-side for encoder layers and target-side for decoder layers. We plot in <ref type="figure">Figure 20</ref> the cosine similarity scores between all 53 languages of the ablation dataset, in the first and last encoder MoE layer, and the first and last decoder MoE layer.</p><p>The similarity heatmaps demonstrate that in late decoder layers (see <ref type="figure">Figure 20d</ref>), the languages are being separated, i.e., dispatched to different set of experts. Languages within the same family are highly similar in their choice of experts, i.e., the late decoder MoE layers are language-specific. This is particularly the case for languages in the Atlantic-Congo family (the rows/columns from cjk to yor) and some pairs like {snd_Arab, urd_Arab} in the Indo-European family or {yue_Hant, zho_Hans} in the Sino-Tibetan family. To a lesser extent, the early encoder MoE layers (see <ref type="figure">Figure 20a</ref>), also show some language-expert specialization. The late encoder MoE layers and the early decoder MoE layers (see <ref type="figure">Figure 20b</ref> and <ref type="figure">Figure 20c</ref>) seem to be language-agnostic.</p><p>In <ref type="figure" target="#fig_0">Figure 21</ref>, we visualize the vectors of expert-distribution per language G &lt;lang&gt; (Equation <ref type="formula" target="#formula_2">(13)</ref>) using UMAP <ref type="bibr" target="#b105">(McInnes et al., 2018)</ref>. The first row displays languages by language family and the second row displays languages by script. Separation of languages is more discernible in the decoder's last layer (last column of <ref type="figure" target="#fig_0">Figure 21</ref>) particularly along language family lines, e.g., Atlantic-Congo in green and Dravidian in pink.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Self-Supervision Strategies on Large-scale Monolingual Corpora</head><p>For low-resource languages, there is generally limited or no bitext data available. In cases where bitext data is publicly available, the domain of the available data could be narrow (e.g. religious texts) or the bitext data could be noisy. In comparison, there is relatively more abundant monolingual data available in low-resource languages. In addition to bitext mining detailed in Section 5.3, another way of leveraging this monolingual data is via incorporating a self-supervised task into the process of training a multilingual machine translation model <ref type="bibr">(Bapna et al., 2022;</ref><ref type="bibr" target="#b60">Chi et al., 2021;</ref>. <ref type="table">Self-supervised  ara Arab  tir  hau  cjk  ewe  fon  fuv  kik  kin  kon  lin  nso  run  tso  twi  wol  yor  swh  tsn  vie  ace Latn  ayr  mal  tam  tel  eus  fas  snd  urd  bel  bul  rus  hin  mar  eng  afr  ast  cym  fra  isl  ita  kea  lav  oci  por  sin  jpn  kor  luo</ref>   <ref type="figure" target="#fig_0">Figure 21</ref>: UMAP Embeddings of the languages in the ablation dataset. We color in the first row according to language family and in the second row according to script.</p><p>learning can learn patterns and constructs of a language from monolingual text. Hence, learning with self-supervised objectives on this additional data could help improve the performance of multilingual machine translation models.</p><p>We first study the effect of the choice of the self-supervised task: (1) language modeling objective, (2) denoising autoencoder objective , or (3) the combination of both. We follow that up with studying the optimal curriculum when combining the task of self-supervised learning (SSL) on monolingual data along with the task of training on multilingual machine translation (MMT) on bitext data. We use the recommendations from this study to decide on our self-supervision strategy for our final model on 200 languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Incorporating Self-Supervised Objectives with Multilingual Machine Translation</head><p>There are different ways of incorporating self-supervision on monolingual data in the training procedure of multilingual machine translation models. Traditionally, self-supervised learning in NLP takes the form of pretraining with a self-supervised objective on monolingual data, followed by finetuning on the task-specific supervised data. Another strategy is to consider SSL on monolingual data and MMT on bitext data as separate tasks in a multi-task learning setup, where examples from both tasks are present in every batch during training. Our third strategy is a combination of the first two strategies. We can first pretrain with the multi-task setting of self-supervision on monolingual data and multilingual machine translation on bitext data, followed by finetuning on multilingual machine translation alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Self-Supervised Learning Objectives</head><p>There are different self-supervised objectives we can use on monolingual data.</p><p>Denoising Autoencoder (DAE). We follow  and use a Transformer encoder-decoder architecture which is the same architecture used for multilingual machine translation. We set the target to be a sentence from the monolingual corpus. We set the source to be a noised version of the target monolingual sentence. We mask random spans of text from the target sentence. Rather than always replacing the masked token with the mask token (&lt;mask&gt;), with a specified probability, we replace the masked token with a random token from the vocabulary. The SSL objective here is to maximize the likelihood of predicting the target given the source which is a noised version of the target.</p><p>Causal Language Modeling (LM). Past work has shown some success with initializing components of a machine translation model with a pretrained decoder Transformer language model. So, we set up the language modeling task in a seq2seq setting where the source is empty and the target is a sentence from a monolingual corpus. The SSL objective here is to maximize the likelihood of predicting the target i.e. the causal language modeling objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Effect of Curriculum of Self-Supervision combined with Multilingual Machine Translation</head><p>In this section, we study the impact of our choice of curriculum when combining the Denoising Autoencoder(DAE) objective with the usual MMT task.</p><p>Experimental Setup. In our setup, we use the multilingual denoising autoencoder objective for self-supervision on the multilingual text corpus for languages in our ablation benchmark. Our baseline is a dense 1.3B model trained for 100K updates on the MMT task with a sampling temperature of 1.0. We train the SSL task variants with the same architecture and sampling temperature for 200K updates each to ensure that SSL task variants are exposed to as much multilingual parallel data as the baseline. We hope to find the best variant that improves the performance on low-resource pairs which do not have sufficient parallel bitext data but relatively abundant monolingual data. We consider three options:</p><p>? Pretraining on SSL, followed by finetuning on MMT (DAE?MMT). We train on SSL for the first 100K updates, followed by finetuning on MMT task for the next 100K updates. We use a sampling temperature of 1.25 for SSL pretraining. across all subsets of pairs compared to a baseline of training on MMT alone. This indicates that there is a fundamental mismatch in the two training tasks and it is hard for finetuning to recover from the initial state at the end of pretraining. Multi-task training on SSL and MMT (DAE+MMT) shows +0.4 chrF ++ on low-resource eng_Latn-xx pairs and +1.1 chrF ++ on very low-resource eng_Latn-xx pairs, thus confirming that the additional monolingual data in the low-resource target languages is useful when presented in a multitask framework. Similarly, we see +1.3 and +1.9 chrF ++ improvements on low and very low-resource pairs in xx-eng_Latn directions as well as +1.1 chrF ++ on xx-yy pairs. We hypothesize that we observe stronger performance translating into English as there is an abundance of very high-quality English monolingual data which is critical for SSL. Finally, we wish to confirm whether there are any benefits to pausing multitask training after the first K updates, and finetuning on MMT only (DAE+MMT?MMT). We see that this performs on par with multitask training on SSL and MMT (DAE+MMT) without finetuning on MMT only. This indicates that when SSL and MMT are jointly trained in a multi-task setup, they do not suffer from interference that could be countered by the final stage of finetuning purely on the final task (MMT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.4">Effect of Self-Supervision Objectives</head><p>In this section, we compare the impact of choosing different self-supervised objectives.</p><p>Experimental Setup. We train all SSL task variants in this section in a multitask learning setup of training SSL and MMT tasks together, because that is the best performing curriculum as demonstrated in Section 6.3.3. Our baseline is a dense 1.3B model trained for 100K updates and SSL models trained for 200K updates. We train the SSL variants with the same architecture and sampling temperature for 200K updates each to ensure that SSL variants are exposed to as much supervised bitext data as the baseline. We hope to find the best self-supervised objective that improves the performance on low-resource pairs which do not have enough bitext data, and relatively abundant monolingual data. We consider three options. Results. First, we observe that the LM task as an SSL objective results in a decline in performance compared to the baseline of training MMT alone. One hypothesis is that self-supervision on the encoder-decoder attention plays an important role, and this is not  present in the LM objective that focuses on training the decoder to generate the text in the monolingual corpus. Next, as also demonstrated in Section 6.3.3, we see significant improvements when using the DAE objective. Finally, we explore whether there are complementary gains of combining DAE along with the LM task. The results show a decline in performance compared to using only the DAE task. This suggests that there might be some interference between the different tasks, which reduces the overall performance when combining them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.5">Discussion</head><p>Recent works <ref type="bibr">(Bapna et al., 2022;</ref><ref type="bibr" target="#b60">Chi et al., 2021;</ref> have demonstrated that denoising and similar self-supervised objectives are very useful for improving model performance when trained along with machine translation task in a multitask setup. In our work, we try two SSL objectives, DAE and LM and experimented with different combinations of both along with the MMT task. We observed that only DAE performs best when trained with MMT. Benefits of the LM task in a multitask setup with MMT is still not evident and future work could reveal a deeper understanding regarding this finding. We also study different curriculum learning strategies with the SSL tasks and find that multitask learning of DAE and MMT is usually the best setup, similar to findings in <ref type="bibr" target="#b60">(Chi et al., 2021;</ref>. Section 8.1.2 further discusses how SSL+MMT multitask training improves model performance for generating higher quality backtranslated data. Self-supervised learning is a powerful technique for optimally utilizing monolingual data and there is scope to study and design better SSL objectives for this. In addition, designing curricula to combine these objectives in a multitask framework is also an interesting direction for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Data Augmentation</head><p>Another way of leveraging monolingual data is through backtranslation <ref type="bibr">(Edunov et al., 2018;</ref><ref type="bibr">Sennrich et al., 2016a)</ref>, a technique which involves creating parallel corpora that are noisy on the source side via machine translation. However, when it comes to low-resource languages, the machine translation models that are used to generate backtranslation data are often not good enough, and hence the generated data is often noisy and degenerate. <ref type="bibr" target="#b88">Hoang et al. (2018)</ref> proposed iterative backtranslation to offset this, as better models are used to generate backtranslations in every iteration. However, this is an expensive endeavor.</p><p>Since the aim is to use the best possible models to generate backtranslated data, this means using massively multilingual models, which are computationally intensive to run both at training and inference time. In this section, we discuss how we can effectively generate high-quality backtranslation data for low-resource languages, using a single iteration to be computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Different sources of data</head><p>Backtranslation is a source of synthetically augmented data for translation models. We contrast it with two other sources of data: (1) primary bitext data, i.e. high-quality parallel corpora that have been human-translated, and (2) mined bitext data, i.e. parallel corpora obtained via mining (see Section 5.3). Backtranslation relies on the availability of an initial translation system, a teacher, to produce a noisy parallel corpus from a monolingual corpus. A natural choice for a teacher system is to use a transformer-based model similar to our previous approaches. However, these neural systems are often thought to be data-inefficient when compared with traditional phrase or rule based statistical machine translation (Koehn and Knowles, 2017) models. In light of this, we also experiment with using a traditional phrase-based statistical translation model as a teacher for backtranslation <ref type="bibr">(Schwenk, 2008)</ref>. We will therefore distinguish four different sources of data in the following experiments, one primary and three augmented sources: human-translated data; mined data; backtranslated data via multilingual neural machine translation; and backtranslated data via statistical machine translation. For brevity and further use, we show the characteristics of these data sources in <ref type="table" target="#tab_6">Table 20</ref> and use the following abbreviated terms for the data sources:</p><p>? NLLB-Seed: our professionally-translated seed datasets as described in Section 4.2.</p><p>? PublicBitext: publicly available parallel corpora. These datasets may be humantranslated but are often automatically aligned.</p><p>? Primary: the combination of the above two sources.</p><p>? Mined: mined data as described in Section 5.3.</p><p>? MmtBT: backtranslations obtained via a 1.3B-parameter dense multilingual neural model.   In the second set of experiments, we try to understand the benefits of adding an additional source of backtranslated data, SmtBT. For this, we train bilingual statistical machine translation (SMT) models on Primary+Mined bitexts. We compare the performance of these models against the multilingual machine translation (MMT) model trained on the same data, and pick the directions where the SMT models are either better or comparable to the MMT models. For the directions we pick, we generate backtranslation data using the SMT teacher models. This gives us the SmtBT dataset. For comparing the complementary benefits of SmtBT, we combine all sources of data Primary+Mined+MmtBT+SmtBT, train a similar 1.3B dense model and compare its performance. <ref type="table" target="#tab_3">Table 21</ref>, we report the performance of the baseline model using only Primary data, and then the other three models trained by incrementally augmenting the training data with the Mined, MmtBT, and SmtBT datasets. We observe that the highest performance is achieved when using all sources of data. Despite recent advances which cast into doubt the supposed data inefficiency of neural machine translation (Sennrich and Zhang, 2019), we see that using SMT as a source of backtranslation still leads to improvements for very low-resource directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. In</head><p>We can further probe into backtranslation quality by looking at the performance of our two teacher models, the MMT model and the set of bilingual SMT models, which are  We also report the number of directions at which each method does best.</p><p>trained on the same Primary+Mined data. In <ref type="table" target="#tab_6">Table 22</ref> the MMT teacher is, as expected, outperforming traditional SMT on all but a few directions. Although the average SMT performance might be low, we hypothesize that the combination of different, complementary sources of noise is the reason why its addition is still beneficial to the overall performance of the model. The differences between generations can be visualized by plotting the total token frequencies generated by the MMT and SMT teachers when translating the same corpus.</p><p>We refer readers to <ref type="figure" target="#fig_39">Figure 42</ref> in Appendix D for two such histograms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Data Tagging</head><p>Tagged backtranslation <ref type="bibr">(Caswell et al., 2019)</ref> is a technique to help the model discern between the different sources of data it is being exposed to during training. This is achieved by pre-pending special tokens to backtranslated training examples, and has been shown to boost performance by helping the model distinguish noisy data and avoid overfitting on it . In the experiments of the previous section we used an extended tagging scheme, using special tokens for each of the three data sources: &lt;MINED_DATA&gt; for Mined, &lt;MMT_BT_DATA&gt; for MmtBT and &lt;SMT_BT_DATA&gt; for SmtBT. We study the effects of ablating away this tagging scheme, experimenting with using just a single tag to mark all secondary data (&lt;SECONDARY_DATA&gt;) as well as using no tags at all.</p><p>Experimental Setup. We train MMT models on the full dataset made up of Primary, Mined, MmtBT and SmtBT, but ablate different tagging schemes. The no tags setting does not use any tags at all, the single tag setting uses the same tag for Mined, MmtBT and SmtBT. Finally the finegrained tags setting uses separate tags for Mined, MmtBT and SmtBT. <ref type="table" target="#tab_6">Table 23</ref> demonstrate the benefits of using finegrained tags. This provides further evidence to support the hypothesis of Caswell et al. (2019) that tagging is useful to help the model distinguish between synthetic and natural data. It also suggests that signaling the specific nature of synthetic data can further boost performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. The results in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Bootstrapping models with NLLB-Seed</head><p>For a considerable number of the low-resource languages examined in this work, the parallel corpora which are publicly available for research often have only a few thousand sentences. They frequently come from sources with a highly specific domain such as scripture, and the level of quality assurance is often unclear. While translating millions of sentences with  We compare models trained on the ablation dataset using no tags, a single tag, and finegrained tags. We report chrF ++ scores aggregated by language direction and resource level type. We observe that finegrained tagging gives the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NLLB-SEED</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PUBLICBITEXT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEED PUBLIC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combined</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEED PUBLIC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diminished+NLLB-SEED PUBLIC SEED</head><p>Diminished PUBLIC 6.2k sents removed <ref type="figure">Figure 22</ref>: Dataset Configurations for the ablation experiments of Section 6.5. We quantify the impact of using NLLB-Seed in various experimental settings.</p><p>professional translators is unfeasible, translating a few thousand is possible. It is important to understand whether there is value in such small but high quality human-annotated seed datasets for low-resource languages. Is such a small dataset sufficient to bootstrap a machine translation system for a new low-resource language or finetune an existing machine translation system on a new domain? In this section, we investigate these questions and we aim to measure the effect of training translation models on such a small human annotated seed dataset like NLLB-Seed (see Section 4.2). We are interested in quantifying the importance of a dataset which has been professionally translated, covers a wider domain, and which can be confidently attributed to the specified language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.1">Usefulness of NLLB-Seed</head><p>First, we want to measure the performance of bilingual models trained on NLLB-Seed against those trained on publicly available data (PublicBitext) as well as on the combination of both (NLLB-Seed+PublicBitext). Secondly, given a certain amount of publicly available parallel data, we study the incremental effect of adding NLLB-Seed, compared to simply adding more data from the original domain of the public bitext. To answer this we create for each language a new dataset, which we call Diminished+NLLB-Seed. This is obtained by subtracting from PublicBitext a random sample of sentences of the same size as the seed dataset (?6.2k), and swapping in NLLB-Seed in its place. The result is a dataset which is of the same size as PublicBitext, but which also contains NLLB-Seed.</p><p>The dataset compositions that we study here are depicted in <ref type="figure">Figure 22</ref>.  Experimental Setup. We select eight low-resource directions covered by NLLB-Seed data. For fair comparison to existing datasets we only select directions for which we could also find a minimum of 10 k sentences of publicly available parallel text. These are ban_Latn, dik_Latn, fuv_Latn and mri_Latn, translated into and out of eng_Latn. We train bilingual models for each direction, using a transformer architecture with 6 encoder layers and 6 decoder layers trained with an inverse square-root learning rate schedule with warm-up. Each language pair uses a custom SentencePiece vocabulary of size 1k. To study the incremental effect of adding NLLB-Seed, we prepare Diminished+NLLB-Seed datasets for each of the above languages. Then we train bilingual models and compare against PublicBitext in a similar setup as described above. <ref type="table" target="#tab_6">Table 24</ref> reveal that, despite the considerably larger size of publicly available training data, training on NLLB-Seed leads to markedly higher performance on average. Unsurprisingly, the best performance is obtained by combining all available data. This result is encouraging, especially in light of recent results showing that a larger MT model can be finetuned on a small but high quality dataset such as NLLB-Seed to adapt it to a new low-resource language easily <ref type="bibr">(Adelani et al., 2022)</ref>. Furthermore, from <ref type="table" target="#tab_6">Table 25</ref>, we observe that the base performance of the Diminished+NLLB-Seed model is higher than that of the base PublicBitext model. This demonstrates that increasing the training data of a model with NLLB-Seed is beneficial compared to increasing it by the same amount of publicly available data. To control for noise, we repeat each experiment three times with different subsets of PublicBitext and report the averages and standard deviations for each. The small variance validates that the low scores on PublicBitext data are not due to any bias of sampling but the lower quality of the PublicBitext data compared to NLLB-Seed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. The results in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.2">Effect of NLLB-Seed on Backtranslation</head><p>We investigate whether using NLLB-Seed might additionally affect performance when backtranslation is used, for example, if using a small amount of human-translated data such as NLLB-Seed might increase backtranslation quality. For this, we start with base models  trained on PublicBitext and on Diminished+NLLB-Seed data, use them to perform backtranslation, and then train a new set of models on this augmented data to quantify performance. <ref type="table" target="#tab_6">Table 25</ref>, we observe that the gap in performance between the models trained with and without NLLB-Seed is further increased when backtranslation is applied. Starting with PublicBitext data only, adding BT brings +1.6 chrF ++ improvement, while starting with Diminished+NLLB-Seed data leads to +3.5 chrF ++ gain (despite the total size of both datasets being the same). This indicates that a small amount of high-quality bitext significantly improves the effectiveness of model-based data augmentation such as backtranslation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. From</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Human Evaluation</head><p>For rapid experimental iteration, the vast majority of modeling ablation decisions are assessed using automatic metrics such as BLEU or chrF ++ . However, performance improvements in automatic metrics may not translate to human-perceived quality, especially for minor improvements in scores such as BLEU. In this section, we conduct a human evaluation (see Section 7 for a description of our protocol) to understand if our described modeling improvements correlate with quality improvements detectable through human evaluation.</p><p>Experimental Setting. We evaluate four models: a dense model baseline, our best performing MoE variant, our best performing SSL variant, and our best performing BT variant. For simplicity, we evaluate the same 24 translation directions for all models with 506 source sentences translated per model. 9 directions are translation into English, 11 out of English, and 4 non-English directions for representativeness.</p><p>Results. We investigate the relationship between chrF ++ score and human evaluation score to understand what quantity of automatic metric improvement in a model ablation would be detectable by human translators. Overall, we find that chrF ++ improvements of +0.5 chrF ++ usually correlate to statistically significant human evaluation improvements, with a score of +1 chrF ++ almost always being detectable by human evaluators.</p><p>For our best MoE variant, there were 14 directions with chrF ++ improvements more than +0.5 over the baseline and 10 of these were statistically significant improvements in human evaluation. For these 10 directions, the human evaluation score more than 0.2 (on a 5-point scale) over the baseline with a corresponding chrF ++ improvement of +2.5</p><p>For our best SSL variant, there were 9 directions with chrF ++ improvement more than +0.5 over the baseline and 4 directions were statistically significantly better in human evaluation. For these 4 directions, the human evaluation score improved +0.2 over the baseline with a corresponding chrF ++ improvement of +2.7.</p><p>Finally, for our best BT variant, there were 14 directions with chrF ++ improvement more than +0.5 over the baseline and 10 directions with statistically significant human evaluation improvements. For these 10 directions, the human evaluation score improved +0.18 over the baseline and chrF ++ improved on average by +3.</p><p>In conclusion, we believe that based on our human evaluation studies of model ablations, that an improvement of +0.5 chrF ++ is often detectable by human evaluators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Conclusion</head><p>Improving the performance of low-resource translation in massively multilingual settings faces several challenges. Directly increasing model size is largely ineffective as low-resource pairs start to overfit. In this section, we studied how to most effectively increase capacity through Mixtures-of-Experts and presented multiple novel regularization strategies. These methods reduce the interference between unrelated language directions. Paired with a training curriculum that introduces higher-resourced pairs earlier in training, we achieved strong gains on low-resource directions while maintaining high-resource performance.</p><p>Beyond architectural challenges, low-resource performance is difficult to improve due to data scarcity. We demonstrated how to effectively utilize monolingual data through both self-supervised training and more effective data augmentation. Using multiple different sources of backtranslated data from MMT and SMT models, in combination with mined data, produces significant performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Evaluation</head><p>The ability to quantify performance is critical to the development of machine learning systems, because improving quality of such systems is impossible without a reliable way to track progress. Machine translation is commonly evaluated using automatic metrics as well as human evaluation, e.g., in the WMT evaluation campaign <ref type="bibr" target="#b10">(Akhbardeh et al., 2021)</ref>, in the AmericasNLP Shared Task <ref type="bibr" target="#b100">(Mager et al., 2021)</ref>. In this section, we describe the automatic metrics that we used and the methodology we followed to perform human evaluation. We present the results of various studies conducted on multilingual translation models and analyze the reliability of automatic metrics on such a varied and low-resource set of languages.</p><p>We further focus on analyzing quality along other axes. Metrics such as BLEU and human evaluation often focus on an axis of translation quality heavily grounded in accuracy and  fluency. Beyond these, we work to quantify our translations from a user safety perspective, choosing to focus additionally on quantifying toxicity in the generated translations. As added toxic content is generally undesirable for the user, we focus first on detecting the appearance of such toxicity in all 200 languages, followed by an analysis of mitigation techniques and prevalence on Flores-200. We open source these novel toxicity lists for all 200 languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Automatic Evaluation</head><p>Various metrics for automatic translation quality assessment exist, including model-based metrics such as COMET <ref type="bibr" target="#b133">(Rei et al., 2020)</ref> and <ref type="bibr">BLEURT (Sellam et al., 2020)</ref>. While model-based metrics have shown better correlation with human judgment in recent metrics shared tasks , they require training and are not easily extended to a large set of low-resource languages. Another approach is to use highly approximate metrics based on roundtrip translations such as RttLangIDChrF (Bapna et al., 2022), although roundtrip translation may not correlate well with translation quality <ref type="bibr">(Koehn, 2005)</ref>. While such methods are more easily scaled to new languages, they are highly dependent on factors which makes results difficult to replicate. 27 In this work, we therefore choose not make use of model-based and roundtrip-based metrics, and rely instead on BLEU and chrF ++ . Both measures rely on the core concept that translation quality can be quantified based on how 27. More specifically, for the case of RttLangIDChrF, neither the full language identification model nor the corpus of sentences used to compute the metric are made public. Furthermore, the main version of the metric used throughout Bapna et al.'s paper does not penalize a translation model's failure to produce text in the correct language, effectively cherry-picking which sentences to evaluate on. The stricter version of the metric, which includes a penalty, is shown to correlate poorly with human judgments. similar a machine translation is compared to that produced by a human translator. We briefly describe both metrics and a variant of BLEU below.</p><p>BLEU. The BLEU score <ref type="bibr" target="#b119">(Papineni et al., 2002)</ref> has been the standard metric for machine translation evaluation since its proposal two decades ago. It measures overlap between machine translation and a human reference translation by combining precision of 1-grams to 4-grams with a brevity penalty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>spBLEU.</head><p>A major downside of BLEU is that it is tokenization-dependent. Efforts such as sacrebleu <ref type="bibr" target="#b124">(Post, 2018)</ref> have taken strides towards standardization, supporting utilizing community-standard tokenizers under the hood. However, these tokenizers do not extend to many languages. <ref type="bibr">Goyal et al. (2022)</ref> propose spBLEU, a BLEU metric based on a standardized SentencePiece model (SPM) covering 101 languages, released with Flores-101. In this work, we provide SPM-200 along with Flores-200 to enable measurement of spBLEU. <ref type="bibr">28</ref> We describe this in greater detail in Section 8.</p><p>chrF ++ . The chrF ++ score <ref type="bibr" target="#b123">(Popovi?, 2017)</ref> overcomes the limitation of the BLEU score which requires that a sentence can be broken up into word tokens. However, some languages, such as Chinese or Thai, do not use spaces to separate words and word segmentation tools may not be readily available or even exist. There is also a concern about highly agglutinative languages where BLEU fails to assign any credit to morphological variants. chrF ++ overcomes this weakness by basing the overlap calculation on character-level n-grams F-score (n ranging from 1 to 6) and complementing with word unigrams and bi-grams. In this work, we primarily evaluate using chrF ++ using the settings from sacrebleu. However, when comparing to other published works, we utilize BLEU and spBLEU where appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Human Evaluation</head><p>While automatic scores are a great tool to drive research, human evaluation is essential to ensure meaningful assessments of translation quality <ref type="bibr" target="#b10">(Kocmi et al., 2021)</ref>. We use two advances -the XSTS evaluation protocol and the use of calibration sets -to enable meaningful human evaluation scores that are comparable across language pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Methodology</head><p>When building machine translation systems for many different language pairs, a core question is which language pairs reach certain levels of quality. Hence, we need meaningful scores that are comparable across language pairs. XSTS Evaluation Protocol. We adapt the recently proposed crosslingual Semantic Text Similarity (XSTS) methodology from <ref type="bibr" target="#b7">Agirre et al. (2012)</ref>. In short, XSTS is a human evaluation protocol that focuses on meaning preservation far more than fluency. For lowresource languages, translations are usually of weaker quality, and so we focus far more on usable (meaning-preserving) translations, even if they are not fully fluent. 100 point scale), work has found that XSTS yields higher interannotator agreement <ref type="bibr">(Licht et al., 2022)</ref>. XSTS rates each source sentence and its machine translation on a five-point scale, where 1 is the lowest score and 5 is the highest score. Each point on the scale is as follows:</p><p>1. The two sentences are not equivalent, share very little details, and may be about different topics. If the two sentences are about similar topics, but less than half of the core concepts mentioned are the same, then 1 is still the appropriate score.</p><p>2. The two sentences share some details, but are not equivalent. Some important information related to the primary subject/verb/object differs or is missing, which alters the intent or meaning of the sentence.</p><p>3. The two sentences are mostly equivalent, but some unimportant details can differ. There cannot be any significant conflicts in intent or meaning between the sentences, no matter how long the sentences are.</p><p>4. The two sentences are paraphrases of each other. Their meanings are near-equivalent, with no major differences or missing information. There can only be minor differences in meaning due to differences in expression (e.g., formality level, style, emphasis, potential implication, idioms, common metaphors).</p><p>5. The two sentences are exactly and completely equivalent in meaning and usage expression (e.g., formality level, style, emphasis, potential implication, idioms, common metaphors).</p><p>Calibration Set. To enable meaningful scores that are comparable across language pairs, we ask each evaluator to provide assessments using the XSTS scale on exactly the same set of sentence pairs. The purpose of this is to identify which sets of annotators have a systemic tendency to be more harsh or generous in their scoring, and correct for this effect. While evaluators assess different languages, the calibration set consists of machine translation output into English paired with an English reference translation. Based on how evaluators use the XSTS scale on this calibration set, we adjust their raw scores on the actual evaluation task to ensure consistency across evaluators. While the monolingual task does not precisely mimic the bilingual XSTS task, it is a reasonable first approximation and has been shown to increase the correlation between human and automatic metrics, primarily by reducing one source of 'noise' in the human evaluations; the lack of score calibration between annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Obtaining Aggregate Human Quality Metrics from Multiple Studies.</head><p>To obtain an aggregate human quality metric for each language direction in an evaluation study, we take the majority XSTS score for each sentence and average these majority scores over all evaluated sentences. In a given study, the aggregate human evaluation score for a source, target language pair l s ? l t is</p><formula xml:id="formula_13">H ls?lt = 1 |T ls?lt | (S,T )?T ls?l t median{X ls?lt,i (S, T ) | 1 ? i ? M ls?lt },<label>(14)</label></formula><p>where l s ? l t denotes a source language, target language pair, X ls?lt,i (S, T ) denotes the XSTS score of the i-th evaluator evaluating sentences in a given translation direction l s ? l t for a source sentence S and target sentence T . M ls?lt denotes the total amount of evaluators evaluating the (source, translation) sentence pair (S, T ) for translation direction l s ? l t and</p><formula xml:id="formula_14">T ls?lt = {(S ls?lt,k , T ls?lt,k ) | 1 ? k ? N ls?lt } is the set of N ls?lt (source, translation) sentence pairs being evaluated for translation direction l s ? l t .</formula><p>Every evaluator in a given study s is also asked to provide ratings for all or part of a calibration set,</p><formula xml:id="formula_15">C s = {(S s,k , T s,k ) | 1 ? k ? K s },</formula><p>where S s,k denotes the k-th source sentence in the calibration set for evaluation study s, T s,k denotes the translated sentence corresponding to S s,k , and K s = |C s | is the number of sentence pairs in the calibration set for evaluation study. The calibration sets for all evaluation studies are drawn from a set of source, target sentence pairs consisting of K = 1000 backtranslated Flores-200 sentences of varying quality.</p><p>For each language direction evaluated in a study, we obtain the mean median XSTS score ("majority score") on the calibration set:</p><formula xml:id="formula_16">C (s) ls?lt = 1 |C s | (S,T )?Cs median{X (s) l,i (S, T ) | 1 ? i ? M (s) ls?lt },<label>(15)</label></formula><p>where X (s) l,i (S, T ) denotes the XSTS score provided by the i-th evaluator for the language direction l s ? l t in study s that evaluated a given source sentence S and a translated sentence T in the study's calibration set C s .</p><p>To obtain aggregate calibrated XSTS scores on the language direction level, we explored several different calibration methodologies of the form</p><formula xml:id="formula_17">H (s) ls?lt = f (H (s) ls?lt , C (s) ls?lt )<label>(16)</label></formula><p>Including a linear shift</p><formula xml:id="formula_18">H (s) ls?lt [lin] = H (s) ls?lt ? (C (s) ls?lt ?C),<label>(17)</label></formula><formula xml:id="formula_19">whereC = s ls?lt C (s) ls?lt s ls?lt 1<label>(18)</label></formula><p>is the mean majority XSTS score on the calibration set across all evaluated language directions across all studies, which in practice is close to 3 (3.01) and therefore for analysis of individual studies we often replaceC with 3 to obviate the need for interacting with all evaluation data across all studies. We also explored other calibration strategies, including clipping the strength of the calibration, adding a multiplicative factor H (s)</p><formula xml:id="formula_20">ls?lt ? ?(C (s) ls?lt ?C)</formula><p>, as well as a more sophisticated heuristic calibration adjustment we name "moderated calibration" designed to keep the calibrated scores within the same [1, 5] domain as the initial majority XSTS scores, to attenuate extreme calibration shifts, and to attentuate calibration shifts when the XSTS score is close to extreme values: </p><formula xml:id="formula_21">C = (C (s) ls?lt ?C) (19) S = tanh (?C) (20) E = ? ? ? ? tanh H (s) ls?lt ? 5 , if C ? 0 tanh H (s) ls?lt ? 1 , if C &gt; 0 ? ? ? (21) H[mod] (s) ls?lt = H (s) ls?lt + S ? E<label>(22)</label></formula><p>None of the calibration methods we investigated showed a dramatic difference in correlation with automated scores, and all calibration methodologies we explored provided superior correlation compared with uncalibrated XSTS scores. For this paper, any references to calibrated scores refer to H[mod]</p><formula xml:id="formula_22">(s) ls?lt .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Results</head><p>The performance of machine translation models according to human evaluators has been extensively analyzed for bilingual models and specific domains. For example, yearly evaluations at the Workshop for Machine Translation <ref type="bibr" target="#b10">(Akhbardeh et al., 2021</ref>) examine a handful of translation directions in the news domain. Another prominent evaluation campaign (IWSLT) puts a focus on speech translation <ref type="bibr" target="#b15">(Anastasopoulos et al., 2021)</ref>. In contrast, we focus on multilingual translation. In this section, we analyze the correlation between human evaluation scores and automatic metrics such as chrF ++ , examine the difficulty of Flores-200 as judged by human evaluators in preliminary studies, and discuss variation in human evaluation scores across languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Evaluation Studies of Translation Quality.</head><p>While human evaluation is the gold standard for understanding true translation quality, automatic evaluation is critical for model design. Comparing the performance of 10 models in a parameter sweep, for example, will rely on automatic metrics. We use aggregated results from three large-scale multilingual human evaluation studies <ref type="formula">(</ref> For each large-scale evaluation study, each combination of translation model and translation direction was assigned a group of evaluators to evaluate a set of source sentence and translation sentence pairs. Each (source, translation) pair was scored by 3 evaluators, though the evaluators may (rarely) change between different pairs of (source, translation) sentences. Study B was an exception: the study was conducted in two parts, with one set of evaluators evaluating the first half of the evaluations and another evaluating the second (though evaluator overlap was allowed). The evaluated sentence pairs, calibration sentence pairs, and evaluators differed in each part.</p><p>The source, translation pairs come from the Flores-200 dataset (1,000 sentences), however some language directions in some studies were evaluated on a randomly chosen subset of Flores-200 containing only 500 sentences.</p><p>Studies A and B shared the same calibration set of 1,000 items, and Study C contained a randomly chosen subset of 500 calibration sentences drawn from the original calibration set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How does Human Evaluation Correlate with chrF ++ and spBLEU?</head><p>We find that automated metrics like spBLEU and chrF ++ correlate reasonably well with calibrated human evaluations of translation quality, as seen in <ref type="figure" target="#fig_14">Figure 24</ref>. In particular, we find that the Spearman R correlation coefficients between aggregated XSTS and spBLEU, chrF ++ (corpus) and chrF ++ (average sentence-level) are 0.710, 0.687, and 0.694 respectively. Other correlation coefficients (Kendall's ? and Pearson's R) have the same ordering. Corpus spBLEU provides the best nominal correlation, followed by average sentence-level chrF ++ with corpus chrF ++ being the least well correlated out of the three.</p><p>We also find that calibrated human evaluation scores correlate more strongly with automated scores than uncalibrated human evaluation scores across all automated metrics and choices of correlation coefficient. In particular, uncalibrated human evaluation scores have a Spearman R correlation coefficient of 0.625, 0.607, and 0.611 for spBLEU, chrF ++ (corpus) and chrF ++ (average sentence-level), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How do Human Evaluation scores differ across Languages?</head><p>We also inspect the individual score distributions for the NLLB-125 model. We observe three rough categories of XSTS score distribution. The first is high performance across the board, meaning that all All translations were generated with the NLLB-125 model, and all scores are from a single evaluation study. We show six distributions that illustrate the three rough categories of score distributions seen in our dataset.</p><p>sentences for that translation direction are rated highly. This is displayed in <ref type="figure" target="#fig_2">Figure 25</ref>'s first column. Such a distribution indicates the translation for almost all sentences evaluated has strong performance. The second pattern is shown in <ref type="figure" target="#fig_2">Figure 25</ref>'s second column, displaying varied quality. For these languages, while the average score can be high, there are many sentences that are rated poorly. Finally, the third pattern we observe is large numbers of poor-quality translations (XSTS scores of 1) along with high rates of incoherent sentences, meaning the evaluator specifically marked the translation as incoherent. These are shown in <ref type="figure" target="#fig_2">Figure 25</ref>'s third column and often represents text that is mostly incomprehensible or has completely distorted wording.</p><p>Human Evaluation for Into English v. Out of English. Several previous works (Arivazhagan et al., 2019) and our findings in Section 6 indicate that translation from various languages into English yields higher BLEU scores than translation out of English. We compare human evaluation differences in into English and out of English performance in <ref type="figure">Figure 26</ref>. Generally we find that, as suggested by automated scores like chrF ++ and spBLEU, human evaluation scores seem to also reflect that into English translation quality is typically better than out of English translation quality, with some exceptions such as snd and azj where into English performance is notably worse on both automated metrics and human evaluation metrics.  <ref type="formula" target="#formula_1">(2)</ref> wol_Latn <ref type="formula">(1)</ref> zho_Hans <ref type="formula">(1)</ref> wol_Latn <ref type="formula" target="#formula_1">(2)</ref> zho_Hans <ref type="formula" target="#formula_1">(2)</ref> mean(median(XSTS)) eng_Latn-xx mean(median(XSTS))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>xx-eng_Latn</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 26: Comparison of into English vs. out of English Translation Quality.</head><p>The left plot compares chrF ++ scores and the right plot compares mean median XSTS scores between into English and out of English translation directions. All translations were generated using the M2M-100 (12B) translation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Toxicity</head><p>Toxicity detection in digital content has received significant attention in recent years, both for user-generated language <ref type="bibr">(Kiritchenko et al., 2021;</ref><ref type="bibr" target="#b112">Mishra et al., 2019;</ref><ref type="bibr" target="#b160">Vidgen et al., 2019;</ref><ref type="bibr">Zampieri et al., 2019)</ref> and machine-generated text <ref type="bibr" target="#b121">(Bender et al., 2021;</ref><ref type="bibr" target="#b176">Xu et al., 2020)</ref>. The generation of toxic content has been explored for various sentence classification and dialogue tasks, but not extensively in translation. However, added toxicity terms, i.e., translated content containing toxic words that were not present in the original text, may have a significant negative impact beyond the overall translation quality. It could, for example, lead to decay of user trust. Our goal in this section is to provide an analysis of toxicity in multilingual MT models. We provide the first baseline to evaluate toxicity in a massive number of languages by collecting and releasing toxicity wordlists in 200 languages (Section 7.3.1). Subsequently, in Section 7.3.3 we propose and evaluate simple yet scalable toxicity detectors that can be optimized in precision or recall depending on the particular application (i.e., filtering or detection, respectively). Then, we propose a filtering strategy to mitigate toxicity imbalance in training data and visualize the source contributions of several examples with added toxicity in Section 7.3.3. Finally, we discuss open challenges and ethical considerations in Section 7.3.4. Note that in this section we will be giving translation examples that contain toxic language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">Preliminaries</head><p>What is Toxicity? Toxicity in natural language processing can be defined as the use of words or phrase structures that induce offensive utterances and bad sentiments <ref type="bibr">(Google Jigsaw, 2017)</ref>. In the context of translation, toxicity may originally be present in the source text or it can be generated de-novo in the target text (added toxicity). This added toxicity can come from a mistranslation (e.g., wrong lexical choice) or as a hallucination of a new target word from zero -both produce inaccurate translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Goal of Toxicity Detection and Mitigation.</head><p>Reliable general-purpose MT systems should be able to translate any source content adequately regardless of the domain or register, which includes translating language that may be regarded as toxic. However, they should remain faithful to the source content, and should not add through the translation process any elements of toxicity that are not found in the source. Our main purpose is to improve translation safety through minimizing the probability of catastrophic mistranslations. Note that added toxicity represents one of several types of catastrophic mistranslations <ref type="bibr" target="#b146">(Specia et al., 2021)</ref>, along with mistranslations of named entities, genders (Levy et al., 2021), numbers and units, and reversal of semantic polarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">Toxicity Lists for 200 Languages</head><p>To enable toxicity detection at scale, we focus on a wordlist-based approach. In this section, we describe what we consider a toxic item to be included in a list and how we scale the creation of these lists to hundreds of languages.</p><p>What Content is Toxic? Due to the subjective nature of toxicity, definitions of toxic language will vary. We include items that are commonly referred to as vulgar or profane language. <ref type="bibr">29</ref> In addition to these, we include items more specifically associated with depictions of pornographic content or sexual acts, some frequently used hate speech expressions, and some bullying expressions (including language that can cause trauma or be used with the purpose of silencing someone). We also include items, vulgar or not, referring to body parts that are commonly associated with sexual practices.</p><p>Collection Methodology. Toxicity is culturally sensitive, which constitutes a challenge when starting from one source language (in this case, English) and attempting to find equivalents in such a largely multilingual setting. Hate speech terms such as racial or ethnic slurs, for example, may well be the most challenging of all. We begin based on the professional translation of an initial list assembled in English, and allow additions to adapt to cultural specificities.</p><p>We iteratively designed a template for toxicity translation that provides information for disambiguation and contextualization purposes. In the latest iteration of the template, the additional information includes a breakdown into domains (e.g., slurs, sex-related terms, abbreviations), part-of-speech information, pointers to the dictionary definitions of the words in their toxic sense, indications as to the language register(s) (slang, vulgar, formal, etc.). In addition, the template provides clearly identified areas for morphological variants to be added (if the target language is morphologically rich). For polysemous terms, which may or may not be toxic depending on context, the template offers additional room and guidance as to best disambiguation practices through suggesting much less ambiguous, short n-grams (typically, 0 &lt; n &lt; 4). For the purpose of reducing cultural blind spots, another section of the template gives translators the possibility to insert common toxic language for which it may be difficult to find direct English equivalence. The translators are asked to provide explanations or verbatim descriptions. Suggestions were limited to around forty 29. Note that vulgar or profane language is not always necessarily toxic (some common slang, for instance, may be considered vulgar but it is not necessarily toxic).</p><p>items without specific restrictions as to the number of derived word forms per item apart from the general guidance of keeping within the boundaries of frequently used word forms (i.e., steering clear of infrequent and archaic word forms). Translators were not asked to produce leetspeak or nonstandard spelling variants, yet not discouraged from including them where they saw fit. Finally, the template allows for a second translator to act as a peer-reviewer, and insert comments and additional suggestions. The end product is a list of n-grams that mainly represent: common profanities, frequent insults, pornographic terms, frequent hate speech terms, some terms used for the purpose of bullying, and body parts generally associated with sexual activity.</p><p>Toxicity Lists at a Glance. To summarize our toxicity detection lists across all 200 languages, the average list length is 271 entries and the median number of entries is 143. The latter may be a better measure of central tendency than the mean average, given that languages with a rich inflectional morphology constitute extreme outliers (e.g., the Czech list has 2,534 entries, the Polish list 2,004). The shortest list has 36 entries and the longest 6,078.</p><p>Related Work. To detect toxicity in natural language, different approaches can be based on wordlists 30 or on machine-learning techniques. 31 Much recent toxicity analysis in NLP is based on the training of supervised classifiers at the sentence level mostly for English, and extended up to a few other languages <ref type="bibr">(Lees et al., 2021)</ref> in multilingual classification. However, we are not aware of a machine translation analysis that evaluates added toxicity, or toxicity imbalances in parallel training datasets; and scales these analyses to a large number of languages. In general, this training of supervised classifiers requires large amounts of labeled data, which means they are often limited to high-resource languages. But, more importantly, recent studies have claimed the strong biases that these classifiers have in wrongly associating the language of underrepresented social groups with toxicity <ref type="bibr" target="#b175">(Xu et al., 2021a)</ref>. While we can not catch non-lexical toxicity nor toxicity terms that are not included in our wordlists, our approach is more scalable and thus capable of extending to hundreds of languages. Furthermore, using a wordlist-based approach provides higher degrees of transparency and explainability. 32 Finally, our wordlists can potentially be used for other applications in NLP. We discuss possible limitations of wordlists in Section 7.3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.3">Toxicity Detection</head><p>Methodology. Our toxicity detectors identify toxicity using the following two criteria: number of toxic items and matched toxicity. This is illustrated in <ref type="figure" target="#fig_3">Figure 27</ref>. We define toxic items as short n-grams (with 0 &lt; n &lt; 4, most commonly) present in our lists described in Section 7.3.2. Among the number of toxic items we explore two cases: 1 or more toxic item and 2 or more toxic items. The toxicity can either be matched or non-matched, where non-matched indicates that there are toxic items either in the source or in the target part of the bitext. In contrast, matched toxicity indicates that toxic items are present on both sides of the bitext, but it does not necessarly mean that toxic words are correctly translated.  Therefore, we have the following toxicity detectors' options: (1) 1+ toxic item non-matched, (2) 2+ toxic items non-matched; (3) 1+ toxic items matched; (4) 2+ toxic items matched.</p><p>A toxic item (or phrase) is considered detected if it is present in a line to be examined, and it is surrounded by spaces or the start/end of a line (e.g., we do not detect bass if looking for ass). We track the number of unique toxic items found in a line, but do not count a phrase again if it has multiple occurrences. For some languages (i.e., Assamese, Burmese, Oriya, Korean, Chinese) space tokenization is not sufficient to distinguish words from one another. In those cases we utilize SentencePiece tokenization of both the sentences and the toxic wordlist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation.</head><p>To quantify the quality of our toxicity lists and detectors, we aim to utilize a general-purpose toxicity benchmark first before application to machine translation. We used the Jigsaw dataset, 33 which aims to take an input sentence and detect if this sentence is toxic or not. This dataset classifies toxicity among the following sub-classes: toxic, any toxic, obscene, threat, insult, severe toxic, identity hate.</p><p>We used the test set partition available from the first challenge and filtered comments by applying these criteria: lines with less than 600 characters, less than 100 words, and with at least 1 word token. Then, we filtered down to only those rows which actually contained at least 1 English letter to filter out only numbers, emoji, etc. Our final set is 86.2% benign and the remaining 13.8% sentences have 1 or more toxic labels (with non-exclusive classes). As no comparable baseline exists over so many languages, we provide the performance of better than chance 'random baseline model' which we compute as randomly generating the toxic/non-toxic labels (both at random 50/50 toxic/non-toxic rates, and at 13.8%/86.2% toxic/non-toxic rates). Then, we use NLLB-200 translation model to translate this English set into all other available languages in the model. Note that using a translation model to generate non-English toxicity references will affect the analysis of quality of our detectors as translation can modify the toxicity level in the references. However, we did not have the alternative of evaluating on toxic labelled data for our 200 languages. <ref type="table" target="#tab_6">Table 26</ref> reports the results of our detectors in English, French (as an example of high-resource language), and an average over all languages with chrF ++ &gt; 45. The best 33. https://www.kaggle.com/c/Jigsaw-toxic-comment-classification-challenge  <ref type="table" target="#tab_6">Table 26</ref>: Detecting Toxicity with Various Methods. We display performance for random choice + majority class baseline, English as source, French as translation from English, and the mean score over all translation languages with a chrF ++ score greater than 45. In bold best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recall Precision FPR FNR</head><p>performing toxicity detector has 66.8% F1 score in English and 61.6% of F1 score in the high-resource language French. These are far better than our baselines of better than chance. <ref type="figure" target="#fig_20">Figure 28</ref> shows decreasing F1 scores for 1+ toxic item vs 2+ toxic items detectors. Dotted lines in the figure are random chance performance. The language set is a uniform sample of several high and low-resource languages. In order to minimize the confounding factor of quality of translation and detectors, we limited the sample to those with chrF ++ above 45 as model quality. The mean is computed both on languages with chrF ++ above 45 and on all languages, without taking into account the chrF ++ threshold. On average, our detectors are better than chance, even for low-resource languages, which demonstrates the usefulness of our detectors. However, there is a significant difference between high and low-resource languages. The results for the languages with the lowest performance in <ref type="figure" target="#fig_20">Figure 28</ref>, i.e., Hindi (hin_Deva), Kannada (kan_Knda), Maithili (mai_Deva), Telugu (tel_Telu), and Magahi (mag_Deva), may be partially explained by the fact that the scripts in which these languages are written are not always adequately tokenized by our detectors. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimated Toxicity in the Training Data.</head><p>We used our detectors (1+ toxic item non-matched) to count potentially toxic items in the bitext training data. <ref type="figure" target="#fig_4">Figure 29</ref> shows percentage of toxic items per corpora in the English-side of our bitext pairs only. <ref type="figure" target="#fig_1">Figure 30</ref> shows the percentage of toxic items per corpora in the non-English-side. Since we are comparing across languages, we use the Christian Bible corpus to calibrate the baseline level. We use this corpus since it is highly multilingual and it should be the most consistent in content, quality and level of toxicity across languages, if the Christian Bible corpus was not available, results are left uncalibrated). We see that the amount of potential toxicity varies among languages and corpora. Overall, OpenSubtitles and qed have a larger amount of potential toxicity in several languages and e n g _ L a t n i t a _ L a t n d e u _ L a t n f r a _ L a t n d a n _ L a t n e l l _ G r e k p o r _ L a t n p o l _ L a t n n l d _ L a t n t u r _ L a t n e s t _ L a t n m k d _ C y r l b u l _ C y r l v i e _ L a t n e u s _ L a t n l i t _ L a t n l i n _ L a t n u k r _ C y r l f i j _ L a t n k i n _ L a t n h a t _ L a t n c y m _ L a t n s m o _ L a t n s o t _ L a t n u z n _ L a t n h i n _ D e v a m a i _ D e v a t e l _ T e l u m a g _ D e v a k a n _ K n d a M e a n c h r F + + &gt; a n _ L a t n n l d _ L a t n e s t _ L a t n f i j _ L a t n f r a _ L a t n l u g _ L a t n d e u _ L a t n e l l _ G r e k g r n _ L a t n h a t _ L a t n k h k _ C y r l h i n _ D e v a i t a _ L a t n j p n _ J p a n k a m _ L a t n k a n _ K n d a k i n _ L a t n l i n _ L a t n l i t _ L a t n m Mined Data has a lower amount. We examined a sample of the potential toxicity of these corpora and found that a great proportion were misaligned bitext. Toxicity was present only in one side of these bitext, either through complete omission (by far the most common) or significant detoxification. Training in this kind of misaligned bitext can encourage mistranslations with added toxicity.</p><p>Mitigating Toxicity by Filtering the Training Data. Added toxicity can arise from toxic items present in the training data. The objective of data filtering is to detect imbalances in toxicity, which could eventually generate toxic mistranslations. Note that we do not want to filter out all toxicity from training corpora because this would introduce bias and prevent the model from correctly translating potentially toxic items, even under benign usage. Thus, we are interested in discarding sentences that are really toxic on one side and not on the other, optimizing for precision. The analysis on detector quality reveals that when our detectors detect multiple instances of toxicity (2+ toxic items) in a sentence, we have a higher precision in toxicity detection. Based on this, we propose to filter training sentences that contain a difference of multiple toxic items when comparing the source and target sides. a n _ L a t n n l d _ L a t n e s t _ L a t n f i j _ L a t n f r a _ L a t n l u g _ L a t n d e u _ L a t n e l l _ G r e k g r n _ L a t n h a t _ L a t n k h k _ C y r l h i n _ D e v a i t a _ L a t n j p n _ J p a n k a m _ L a t n k a n _ K n d a k i n _ L a t n l i n _ L a t n l i t _ L a t n m   <ref type="table" target="#tab_6">Table 27</ref>: Experiments with Filtering for bilingual translation models. We report for each direction the number of training sentences, chrF ++ and added toxic items.</p><p>To test this methodology, we included toxicity filtering to the filtering pipeline which will be described in more detail in Section 8.1.4. We then trained bilingual models with and without this filtering using the architecture described in Section 6.5. <ref type="table" target="#tab_6">Table 27</ref> shows results on the Flores-200 dev. This table includes the number of training sentences, the translation quality (chrF ++ ) and the number of toxic items in the translation output. All figures are reported before and after filtering. We observe that a filtering pipeline which includes toxicity filtering not only improves translation performance but also tends to reduce the number of toxic items in the translation.</p><p>Interpretability of Added Toxicity. Added toxicity in translation output may come from hallucination or mistranslation. Note that hallucinated toxicity means that there is a toxic item in the target output that appears without having any direct source correspondence. In contrast, mistranslated toxicity means that we are translating a non-toxic source word into a toxic target word. To differentiate between these two cases cases, we use ALTI+ <ref type="bibr">(Ferrando et al., 2022)</ref>, which enables examination of source and target prefix contributions to model predictions. <ref type="figure" target="#fig_0">Figure 31</ref> reports the input attributions of a particular word. Each example contains the source sentence, the target output sentence and the translation English reference sentence.</p><p>? Sentence 1, which is a translation from Wolof to English, shows the input attributions of the word idiots. We observe that the input contributions of idiots is concentrated on the target prefix word of self-absorbed. The total source contribution for the predicted word idiots is 0.35, which is significantly lower than the total target prefix contribution of 0.65. Again, this is coherent with the fact that this word is hallucinated, since it does not have a direct correspondence to a source token.</p><p>? Sentence 2, which is a translation from Quechua to English, shows the input attributions of the word ass. We observe that the input contributions of ass is concentrated on the target prefix words of pain, in, the. The total source contribution for the predicted word ass is 0.20, which is significantly lower than the total target prefix contribution of 0.80. This is coherent with the fact that this word is hallucinated, since it does not have a direct correspondence to a source token.</p><p>? Sentence 3 shows the input contributions of the word penis, when translating from Northern Kurdish to English. We observe that these contributions clearly include the source word Penceya. The total source contribution for the predicted word penis is 0.68, which is significantly higher than the total target prefix contribution of 0.32. We conclude that this word is mistranslated, since it does have a direct correspondence to a source token Penceya, which should be translated as claw.</p><p>? Finally, Sentence 4 shows the input contributions of the word Nazis, when translating from German to English. The total source contribution for the predicted word Nazis is 0.77, which is significantly higher than the total target prefix contribution of 0.23. This example is added to compare the behaviour of ALTI+ in an accurate toxic translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.4">Open Challenges in Toxicity for Machine Translation</head><p>There exist many open research questions for the challenge of detecting toxicity at scale for hundreds of languages. Since we are evaluating our approach in a translated dataset, the quality of translation may be a confounding factor that will be worth exploring. For example, the quality of the toxicity detection can be affected by the amount of resources available per language. Alternatively, the quality and efficiency of our detectors which detect or filter toxicity may vary depending on the wordlist length, the segmentation accuracy, the variation of toxicity class, and the amount of non-lexicalised toxicity. Challenges in the wordlists include morphological aspects such as case sensitivity. Differences in casing are often related to word order; i.e., the initial characters of words in sentence-initial position are often capitalized. Our detectors lowercase all items prior to detection, which makes it impossible to differentiate between certain homographs that differ only in the casing of their initial characters. The expansion and disambiguation of small toxicity lists are critical areas for future work, which likely require close collaboration with a larger number of native speakers. A first step towards disambiguation can be to contextualize polysemous words by replacing single tokens with n-grams that have a much higher probability of representing true toxic content. Finally, we know that added toxicity can be caused by the phenomena of hallucination or by an error in the translation. Our visualization examples with alti+ show that a low amount of source contribution in the toxic item, computed with this method, is a Hallucinated Toxicity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mistranslated Toxicity</head><p>Sentence 1 Sentence 3</p><p>Montaru biologie ay osiyat?r yu yor seen bopp la?u yuy wey ci ab diir bu moom boppam donte amul ay ju?ju biti.</p><p>Penceya w? ya didoy? mezintir b?, ? b? sebeb? nav? Hesperonychus-?</p><p>It's about a bunch of self-absorbed idiots who live in their own little world, and they don't have time for anything else.</p><p>His dodo's penis was larger, and was the reason for the name Hesperonychus Reference 1 Reference 3</p><p>Biological clocks are self sustaining oscillators which will continue a period of free-running cycling even in the absence of external cues.</p><p>Its second claw was larger, giving rise to the name Hesperonychus which means "western claw."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correctly Translated Toxicity</head><p>Sentence 2 Sentence 4</p><p>Kayqa nanaykuna ta?ichiqina imaymanamkunapi. Machen Sie keine Witze ?ber den Holocaust oder die Nazis.</p><p>It's a pain in the ass. do not make jokes about the Holocaust or the Nazis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference 2 Reference 4</head><p>This is just like symptomatic treatment in many cases.</p><p>Do not make jokes about the Holocaust or Nazis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 31: ALTI+ Visualizations of Source and Target Contributions.</head><p>Hallucinated toxicity (sentence 1) shows input contributions for the toxic item idiots in Wolof-to-English; (sentence 2) shows input contributions for the toxic item ass in Quechua-to-English; Mistranslated toxicity (sentence 3) shows input contributions for the toxic item penis in Kurdish-to-English. English reference is shown in each example; Correctly translated toxicity (sentence 4) shows a perfect translation for comparison with previous examples.</p><p>strong indicator for hallucination. We want to use this information to further quantify and mitigate added toxicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.5">Ethical Considerations for Toxicity Research</head><p>The evaluation of machine translation has been deeply studied in terms of quality assessment both using automatic and human annotation approaches. Recently, responsible MT evaluation is emerging, motivated by the more general responsible artificial intelligence area. This evaluation aims at measuring fairness, ethical, and social aspects of our technologies. For example, it allows us to quantify the toxicity or biases that our model keeps, generates, or potentially amplifies; e.g., Blodgett et al. <ref type="formula" target="#formula_1">(2020)</ref>; Costa-juss? (2019); <ref type="bibr">Renduchintala and Williams (2021)</ref>. Among the different alternatives in responsible evaluation, we have started with the toxicity challenge because it contributes highly to harmful internet content (see Section 2). In this subsection, we discuss ethical issues related to toxicity detection in translation.</p><p>Unintended use of Toxicity Lists. It should not be assumed that our lists could be used to moderate content or suppress machine-generated toxic language. As is the case with human translators, we believe that machine-translation systems should remain faithful to the input text, which is why we focus solely on detecting added toxicity. 34 Even informative and 34. A single exception was made in the case of backtranslated data, due to the nature of web crawled data which contains a high proportion of pornographic and toxic content.</p><p>educational sources that contain either some degree of toxicity or other tokens that may not be toxic in all contexts but will be matched by our lists; for example, some Wikipedia pages contain descriptions of sexual acts, and others contain vocabulary describing intimate body parts for the purpose of providing information about human biology and health. However, we acknowledge that our toxicity lists could be used for other purposes that are different from, or opposed to, ours. They could be used to inform adversarial attacks against toxicity classifiers <ref type="bibr">(Kurita et al., 2019)</ref>, or for the enforcement of policies that aim to surveil or suppress toxic language. While acknowledging potential unintended uses of our lists, we remain mindful of the likely possibility that similar lists may have already been separately collected for the aforementioned purposes by entities who have not publicly disclosed them.</p><p>Biases in List Building. It is likely that toxicity lists themselves include biases due to errors, omissions, or insufficiently diverse cultural backgrounds <ref type="bibr" target="#b46">(Davidson et al., 2019;</ref><ref type="bibr" target="#b63">Gehman et al., 2020;</ref><ref type="bibr" target="#b144">Ross et al., 2017;</ref><ref type="bibr">Sap et al., 2019)</ref>. We hypothesize two major causes for these biases: (1) ambiguities that are inherent to toxic language itself and (2) cultural biases that can be introduced at any step of the list-building process, since our lists were for the most part translated from English. First, we discuss ambiguities in toxic language itself. Due to the discomfort or potential harm toxic language can cause, there have always been attempts to curb its use in social interactions. This, in turn, has led social actors to design new means of expression to circumvent censorship or avoid opprobrium; metaphors and innuendos would be good representatives of such means of expression. For example, common linguistic camouflage schemes include referring to animal names for insults or references to body parts. For these reasons many words and phrases that make part of toxic language are also common words that can be used innocuously. Conversely, what may now be considered very specific technical terms (but were more widely used at different times in our history) have been known to sometimes take on a toxic meaning (e.g., the English noun slag). These linguistic phenomena cause ambiguities, which in turn lead to mistranslations. To prevent misunderstandings and resolve ambiguities, the latest mitigation steps included the creation of a one-hour translator training session and the search for more accurate n-grams as replacements for ambiguous single tokens. Periodic iterative testing on a variety of input texts followed by both quantitative and qualitative analyses of the results allow for more accurate n-gram substitutions, although it remains clear that such continuous improvement method eventually reaches a point of diminishing returns.</p><p>Apart from known ambiguities that are inherent to toxic language, cultural biases can also infiltrate toxicity lists either via the building of the reference list or via the translation process. Cultural biases can be introduced by the fact that the first drafts of almost all lists were translations from English (mostly American English). Some English entries have no clear direct equivalents in other languages. In this case, errors and biases can be introduced when expectations are not clearly stated and translators attempt to provide translations for all entries at all costs. In addition to clarifying expectations, this problem can be mitigated by asking the translators, as an intermediate step in the translation task, to indicate whether they think that a direct equivalent can be found in their native language, which helps them remember that they should refrain from providing translations if such is not the case. Conversely, some toxic items in languages other than English have no clear direct equivalents in English, which further increases the coverage discrepancy in between lists. Such discrepancy has proven more difficult to mitigate. Unrestricted additions generate noise, while restricted additions generate friction. This puts into perspective the efficiency of the alternative to list building through translation of a reference list, which would be building lists solely by collecting free-form suggestions.</p><p>Finally, there are also potential biases in the translation process. Biases can be introduced through unequal availability of translators and consultants across languages. For highresource languages, numerous translators and consultants are available and compete on the language service market. This is unfortunately not the case for low-resource languages, where access to fewer professionals with diverse backgrounds and specializations increases the risk of biases in favor of a single worldview.</p><p>Translator and Consultant Safety. By definition, the toxicity area of work is one where translators and consultants may feel too uncomfortable to work <ref type="bibr" target="#b168">(Welbl et al., 2021)</ref>. Additionally, regardless of one's willingness to work on potentially toxic language, we are mindful of the fact that the production of toxic language, be it through translation, may not be perceived equally in all parts of the world. In parts of the world where free speech is generally protected by law, toxic language may be seen as offensive, triggering, or harmful, and its unsolicited use can be subject to consequences in certain venues (such as in the workplace), but anyone is theoretically free to voluntarily work on the collection, curation, translation, or analysis of toxic language. We cannot safely assume that such is the case in all other parts of the world. Bearing this in mind, translators and consultants are made aware that the work contains toxic language, and can reject it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Conclusion</head><p>Evaluation plays a vital role in the development of machine translation systems, and also allows potential users to assess if translation quality is good enough for their purposes. For system development, we rely on automatic metrics. Due to the diverse set of languages we address, with some lacking visible indicators of word boundaries, we have to move beyond the common practice of using BLEU scores. We use two metrics: spBLEU, a modification to BLEU that uses a sentencepiece model to ensure tokenization of any text in any language, and chrF ++ that operates primarily on character-level.</p><p>While automatic scores are an essential tool, they fail in one important aspect: providing meaningful scores that enable deployment decisions. Hence, we add human evaluation as a final system evaluation step. However, due to variance between human evaluators, this does not guarantee meaningful scores either. We achieve the goal of meaningful scores by a new and more clearly defined scoring metric (XSTS) and the use of a calibration set that not only allows us to adjust raw scores from evaluators working on the same language pair but also to obtain consistent scores across language pairs. Moreover, we are concerned about harmful content generated from our translation models. We developed novel methods to detect toxicity, i.e., translated content containing toxic words that were not present in the original text. We were able to extend this work to all 200 languages of the NLLB effort due to a language resource that we created for all these languages: lists of toxic terms. Armed with this resource, we built classifiers to detect and mitigate toxicity in our translations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Bringing it All Together</head><p>We have seen in previous sections how multiple areas of interdisciplinary research have addressed different parts of the problem of scaling human-centered translation for lowresource languages. Buttressed by our guiding principles described in Section 2.2, we bring all the interdisciplinary work from the previous sections and combine them in a manner that improves the performance of multilingual translation systems for low-resource languages from underserved communities. We share everything transparently, our roadblocks and successes. To enable the community to leverage and build on top of NLLB, we open source all our evaluation benchmarks, models, datasets, training and inference code as well as other modeling and data artifacts. Driven by our guiding principle of being reflexive, we hope sharing our work and tools will help the community to examine the current practices and improve where we fail, in a mission towards the north star goal of no language left behind.</p><p>In the following sections, we describe how we combine our different datasets and different improvements from previous sections to build one massive multilingual machine translation model, NLLB-200 covering 202 languages. We compare the performance of NLLB-200 on Flores-200, with both automated metrics and human evaluation results and demonstrate we get state-of-the-art performance. We analyze toxicity in NLLB-200 generations and study the extent to which data and toxicity filtering methods can help reduce it. We also compare the performance of NLLB-200 on Flores-101 and several common MT benchmarks and show NLLB-200 generalizes well and achieves competitive performance. Next, we discuss model distillation and how some of our distilled models are providing translations for Wikipedia. Following that, we discuss transliteration and dialectal translations, and where <ref type="figure" target="#fig_1">Figure 33</ref>: Final Dataset Construction. We depict how we combine together various data sources, data augmentation and data filtering techniques. The datasets created in our effort are shown in blue, and models in orange. multilingual systems have room to improve these. Finally, we discuss the environmental impact of NLLB-200 and the implications of compute-efficient decisions in scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Preparing the Data</head><p>We describe how we aggregate various data sources discussed in the previous sections and augment with additional data through backtranslation. We experimentally demonstrate the advantages of leveraging large-scale mined and backtranslated data to significantly improve the performance of low-resource translation. Our overall process is summarized in <ref type="figure" target="#fig_1">Figure 33</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.1">Training a Tokenizer for 200+ languages</head><p>To represent the 200+ languages of No Language Left Behind, we trained a new SentencePiece (SPM; Kudo and Richardson, 2018) tokenizer. To train this SentencePiece model, we sample a total of 100M sentences from primary bitext corpora. Given that most of the languages in NLLB are low-resource languages (150), uniform sampling would over-represent high-resource languages and under-represent low-resource languages, leading to too much fragmentation of low-resource language text. To mitigate this, we apply temperature sampling (with temperature T = 5), which effectively downsamples high-resource languages and upsamples low-resource languages. This results in a more balanced distribution of samples over all languages.</p><p>To validate the quality of the SPM, we first examine the rate of unknown tokens (&lt;unk&gt;) for each language. We observe that even after using a high temperature for sampling, certain languages such as zho_Hans, zho_Hant and yue_Hant had higher &lt;unk&gt; error rates, due to the very large character set of their scripts. To compensate, we further upsample those specific languages by a factor of 5 during training. With these modifications, the &lt;unk&gt; error rate for all languages is below 1%. Another important factor for quality is the tokenization rate, or the average number of tokens per sentence for each language <ref type="bibr" target="#b109">(Mielke et al., 2021)</ref>. Since SentencePiece identifies subword units based on language perplexity (roughly, frequency), underrepresented languages tend to have a higher tokenization rate than high-resource ones, leading to a near character-based model for those languages. This makes modeling more challenging, especially for long range dependencies and for synthesizing words from near character-level tokens. Based on the above two factors, we choose a vocabulary of size 256,000 for our SentencePiece model to allow for enough capacity to represent the wide spectrum and large number of languages we cover. As we achieve reasonable tokenization quality with a vocabulary size of 256k, we do not train SentencePiece models with even larger vocabulary sizes (e.g. 384k or more), as a larger vocabulary size would significantly increase the number of model parameters.</p><p>To evaluate with spBLEU, we use this SPM-200 as the tokenizer to better support the languages of Flores-200. We open source this SentencePiece model along with the Flores-200 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.2">Datasets</head><p>To train our systems, we leverage three different types of data, as listed below: Primary Bitext. We use a set of publicly available parallel corpora from a variety of sources, including NLLB-Seed (Section 4.2). We add a total of 661 sets of primary bitext data. We choose all English-centric sets when available and also add non-English-centric pairs if they have a low resource language either as source, target, or both. <ref type="table" target="#tab_6">Table 52</ref> in Appendix E.1.1 provides further information on the list of public bitext corpora we used for training. We use the term Primary to refer to this type of data.</p><p>Mined Bitext. We use bitext corpora retrieved by large-scale bitext mining, as detailed in Section 5.3. We add mined data for a total of 784 directions. These include all the English-centric directions and a subset of non-English-centric directions. Non-English-centric mined data is effective in improving performance of multilingual translation systems <ref type="bibr" target="#b151">(Fan et al., 2020)</ref>. However, 200+ languages implies more than 40,000 non-English-centric pairs. Adding all the pairs could be detrimental, as some pairs will not have high quality mined bitexts. To subselect based on projected quality, we first pick the directions which have xsim error rate under 5. As a further restriction, we add mining data primarily for pairs containing low-resource languages within a given language family or a geographical region. This is an imperfect approximation to ensure improved transfer learning between similar languages. In Appendix E.1.3, we share the full list of bitext mined pairs used for training. We use Mined to refer to this type of data.</p><p>Monolingual Text. Details about our monolingual data can be found in Section 5.2. We use monolingual data for a total of 192 languages. This data is used for self-supervised learning and for generating backtranslated data (described in the next section). We use Monolingual to refer to this type of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.3">Large Scale Backtranslation</head><p>Backtranslated data provides a form of weak supervision which is crucial for improving translation performance of low-resource languages. As we observed in Section 6.4.1, combining backtranslation data generated from multiple sources improves performance of a translation model due to increased backtranslation diversity. Following this result, we generate backtranslated data from two models: (1) a multilingual neural machine translation model (MmtBT) and (2) a set of bilingual statistical machine translation models (SmtBT). We next describe how we chose the language pairs for backtranslation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identifying Backtranslation Directions.</head><p>While effective, backtranslation is computationally expensive as it requires training massively multilingual as well as bilingual models and generating translations for up to tens of millions of sentences per direction. We describe how we identify which translation directions would benefit the most from augmented backtranslation.</p><p>We first train a baseline multilingual neural machine translation model (3.3B parameters, dense) on a dataset composed of all the primary bitext pairs and English-centric mined pairs. We train this model in a multitask learning setup, comprised of both MMT and SSL tasks, to produce the strongest possible model. We find that it is extremely important to backtranslate with the best possible model, as the quality of generated backtranslations is highly correlated to the performance of the model used to generate BT data (see <ref type="figure" target="#fig_1">Figure 43</ref> for details). In Section 6.3, we observed that training on self-supervised objectives in addition to the MMT task in a multitask setup improves performance particularly when trained on Primary+Mined data (see Section 8.2.1). Improvements on xx-eng_Latn directions are more significant compared to eng_Latn-xx directions for low-resource languages. This is an added advantage for a model to generate backtranslations for eng_Latn-xx.</p><p>After training the above baseline model, we select the subset of languages for which we generate SmtBT data. We select every language xx for which the baseline model achieves spBLEU&lt; 10 on eng_Latn-xx directions or spBLEU&lt; 15 on xx-eng_Latn directions on Flores-200 dev set. These thresholds are chosen to keep the number of backtranslated directions manageable given computational constraints, and are also informed by previous preliminary experiments which showed that gains from using SmtBT were concentrated in directions on which the baseline model gave poor performance. This is usually true for very low and underserved languages. For each of the selected languages we then produce SmtBT data both in and out of English.</p><p>Finally, we identify the directions for which to obtain MmtBT data. As neural models are particularly effective for high-resource scenarios, we increase the threshold to capture a wider range of languages. We select every direction into and out of English where the Flores-200 dev set performance of the baseline model is below 30 spBLEU. Overall, using these criteria, we selected 76 English-centric directions for backtranslation through the SmtBT pipeline and 261 directions through MmtBT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scaling Backtranslation Generation.</head><p>To perform MmtBT on a total of 261 directions with a 3.3B-parameter dense neural model, we leverage the model inference/generation framework in fairseq . For SmtBT, we backtranslate 76 directions using the same MOSES setup described in Section 6.4.1. This setup consists of individual CPU-bound bilingual models, and the cost of scaling is linear in the number of directions. By optimizing our pipelines to improve GPU/CPU utilization, we improve efficiency of this expensive process to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.4">Filtering Strategy</head><p>The addition of data from backtranslation and mining can yield considerable gains in model performance. However, these processes typically yield sentence pairs that are much noisier than human-translated data. To benefit from such data while at the same time limiting the negative effects of noise, it is necessary to perform a series of bitext filtering steps. Corpus filtering has been used for many years in machine translation <ref type="bibr" target="#b88">(Koehn et al., 2018</ref><ref type="bibr" target="#b70">(Koehn et al., , 2019</ref>. Our filtering pipeline performs several types of checks meant to determine whether a given data point is unlikely to be a real translation pair. We can divide these into several families.</p><p>LASER Filtering. The LASER filter operates on mined data and can remove bitext pairs whose LASER score falls below a given threshold (see Section 5.3). For all our mined bitext filtering we apply a threshold of 1.06 <ref type="bibr">(Schwenk et al., 2021b)</ref>.</p><p>Length Filtering. The length-based filter identifies sentences that are not within provided maximum and minimum length ratios. It also filters out sentence pairs with highly skewed length ratios, as such pairs are typically unlikely to represent real translations.</p><p>The concept of length is not straightforward when working with a massively multilingual model due to the inherent differences in sentence length distributions across different languages. In order to account for this phenomenon, we measure sentence lengths based on Unicode code points, and apply a language-specific correction factor. For a given language , the factor is computed as ? = Neng_Latn N , where N is the total length of the Flores-200 validation set for language . Concretely, multiplying all lengths by this factor allows us to express a single length threshold in terms of typical English-language character lengths, and have a length discount or penalty applied for languages with generally shorter or longer sentences respectively. For mined and backtranslated data we filter out sentences with length ratios above 9.0. For backtranslations we additionally filter out sentences of length below 15 (corresponding to about three words in English) as we empirically found such short sentences to be a source of noise. LID Filtering. Another important step is to discard pairs whose sentences do not appear to be written in the expected languages. This can be performed automatically using language identification (Section 5.1), with thresholds chosen appropriately based on the reliability of LID scores for each given language.</p><p>Toxicity Filter. Based on the techniques proposed in Section 7.3, we implement a toxicity filter. This removes sentence pairs which have toxicity imbalance, i.e., when the difference in number of toxic items detected in source and target is above a certain threshold. An alternative mode of operation supported by the filter is to remove all pairs in which one or both sentences contain toxic elements above a given threshold, regardless of their relative difference. Further details are available in Section 7.3.3.</p><p>Deduplication. <ref type="bibr" target="#b109">Lee et al. (2021)</ref> demonstrates that training data deduplication is critical for large language model training. Deduplication is important for machine translation as well, so in the final step of the filtering pipeline we remove duplicates. In order to determine if two texts are duplicates, we apply a normalization process which removes punctuation and non-printing characters, then replaces all digits. The filtering can remove pair duplicates, defined as cases where two data points have identical source and target; or source/target duplicates, i.e. data points that have the same source side but might have different targets and vice versa. Source deduplication is useful for backtranslated data to catch cases where the backtranslation model decodes to the same sequence regardless of its input. Target deduplication is useful for Mined, where same source sentences are aligned with multiple target sentences. The effect of filtering is discussed in detail in Section 7.3.3, where we experiment with training bilingual models for six directions on filtered and unfiltered data, comparing their performance as well as the amount of hallucinated toxicity produced. The results in <ref type="table" target="#tab_6">Table 27</ref> confirm that filtering leads to an improvement in performance as well as a reduction in hallucinated toxicity. Our filtering configuration for various data sources is made available along with our other training scripts. 35</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.5">Effect of using Different Data Sources on Performance</head><p>We expect to have cumulative benefits by combining the different sources of data. We empirically investigate this hypothesis in this section.</p><p>Experimental Setup. We train dense 3.3B Transformer encoder-decoder models with model dimension 2048, FFN dimension 8192, 16 attention heads and 48 layers (24 encoder, 24 decoder) for these data ablation experiments. We train these models on three sets of data:</p><p>(1) Primary, (2) Primary+Mined, and (3) Primary+Mined+MmtBT+SmtBT to compare the cumulative improvements coming from adding each source of data. All models are trained for a total of 300k iterations and we report the results with best chrF ++ score checkpoints. <ref type="figure" target="#fig_1">Figure 34</ref>, we demonstrate the impact of adding different data sources over Primary data. We aggregate results over language pair type and resource level. We observe that across all language pairs, performance improves significantly by adding Mined data and further by adding MmtBT+SmtBT backtranslated data. Focusing our observation on resource levels, we observe that low-resource languages improve more compared to highresource languages. This is not surprising, as high-resource languages already have significant amounts of Primary bitext data publicly available. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Mining and Backtranslation on Very Low-Resource Languages.</head><p>Looking deeper at the results, we investigate how mined and backtranslated data sources impact very low-resource languages. We define very low-resource as languages with fewer than 100K unique sentence pairs across all language pairings available in public bitext corpora, with 84 total. On aggregate, our proposed techniques of mining and backtranslation improve low-resource and very low-resource language directions significantly (see <ref type="figure" target="#fig_1">Figure 34</ref>). Most prominently, very low-resource into English directions improve by +12.5 chrF ++ with mined data and +6.1 chrF ++ with additional BT data, with an overall improvement of +18.6 chrF ++ .</p><p>Similarly, we observe that out of English directions improve by +4.7 chrF ++ when adding mined data and +1.9 chrF ++ when adding backtranslated data, with an overall improvement of +6.6 chrF ++ . For non-English-centric pairs, we see an improvement of +7.5 chrF ++ when adding mined data and +1.4 chrF ++ when adding backtranslated data, with an overall improvement of +8.9 chrF ++ . These results show that our improvements in bitext mining and backtranslation increase the data quantity as well as quality for low-resource languages that are often underserved or excluded by existing translation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.6">The 200 Language Dataset</head><p>We observed the benefits of both mining and backtranslation on low-resource languages. Combining all the sources of data, we prepare our final dataset, covering 202 languages. <ref type="bibr">36</ref> The dataset comprises primary bitext for 661 language pairs, mined bitext for 784 language pairs, and 261 directions of backtranslated bitext. In total, there are 1220 language pairs or 2440 directions (xx-yy and yy-xx) for training. These 2440 directions sum to over 18B total sentence pairs. <ref type="figure" target="#fig_1">Figure 35</ref> displays the distribution of samples across the 1220 language pairs -the majority of the pairs have fewer than 1M sentences and are low-resource directions.</p><p>36. Two languages among the 204 in Flores-200, arb_Latn and min_Arab, have no available training data and hence we did not include them in the model training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Preparing the Model</head><p>In the previous section, we discuss how we improve data quantity and quality through mining, backtranslation, and filtering, leading to significant gains in model performance on lowresource languages. In this section, we discuss how we scale and adapt our model architecture and training procedure to build multilingual machine translation models for more than 200 languages and thousands of language directions. Training large models in a massively multilingual setting is a challenging problem due to the extreme data imbalance between language pairs as shown in <ref type="figure" target="#fig_1">Figure 35</ref> and varying levels of translation difficulty. Learning objectives in the multilingual setting have complex and unknown dynamics and often compete with each other due to gradient interference <ref type="bibr" target="#b166">(Wang et al., 2020c)</ref>. Low-resource language pairs quickly overfit while high-resource language pairs usually benefit from longer training. Overall, these conflicting training dynamics make it a difficult optimization problem. We addressed some of these challenges in Section 6.2 by showing how Sparsely Gated Mixture of Expert models with different regularization strategies and curriculum learning help improve the performance of massively multilingual machine translation models, especially for low-resource languages. In Section 6.3, we demonstrated how monolingual data can be leveraged to improve multilingual machine translation via self-supervision in the form of an additional denoising autoencoder task during training. In Section 6.4, we saw another way of leveraging monolingual data through large-scale backtranslation.</p><p>We now apply these strategies on the full training dataset as described in the previous section. First, we analyze the benefits of self-supervised learning (SSL) with the denoising autoencoder (DAE) task when training with and without backtranslated data. This helps us understand whether SSL helps further on top of mining and backtranslation, since all the approaches leverage the exact same monolingual data. Next, we apply the most promising regularization and curriculum learning strategies from Section 6.2 and train Sparsely Gated Mixture of Expert (MoE) models on the full dataset. We analyze the impact of MoE layer frequency in the model. With the best MoE layer frequency and regularization strategy, we then analyze the impact of introducing language pairs with a curriculum, based on the overfitting properties of each language pair. Based on these experiments, we propose the model architecture and training recipe to build our final model, NLLB-200, a massively multilingual machine translation model covering 202 languages and capable of translating 40k+ language directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.1">Does Self-supervised Learning help on top of Mining and</head><p>Backtranslation?</p><p>Using large scale bitext mining as discussed in Section 5.3, we already leverage a subset of the monolingual sentences in low-resource languages that can be successfully mapped to sentences in other languages. As detailed in Section 6.3, self-supervised learning objectives can help further leverage monolingual data to improve performance on low-resource languages <ref type="bibr">(Bapna et al., 2022;</ref><ref type="bibr">Liu et al., 2021a;</ref>. We saw in Section 8.1.3 that self-supervised learning brings strong improvements to the multilingual neural machine translation model trained on primary bitext and mined data, and that model is then used to generate backtranslation data. It is important to note that the same monolingual data is again used for backtranslation generation. Low-resource translations improve significantly as we We compare on chrF ++ scores. We observe that as we add more and more data from mining and/or backtranslation, we start to see diminishing improvements from the SSL task.</p><p>add mined bitext and backtranslated bitext, as observed from the results in Section 8.1.5.</p><p>Here we show some experiments and associated findings to understand how much more improvement self-supervised learning can bring when combined with training on mined as well as backtranslated data.</p><p>Experimental Setup. We use the setup similar to the experiments in Section 8.1.5 on different sets of data and train with and without self-supervised (SSL) multitasking. The datasets used are (1) Primary, (2) Primary + Mined, and (3) Primary + Mined + MmtBT + SmtBT. We follow a similar setup as in Section 6.3 for the multitask training and train 3.3B dense models for 300,000 updates. We compare the models on the best chrF ++ checkpoints.</p><p>Results. In <ref type="figure" target="#fig_1">Figure 36</ref>, we observe that when using the additional self-supervised denoising autoencoder task (DAE) on the monolingual data in addition to the multilingual machine translation (MMT) task, the improvements decrease as we add Mined data, and become negligible once we add MmtBT+SmtBT backtranslated data. This demonstrates that mining and backtranslation provide good quality augmented bitexts from monolingual data, which improves low-resource translation. When combined with backtranslation, MMT+SSL multitask training begins to perform worse on eng_Latn-xx directions with no improvements on xx-eng_Latn and xx-yy directions. Based on these results, for our final model training, we do not use SSL and simply train on the MMT task. That said, there certainly could be situations when it is not feasible to get enough good quality mined or backtranslated data and some form of self-supervised learning on better quality and/or relatively larger monolingual corpora could be promising. The best strategy to utilize monolingual data is an active field of study and future work could reveal a deeper understanding of the relative benefits of mining, backtranslation, self-supervised training and the best ways to combine them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.2">Scaling Model Architecture</head><p>As we prepare to train on the final 202 language dataset comprising of over 18B sentence pairs and 2440 language directions, we need to scale up our models to ensure we have enough capacity to model all languages and language directions well. We increase our dense model size moderately to 3.3B parameters. Beyond that, we scale model size using the conditional compute strategy of Sparsely Gated Mixture of Expert (MoE) models for two reasons: (1) Adding expert capacity helps high-resource languages due to increased model capacity and also helps low-resource languages by minimizing interference with unrelated languages.</p><p>(2) The computational overhead (FLOPs) of Sparsely Gated MoE models does not increase proportional to the number of parameters. However, large models, especially MoE, are prone to rapid overfitting on low-resource directions, so we apply the regularization and curriculum learning strategies discussed in Section 6.2 to optimize the complex training dynamics while training 2440 different language pairs with varying resource levels, varying difficulty and varying relatedness to other language pairs. Sparsely Gated MoE Layers. We now describe how we incorporate Sparsely Gated MoE layers into our models. By default, existing literature <ref type="bibr" target="#b23">(Artetxe et al., 2021;</ref><ref type="bibr">Fedus et al., 2022;</ref><ref type="bibr">Lepikhin et al., 2020)</ref> replaces the Feed Forward Network (FFN) layer at every alternate Transformer block with an MoE layer. The number of experts chosen are dependent on how much we want to increase MoE model capacity and the nature of the task being trained. Scaling the number of experts in MoE models comes with an increase in distributed communication cost during training, due to the expensive All-to-All communication primitive, which is performed twice (dispatch and combine) for every layer on each model update <ref type="bibr">(Lepikhin et al., 2020)</ref>. Another disadvantage is that the All-to-All communication primitive scales sub-linearly with the distributed world size (Lepikhin et al., 2020). To improve communication efficiency of our models during training, we use 128 experts. This decision is based on the fact that we observe that 32 experts perform as well as 64 experts on the 53 language benchmark when the overall dropout is tuned separately for both settings in Section 6.2. Another approach to increase training efficiency is to reduce the frequency of MoE layers by placing MoE layers at wider intervals in the encoder and decoder of the model. We experiment with placing MoE layers at every 2 nd , 4 th and 6 th layer of a model with 24 encoder and 24 decoder layers. We then pick the variant with the best efficiency-accuracy trade off.  these models and keep all other parameters same. All the models are trained for 200k updates and we pick the checkpoint with the best chrF ++ score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choosing the Optimal Regularization</head><p>Results. We see in <ref type="table" target="#tab_6">Table 28</ref> that inserting MoE layers at an interval of every 4 Transformer blocks exhibits the best performance, in particular improving performance in very-low resource settings. In terms of training efficiency, the MoE model with f MoE =4 is 28% more efficient than f MoE =2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.3">Designing an Optimized Training Curriculum</head><p>Agnostic to the model architecture, an optimal curriculum in a massively multitask setting helps to find a better local minimum and acts as a strong regularizer <ref type="bibr">(Bengio et al., 2009;</ref><ref type="bibr" target="#b95">Lu et al., 2020)</ref>. There are several types of curriculum learning strategies based on how we order tasks -in terms of their difficulty level or their convergence patterns. For massively multilingual machine translation, high-resource pairs continue to converge when trained longer, whereas low-resource pairs overfit relatively quickly given smaller training datasets and higher model capacity. We propose a simple strategy where we introduce high-resource directions first, and introduce pairs that overfit later on in the training process. Section 6.2.3 indicates the benefit of this strategy on low-resource and very low-resource directions. Based on observing when different directions overfit at different points during training, we experiment with how to optimally bucket language pairs to introduce at different phases during training.</p><p>Experimental Setup. We compare three curriculum learning strategies here: (1) no curriculum, (2) curriculum with naive bucketing based on training example counts, and (3) curriculum with bucketing into multiple phases based on observed overfitting when training with no curriculum. We train 128-expert MoE models with f MoE =4 with each strategy, and set T = 300k, where T is the total number of updates for the entire training.</p><p>In the naive bucketing curriculum of variant (2), we divide language pairs into (2a) those with more than 9M training sentence pairs, and (2b) those with fewer than 9M sentence pairs. We begin training with the language pairs with more than 9M sentences (261 language pairs, including 128 low-resource ones). At 200k updates, we then introduce language pairs with fewer than 9M sentences (2179 language pairs, including 2010 low-resource ones) and train all the pairs for a total of T updates. This threshold was chosen empirically based on  the observed relationship between each language pair's training example count and number of updates before overfitting.</p><p>In the phased curriculum variant (3), we first train the model with variant (1) no curriculum, and then divide language pairs into n different buckets b 0 , b 1 , . . . , b n?1 based on when they start to overfit. Then we restart training, introducing a particular bucket b i at T ? k i updates, where k i is the median number of updates after which all directions in bucket b i start to overfit. In our particular instantiation, we set k 0 = 300k, k 1 = 130k,  <ref type="table" target="#tab_6">Table 29</ref>, we observe that using a curriculum i.e. variants (2) or (3), is better on this massively multilingual dataset compared to using no curriculum, variant (1). This is in contrast with our observation from Section 6.2.3, where we saw no benefits of curriculum learning on top of MoE EOM. This difference is likely due to training on a dataset with 4x more languages and thousands of more directions, the majority of which are low to very low-resource and are prone to overfitting even after strong regularization techniques are applied. eng_Latn-xx and xx-yy directions see improvements of &gt;+0.6 chrF ++ with the majority of improvements coming from low and very low-resource directions. On xx-eng_Latn we see +1 chrF ++ improvement and most significantly on very low-resource(+1.9 chrF ++ ). Next, we observe that, on average, the 4-phase curriculum strategy performs best among the two variants of curriculum we test. Compared to the (2) naive curriculum, the (3) phased curriculum shows significant improvements on low (+0.7 chrF ++ ) and very lowresource (+1.2 chrF ++ ) pairs for xx-eng_Latn directions. Both variants show comparable performance on eng_Latn-xx and non-English-centric directions. We analyze the effect of phased curricula in more depth in Section 8.5.2.</p><formula xml:id="formula_23">k 2 = 70k, k 3 = 30k.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. From</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.4">The 200 Language Model: NLLB-200</head><p>Based on the experiments and analyses detailed in this section, we now summarize the best recipe we found to build the NLLB-200 model. Our final model is a Transformer encoder-decoder model in which we replace the Feed Forward Network (FFN) layer in every 4 th Transformer block with a Sparsely Gated Mixture of Experts layer containing 128 experts. We use model dimension 2048, FFN dimension 8192, 16 attention heads, 24 encoder layers and 24 decoder layers. We use Pre-LayerNorm  as described in Section 6.1.1. We share the embedding weights of the encoder input embedding, decoder input embedding and decoder output embedding layers. We use an overall dropout of 0.3, attention dropout 0.1 and EOM with p eom =0.2. The model has a total of 54.5B parameters and FLOPs similar to that of a 3.3B dense model.</p><p>Training Details. We train the model for 300k steps using the 4 phase curriculum described in Section 8.2.3. We use an effective batch size of 1M tokens per update. The maximum sequence length during training is 512 for both the encoder and the decoder. We use the Adam optimizer (Kingma and Ba, 2015) with ? 1 = 0.9 and ? 2 = 0.98 and = 10 ?6 . For training efficiency, we use memory efficient FP16 for training as implemented in the fairseq library and also maintain Adam optimizer states in FP16 <ref type="bibr" target="#b51">(Dhariwal et al., 2020)</ref>. We linearly increase the learning rate from 10 ?7 to 0.002 for 8000 warmup steps and then follow the inverse square root learning rate schedule. The loss is cross-entropy with label smoothing of 0.1 <ref type="bibr">(Szegedy et al., 2015)</ref>. For balancing expert utilization, we use the additional load balancing loss as described in Section 6.2 with a weight of 0.01. The capacity factor of each expert is set to 2, i.e., each expert can process up to 2 ? T /E tokens, where T is the number of tokens in the mini-batch and E is the number of experts. During generation, we set the capacity to be equal to the batch size so that all tokens can be routed to a single expert if needed. This would be useful for low-resource pairs that might prefer a small subset of experts. Finally, the model is trained to accept the source language token as prefix for the source sequence and the target language token as the first token input to the decoder as explained in Section 6.1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Results on Flores-200</head><p>We present results comparing NLLB-200's performance against existing state-of-the-art models on Flores-101 languages. We dive into performance on Flores-200 languages, focusing on English-centric, non-English-centric and zero-shot performance. We primarily show results with the chrF ++ metric, but also list spBLEU for convenience. We follow this with a human evaluation on NLLB-200's translations on Flores-200. Finally, we discuss the prevalence of hallucinated toxicity in NLLB-200's generations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.1">Performance on Flores-101 and Comparison to State-of-the-Art</head><p>We evaluate NLLB-200 on Flores-101 to compare its performance with state-of-theart models. We use the same evaluation method as used for Flores-101, with the 101 language SentencePiece (SPM) tokenizer officially provided. In <ref type="table" target="#tab_9">Table 30</ref>, we observe that NLLB-200 outperforms the nearest state-of-the-art by almost +7.3 spBLEU on average -a 44% improvement. We then compare with a few other state-of-the-art models such as Deepnet     state-of-the-art systems by a significant margin despite covering 200+ languages -twice as many languages (or more than 30k additional directions) compared to any previous work.</p><p>Performance on African and Indian Languages Next, we compare NLLB-200's performance for two specific language groups -African and Indian languages. Several recent works in NLP have focused on African languages <ref type="bibr" target="#b2">(Abbott and Martinus, 2019;</ref><ref type="bibr">Adelani et al., 2022;</ref><ref type="bibr" target="#b28">Azunre et al., 2021c;</ref><ref type="bibr" target="#b44">Dabre and Sukhoo, 2022;</ref><ref type="bibr" target="#b6">Emezue and</ref><ref type="bibr">Dossou, 2021, 2020;</ref><ref type="bibr" target="#b74">Hacheme, 2021;</ref><ref type="bibr">Nekoto et al., 2020;</ref><ref type="bibr">Siminyu et al., 2021)</ref>. Here we compare against two recent works: MMTAfrica (Emezue and Dossou, 2021) and Mafand-MT <ref type="bibr">(Adelani et al., 2022)</ref>. Mafand-MT uses an M2M-100 model finetuned on the MAFAND dataset. For MMTAfrica, we take the max score of their BT and BT&amp;REC methods. In <ref type="table" target="#tab_3">Table 31</ref>, we observe that NLLB-200 outperforms both models significantly on most of the eng_Latn-xx and all</p><formula xml:id="formula_24">eng_Latn-xx xx-eng_Latn (a) (b) (c) (d) NLLB-200 (a) (b) (c) (d) NLLB-200</formula><p>asm -/ 6.9/--/-/--/-/--/13.6/-7.9/11.7/35.9 23.3/-/--/-/--/-/-24.9/-/-33.9/-/57.8 ben -/20.   of xx-eng_Latn directions. On eng_Latn-yor_Latn and eng_Latn-swh_Latn, MMTAfrica is slightly better than NLLB-200. NLLB-200 improves significantly on all other directions as it benefits from multilingual transfer by handling 55 African languages, in addition to data and modeling improvements. In comparison, the other works train on only 6 to 10 African languages. In <ref type="table" target="#tab_9">Table 53</ref>, we further compare NLLB-200's performance on non-English-centric African language directions.</p><p>In recent years, Indian languages have seen a lot of progress in low-resource multilingual NLP <ref type="bibr">(Bhattacharjee et al., 2022;</ref><ref type="bibr" target="#b45">Dabre et al., 2021;</ref><ref type="bibr" target="#b132">Ramesh et al., 2022)</ref>. We compare NLLB-200's translation performance with (a) IndicTrans <ref type="bibr" target="#b132">(Ramesh et al., 2022)</ref>, (b) IndicBART <ref type="bibr" target="#b45">(Dabre et al., 2021)</ref>, and commercial translation systems such as (c) Google Translate, (d) Microsoft Translate. IndicTrans reports results using the default 13a Moses tokenizer from SacreBLEU <ref type="bibr" target="#b124">(Post, 2018)</ref> for xx-eng_Latn and IndicNLP tokenizer 37 for eng_Latn-xx directions. IndicBART reports scores with the default 13a Moses tokenizer from SacreBLEU for all directions. We report the scores with both these variants as well as chrF ++ . From <ref type="table" target="#tab_6">Table 32</ref> we observe that NLLB-200 outperforms all the models significantly on the xx-eng_Latn directions. On eng_Latn-xx directions, NLLB-200 performs better than (a) and (b) but worse than commercial systems (c) and (d). Overall, on an average over all directions, NLLB-200 outperforms all the above systems. NLLB-200's training dataset includes 25 Indian languages 38 , which is almost twice the languages covered by (a) and (b). The performance improvements can be attributed to more multilingual transfer, along with improved mined and backtranslated data quality for the Indian language family.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.2">Performance on Flores-200</head><p>We present the performance of NLLB-200 on the full    <ref type="table" target="#tab_9">Table 33</ref> with both chrF ++ and spBLEU metrics.</p><p>English-Centric Performance. We first discuss performance on English-centric directions. We observe that, on average, xx-eng_Latn directions perform much better than eng_Latn-xx directions -a general trend we have previously observed in Section 6. We hypothesize this is due to several reasons: (1) xx-eng_Latn directions all require the decoder to decode into the same language, and English is the majority language in the training dataset;</p><p>(2) Due to abundant and high-quality monolingual data in English, bitext mining and backtranslation produce higher quality data on the English side, and;  In contrast with the ?2k supervised pairs, where the model achieves 39.5 chrF ++ , we see only a -4.1 chrF ++ drop on average over the ?38k zero-shot pairs. Zero-shot low and very low-resource directions are on average only around -5.0 chrF ++ worse compared to supervised pairs. The model has reasonable zero-shot translation performance likely due to its massively multilingual nature. Following Fan et al. <ref type="formula" target="#formula_1">(2020)</ref>, we added non-English-centric mined bitext data to our training set, which also contributes to improving zero-shot. Further, we carefully chose the mined bitext directions which have better aligned bitexts and are from languages which are similar to each other.</p><p>Comparison against Google Translate. We compare NLLB-200's performance against commercial translation systems like Google Translate. Since these systems are proprietary, fair comparisons are challenging as there exists little information about model architecture, training settings, or number of models. Thus, we provide these results only for approximate comparison. We focus our observation on low and very low-resource language directions that overlap between Flores-200 and the Google Translate API 41 -note that NLLB-200 covers far more languages. There are 102 directions which are low-resource and among those 30 are 41. https://cloud.google.com/translate/docs/languages g a z _ L a t n -e n g _ L a t n h y e _ A r m n -e n g _ L a t n r o n _ L a t n -e n g _ L a t n s s w _ L a t n -e n g _ L a t n w o l _ L a t n -e n g _ L a t n z h o _ H a n s -e n g _ L a t n z u l _ L a t n -e n g _ L a t n a m h _ E t h i -g a z _ L a t n a m h _ E t h i -t i r _ E t h i f r a _ L a t n -b a m _ L a t n f r a _ L a t n -e w e _ L a t n f r a _ L a t n -o c i _ L a t n f r a _ L a t n -w o l _ L a t n h i n _ D e v a -k a n _ K n d a h i n _ D e v a -m a r _ D e v a i n d _ L a t n -j a v _ L a t n i t a _ L a t n -l i j _ L a t n j p n _ J p a n -k o r _ H a n g p b t _ A r a b -u r d _ A r a b r u s _ C y r l -b e l _ C y r l r u s _ C y r l -t g k _ C y r l s p a _ L a t n -a y r _ L a t n s p a _ L a t n -q u y _ L a t n s w e _ L a t n -n n o _ L a t n z h o _ H a n s -y u e _ H a n t e n g _ L a t n -a r b _ A r a b e n g _ L a t n -a r y _ A r a b e n g _ L a t n -b u l _ C y r l e n g _ L a t n -c e s _ L a t n e n g _ L a t n -c k b _ A r a b e n g _ L a t n -d e u _ L a t n e n g _ L a t n -e l l _ G r e k e n g _ L a t n -f u v _ L a t n e n g _ L a t n -g l a _ L a t n e n g _ L a t n -h a u _ L a t n e n g _ L a t n -h e b _ H e b r e n g _ L a t n -h i n _ D e v a e n g _ L a t n -i b o _ L a t n e n g _ L a t n -i s l _ L a t n e n g _ L a t n -j p n _ J p a n e n g _ L a t n -l u g _ L a t n e n g _ L a t n -l u o _ L a t n e n g _ L a t n -s o m _ L a t n e n g _ L a t n -s w e _ L a t n e n g _ L a t n -t a m _ T a m l e n g _ L a t n -t u r _ L a t n e n g _ L a t n -t w i _ L a t n e n g _ L a t n -v i e _ L a t n e n g _ L a t n -w o l _ L a t n e n g _ L a t n -y d d _ H e b r e n g _ L a t n -z h o _ H a n s  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.3">Human Evaluation</head><p>We next evaluate the quality of translations with professional human translators following the XSTS evaluation protocol and calibration methodology described in Section 7.</p><p>Experimental Setting. We compare two models: NLLB-200 and a baseline 3.3B parameter dense model. Both are trained on the same data and cover 202 languages. To understand model performance across a diverse set of language pairs, we evaluate 51 different translation directions. 26 out of English, 18 non-English, and 7 into English directions that represent many everyday translation needs that were described in our survey studies with low-resource language speakers (see Section 2). Each evaluation uses the 1,012 sentences of Flores-200 devtest.</p><p>Results. Results are shown in <ref type="figure" target="#fig_1">Figure 37</ref>. Overall, NLLB-200 achieves an average XSTS score of 4.22 averaged across all directions, and the dense model achieves an average XSTS score of 3.66. Compared to the baseline dense model, the performance of NLLB-200 is statistically significantly stronger. NLLB-200 achieves an average XSTS score of 4.09 for translation into English, 4.33 for translation out of English, and 4.12 for translation for non-English directions. Certain directions have a significant difference, such as rus_Cyrl-tgk_Cyrl and eng_Latn-gla_Latn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.4">Prevalence of Toxicity</head><p>Our goal is to produce high-quality safe translations for each of our 200 languages. Deviating in meaning from the source sentence is not desired behavior, but adding toxicity is worse. We use the toxicity detectors proposed in Section 7.3 (in particular, the 1+ toxic item detector) to evaluate the added toxicity in the NLLB-200 translations. We conduct our evaluation on the Flores-200 devtest set. Note that this data has an extremely low prevalence of true toxicity (only 3 toxic items in the devtest set) due to its Wikimedia domain.</p><p>Using NLLB-200, we evaluated translation outputs into and out of English. Potential added toxicity with the 1+ toxic item detector was detected in 130 out of 201 eng_Latn-xx directions (1,636 sentences) and 200 out of 201 xx-eng_Latn directions (555 sentences). <ref type="figure">Figure 44</ref> in Appendix E.3 contains a more detailed breakdown. Overall, our analysis indicates a low prevalence of potential added toxicity in the translation of Flores-200. However, it does not mean that our models generate low toxicity in general. They could generate higher levels of toxicity in other domains (i.e., different topics, registers, or discourse types) than those found in Flores-200. These other domains may contain a higher prevalence of potential triggers, such as out of vocabulary tokens, infrequent words or word forms used with an unusual part-of-speech value (e.g., a doff grandpa) or in unusual constructions (e.g., potbellied veterans). In addition, we observe that there is a big difference in the potential added toxicity when translating out of English compared to translating into English. In out of English translations, we have more than 3 times as many potential added toxicity examples (0.80% on average) than into English (0.27% on average). Whether these differences are due to an over-detection by our toxicity lists or due to actual added toxicity by our models when translating particular translation directions is a direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Out-of-domain Generalization: Performance on non-Flores-200 Domains</head><p>We next evaluate our model's generalization capability on some non-FLORES MT benchmarks and compare to existing works. We further analyze how NLLB-200 can be adapted to specialize in various domains and discuss the significant performance improvement that comes from in-domain finetuning.</p><p>Our goal in this section is to examine if we are developing a robust general-purpose MT system capable of translating in various domains. We first evaluate the capability of NLLB-200 to generalize on a wide selection of non-FLORES MT benchmarks from different domains <ref type="bibr">(news, health, governmental, etc.)</ref>. Then, we leverage our own NLLB-MD dataset (see Section 4.3) to validate the transferability of NLLB-200 to other domains by finetuning on small quantities of high-quality bitexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.1">Public Benchmarks</head><p>We use publicly available and widely used MT evaluation benchmarks to assess the performance of NLLB-200 on domains other than the Wikimedia text of Flores-200, and to compare the performance of NLLB-200 to previous state-of-the-art models. <ref type="bibr">42</ref> We select a total of 238 directions from 8 benchmarks. We describe each of these benchmarks and the set of selected directions below.</p><p>42. Note that training setups vary, and thus models are not directly comparable Flores(v1): with a total of 8 directions, the original Flores dataset <ref type="bibr" target="#b70">(Guzm?n et al., 2019)</ref> pairs four low-resource languages (khm_Khmr, npi_Deva, pbt_Arab, sin_Sinh) with eng_Latn in the Wikimedia domain.</p><p>WAT: we select 3 languages (hin_Deva, khm_Khmr and mya_Mymr ) paired with English (6 directions) from the WAT competition.</p><p>WMT: we evaluate on the 15 WMT languages selected in <ref type="bibr" target="#b91">Siddhant et al. (2020)</ref>. This set overlaps with the 10 languages selected in <ref type="bibr" target="#b164">Wang et al. (2020a)</ref> and both are frequently used for benchmarking MMT models <ref type="bibr">(Kim et al., 2021;</ref><ref type="bibr">Kudugunta et al., 2021)</ref>. The 15 languages paired with English in this set are: (ces_Latn, deu_Latn, est_Latn, fin_Latn, fra_Latn, guj_Gujr, hin_Deva, kaz_Cyrl, lvs_Latn, lit_Latn, ron_Latn, rus_Cyrl, spa_Latn, tur_Latn and zho_Hans).</p><p>IWSLT: we select 24 directions from the IWSLT translation competition. With bitexts based on aligned TED talks, the selected directions come from different campaigns (see <ref type="table" target="#tab_12">Table 55</ref> in the appendix for more details on each direction).    Evaluation on Other Benchmarks. We evaluate the translations' accuracy with BLEU, spBLEU, and chrF ++ (choosing to match the evaluation methodology of the individual benchmarks). To measure BLEU, we first detokenize the hypotheses. Then, for each evaluation corpus and for each language, we conform to the tokenization or normalization used in the current state-of-the-art. Once the hypotheses and references are tokenized, we compute BLEU. In most languages, we pass the detokenized output to SacreBLEU <ref type="bibr" target="#b124">(Post, 2018)</ref> and use the default 13a Moses tokenizer. See <ref type="table" target="#tab_12">Table 56</ref> in the appendix for a breakdown of the pre-processing steps for each irregular evaluation direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.2">Results</head><p>FloresV1. On the in-domain test sets of Flores(v1) in <ref type="table" target="#tab_9">Table 35a</ref>, we outperform the state of the art   TICO. <ref type="table" target="#tab_9">Table 37</ref> shows the scores of NLLB-200 and those of the best baseline trained in the original TICO paper <ref type="bibr" target="#b14">(Anastasopoulos et al., 2020)</ref>. We see a considerable gain in accuracy on low and high-resource languages alike. Additional results on TICO can be found in <ref type="table" target="#tab_12">Table 58</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAFAND.</head><p>We compare in <ref type="table" target="#tab_9">Table 38</ref>   <ref type="table" target="#tab_12">Table 57</ref> and <ref type="table" target="#tab_12">Table 58</ref> of the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.3">Effective Domain Adaptation with NLLB-MD</head><p>In the following, we study if NLLB-200 can effectively transfer to other domains and if it lends itself to the common strategy of single-task finetuning with small quantities of in-domain high quality translations <ref type="bibr">(Adelani et al., 2022;</ref><ref type="bibr">Lee et al., 2022;</ref><ref type="bibr" target="#b93">Liu et al., 2021b;</ref>.  Experimental Setup. We experiment with the NLLB-MD dataset (see Section 4.3). It provides high-quality translations in four domains (news, scripted formal speech (scripted), unscripted informal speech (chat) and health), translating from English to 6 languages (5 of which are low-resource). We hold 500 sentences in each language for testing, finetune on 2000 sentences, and use the remainder for validation. In each translation direction (into and out of English), we finetune NLLB-200 on that single task for 50 updates (15-20 epochs) with a learning rate of 5e-5 following an inverse square-root schedule after warming up for 10 updates. We consider two options for finetuning NLLB-200 for the new task: (1) finetuning with the original training objective (label-smoothed cross-entropy with an additional load balancing regularization term) (see Section 6.2) and (2) finetuning without regularization, thus, leaving the MoE's load distribution unconstrained.</p><p>Results. <ref type="figure" target="#fig_1">Figure 38</ref> shows validation chrF ++ scores in the chat domain tasks of the pretrained NLLB-200, the similarly finetuned model with load balancing (NLLB-200+FN+LB), and the finetuned model without load balancing (NLLB-200+FN). On average, finetuning (FN+LB) improves the accuracy by +6.1 chrF ++ points. The performance gain is more considerable when translating into high-resource languages (eng and rus) with an average +8.9 chrF ++ points and an average +2.0 points when translating into the 5 low-resource languages in NLLB-MD. When switching off the load balancing regularization, NLLB-200+FN improves by +7.2 chrF ++ and it is particularly interesting when translating into low-resource languages with an increase of +3.7.</p><p>We next finetune with our best strategy (NLLB-200+FN) on the other 3 domains of NLLB-MD and report chrF ++ scores on the test sets in <ref type="figure" target="#fig_1">Figure 39</ref>. On average, by finetuning   English-centric tasks of NLLB-MD. NLLB-200+FN+LB and +FN refer to finetuning with and without load balancing (LB). We report accuracy in terms of chrF ++ on the validation set.</p><p>NLLB-200, we can improve translation accuracy in new domains by +7.7 in chat, +3.1 in news, +4.1 in health and +5.8 in scripted (in terms of chrF ++ ). These results are evidence of NLLB-200's transferability and adaptability to other domains. The issue of finetuning sparsely activated large models has been raised in prior work <ref type="bibr" target="#b23">(Artetxe et al., 2021;</ref><ref type="bibr">Fedus et al., 2022;</ref><ref type="bibr">Zoph et al., 2022)</ref>. These large models are more prone to overfitting than their dense counterparts, and in some cases they performed poorly when finetuning <ref type="bibr" target="#b23">(Artetxe et al., 2021;</ref><ref type="bibr">Fedus et al., 2022)</ref>. load balancing, we are allowing the model to drop experts, practically activating a few that will be finetuned for the downstream task. This is particularly relevant when finetuning on a single-task for which NLLB-200 has learned to assign specific experts (see Section 8.5.1); adding load balancing loss when the mini-batches are not mixed will considerably shift this learned assignment. We leave the exploration of MoE finetuning strategies with added regularization, selective finetuning and relaxed optimization for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5">Analysis of NLLB-200</head><p>In this section, we analyze several properties of NLLB-200. We first discuss the language co-location of the massively multilingual NLLB-200's experts. Then, we examine how different curriculum learning strategies address the issue of overfitting on low-resource directions while also maintaining performance of high resource directions. Finally we dive into the impact of multilingual transfer on low-resource languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.1">Language Co-location in NLLB-200 experts</head><p>Similar to our analysis of MoE models on the ablation dataset with its 53 languages (see Section 6.2.4), we aim to analyze language co-location in NLLB-200's experts. Following the same steps, we compute a per-language distribution across the 128 experts, evaluated on the dev set of Flores-200, then we embed all the trained languages in 2D with UMAP. Similar to our observations on the ablation dataset, we see in <ref type="figure" target="#fig_36">Figure 40</ref> that late decoder layers and early encoder layers of NLLB-200 process tokens from separate languages by dispatching them to different sets of experts. Languages within the same family are assigned similar sets of experts, and families that are geographically proximate (e.g., Nilotic, Saharan and Atlantic-Congo) or are genealogically related (e.g., Arabic and Hebrew) also get assigned to similar MoE experts of NLLB-200. Detailed similarity scores between the 200 languages in each of the 4 depicted layers can be found in <ref type="figure" target="#fig_2">Figure 45</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.2">Effect of Phased Curriculum on Low-Resource Overfitting</head><p>In <ref type="figure" target="#fig_0">Figure 41</ref>, we compare the effect of no curriculum, naive curriculum, and the phased curriculum training of NLLB-200. We observe that while naive curriculum helps alleviate  some low-resource overfitting, some low-resource language pairs are introduced too early, while others are introduced too late. These effects are mitigated with the phased curriculum. The effects are more prominent for very low-resource pairs, which start to overfit significantly if introduced early, as seen in <ref type="figure" target="#fig_0">Figure 41</ref>(a). Although phased curriculum strategy in general helps alleviate lot of overfitting issues we see with naive curriculum, it is still not optimal. Designing automated curricula that monitor training overfitting patterns can further improve performance, and we keep this direction of research for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.3">Impact of Multilingual Transfer</head><p>One of the main benefits of multilingual models lies in the strong transfer between the languages. To quantify the gains obtained with the massively multilingual NLLB-200, we trained bilingual models on a few low resource directions with the same training data and compared their performance. The results are presented in <ref type="table" target="#tab_9">Table 39</ref>. We observe impressive gains over almost all directions we compare on, which demonstrates the benefits of multilingual transfer. We also notice that the gains are more consistent on xx-eng_Latn directions. For eng_Latn-xx language pairs, improvements are less consistent and we can observe some directions where huge gains (+19.4 chrF ++ ) are seen, but also other directions where slight decreases(-0.6 chrF ++ ) are observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6">Making Large Models More Accessible through Distillation</head><p>While large models often have strong performance, their sheer size limits their utility and makes inference expensive. Adding additional languages, finetuning, or even generating translations can require significant GPU compute. In this section, we explore distillation, a common technique for training smaller student models from larger, better performing teacher models <ref type="bibr">(Bucilu? et al., 2006;</ref><ref type="bibr" target="#b86">Hinton et al., 2015;</ref>. Such techniques yield stronger performance compared to training a small model from scratch <ref type="bibr" target="#b67">(Gordon and Duh, 2020;</ref><ref type="bibr">Kim and Rush, 2016)</ref>. Various approaches have been explored extensively to compress translation models <ref type="bibr" target="#b59">(Freitag et al., 2017;</ref><ref type="bibr" target="#b79">Zhang et al., 2018;</ref><ref type="bibr">Zhou et al., 2020)</ref>, including in multilingual <ref type="bibr" target="#b148">(Sun et al., 2020)</ref> and low-resource <ref type="bibr" target="#b8">(Ahia et al., 2021;</ref><ref type="bibr">Saleh et al., 2020)</ref> settings. We explore and discuss two types of distillation approaches, online and offline distillation. We detail how we apply these techniques to create specialized models for the Wikipedia Content Translation tool <ref type="bibr">(Laxstr?m et al., 2015)</ref>. These models serve translations for 25 languages and 74 directions, translating on-demand with low latency. Finally, we apply a compute efficient approach for distilling our 54B parameter NLLB-200 across all 200 languages. For machine learning practitioners, we hope that making such models available will enable translations for far more languages in various applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6.1">Knowledge Distillation</head><p>Broadly, knowledge distillation is the process of transferring knowledge from a larger teacher model to a smaller student model <ref type="bibr" target="#b86">(Hinton et al., 2015)</ref>. We investigate two forms of distillation: online and offline. We describe both these approaches briefly here.</p><p>Online Distillation. We explore Word-Level knowledge distillation <ref type="bibr" target="#b86">(Hinton et al., 2015)</ref>. In this setting, the student model is trained on the training data but with an additional objective: to minimize the cross-entropy with respect to the word-level distribution of the teacher model.</p><p>Offline Distillation. We explore Sequence-Level knowledge distillation (Kim and Rush, 2016), which can be thought of as a form of data augmentation <ref type="bibr" target="#b66">(Gordon and Duh, 2019;</ref><ref type="bibr">Xu et al., 2021b</ref>) similar to backtranslation. The teacher model is used to generate translations, taking monolingual data as input. The student model is then trained on data generated by the teacher. This approach has the benefit that the student model can learn to mimic the teacher at the sentence level, but not local predictions over individual words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6.2">Creating Models Specialized for the Wikipedia Domain</head><p>In our interviews with native speakers of low-resource languages, a common theme was access to information online, such as educational content. While knowledge has been curated at a global scale on online platforms such as Wikipedia, these platforms remain accessible primarily by those who speak one of the handful of languages that dominate the web -such as English. Driven by our principle of prioritizing underserved communities and sharing, we next explore how our work could be put into practice. We describe how we build multilingual translation models for the Wikipedia Content Translation tool. This is an online interface that Wikipedia editors can use to translate an article that already exists in another language as a starting point for writing a new article. This aids the creation of new articles for underserved languages. We leverage distillation process to create models that are fast enough to meet the latency requirements of an on-demand service, while producing high-quality translations. Such work is broadly useful for machine learning practitioners who may be interested in adapting general-purpose systems to specific domains and deployed applications.</p><p>Language Pairs.  We open-source an end-to-end distillation pipeline in the stopes 44 library which can be used to apply offline distillation given a trained teacher model.</p><p>Applying Online Distillation. We use online word-level distillation <ref type="bibr" target="#b86">(Hinton et al., 2015)</ref> to distill the fine-tuned 1.3B parameter dense teacher model into a smaller 500M parameter dense model for inference efficiency. We use the same monolingual Wikipedia data dumps for source sentences, and we use only the soft cross-entropy with respect to the teacher distribution L KD because we use the monolingual Wikipedia data dumps as source sentences.</p><p>Comparing Online and Offline Distillation. We first examine the performance of online distillation. From <ref type="table" target="#tab_11">Table 40</ref>, we see that the 515 million parameter student model 43. https://dumps.wikimedia.org/other/cirrussearch/ 44. https://github.com/facebookresearch/stopes performs on par with the fine-tuned teacher model, within 0.1 chrF ++ for eng_Latn-xx, fra_Latn-xx, and spa_Latn-xx directions.</p><p>For offline distillation, <ref type="table" target="#tab_11">Table 40</ref> indicates that the 515 million parameter student model performs better than the fine-tuned 1.3 billion parameter teacher model on average across all pairs. Improvements are observed on eng_Latn-xx (+0.4 chrf++), fra_Latn-xx (+0.4 chrf++), and spa_Latn-xx (+0.3 chrf++) directions. We hypothesize that these improvements come in part from the in-domain Wikipedia monolingual source data used for the distillation training dataset. Further, distilled models often produce more translationese which can have effects on the reliability of reference-based metrics <ref type="bibr" target="#b142">(Riley et al., 2020)</ref>.</p><p>Based on these results, we conclude that offline sequence-level distillation performs marginally better than online word-level distillation when distilling our models for Wikipedia Content Translation. However, online distillation still performs well and is more compute efficient as a very large MoE model is not required to autoregressively generate millions of translations, which is important when distilling larger models to more languages. In the next sections, we explore compute efficient online distillation for very large scale multilingual models like NLLB-200. Apart from the major difference of training data for both the teacher and student models, there are several other notable distinctions. Our model begins with a 1.3B parameter dense teacher rather than a 6B parameter model. We cover 25 target languages across eng_Latn, fra_Latn, spa_Latn source languages, and 79 directions in one student model rather than having two student models eng_Latn-xx and xx-eng_Latn each supporting 30 languages and directions. Our final student model is only 500M parameters, compared to the 850M parameter encoder model with a Transformer based encoder and LSTM decoder presented in <ref type="bibr">(Bapna et al., 2022)</ref>. In spite of these, when averaged over the performance of 6 overlapping directions on Flores-101 (eng_Latn-xx for asm_Beng, ckb_Arab, lin_Latn, lug_Latn, nso_Latn, and gaz_Latn), our 500M parameter distilled model has similar performance as the 850M parameter distilled model from <ref type="bibr">(Bapna et al., 2022)</ref>. Across the 6 overlapping directions, both models achieve 24.9 spBLEU on Flores-101 devtest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6.3">Distillation of NLLB-200, a 54B Parameter MoE Model</head><p>We finally explore distillation of NLLB-200, a 54B parameter Mixture-of-Experts model, with the help of online distillation. The final distilled model retains full translation support of all 202 languages. Since inference for the full NLLB-200 is slower than the 1.3B parameter dense model and we distill to more than 30 times the number of language pairs of the Wikipedia model, we choose to use online rather than offline distillation due to the time and compute required to generate sequence-level teacher outputs. We train baseline and distilled dense models, each with 1.3B and 615M parameters, all for 200,000 updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>. <ref type="table" target="#tab_3">Table 41</ref> shows that on average, the distilled dense 1.3B student model performs better than the dense baseline 1.3B model by +0.5 chrF ++ , and the 615M student model performs better than the baseline 615M model by +0.3 chrF ++ . We observe that distillation provides larger improvements over the corresponding baseline dense models for low and  Practically deploying machine learning models, particularly neural methods, is extremely difficult and an active area of research. Our investigation indicates that distillation is a very promising avenue for leveraging multilingual models and specializing them to a subset of desired language directions and to the desired domain. This enables the Wikipedia translation model we create to have strong performance (despite the teacher model being only 1.3B parameters). We hope that such work, and open-sourcing these distilled models, enables others to access translation for their own native languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.7">Effectively Including Languages with Multiple Scripts and Related Languoids</head><p>Languages are not monolithic units, but fluid and full of variation. Some languages are written naturally in multiple scripts -Serbian is a well-known example that uses both Latin script and Cyrillic script. Other languages have a large amount of variation -we term these languoids. This variation can be fairly small differences in spelling (British English compared to American English, for instance) or render the languages mutually unintelligible.</p><p>In Flores-200 (see Section 3), we include multiple scripts for languages and multiple Arabic languoids. In this section, we study how to best represent natural language variation and produce the most accurate, localized translations. We focus on two cases: transliteration between different scripts and translation of closely related Arabic languoids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.7.1">Transliteration</head><p>The languages of the world use a wide variety of writing systems. Examples include logosyllabaries such as the Han script, which use ideographs to represent words or morphemes; syllabaries such as Katakana, which represent syllables; abugidas such as the Devanagari script, whose base units are consonant-vowel pairs; abjads like the Arabic script, which instead only require that consonants be written and may allow for vowels to be represented as diacritics; and alphabets like the Latin and Cyrillic writing systems, whose symbols denote both vowels and consonants; and many more. In many situations, a single language may be commonly communicated through different writing systems. Such cases arise due to historical, geopolitical, religious, or technological reasons. Language boundaries rarely overlap neatly with the borders of geopolitical entities such as nation states. The coexistence of multiple writing systems for a single language leads to two main challenges for creators of language technologies, especially when the language in question falls into the low-resource or endangered classifications: data challenges and ethical considerations.</p><p>Data Challenges. Obtaining transliteration models can be challenging. Whether the technique being tested is rule-based or model-based, the amount of readily available data may not be sufficient. To go from an abjad to an alphabet using a rule-based technique, simple rules which are deductively derived from character-to-character mapping tables, where the mapping of diacritics proves particularly challenging, do not produce satisfactory results. Similarly, model-based techniques require fairly large quantities of textual resources <ref type="bibr" target="#b99">(Madhani et al., 2022)</ref>, which are by definition unavailable for low-resource languages. Having linguistic data spread across different scripts can further reduce the amount of text which can be used for training, leading to poor performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations.</head><p>Developing technologies that favor one script might inadvertently further relegate minorities using a different writing system. In this section, we explore the question of whether a technological solution can be devised to alleviate these concerns. How effective are our translation models at generating as well as assimilating content in any script? Can they be used to perform transliteration, so as to bridge any gaps between writing systems, or are traditional transliteration methods more effective?</p><p>Experimental Setting. For evaluation, we choose the challenging task of transliterating from an abjad to an alphabet, which requires the recovery of vowels. We evaluate transliteration from the Arabic script to the Latin script for Acehnese and Banjar, as well as from the Tifinagh script to the Latin script for Tamasheq. The systems we compare are:</p><p>? The out-of-the box universal romanization tool uroman <ref type="bibr" target="#b84">(Hermjakob et al., 2018)</ref> ? Two online transliteration tools:  attention heads and 256-dimensional embeddings. The size of the SentencePiece vocabulary, which for each language is joint across source and target script, is determined as a function of the character set size. We experiment with vocabularies of size 1.5, 2.5 and 10 times the character set size, and determine the best choice for a given language through validation. Similar architectures have already been shown to be effective at character transduction and low-resource text normalization tasks <ref type="bibr" target="#b96">(Lusito et al., 2022;</ref>.</p><p>? A rule-based approach which naively replaces individual characters according to a transliteration table. We combine this approach with Ejawi as a postprocessing step, as we noticed a number of words still remained in the source script after using this online tool. We also attempt to use this rule-based approach as a preprocessing step for the neural transliterator, with the aim of bringing source and target embedding representations closer to each other.</p><p>Evaluation. We evaluate using CER (character error rate), which is computed as the ratio of edits to reference characters, and is common in tasks such as Optical Character Recognition. All systems are evaluated on Flores-200 devtest.</p><p>Results. Results are shown in <ref type="table" target="#tab_6">Table 42</ref>. We observe that specialized transliteration models perform best at this task. Among these tools, we note that the Ejawi online transliteration service achieves a high error rate on its own, but performs much better when followed by a simple rule-based character replacement step. 47 While the neural transliteration systems we trained do not in general achieve the lowest error rates, their performance could be plausibly improved by collecting larger training datasets. Regardless of the approach, error rates remain relatively high. This highlights the importance of being mindful when designing linguistic technologies, so as to reduce as much as possible the differences which might otherwise be introduced for users of different scripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.7.2">Multidialectal Translation</head><p>We want to consider the possibility of translating from or into different Arabic languoids.</p><p>To test the feasibility and efficacy of multilingual translation models for this task, we focus here on Arabic languoids, presented in <ref type="table" target="#tab_9">Table 43</ref>. Arabic has the advantage of being rich in 47. Based on <ref type="bibr" target="#b16">Andries (2004)</ref> for the Tifinagh script, and on the Arabic ICU transliteration rules for Jawi.  dialectal variation that has been well studied and documented <ref type="bibr" target="#b12">(Alshargi et al., 2019;</ref><ref type="bibr" target="#b72">Habash, 2010;</ref><ref type="bibr" target="#b73">Habash et al., 2013;</ref><ref type="bibr">Salameh et al., 2018)</ref>. We should note that a commonly stated characteristic of Arabic is the sociolinguistic phenomenon termed diglossia. In diglossic situations, users code-switch between a more vernacular dialect used for informal and casual tasks (e.g., day-to-day conversations, social media chats or posts) and another dialect used for tasks that are typically performed at a higher register of language (e.g., technical or scientific discourse, educational or informative content; see <ref type="bibr">Ferguson, 1959)</ref>. Such is the case for Arabic languoids, where MSA serves for higher-register use. The direct implication of this phenomenon is that sentences being produced in any Arabic languoid at a higher register will likely have more in common with MSA than sentences produced at a lower register (e.g., informal social media chats or posts).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dialectness Level Metric and Calculation on Flores-200. Dialectness Level (DL;</head><p>see <ref type="bibr">Sajjad et al., 2020)</ref> serves as a way of measuring the degree of lexical variation present in these languoids when compared to MSA. Specifically, the DL is a representation of the amount of tokens that are present in the languoid which are not present in MSA, including instances of semantic differences for a token. This means that in instances where the meaning of a given token in MSA is different from that of the same token in another languoid, it will not be taken into account as part of the DL score. The DL is represented as a range from 0% to 100% wherein the higher the range, the higher the level of dialectness in that specific dialect. The DL is calculated in two ways for our purposes: at the corpus and the sentence levels. The first metric, corpus-level DL (henceforth cDL), is computed for a given non-MSA languoid as the fraction of all tokens in its Flores-200 devtest dataset which are not present in MSA. The second DL measurement, sentence-level DL (henceforth sDL) is computed analogously but at a higher level of granularity. We work out, for each line in a languoid's Flores-200 devtest dataset, the fraction of its tokens which are not present   in the corresponding MSA sentence. This number is then averaged across all lines in the dataset. As in Sajjad et al. <ref type="formula" target="#formula_1">(2020)</ref>, to compute these metrics we normalize our Arabic text by replacing different forms of Alif and Hamzah as well as Indo-Arabic numerals. Additionally, we also use the same metrics used throughout this paper for evaluating machine translation, spBLEU and chrF ++ , as described in Section 7.</p><p>Results. Given that Flores-200 is aligned, we start by measuring the differences between the Flores-200 reference data for six Arabic languoids: Mesopotamian, Ta'izzi-Adeni, Tunisian, South Levantine, Najdi and Egyptian. It should be noted that data for South Levantine (ajp_Arab) and Egyptian (arz_Arab) was obtained by translating English, whereas the datasets of the other four languoids were adapted from MSA directly. While we decide to include these two languoids in our analyses to provide additional context, one should bear in mind that any results involving them might be skewed due to the slightly different data collection process. For this reason, we keep them separate in results tables. The results in <ref type="table" target="#tab_9">Table 43</ref> show that, with the exception of the two languoids whose data was obtained via translation, all other languoids are close to MSA, with spBLEU scores exceeding 50 points. Of the adapted datasets, Tunisian (aeb_Arab) is the one diverging the most from MSA, and Najdi (ars_Arab) matches MSA almost exactly.  We see that, among the languoids whose evaluation data was adapted from the MSA dataset (first four rows), the trends observed in <ref type="table" target="#tab_9">Table 43</ref> are still visible. Performance is low for Tunisian (aeb_Arab) and Mesopotamian (acm_Arab), and best for Najdi (ars_Arab), which can likely be explained by how much each model benefits from transfer learning via the much higher resourced MSA. Conversely, the two directions whose evaluation data was directly translated from English (bottom two rows) are achieving relatively high translation scores, despite their references showing the highest differences from MSA. We can formulate two hypotheses as to why. First, regardless of the dialectal variation shown for these languages in <ref type="table" target="#tab_9">Table 43</ref>, they might in fact be relatively close to MSA, and their perceived dialectal differences might largely be explained away by the slightly different procedure which was used to collect their evaluation data. Second, since their evaluation data was translated directly from English (as opposed to being adapted from MSA which was itself translated from English), it may actually be closer semantically and syntactically to the English source, making the task of translating it easier. We then analyze how distinct the dialects generated by our translation model are. In the rightmost part of Tables 44 and 45, we measure the differences between the generated non-MSA Arabic text and the generated MSA. For the first four languoids in the group, the major trends observed in <ref type="table" target="#tab_9">Table 43</ref> still hold, with Tunisian and Mesopotamian being the farthest away from MSA, and Najdi being the closest. More generally however, we observe a slight flattening of the differences in dialectness levels of the text translated from English compared to the human-annotated evaluation data. Indeed, the sentence-level DL scores have a standard deviation of 14.6 in the evaluation data, which reduces to 8.8 for the data generated by the model.</p><p>Additionally, we examine the translation model's ability to process various Arabic languoids, by looking at its performance when translating them into English and French. The results are reported in <ref type="table" target="#tab_11">Table 46</ref>. As expected, for both target languages the highresource MSA (arb_Arab) performs best, and is followed by the low-resource South Levantine (ajp_Arab) and Najdi (ars_Arab) languoids which benefit from transfer learning. Of the  We report average performance and standard deviation for the translation of Arabic languoids into and out of eng_Latn and fra_Latn. We also report the number of directions in each set for which a given model achieves the top performance.</p><p>four languoids whose evaluation data was collected by adaptation, we see that the relative ranking of translation performance figures matches the order of the dialectness level rankings in <ref type="table" target="#tab_9">Table 43</ref>: dialects closest to MSA are the ones most effectively understood by the model. Finally, in <ref type="table" target="#tab_11">Table 47</ref> we compare the performance of the large NLLB-200 model against that of a more targeted, smaller scale model that only focuses on translating between Arabic languoids, eng_Latn and fra_Latn. This custom model uses a smaller dense transformer architecture with 12 encoder layers and 12 decoder layers, FFN dimension 4096, 16 attention heads, and which otherwise follows the setup of the baseline model used in Section 6.2.1. We see that on average NLLB-200 outperforms the custom model, achieving the top performance for 17/28 directions as well as a higher average score. Performance for the large model is especially much higher when translating into French. This can be explained by difference in number of fra_Latn primary training sentences that each model is exposed to. The NLLB-200 model, having access to a much wider number of directions, can count on over 53M unique primary French sentences. Limiting training corpora to English-centric and Arabic-centric directions instead reduces this number to under 38M for the custom model. On the other hand, the smaller model outperforms NLLB-200 when translating into Arabic languoids, showing that a large multilingual model is not uniformly better for this set of languages.</p><p>Conclusion. In conclusion, these results mirror other similar efforts, such as the AraBench benchmark <ref type="bibr">(Sajjad et al., 2020)</ref>, highlighting the benefits of translations for Arabic languoids made using an MSA system. Their conclusions, correlating Dialectness Level and translation quality of generations, can also be seen in our results. As stated previously, of the Arabic languoids that we focused on, Najdi show this best as it has consistently low DL, and subsequently, it also represents the high end of translation quality. Conversely Mesopotamian and Tunisian, showing high DL in <ref type="table" target="#tab_9">Table 43</ref>, achieve consistently low translation performance. These results, as well as the AraBench benchmark findings, wherein they conclude that for translations made using an MSA system, a languoid with a lower DL will generate a higher quality translation, provide a way of overcoming the resource gap that might exist for these languoids. Their short linguistic distance to MSA allows them to benefit largely through transfer learning serving as a means of improving low-resource MT and multidialectal translation. Finally, our comparison of NLLB-200 and a more targeted model shows that while a massively multilingual model achieves the best average score, the smaller model can still outperform it on specific directions, highlighting the importance of more focused research on closely related languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.8">Environmental Impact of NLLB</head><p>Carbon emission estimates are not precise as the community lacks tools to accurately measure the factors that contribute to the emissions. Previous works have reported estimates and recommendations in Bender et al. <ref type="formula" target="#formula_1">(2021)</ref>; <ref type="bibr" target="#b52">Dodge et al. (2022)</ref>; <ref type="bibr" target="#b120">Patterson et al. (2021)</ref>; <ref type="bibr" target="#b171">Wu et al. (2022)</ref>. In this work, we rely on the best available power consumption estimates of GPU devices and carbon efficiency. Note that estimates of cloud providers are still inexact. There are several factors that affect the accuracy of these measurements: the real GPU power usage depends on GPU utilization and is likely different from Thermal Design Parameter(TDP) that we use as GPU power. Additionally, we did not include additional power costs, such as InfiniBand (IB) power consumption or non-GPU power consumption of the servers or datacenter cooling. Furthermore, manufacturing carbon cost for AI systems, such as GPUs, can introduce additional carbon footprint <ref type="bibr">(Gupta et al., 2022a,b)</ref>. We hope the carbon footprint analysis for NLLB helps provide transparency to understand the environmental implications of AI technologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Carbon Emissions for Training NLLB-200.</head><p>The training of NLLB-200 was performed on NVIDIA A100 GPUs. Using the NVIDIA A100 system specifications <ref type="bibr" target="#b36">(Choquette et al., 2021)</ref>, we use TDP 400W as the power per processor. To train NLLB-200, a cumulative of 51968 GPU hours of computation was performed on hardware of type A100-SXM-80GB (TDP of 400W). We estimate the total emissions for training NLLB-200 to be 8.39 tCO 2 eq of which 100% were directly offset by the provider's sustainability program. 48</p><p>Total Carbon Footprint of the entire No Language Left Behind Effort. The above only captures the carbon footprint of our final model. However, there are several steps in the research process before training a final model and steps afterwards (such as producing translations human evaluation steps) which we must also consider <ref type="bibr" target="#b171">(Wu et al., 2022)</ref>. Most previous works simply report the carbon footprint of training their largest models and multiply it with a factor (usually 2x) to report the total emissions. Instead, we try to report the carbon footprint for all the steps that have GPU utilization. This also provides useful insights to the community about the compute requirements and efficiency of each stage. Our detailed report includes steps for data preparation comprising large scale bitext mining (Section 5.3) and backtranslation (Section 8.1.3), all modeling experiments to design our architecture and training methods (Section 6), final model ablations for all 200 languages (Section 8.2), and model evaluations (Section 8.3). In <ref type="table" target="#tab_11">Table 48</ref>, we report each calculation in detail and observe that the experimentation phase of our research is the most compute expensive of all. Total emissions for the NLLB project as a whole is estimated to be 104.31 tCO 2 eq of which 100% were directly offset by the provider.</p><p>Sparse Mixture-of-Expert models may have a huge number of total parameters, but they are only sparsely activated when processing tokens during training. Hence, they can have greater compute efficiency compared to their dense counterparts and scaling 48. https://sustainability.fb.com/2021-sustainability-report/  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">No Language Left Behind: Social Impact &amp; Concluding Thoughts</head><p>In this effort, we took on the challenge of creating high-quality machine translation systems for 200+ languages. Faced with major obstacles such as the lack of reliable evaluation and training data, progress in low-resource translation has been slow compared to its highresource counterpart. In NLLB, we use novel approaches to make several major contributions aimed at bridging these gaps: (1) Flores-200, a high-quality human-translated evaluation dataset, and NLLB-Seed, a dataset comprising of human-translated bitext for 43 languages, (2) a novel bitext mining method that creates hundreds of millions of aligned training sentences for low-resource languages using our open-source mining library stopes and language identification model, and (3) various modeling techniques specifically devised to dramatically improve low-resource multilingual translation by reducing over-fitting. Beyond these, we also created smaller, distilled models so that the research community and various machine learning practitioners can more easily deploy this work.</p><p>To conclude, we discuss the potential social impact of our work. As is the case with most AI advancements, measuring NLLB's social impact requires a systematic evaluation framework and a longitudinal outlook. While its delivery could bring benefits to several stakeholders, including low-resource language groups and the scientific community at large, we also recognize that such an intervention has its potential downsides. As such, we reflect on the possibilities and limitations of NLLB, and ways to maximize its benefits while minimizing harm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Expanding Information Access</head><p>In the summer of 2016, the United Nations declared internet access as a basic human right <ref type="bibr" target="#b90">(Howell and West, 2016)</ref>. While the intent of this declaration was to compel countries to limit censorship and allow for information and ideas to flow without interference, much of the internet remains inaccessible to many due to language barriers. NLLB has the potential to alter the status quo by making the internet more accessible for many.</p><p>For many low-resource language communities, NLLB's offering would be the first model designed to support translation of their languages. Adopters of NLLB's tooling might be able to access content previously unavailable to them, allowing bolstered exposure to information and media. While its impact could cut across many domains of everyday lives, its impact on education, which other machine translation studies have also examined (Lee, 2020), could be significant. In formal educational settings, for instance, students and educators belonging to low-resource language groups would be able to tap into more books, research articles, and archives than before. Within the realms of informal learning, low-resource language speakers could experience greater access to information from global news outlets and social media platforms, as well as online encyclopedias such as Wikipedia. In these latter spaces, where the production of knowledge and content moves at a breakneck speed, the value of translation cannot be downplayed <ref type="bibr">(Bywood et al., 2017;</ref><ref type="bibr">Singh et al., 2012)</ref>.</p><p>The benefits of better quality translation are not exclusive for underserved communities. For communities currently being served by other translation services, the improvement in translation quality would boost their overall accessibility and utilization of the web's offerings. Such quality improvements could also lead to more streamlined knowledge acquisition and communicative processes. The cognitive energy one saves from deciphering poorly translated content could then be channeled to performing other more important tasks.</p><p>Because language is intrinsically tied to culture, for many low-resource languages facing endangerment, the threat of losing one's language could also mean the erosion of one's heritage <ref type="bibr">(Sallabank, 2013)</ref>. NLLB could motivate more low-resource language writers or content creators to share localized knowledge or various aspects of their culture with both cultural insiders and outsiders through social media platforms or websites like Wikipedia. Giving individuals access to new translation tools could thus open up a valuable avenue for bidirectional learning. In the long run, such generative processes could create dents on the global knowledge system, challenge Western-centric modes of knowledge production and dissemination, and aid in the revitalization of certain minority cultures and languages <ref type="bibr">(Bird, 2019;</ref><ref type="bibr">Bird and Chiang, 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">The Janus-faced Nature of Digital Participation</head><p>The benefits of a technological intervention like NLLB needs to be carefully weighed against the costs and risks it might incur on low-resource language groups and other stakeholders.</p><p>An increase in digital participation and linguistic representation, for example, heightens the visibility of a group <ref type="bibr">(Bucher, 2012)</ref>. Such visibility may amplify the odds of groups becoming targeted for surveillance and censorship <ref type="bibr" target="#b155">(Trer?, 2016;</ref><ref type="bibr">Zuboff, 2019)</ref>. Relatedly, affected communities may also become more susceptible to misinformation, online scams, or hate speech <ref type="bibr" target="#b64">(Gereme et al., 2021;</ref><ref type="bibr" target="#b89">Hossain et al., 2020)</ref>. In other words, the expansion of language and information access renders certain groups more vulnerable to longstanding issues plaguing digital communities at large. While no simple solution exists for these complex issues, we hope that NLLB could be leveraged for its cross-lingual potential <ref type="bibr" target="#b41">(Conneau et al., 2018)</ref> to strengthen existing (and typically monolingual) tooling designed to detect and classify hate speech, phishing, and other socially harmful online texts in low-resource languages <ref type="bibr">(Khonji et al., 2013;</ref><ref type="bibr" target="#b98">MacAvaney et al., 2019)</ref>. Recognizing that such tools act as a first defense, we believe that long term, structural investments aimed at curbing nefarious digital activities and improving online literacy need to go hand in hand with the introduction of new tools such as NLLB.</p><p>While access to translation could boost overall digital participation, it could also exacerbate existing digital inequities at a local or community-level. For one, those with technological know-how will benefit from NLLB more than those without. Demographically, these patterns are reflected through differences in factors such as age, education, economic standing, and rurality <ref type="bibr">(Elena-Bucea et al., 2021;</ref><ref type="bibr" target="#b85">Hindman, 2000)</ref>. Moreover, because technological infrastructure is unevenly distributed in many parts of the world, communities that are already lagging behind when it comes to internet access may experience aggregated information gaps compared to their better-served counterparts. In other words, the disparities in knowledge access, social connectivity, and economic mobility could deepen if the structural measures needed to rectify existing challenges that affect low-resource language communities are not in place.</p><p>Given that the primary goal of NLLB is to reduce language inequities in a global context, more and more low-resource languages will be incorporated into the project (or others alike) in the long run. Along this trajectory, those within this research space will inevitably encounter an increasing number of vulnerable communities that may resist the idea of letting technological entities they have little ties to capitalize on their languages <ref type="bibr" target="#b39">(Coffey, 2021)</ref>. To this end, being reflexive in our approach and prioritizing relationships with local institutions and community members to better understand their needs and concerns is of utmost importance in any expansion efforts. This motivation further explains why we have developed long-term in-depth interview and fieldwork studies with speakers of low-resource languages to understand how our intervention might impact their day-to-day lives. Moreover, collaborations with research groups that already possess vested interest in the topic at hand are imperative to any future success. We hope that spotlighting mutual interests and shared moral visions would facilitate resource and knowledge pooling, paving the way for long-term cooperation amongst various stakeholders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">The Future of NLLB: A Collective Responsibility</head><p>Recognizing that solving language disparities through machine translation is a mammoth task, NLLB's decision to make datasets and models publicly available encourages innovation through community production and collaboration <ref type="bibr" target="#b167">(Weber, 2004)</ref>. Open-sourcing our datasets and models not only advocates for transparency in the development of AI technologies, it further prevents the duplication of effort and allows machine translation practitioners to devote their energy at identifying gaps and building on the work we have done <ref type="bibr">(Kogut and Metiu, 2001)</ref>. Furthermore, we are actively developing mechanisms that would allow us to support data scientists and researchers who wish to use and adapt our models to meet their own needs, and provide the necessary assistance when needed. We are cognizant that open-sourcing does not translate into equitable access; the technological and infrastructural barriers to deploying our models and datasets remain high for researchers in many parts of the world. As such, to alleviate issues around the uneven distribution of computing power, we plan to develop toolkits and issue grants to under-resourced labs to assist them in their research endeavors. We believe that a collaborative mindset, alongside systematic and long-term documentation, will allow us to better assess the impact we have on the various communities implicated in our project.</p><p>Moreover, sharing NLLB with the larger scientific and research community will allow those with diverse expertise to contribute to the advancement of the project. In many ways, the composition of the NLLB effort speaks to the centrality of interdisciplinarity in shaping our vision. Machine translation lies at the intersection of technological, cultural, and societal development, and thus requires scholars with disparate training and standpoints to fully comprehend every angle <ref type="bibr">(Kusters et al., 2020)</ref>. It is our hope in future iterations, NLLB continues to expand to include of scholars from fields underrepresented in the world of machine translation and AI, particularly those from humanities and social sciences background. More importantly, we hope that teams developing such initiatives would come from a wide range of race, gender, and cultural identities, much like the communities whose lives we seek to improve.</p><p>Finally, we want to stress that overcoming the challenges that prevent the web from being truly accessible to speakers of all languages requires a multifaceted approach. As a single technological intervention, NLLB is all but one piece in a massive puzzle. Policy interventions aimed at more fundamental issues surrounding education, internet access, and digital literacy are imperative to eradicating the structural problem of language disparities. We are committed to working together with various stakeholders as we continue our path to materialize translation technologies that make the web a more accessible place, regardless of the language one speaks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Contributions</head><p>We outline the contributions of each member of No Language Left Behind, grouped by section and sorted alphabetically by last name. Each person is only mentioned once even though many contributed to several areas. No amount of space could fully describe the passion and contributions of every single person involved in bringing this effort to life.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>Bapi Akula -monolingual data pipeline to go from CommonCrawl to deduplicated, filtered sentences Pierre Andrews -engineering lead for data, led the development of stopes, mining and monolingual cleaning pipelines </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Acknowledgements</head><p>We thank our interns for the energy and discussions they brought in: Christos Baziotis, Dheeru Dua, Alex Guo, Oana Ignat, Ammar Kamran, Tasnim Mohiuddin, Andre Niyongabo Rubungo, Simeng Sun, Steven Tan, Haoran Xu, Shijie Wu, Yuwei Zhang. We thank the Wikimedia Foundation staff and Wikimedia volunteers who worked with us and provided feedback to our model. We thank Vishrav Chaudhary for help with the data pipeline. We thank Edouard Grave for his help in scaling fasttext to all Flores-200 languages. We thank Mona Diab for XSTS work and Lucia Specia for discussions on Toxicity and XSTS. We thank Javier Ferrando and Carlos Escolano for their invaluable help in using the ALTI+ method. We thank Brian O'Horo and Justine Kao for their insights and guidance. We thank Gloria Chang, Carole-Jean Wu and Ramya Raghavendra for helping us compute the CO 2 cost of our models. We thank Anjali Sridhar for help with FSDP. We thank Scott Jeschonek, Giri Anantharaman, Diego Sarina, Joaquin Colombo, Sanjana Krishnan, Dinesh Kannappan, Kalyan Saladi, Vivek Pai, Amit Yajurvedi, and Shubho Sengupta for their help with training infrastructure. We thank Kyle Johnson for his help with UXR studies and model evaluation. We thank Antoine Bordes, Marina Zannoli, and Chris Moghbel for supporting this project. We thank Pascale Fung for inspirational and generative discussions on the human-centered objectives of the project. We thank Nicolas Usunier, Sebastian Riedel, Shubho Sengupta, and Emily Dinan for helpful feedback on the paper. Finally, we thank all of the translators, reviewers, human evaluators, linguists, as well as the translation and quality assurance agencies we partnered with, for helping create Flores-200, NLLB-Seed, NLLB-MD, Toxicity-200, performing our human evaluations, and teaching us about their native languages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Languages</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Ethical Considerations around Language Standardization</head><p>Our work contains many languages that have varying levels of standardization. What may appear as a single language might in fact be hiding several competing standards for script, spelling, word formation, the acceptance of neologisms and borrowed terms, and more generally grammatical and style guidelines. Examples include languages such as Fulah, which includes several distinct languages and languages such as Kashmiri and Central Kanuri which represent languages where multiple scripts are in common use. The history of standardization of languages is complex, and has often historically been related to power and identity. Certain governments or institutions might push for a certain standard, or more subtle programs by specific circles (cultural elites, religious groups, economically powerful regions, major publishing houses) might exist with the aim of "civilizing" others or establish distinction <ref type="bibr">(Bourdieu, 1987;</ref><ref type="bibr" target="#b47">De Mauro, 2014;</ref><ref type="bibr" target="#b80">Haugen, 1959;</ref><ref type="bibr" target="#b141">Rickford, 2012)</ref>. At the same time, we may observe anti-standardization, groups of people that reject and contest the separate status of languages of the elite as 'the standard' <ref type="bibr" target="#b19">(Armstrong and Mackenzie, 2013)</ref>.</p><p>Work on language technologies has the potential to affect the way people use language and how it evolves, which opens up questions about responsibility. In order to consider the effect of our models on language communities, we examine two diametrically opposed cases: centrally and distributedly standardized languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Centralized standardization.</head><p>A large group of languages in our scope have generally recognized central regulatory bodies, in the form of either governmental, academic or community-run structures that possess deep organizational resources and powers. For these languages, grammars and dictionaries will typically be available, as well as formal ways to decide on the adoption of neologisms. While such resources are usually not directly used by translation technologies such as neural machine translation or language identification, they can be used as references for data collection, provided to annotators as guidelines, or used by scientists to decide how training datasets should be filtered. In training our models, however, we also make use of large amounts of linguistic data mined from the web. This data has the potential of capturing the more fluid state of language development and of including neologisms and language constructs that may not be officially sanctioned <ref type="bibr" target="#b122">(Plank, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distributed standardization.</head><p>In contrast to centrally institutionalized languages, many other languages might have an existing standardizing body whose work is however not recognized by large parts of the community; or multiple standardizing bodies might coexist, providing conflicting guidance; or multiple weakly-standardized variants might have arisen without any regulatory bodies. Many of the languages in this category are considered low resource due to the absence or extreme scarcity of annotated data. In order to develop models, it was necessary to annotate some datasets, which in turn required the development of guidelines for the annotators. This may steer the development of languages in potentially undesirable ways, such as favoring one variant over others. As a result, certain groups might end up feeling left out, or forced to comply with standards that feel alien to them. While only specific variants of certain languages might be supported out of the box, it should be possible to extend support to other variants with only limited data annotation efforts. Indeed, it' s been shown that closely related languages strongly benefit from transfer learning <ref type="bibr" target="#b151">(Fan et al., 2020;</ref><ref type="bibr">Gu et al., 2018;</ref><ref type="bibr">Sajjad et al., 2020;</ref><ref type="bibr">Zoph et al., 2016)</ref>, in such a way that "competing" variants might end up benefiting from each other' s presence in the dataset. Furthermore, simple adaptation between certain variants can be performed in an automatic manner, as is the case of transliteration (see Section 8.7) or mapping between different spelling standards <ref type="bibr">(Bollmann, 2019;</ref><ref type="bibr" target="#b96">Lusito et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Toxicity Lists</head><p>The toxicity lists for languages other than English were created through human translation using an English-language prompt, except the Luxembourgish and Asturian lists, the human translation of which was pivoted via German and Spanish translations of the English prompt, respectively. However, the process was iterative, and the prompt had to be modified in order to reduce translation errors caused by ambiguities and misunderstandings.</p><p>For most of the languages that were part of FLORES 101, we used existing lists produced over the course of a previous project. The English prompt for this iteration did not provide any definitions for the words it included, or part-of-speech and register information. Apart from translations directly related to the prompt, the lists could be expanded without any restrictions to allow for the inclusion of cultural specificities and spelling variants (including inflection, leetspeak, and nonstandard spelling). The lists resulting from this first method comprise numerous items that cannot be traced back to any known dictionary form, which makes checking for quality and anomalies a particularly daunting task. It is already not uncommon for toxic words to be excluded from widely available dictionaries, even in their more standard form; nonstandard forms further complicate the matter, especially when any number of suggestions can be freely added. It is unclear how important the inclusion of variants such as leetspeak and nonstandard spelling is. Nonstandard variants can, by definition, be nonstandard in a number of ways, which string-matching methods cannot cover exhaustively.</p><p>For languages that were added to those mentioned above, the English prompt was supplemented with part-of-speech and register information, as well as links to definitions, in an effort to reduce errors associated with polysemy and other ambiguities. Translators were encouraged to indicate whether a prompt item had an equivalent in their languages, and to suggest items that an English-centric prompt would miss. However, suggestions were limited to around forty items without specific restrictions as to the number of derived word forms per item apart from the general guidance of keeping within the boundaries of frequently used word forms (i.e. steering clear of infrequent and archaic word forms). In this method, translators were not asked to produce leetspeak or nonstandard spelling variants, yet not discouraged from including them where they saw fit.</p><p>Both methods are met with at least one common challenge. Despite the additional grammatical and semantic information provided by the second type of prompt, as well as the complementary training, translators seem to face similar difficulties when deciding whether certain items qualify as slurs or as nontoxic language. This is particularly salient in the case of some racial slurs and of slurs against the LGBTQ+ community. One important factor may be, as was previously noted, that it is sometimes difficult to find translators with diverse backgrounds, who will accept this sort of assignment. Another factor may be that some of these items may have been reclaimed by the community of reference but even allies of the community who are not themselves members thereof would hesitate to use them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Data</head><p>To assess the performance of our language identification (LID) system, we leverage the high-quality annotations from the Flores-200 dataset. Precision, recall and F1 scores across all languages on Flores-200 devtest are shown in <ref type="table" target="#tab_11">Table 49</ref>.</p><p>As discussed in Section 5.1.4, there is a significant domain mismatch between the data used to train our LID system and the web corpus that system is used on in our pipeline. To assess the impact, we conducted an extensive human evaluation. We selected 74 low-resource languages on which a preliminary LID model yielded low F1 scores. We randomly picked several thousand sentences predicted to be among those languages and asked annotators to assess whether each prediction was correct. We built a challenge set based on these annotations to benchmark our final LID model.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Modeling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Ablation Dataset</head><p>In <ref type="table" target="#tab_3">Table 51</ref> we list all the languages in our ablation dataset used for experimentation in Section 6. <ref type="table" target="#tab_3">Table 51</ref> contains the exact set of 110 language pairs used in the ablation dataset for experiments in Section 6. In Section 6.4.1 we observed that combining backtranslations from Statistical Machine Translation (SMT) and Multilingual Neural Machine Translation (MMT) is beneficial. We hypothesized that this is because the two models provide different, complementary sources of noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 SMT vs MMT</head><p>One way to visualize these differences is to plot the token frequencies of the translations produced by the two types of model. We show this in <ref type="figure" target="#fig_39">Figure 42</ref> for two directions. Namely, <ref type="figure" target="#fig_39">Figure 42a</ref> shows the same set of kau_Arab sentences translated into eng_Latn by the two models; and <ref type="figure" target="#fig_39">Figure 42b</ref> does this for awa_Deva.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Pairs</head><p>arb_Arab-sin_Sinh eng_Latn-pes_Arab eng_Latn-kin_Latn eng_Latn-sin_Sinh eng_Latn-yue_Hant sin_Sinh-arb_Arab pes_Arab-eng_Latn kin_Latn-eng_Latn sin_Sinh-eng_Latn yue_Hant-eng_Latn eng_Latn-ace_Latn eng_Latn-fin_Latn eng_Latn-kon_Latn eng_Latn-snd_Arab eng_Latn-zho_Hans ace_Latn-eng_Latn fin_Latn-eng_Latn kon_Latn-eng_Latn snd_Arab-eng_Latn zho_Hans-eng_Latn eng_Latn-afr_Latn eng_Latn-fon_Latn eng_Latn-lvs_Latn eng_Latn-tam_Taml eus_Latn-por_Latn afr_Latn-eng_Latn fon_Latn-eng_Latn lvs_Latn-eng_Latn tam_Taml-eng_Latn por_Latn-eus_Latn eng_Latn-arb_Arab eng_Latn-fra_Latn eng_Latn-lin_Latn eng_Latn-tel_Telu fra_Latn-hau_Latn arb_Arab-eng_Latn fra_Latn-eng_Latn lin_Latn-eng_Latn tel_Telu-eng_Latn hau_Latn-fra_Latn eng_Latn-ast_Latn eng_Latn-fuv_Latn eng_Latn-luo_Latn eng_Latn-tir_Ethi fra_Latn-kon_Latn ast_Latn-eng_Latn fuv_Latn-eng_Latn luo_Latn-eng_Latn tir_Ethi-eng_Latn kon_Latn-fra_Latn eng_Latn-ayr_Latn eng_Latn-hau_Latn eng_Latn-mal_Mlym eng_Latn-tso_Latn fra_Latn-lin_Latn ayr_Latn-eng_Latn hau_Latn-eng_Latn mal_Mlym-eng_Latn tso_Latn-eng_Latn lin_Latn-fra_Latn eng_Latn-bel_Cyrl eng_Latn-hin_Deva eng_Latn-mar_Deva eng_Latn-twi_Latn fra_Latn-swh_Latn bel_Cyrl-eng_Latn hin_Deva-eng_Latn mar_Deva-eng_Latn twi_Latn-eng_Latn swh_Latn-fra_Latn eng_Latn-bul_Cyrl eng_Latn-isl_Latn eng_Latn-nso_Latn eng_Latn-urd_Arab hin_Deva-tam_Taml bul_Cyrl-eng_Latn isl_Latn-eng_Latn nso_Latn-eng_Latn urd_Arab-eng_Latn tam_Taml-hin_Deva eng_Latn-cjk_Latn eng_Latn-ita_Latn eng_Latn-oci_Latn eng_Latn-vie_Latn jpn_Jpan-kor_Hang cjk_Latn-eng_Latn ita_Latn-eng_Latn oci_Latn-eng_Latn vie_Latn-eng_Latn kor_Hang-jpn_Jpan eng_Latn-cym_Latn eng_Latn-kea_Latn eng_Latn-run_Latn eng_Latn-wol_Latn rus_Cyrl-tat_Cyrl cym_Latn-eng_Latn kea_Latn-eng_Latn run_Latn-eng_Latn wol_Latn-eng_Latn tat_Cyrl-rus_Cyrl eng_Latn-ewe_Latn eng_Latn-kik_Latn eng_Latn-rus_Cyrl eng_Latn-yor_Latn swh_Latn-tsn_Latn ewe_Latn-eng_Latn kik_Latn-eng_Latn rus_Cyrl-eng_Latn yor_Latn-eng_Latn tsn_Latn-swh_Latn  <ref type="bibr" target="#b153">(Tiedemann, 2012)</ref> and with the help of the mtdata tool <ref type="bibr">(Gowda et al., 2021)</ref>. Direction counts refer to the number of directions used in this work, which may differ from to the total number of directions made available by the corpus. They also do not include reverse directions, such that e.g. eng_Latn-fra_Latn does not also contribute to the count as fra_Latn-eng_Latn. Complete training configuration files are available in our repository 49 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1.2 Importance of Backtranslation Quality on Model Scaling</head><p>We study the importance of backtranslation quality on model scaling. Data augmentation strategies such as backtranslation, self-training, and even large-scale mining form a significant portion of training data for modern translation systems. However, they are not as high quality as human translated data and data augmentation quality may limit a translation model's overall quality.</p><p>We train a multilingual model on 60 African language translation directions, to and from French and English. Subsequently, we investigate the importance of BT quality on model performance for 8 languages: fuv_Latn, kmb_Latn, lug_Latn, nya_Latn, swh_Latn,   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2.1 Performance on African Languages</head><p>In  directions, GT performs better on eng_Latn-xx directions. NLLB-200 performs slightly better overall but significantly better on low and very low-resource pairs. Note that several of the high-resource directions where GT performs better are from Flores-101. This is likely because the workflow of professional translators usually begins with machine translation followed by post-editing, which advantages the heavily-used GT. Hence we see GT is significantly better on such eng_Latn-xx high-resource pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Toxicity Evaluation</head><p>We present toxicity detection on translation into English and out of English, separating high and low-resource languages in <ref type="figure">Figure 44</ref>. We find that detections are dominated by over-detection of benign terms, but there is a fraction of real hallucinated toxicity within some of these detections as well. A small baseline detection level is present on nearly all into English directions, while the out of English side is much more inconsistent with languages both very high and low; likely due to differences in toxicty list over-detection and actual tendency of the model to hallucinate into some languages more than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Out-of-domain Generalization: Performance on non Flores-200 Domains</head><p>We present full evaluation results on various non-Flores-200 datasets and domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 Analysis of NLLB-200</head><p>We present full cosine similarity scores for all Flores-200 languages in NLLB-200 in <ref type="figure" target="#fig_2">Figure 45</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.6 Full Distillation Results.</head><p>We present the results for each distillation direction for Wikipedia-domain models. As described in Section 8.6, we use offline sequence-level distillation to create a smaller model for Wikipedia-domain translation. The score for translation from English, French, and Spanish is shown as well as several Wikipedia-requested translation directions.                                 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head><p>? Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF ++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Data</head><p>? Datasets: Flores-200 dataset is described in Section 4 ? Motivation: We used Flores-200 as it provides full evaluation coverage of the languages in NLLB-200 ? Preprocessing: Sentence-split raw text data was preprocessed using SentencePiece. The SentencePiece model is released along with NLLB-200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Data</head><p>? We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>? In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Caveats and Recommendations</head><p>? Our model has been tested on the Wikimedia domain with limited investigation on other domains supported in NLLB-MD. In addition, the supported languages may have variations that our model is not capturing. Users should make appropriate assessments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Carbon Footprint Details</head><p>? The carbon dioxide (CO 2 e) estimate is reported in Section 8.8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Creation</head><p>? Curation Rationale Script, dialect, spelling and translation approaches were first established and aligned on from Flores-200. Translators referenced these linguistic alignments while working on NLLB-Seed translations. The datasets were translated directly from English for 39 languages, half the data for Ligurian (3000 sentences) were first translated from English to Italian, then translated from Italian to Ligurian while the other half was translated directly from English, and three Arabic script languages (Acehnese, Banjar, Tamasheq) were transliterated from their respective Latin script datasets that were translated from English. Following the translation or transliteration phase was a linguistic quality assessment phase in which the completed datasets were checked against the linguistic alignments from Flores-200 along with basic quality sanity checks. The datasets were then finalized and completed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Source Data</head><p>Source Data includes 6193 English sentences sampled from Wikipedia Articles in 11 categories: Anthropology, Arts, Biology, Geography, History, Mathematics, People, Philosophy, Physical, Society, Technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Annotations</head><p>There are no extra annotations with the bitext.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Personal and Sensitive Information</head><p>Not applicable</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Considerations for Using the Data</head><p>? Social Impact of Dataset The dataset is specifically built to increase the translation quality and improve language identification of the extremely low-resourced languages it contains. This helps improve the quality of different languages in machine translation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Discussion of Biases</head><p>Biases on the dataset have not been studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Dataset Curators</head><p>All translators who participated in the NLLB-Seed data creation underwent a vetting process by our translation vendor partners. Translators are required to be native speakers and educated in the target language. They must also have a high level fluency (C1-C2) in English. For non-English translators, they are required to have a high level fluency of their source language. Translators are also required to have at least two to three years of translation experience in the relevant language pair if they have an academic degree in translation or linguistics and three to five years of translation experience if they do not have any relevant academic qualification. Translators also undergo a translation test every 18 months to assess quality of their translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Licensing Information</head><p>We The data was not human annotated.</p><p>-Personal and Sensitive Information The metadata files do not contain any text beyond website urls. However the data in CommonCrawl and ParaCrawl may contain personally identifiable information, sensitive or toxic content that was publicly shared on the Internet. Some of this information may have been referred to in the released dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Considerations for Using the Data</head><p>? Social Impact of Dataset This data can be used to reconstruct a dataset for training machine learning systems for many low resource languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Discussion of Biases</head><p>Biases in the data have not been specifically studied, however as the original source of data is World Wide Web it is likely that the data has biases similar to those prevalent in the Internet. The data may also exhibit biases introduced by language identification and data filtering techniques: lower resource languages may have lower accuracy while data filtering techniques may remove certain less natural utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Dataset Curators</head><p>The data was not curated ? Licensing Information</p><p>We are releasing the metadata and the script to recreate the bitext from it under the terms of CC- <ref type="bibr">BY-NC. b</ref> The text and copyright (where applicable) remains with the original authors or publishers, please adhere to the applicable licenses provided by the original authors. We keep track of the source URL of each individual sentence to allow people to refer to said website for licensing information. a. For this card we use the template available https://huggingface.co/docs/datasets/v1.12.0/ dataset_card.html. We provide details on the metadata released. b. https://creativecommons.org/licenses/by-nc/4.0/legalcode</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>No Language Left Behind: Our low-resource translation effort focuses on four cornerstones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Human-Translated Dataset Contributions of No Language Left Behind:As highlighted, these datasets enable model training and evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Quality of FLORES-200:We depict the quality assurance score for the languages in Flores-200. The minimum acceptable standard is 90 percent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Overview of our Bitext Mining Pipeline. Language identification is applied on web corpora to extract monolingual sentences. Aligned pairs are later identified with LASER3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>Confusion Matrix on Flores-200 on the 21 languages with lowest accuracy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Effect of Sentence Length on Performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :</head><label>11</label><figDesc>LID Score Distribution Patterns on ParaCrawl, illustrated with Kimbundu, Igbo and Yoruba.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>21. https://github.com/facebookresearch/LASER where x and y are the source and target sentences, and N N k (x) denotes the k nearest u r d _ A r a b t e l _ T u l u t g l _ L a t n k a b _ L a t n b e l _ C y r l t a m _ T a m l a m h _ E t h i k a t _ G e o r k a z _ C y r l k h m _ K h m r p b t _ A r a b h y e _ A r m n h a u _ L a t n k m r _ L a t n u z n _ L a t n u i g _ A r a b s n d _ A r a b s o m _ L a t n t g k _ C y r l g l e _ L a t n m y a _ M y m r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 15 :</head><label>15</label><figDesc>Ablation Dataset Counts depicting the amount of training data across all language pairs, ranking from 39,992 to 18.7 million sentence pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>( 1 )</head><label>1</label><figDesc>Denoising Autoencoder (DAE), (2) Causal Language Modeling (LM), and (3) the combination of both (DAE+LM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>15.6 (?0.3) 14.6 (?1.2) 17.3 (?0.6) 18.2 (?0.4) eng_Latn-fuv_Latn 10.0 (?0.9) 13.6 (?0.3) 11.8 (?0.4) 13.1 (?0.2) mri_Latn-eng_Latn 16.7 (?0.5) 20.6 (?0.2) 25.9 (?0.1) 31.8 (?0.4) eng_Latn-mri_Latn 23.0 (?0.2) 24.8 (?0.2) 30.6 (?0.4) 36.2 (?0.5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 23 :</head><label>23</label><figDesc>Evaluation Contributions of No Language Left Behind: As highlighted, we describe our procedure for Human Evaluation and the creation of Toxicity lists for 200+ languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Compared to Direct Assessment (Graham et al., 2013) with a 5-point scale (the original direct assessment uses a 28. Our analyses demonstrate that there are minor differences between the SPM-200 from Flores-200 and SPM-100 model from Flores-101 when measuring on the Flores-101 languages. The major advantage of SPM-200 is that it covers 200+ languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 24 :</head><label>24</label><figDesc>Correlations between aggregated Human Quality Scores andAutomated Metrics. Left figure shows relationship between spBLEU and XSTS, middle figure shows relationship between chrF ++ and XSTS and the right figure shows relationship between average sentence-level chrF ++ and XSTS. All automated scores were computed only on the sentences evaluated for a given model and translation direction (either the full Flores-200 dataset or a subset). Note that NLLB-200 refers to a 55B parameter MoE model, and NLLB-200 Baseline refers to a dense 3.3B parameter model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Study A, Study B, and Study C) to examine relationships between human measures of quality and automated scores like spBLEU and chrF ++ . These evaluation studies contain evaluations of translations from five distinct translation models (NLLB-200 (MoE 55B), M2M-100 12B (Fan et al., 2020), NLLB-125 -a MoE model covering 125 languages -and an English-Centric multilingual WMT2021 Submission covering 7 languages (Tran et al., 2021), and a dense 3.3B NLLB-200 model used as a baseline for NLLB-200 (MoE 55B)) and 86 distinct translation directions evaluated by up to 292 distinct human evaluators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 25 :</head><label>25</label><figDesc>Selected distributions of median XSTS scores for different Translation Directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>30. https://cleanspeak.com/ 31. https://www.perspectiveapi.com/ 32. https://cyber.harvard.edu/publication/2020/principled-ai</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 27 :</head><label>27</label><figDesc>Overview of Toxicity Detectors which vary with number of toxic items and bitext matching of toxic items.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 28 :</head><label>28</label><figDesc>Ordered F1 scores for list-based Toxicity Detectors. 1+ toxic item vs 2+ toxic item. Dotted lines are random chance baselines performances. The language set is a uniform sample of several high and low-resource languages, limited to those with chrF ++ &gt; 45. Mean scores presented for both all languages without and with the chrF ++ threshold for inclusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 29 :</head><label>29</label><figDesc>Percentage of Toxic Items per found per corpora in the English-side. The same list was used across all corpora language pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>_ A r a b e u s _ L a t n b u l _ C y r l m y a _ M y m r d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 30 :</head><label>30</label><figDesc>k d _ C y r l m a g _ D e v a m a i _ D e v a u z n _ L a t n p o l _ L a t n p o r _ L a t n s m o _ L a t n s h n _ M y m r s c n _ L a t n a z b _ A r a b s o t _ L a t n b o d _ T i b t t e l _ T e l u t u r _ L a t n u k r _ C y r l v i e _ L a t n c y m _ L a Percentage of Toxic Items per found per corpora in the non-English-side, calibrated by subtracting the value of the Christian Bible corpus, for consistency across languages</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 32 :</head><label>32</label><figDesc>Bringing it All Together: We describe how we create our final NLLB-200 using all of the datasets and modeling techniques we created.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 34 :</head><label>34</label><figDesc>Comparing Model Performance when trained on data from various sources. We observe significant improvements on adding Mined and MmtBT+SmtBT backtranslated data for all type of language pairs and resource levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 35 :</head><label>35</label><figDesc>Distribution of Amount of Training Sentence Pairs across 1220 language pairs in our dataset. We observe that the majority of pairs have fewer than 1M sentences and are low-resource.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 36 :</head><label>36</label><figDesc>Comparison of only MMT vs MMT+SSL Multitask Performance on aggregate over Into English and Out of English directions, when trained on different sources of data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head></head><label></label><figDesc>Pairs in b 0 are introduced first at the start of model training, pairs in b 1 are introduced at step T ? 130k, and so on. The exact composition of each bucket in the phased curriculum is explained in Appendix E.1.3. Briefly, there are 822 pairs, with 673 low-resource pairs in b 0 . There are 506 pairs, with 449 low-resource pairs in b 1 . There are 455 pairs, with 444 low-resource pairs in b 2 . Finally, there are 657 pairs, with 612 low-resource pairs in b 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head></head><label></label><figDesc>3/-17.3/-/--/23.7/--/22.9/-19.4/22.1/50.0 32.2/-/-30.7/-/-33.6/-/-31.2/-/-38.7/-/62.2 guj -/22.6/-22.6/-/--/26.6/--/27.7/-25.0/25.2/53.3 34.3/-/-33.6/-/-39.5/-/-35.4/-/-44.6/-/66.6 hin -/34.5/-31.3/-/--/38.8/--/31.8/-34.6/36.7/57.3 37.9/-/-36.0/-/-42.7/-/-36.9/-/-44.4/-/66.5 kan -/18.9/-16.7/-/--/23.6/--/22.0/-21.3/22.1/53.4 28.8/-/-27.4/-/-31.7/-/-30.5/-/-36.9/-/61.0 mal -/16.3/-14.2/-/--/21.6/--/21.1/-17.1/18.3/51.6 31.7/-/-30.4/-/-33.4/-/-34.1/-/-39.1/-/62.9 mar -/16.1/-14.7/-/--/20.1/--/18.3/-17.6/17.9/48.0 30.8/-/-30.0/-/-35.5/-/-32.7/-/-40.3/-/63.8 ory -/13.9/-10.1/-/--/22.7/--/20.9/-15.1/16.9/45.7 30.1/-/-28.6/-/-30.3/-/-31.0/-/-41.6/-/64.4 pan -/26.9/-21.9/-/--/29.2/--/28.5/-24.5/27.7/49.0 35.8/-/-34.2/-/-37.8/-/-35.1/-/-44.8/-/66.3 tam -/16.3/-14.9/-/--/20.6/--/20.0/-19.8/19.8/53.7 28.6/-/-27.7/-/-31.2/-/-29.8/-/-36.8/-/60.8 tel -/22.0/-20.4/-/--/26.3/--/30.5/-24.8/25.3/55.9 33.5/-/-32.7/-/-38.3/-/-37.3/-/-43.6/-/65.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head></head><label></label><figDesc>(a) IndicTrans (Ramesh et al., 2022), (b) IndicBART (Dabre et al., 2021), (c) Google Translate, (d) Microsoft Translate. Numbers for (d) are taken from (Ramesh et al., 2022). NLLB-200 outperforms other translation systems on all the xx-eng_Latn directions. On eng_Latn-xx, NLLB-200 outperforms (a) and (b), but performs worse compared to (c) and (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head></head><label></label><figDesc>Flores-200 devtest set. For the 202 languages in NLLB-200, there are a possible 40,602 translation directions in Flores-37. Available in https://github.com/anoopkunchukuttan/indic_nlp_library. 38. Languages recognized by the Indian Constitution and a few other recognized minority languages of India.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 37 :</head><label>37</label><figDesc>XSTS Human Evaluation Quality Delta. Delta between NLLB-200 model and dense baseline -both trained on our NLLB dataset. On average, NLLB-200 outperforms the dense baseline by more than 0.5 XSTS score (on a 5 point scale). Average performance improvement into English is 0.25, average performance improvement translation out of English is 0.44, and average performance improvement of non-English directions is 0.76. very low resource.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head></head><label></label><figDesc>(a) Kim et al. (2021), (b) Tang et al. (2020), (c) Liu et al. (2020), (e) Kudugunta et al. (2021), (f) Ramesh et al. (2022), (g) Provilkov et al. (2020), (i) Bojar et al. (2016). (j) Cettolo et al. (2014) and (k) Cettolo et al. (2017)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure 38 :</head><label>38</label><figDesc>a y r -e n g b h o -e n g d y u -e n g f u r -e n g r u s -e n g w o l -e n g e n g -a y r e n g -b h o e n g -d y u e n g -f u r e n g -r u s e n g -w o l Comparison of NLLB-200 with and without Finetuning on the 12</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Figure 39 :</head><label>39</label><figDesc>Fedus et al. (2022) suggests increasing regularization with expert dropout, effectively applying stronger regularization to the expert parameters, while Zoph et al. (2022) combat overfitting by updating only a subset of model parameters. With MoE Expert Output Masking (EOM), NLLB-200 is heavily regularized and exhibits less overfitting on downstream tasks. We hypothesize that without Performance on NLLB-MD Test Sets (12 tasks in 4 domains) of NLLB-200 and the single-task finetuned models NLLB-200+FN (without load balancing).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure 40 :</head><label>40</label><figDesc>UMAP embedding of the FLORES-200 languages. We embed vectors of expert-language assignment in 2D with UMAP. Languages with similar expert choice (averaged on Flores-200 dev set) are adjacent in the 2D-projected space. We color in the first row according to language family and in the second row according to script. eng_Latn-rus_Cyrl (high)Figure 41: Validation Perplexity with Various Curriculum Strategies. Our proposed phased curriculum is particularly beneficial for low-resource and very low-resource pairs, mitigating the overfitting observed in the baseline and naive curriculum variants. With the naive curriculum, introducing pairs too early (a) or too late (b) hurts performance. The curriculum does not affect high-resource pairs much (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head></head><label></label><figDesc>Bapna et al. (2022) While significant differences prevent fair comparison, we briefly compare our Wikipedia distilled model to Bapna et al. (2022)'s distilled models on Flores-101 devtest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Onur</head><label></label><figDesc>?elebi -LID for 200+ languages, open source of mined data, implementation and discussions to develop LASER3 and monolingual pipeline Kenneth Heafield -technical feedback on monolingual data quality, open source of mined data Kevin Heffernan -implementation and experimentation to create LASER3, produced mined bitext Semarley Jarrett -worked on data partnerships Holger Schwenk -research lead for data, also led the development of LASER3 and bitext mining Guillaume Wenzek -technical feedback on data open-source and monolingual data, implemented infrastructure for Wikipedia model deploymentModeling Loic Barrault -visualization of model scores Shruti Bhosale -research lead for modeling, led experimentation and research direction on MoE James Cross -experimentation for incorporating SSL Maha Elbayad -implementation and experimentation with various MoE architectures, analysis of model quality and properties of MoE models Vedanuj Goswami -research lead for modeling, led experimentation and research direction on BT, SSL, led execution on the 200 languages goal Jean Maillard -creation of bitext filtering pipeline, experimentation on backtranslation, effect of NLLB-Seed, and transliteration Kaushik Ram Sadagopan -experimentation on data quality on model training, effect of NLLB-Seed Anna Sun -experimentation with effective MoE regularization, creation of Wikipedia Translation models, model distillation, experimentation curriculum learning Chau Tran -experimentation with self-supervised learning, model distillation Evaluation Marta R. Costa-juss? -analysis, mitigation and interpretability of toxicity, created data&amp;model sheets, worked on ethics research Cynthia Gao -led and worked on all human data collection and annotations (FLORES 200, NLLB-Seed, Human Evaluations) John Hoffman -analysis of translation quality and human evaluation experiments Elahe Kalbassi -worked on all human data collection and annotations (FLORES 200, NLLB-Seed, Human Evaluations) Philipp Koehn -technical feedback on mining and monolingual data quality, development of XSTS and human evaluation study analysis Daniel Licht -analysis of toxicity in translation and human evaluation experiments, development of XSTS Dirk Rowe -designed figures and UI for human studies Shannon Spruit -advised ethics research on the creation of machine translation models Skyler Wang -helped design and conducted interview studies to understand the impact of translation, advised research on ethics of translation and development of models Al Youngblood -designed and conducted interview studies to understand the impact of translation on people, worked on ethics research Linguistics Gabriel Mejia Gonzalez -Arabic dialectal variation, transliteration Prangthip Hansanti -transliteration, model output quality, toxicity detection Janice Lam -LID improvement, FLORES 200 language information and codes, FLORES data quality, toxicity detection Christophe Ropers -linguist lead, FLORES 200 language information and codes, model output quality, toxicity detection, ethics Organization Necip Fazil Ayan -research director, helped with the overall direction and strategy Sergey Edunov -manager lead, provided engineering and open-source direction Angela Fan -research and project lead, provided research direction for the entire project Francisco Guzm?n -research and engineering manager, provided direction for evaluation research Alexandre Mourachko -engineering manager, provided direction for data research Safiyyah Saleem -technical program manager Jeff Wang -product manager, led Wikimedia Foundation collaboration</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>Figure 42 :</head><label>42</label><figDesc>awa_Deva-eng_Latn, ? = 0.52, ? = 0.45 Comparison of token counts generated by MMT and SMT for the same set of source sentences. Tokens on the x-axis are sorted by decreasing SMT generation counts. We also report Spearman's ? and Kendall's ? rank correlation coefficients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head></head><label></label><figDesc>0.83 0.83 0.82 0.83 0.82 0.83 0.87 0.83 0.75 0.85 0.96 0.84 0.96 0.80 0.84 0.74 0.87 0.85 0.75 0.99 0.90 0.96 0.86 0.78 0.77 0.84 0.85 0.81 0.85 0.85 0.81 0.81 0.77 0.84 0.71 0.85 0.82 0.85 0.85 0.81 0.85 0.86 0.84 0.79 0.99 0.85 0.85 0.85 0.85 0.86 0.84 0.65 0.74 0.77 0.66 0.81 0.81 0.81 0.85 0.81 0.80 0.75 0.84 0.84 0.83 0.80 0.83 0.83 0.80 0.86 0.85 0.85 0.86 0.84 0.84 0.83 0.84 0.85 0.86 0.87 0.86 0.86 0.86 0.85 0.85 0.85 0.85 0.80 0.84 0.87 0.86 0.87 0.86 0.82 0.86 0.86 0.87 0.87 0.87 0.84 0.86 0.85 0.86 0.82 0.87 0.85 0.84 0.84 0.81 0.84 0.84 0.83 0.83 0.84 0.82 0.82 0.81 0.87 0.82 0.85 0.82 0.84 0.85 0.72 0.82 0.81 0.84 0.81 0.84 0.83 0.86 0.85 0.84 0.85 0.84 0.84 0.85 0.84 0.82 0.84 0.83 0.83 0.84 0.84 0.84 0.86 0.85 1.00 1.00 0.77 0.84 0.82 0.98 0.83 0.85 0.86 0.85 0.82 0.81 0.84 0.87 0.86 0.86 0.86 0.84 0.99 0.83 0.86 0.88 0.86 0.85 0.86 0.84 0.83 0.82 0.84 0.84 0.82 0.83 0.84 0.82 0.81 0.81 0.82 0.81 0.82 0.85 0.82 0.74 0.84 0.97 0.82 0.96 0.78 0.82 0.73 0.85 0.84 0.75 0.99 0.88 0.97 0.84 0.78 0.75 0.83 0.84 0.81 0.83 0.84 0.80 0.80 0.75 0.83 0.69 0.83 0.81 0.83 0.83 0.80 0.84 0.85 0.83 0.78 0.99 0.83 0.83 0.84 0.84 0.85 0.83 0.64 0.73 0.77 0.65 0.80 0.80 0.80 0.83 0.80 0.79 0.74 0.83 0.83 0.82 0.79 0.82 0.82 0.78 0.85 0.83 0.84 0.85 0.83 0.82 0.82 0.83 0.84 0.85 0.85 0.85 0.85 0.84 0.83 0.83 0.84 0.84 0.79 0.83 0.85 0.85 0.86 0.84 0.81 0.84 0.84 0.85 0.85 0.86 0.83 0.85 0.83 0.84 0.80 0.85 0.83 0.83 0.83 0.80 0.83 0.83 0.82 0.81 0.82 0.81 0.81 0.80 0.86 0.81 0.83 0.80 0.83 0.83 0.70 0.80 0.79 0.82 0.79 0.82 0.81 0.84 0.83 0.82 0.84 0.83 0.83 0.83 0.82 0.81 0.83 0.81 0.81 0.82 0.83 0.83 0.85 0.83 1.00 1.00 0.76 0.83 0.79 0.98 0.82 0.84 0.85 0.83 0.82 0.81 0.83 0.85 0.85 0.84 0.84 0.82 0.99 0.82 0.85 0.86 0.85 0.84 0.85 0.83 0.82 0.81 0.83 0.83 0.81 0.82 0.82 0.83 0.83 0.82 0.84 0.82 0.83 0.89 0.88 0.75 0.87 0.64 0.88 0.90 0.80 0.85 0.72 0.84 0.85 0.76 0.80 0.86 0.65 0.86 0.78 0.76 0.84 0.86 0.83 0.86 0.86 0.82 0.81 0.76 0.85 0.71 0.86 0.81 0.85 0.86 0.82 0.86 0.87 0.84 0.81 0.72 0.85 0.85 0.86 0.89 0.87 0.88 0.63 0.73 0.77 0.66 0.81 0.81 0.80 0.87 0.82 0.79 0.75 0.85 0.84 0.83 0.81 0.83 0.85 0.81 0.88 0.88 0.87 0.88 0.85 0.89 0.88 0.88 0.88 0.88 0.88 0.88 0.88 0.90 0.89 0.90 0.89 0.89 0.85 0.90 0.89 0.88 0.88 0.89 0.88 0.88 0.88 0.88 0.88 0.87 0.87 0.89 0.85 0.85 0.85 0.87 0.89 0.87 0.88 0.83 0.87 0.86 0.86 0.82 0.87 0.87 0.84 0.83 0.88 0.84 0.89 0.83 0.87 0.87 0.77 0.85 0.81 0.87 0.86 0.89 0.82 0.88 0.87 0.84 0.86 0.87 0.88 0.88 0.88 0.85 0.86 0.88 0.84 0.86 0.87 0.87 0.89 0.89 0.77 0.76 1.00 0.91 0.84 0.71 0.84 0.87 0.87 0.86 0.83 0.81 0.88 0.87 0.87 0.88 0.88 0.86 0.76 0.82 0.86 0.86 0.86 0.87 0.87 0.84 0.81 0.82 0.85 0.83 0.83 0.84 0.86</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><head></head><label></label><figDesc>0.75 0.75 0.74 0.75 0.75 0.76 0.80 0.77 0.68 0.78 0.99 0.76 0.93 0.74 0.77 0.66 0.79 0.77 0.69 0.97 0.83 0.99 0.79 0.70 0.71 0.76 0.77 0.74 0.77 0.77 0.74 0.74 0.71 0.76 0.65 0.77 0.76 0.78 0.77 0.74 0.77 0.78 0.77 0.72 0.99 0.77 0.78 0.78 0.78 0.79 0.77 0.62 0.69 0.72 0.62 0.75 0.75 0.75 0.78 0.75 0.74 0.70 0.77 0.77 0.77 0.74 0.76 0.77 0.73 0.80 0.77 0.79 0.79 0.77 0.77 0.76 0.77 0.78 0.79 0.79 0.79 0.79 0.78 0.77 0.77 0.78 0.78 0.74 0.77 0.80 0.79 0.80 0.78 0.76 0.79 0.78 0.80 0.80 0.80 0.77 0.80 0.78 0.79 0.75 0.79 0.77 0.77 0.77 0.75 0.77 0.78 0.78 0.77 0.77 0.76 0.76 0.74 0.81 0.76 0.77 0.74 0.77 0.78 0.66 0.75 0.76 0.76 0.74 0.76 0.77 0.77 0.78 0.77 0.78 0.77 0.77 0.77 0.77 0.76 0.77 0.76 0.76 0.77 0.77 0.77 0.80 0.78 0.98 0.98 0.71 0.77 0.76 1.00 0.77 0.78 0.79 0.78 0.76 0.76 0.77 0.79 0.80 0.78 0.79 0.77 0.98 0.76 0.79 0.81 0.79 0.78 0.79 0.77 0.76 0.75 0.77 0.77 0.75 0.75 0.76</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_49"><head></head><label></label><figDesc>0.82 0.82 0.81 0.82 0.81 0.82 0.86 0.83 0.74 0.84 0.97 0.83 0.96 0.78 0.83 0.71 0.85 0.83 0.74 0.99 0.88 0.97 0.84 0.76 0.75 0.82 0.83 0.80 0.83 0.83 0.79 0.79 0.75 0.82 0.68 0.84 0.80 0.84 0.83 0.79 0.84 0.84 0.82 0.78 0.99 0.83 0.83 0.84 0.84 0.85 0.83 0.65 0.74 0.77 0.66 0.81 0.81 0.81 0.84 0.80 0.79 0.76 0.83 0.83 0.82 0.81 0.83 0.83 0.80 0.85 0.83 0.84 0.85 0.83 0.83 0.82 0.83 0.84 0.85 0.86 0.85 0.85 0.85 0.84 0.84 0.84 0.84 0.80 0.84 0.86 0.85 0.86 0.85 0.82 0.85 0.85 0.86 0.86 0.86 0.83 0.85 0.83 0.84 0.80 0.86 0.84 0.84 0.83 0.80 0.83 0.83 0.83 0.81 0.83 0.83 0.81 0.80 0.87 0.81 0.84 0.80 0.83 0.84 0.72 0.82 0.80 0.82 0.81 0.83 0.81 0.84 0.83 0.82 0.84 0.84 0.83 0.83 0.83 0.83 0.83 0.82 0.82 0.84 0.83 0.83 0.86 0.84 0.99 0.99 0.76 0.83 0.80 0.98 0.82 0.84 0.85 0.84 0.81 0.81 0.83 0.85 0.85 0.85 0.84 0.83 1.00 0.81 0.84 0.86 0.85 0.84 0.85 0.82 0.81 0.80 0.82 0.82 0.81 0.81 0.83</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_50"><head></head><label></label><figDesc>1.00 0.94 0.98 0.97 0.95 0.99 0.89 0.88 0.91 0.88 0.87 0.91 0.89 0.91 0.88 0.87 0.72 0.80 0.76 0.82 0.88 0.78 0.86 0.88 0.77 0.87 0.90 0.78 0.89 0.86 0.87 0.86 0.83 0.88 0.80 0.93 0.92 0.89 0.92 0.90 0.91 0.89 0.81 0.81 0.78 0.92 0.85 0.92 0.81 0.87 0.91 0.80 0.86 0.83 0.85 0.89 0.79 0.87 0.92 0.87 0.85 0.88 0.87 0.87 0.86 0.87 0.86 0.92 0.81 0.91 0.89 0.90 0.91 0.91 0.87 0.88 0.87 0.89 0.90 0.89 0.90 0.91 0.90 0.91 0.89 0.91 0.91 0.88 0.91 0.92 0.92 0.93 0.92 0.84 0.91 0.90 0.91 0.90 0.84 0.90 0.93 0.89 0.91 0.89 0.89 0.90 0.93 0.91 0.89 0.92 0.91 0.91 0.87 0.92 0.91 0.84 0.85 0.92 0.92 0.90 0.87 0.93 0.80 0.77 0.90 0.83 0.88 0.91 0.77 0.87 0.87 0.91 0.88 0.91 0.88 0.91 0.90 0.89 0.91 0.90 0.83 0.84 0.92 0.88 0.84 0.90 0.91 0.76 0.86 0.86 0.91 0.79 0.85 0.85 0.88 0.84 0.81 0.83 0.82 0.86 0.91 0.88 0.90 0.91 0.85 0.89 0.88 0.90 0.92 0.92 0.92 0.91 0.91 0.86 0.88 0.89 0.92 0.83 0.84 0.84 0.94 1.00 0.98 0.87 0.99 0.92 0.90 0.90 0.91 0.89 0.88 0.93 0.90 0.92 0.88 0.88 0.74 0.82 0.78 0.83 0.90 0.80 0.86 0.89 0.77 0.88 0.91 0.79 0.90 0.87 0.88 0.86 0.84 0.89 0.82 0.93 0.92 0.89 0.93 0.91 0.91 0.91 0.82 0.82 0.80 0.92 0.86 0.92 0.84 0.90 0.93 0.81 0.87 0.84 0.86 0.89 0.82 0.88 0.93 0.88 0.86 0.89 0.88 0.89 0.87 0.89 0.88 0.92 0.83 0.92 0.91 0.90 0.91 0.92 0.90 0.89 0.88 0.90 0.91 0.91 0.91 0.91 0.91 0.93 0.90 0.92 0.92 0.89 0.92 0.93 0.93 0.93 0.93 0.86 0.92 0.92 0.92 0.91 0.85 0.93 0.93 0.90 0.92 0.90 0.91 0.90 0.92 0.91 0.91 0.93 0.92 0.93 0.90 0.92 0.91 0.85 0.86 0.93 0.94 0.90 0.88 0.94 0.81 0.79 0.93 0.84 0.88 0.93 0.78 0.89 0.88 0.92 0.91 0.92 0.91 0.91 0.90 0.89 0.93 0.92 0.84 0.85 0.94 0.89 0.86 0.93 0.92 0.78 0.87 0.87 0.87 0.81 0.87 0.86 0.89 0.86 0.83 0.85 0.83 0.88 0.92 0.90 0.92 0.92 0.87 0.90 0.90 0.92 0.98 0.98 0.92 0.98 0.92 0.88 0.90 0.91 0.97 0.83 0.84 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_51"><head></head><label></label><figDesc>0.85 0.91 0.88 0.90 0.89 0.90 0.87 0.86 0.91 0.89 0.90 0.85 0.86 0.72 0.79 0.75 0.81 0.88 0.80 0.85 0.87 0.76 0.86 0.90 0.77 0.90 0.86 0.87 0.85 0.83 0.88 0.82 0.92 0.91 0.88 0.92 0.90 0.91 0.89 0.80 0.80 0.79 0.91 0.85 0.90 0.82 0.89 0.91 0.80 0.85 0.83 0.85 0.89 0.79 0.86 0.92 0.88 0.84 0.88 0.87 0.87 0.86 0.87 0.86 0.91 0.81 0.91 0.88 0.90 0.91 0.91 0.88 0.89 0.88 0.90 0.91 0.90 0.91 0.91 0.89 0.92 0.91 0.90 0.94 0.88 0.92 0.91 0.91 0.92 0.91 0.83 0.91 0.91 0.90 0.90 0.83 0.92 0.92 0.89 0.90 0.99 0.89 1.00 0.92 0.89 0.89 0.91 0.90 0.90 0.88 0.91 0.91 0.85 0.86 0.93 0.93 1.00 0.86 0.92 0.79 0.79 0.91 0.82 0.89 0.92 0.79 0.87 0.88 0.90 0.88 0.90 0.88 0.90 1.00 0.88 0.92 0.90 0.82 0.85 0.92 0.88 0.85 0.91 0.90 0.76 0.85 0.87 0.84 0.79 0.86 0.85 0.88 0.84 0.81 0.85 0.82 0.87 0.91 0.88 0.90 0.91 0.85 0.89 0.88 0.90 0.90 0.91 0.91 0.89 0.91 0.86 0.88 0.89 0.92 0.85 0.84 0.83 0.87 0.88 0.88 0.81 0.89 0.85 0.87 0.85 0.87 0.85 0.84 0.87 0.86 0.88 0.83 0.83 0.70 0.77 0.72 0.78 0.85 0.78 0.82 0.85 0.74 0.84 0.87 0.74 0.86 0.83 0.84 0.82 0.80 0.85 0.77 0.90 0.87 0.85 0.89 0.85 0.86 0.86 0.78 0.77 0.76 0.88 0.83 0.88 0.80 0.86 0.88 0.77 0.83 0.81 0.82 0.86 0.76 0.83 0.89 0.84 0.82 0.85 0.83 0.84 0.84 0.85 0.83 0.88 0.80 0.88 0.86 0.88 0.88 0.88 0.86 0.85 0.85 0.86 0.88 0.86 0.88 0.88 0.86 0.88 0.85 0.88 0.87 0.86 0.88 0.88 0.88 0.89 0.88 0.81 0.87 0.87 0.88 0.87 0.80 0.89 0.89 0.87 0.87 0.86 0.87 0.87 0.89 0.87 0.87 0.88 0.87 0.87 0.99 0.88 0.88 0.82 0.84 0.89 0.90 0.86 1.00 0.89 0.77 0.77 0.88 0.79 0.86 0.89 0.76 0.84 0.85 0.87 0.87 0.88 0.86 0.87 0.86 0.86 0.89 0.87 0.80 0.82 0.89 0.86 0.81 0.89 0.88 0.73 0.83 0.84 0.82 0.78 0.83 0.81 0.85 0.82 0.79 0.81 0.79 0.84 0.88 0.85 0.88 0.88 0.82 0.87 0.87 0.88 0.88 0.89 0.89 0.87 0.95 0.98 0.98 0.98 0.92 0.81 0.81 0.80 0.93 0.94 0.94 0.90 0.94 0.92 0.93 0.91 0.92 0.90 0.90 0.93 0.92 0.93 0.90 0.89 0.74 0.81 0.77 0.84 0.92 0.80 0.88 0.89 0.79 0.90 0.91 0.78 0.92 0.89 0.89 0.87 0.85 0.89 0.83 0.95 0.94 0.89 0.94 0.91 0.92 0.91 0.82 0.81 0.80 0.93 0.89 0.93 0.87 0.93 0.94 0.82 0.87 0.84 0.87 0.92 0.81 0.90 0.94 0.89 0.87 0.91 0.89 0.89 0.88 0.90 0.89 0.92 0.86 0.95 0.93 0.94 0.95 0.94 0.91 0.92 0.90 0.93 0.95 0.92 0.92 0.95 0.93 0.94 0.91 0.94 0.94 0.91 0.94 0.93 0.93 0.96 0.93 0.86 0.92 0.92 0.93 0.93 0.86 0.95 0.95 0.91 0.93 0.92 0.96 0.92 0.96 0.93 0.93 0.94 0.93 0.93 0.90 0.96 0.95 0.87 0.88 0.96 0.95 0.92 0.89 1.00 0.81 0.80 0.97 0.83 0.90 0.96 0.80 0.89 0.90 0.94 0.90 0.93 0.96 0.92 0.92 0.96 0.95 0.92 0.85 0.86 0.98 0.92 0.87 0.95 0.92 0.77 0.87 0.89 0.91 0.82 0.89 0.87 0.89 0.86 0.83 0.86 0.88 0.89 0.92 0.91 0.92 0.94 0.91 0.94 0.90 0.93 0.93 0.94 0.94 0.93 0.93 0.87 0.89 0.91 0.93 0.85 0.86 0.87 0.80 0.81 0.81 0.74 0.83 0.77 0.80 0.80 0.80 0.78 0.78 0.82 0.80 0.80 0.77 0.78 0.65 0.72 0.68 0.71 0.78 0.70 0.76 0.79 0.68 0.78 0.80 0.71 0.81 0.77 0.77 0.75 0.75 0.78 0.72 0.82 0.81 0.78 0.81 0.79 0.80 0.80 0.72 0.72 0.70 0.81 0.75 0.80 0.74 0.79 0.81 0.72 0.77 0.74 0.77 0.78 0.71 0.76 0.81 0.78 0.76 0.77 0.78 0.76 0.78 0.77 0.76 0.80 0.73 0.81 0.78 0.79 0.80 0.82 0.80 0.79 0.79 0.79 0.80 0.80 0.80 0.82 0.81 0.82 0.81 0.81 0.82 0.80 0.83 0.83 0.81 0.83 0.82 0.75 0.81 0.79 0.81 0.81 0.74 0.81 0.82 0.80 0.79 0.80 0.79 0.79 0.81 0.80 0.80 0.81 0.80 0.80 0.78 0.80 0.82 0.76 0.76 0.87 0.83 0.79 0.77 0.81 1.00 0.72 0.82 0.72 0.79 0.81 0.72 0.78 0.79 0.79 0.79 0.80 0.80 0.80 0.79 0.77 0.83 0.80 0.75 0.76 0.82 0.79 0.75 0.83 0.81 0.69 0.77 0.79 0.73 0.72 0.76 0.75 0.80 0.76 0.73 0.75 0.71 0.79 0.82 0.78 0.81 0.81 0.75 0.80 0.78 0.81 0.81 0.81 0.81 0.79 0.81 0.77 0.79 0.79 0.81 0.74 0.74 0.74 0.77 0.79 0.79 0.72 0.81 0.74 0.79 0.79 0.79 0.76 0.77 0.80 0.79 0.83 0.76 0.77 0.65 0.71 0.67 0.72 0.78 0.70 0.75 0.78 0.67 0.78 0.80 0.69 0.79 0.76 0.77 0.75 0.74 0.78 0.72 0.81 0.81 0.77 0.80 0.80 0.80 0.79 0.71 0.70 0.68 0.80 0.76 0.79 0.73 0.79 0.79 0.71 0.76 0.72 0.77 0.79 0.69 0.76 0.79 0.76 0.74 0.78 0.76 0.76 0.75 0.78 0.76 0.78 0.73 0.81 0.78 0.79 0.80 0.82 0.80 0.78 0.77 0.77 0.78 0.81 0.80 0.79 0.80 0.79 0.78 0.80 0.80 0.79 0.81 0.80 0.80 0.81 0.80 0.74 0.79 0.78 0.79 0.79 0.74 0.81 0.82 0.80 0.78 0.80 0.77 0.79 0.79 0.79 0.79 0.80 0.79 0.74 0.78 0.78 0.82 0.77 0.77 0.79 0.82 0.79 0.77 0.80 0.72 1.00 0.82 0.75 0.79 0.82 0.72 0.77 0.78 0.79 0.79 0.80 0.77 0.79 0.78 0.77 0.82 0.79 0.74 0.75 0.81 0.79 0.73 0.81 0.79 0.67 0.77 0.78 0.72 0.72 0.76 0.75 0.76 0.73 0.70 0.74 0.71 0.78 0.82 0.77 0.81 0.83 0.77 0.80 0.77 0.81 0.80 0.80 0.81 0.79 0.82 0.77 0.80 0.79 0.81 0.75 0.72 0.730.90 0.93 0.92 0.86 0.94 0.89 0.92 0.90 0.91 0.88 0.94 0.92 0.91 0.95 0.89 0.88 0.74 0.80 0.77 0.82 0.95 0.80 0.87 0.89 0.78 0.90 0.91 0.78 0.90 0.87 0.88 0.86 0.90 0.88 0.82 0.93 0.93 0.89 0.92 0.91 0.91 0.90 0.81 0.80 0.87 0.92 0.87 0.91 0.87 0.92 0.91 0.81 0.86 0.83 0.87 0.91 0.79 0.88 0.91 0.87 0.86 0.89 0.88 0.88 0.87 0.89 0.88 0.90 0.85 0.93 0.92 0.92 0.93 0.94 0.90 0.92 0.88 0.91 0.93 0.92 0.91 0.93 0.92 0.93 0.90 0.92 0.94 0.90 0.92 0.92 0.92 0.94 0.92 0.86 0.92 0.92 0.92 0.91 0.86 0.94 0.94 0.91 0.91 0.91 0.93 0.90 0.93 0.91 0.96 0.92 0.91 0.89 0.90 0.93 0.98 0.89 0.88 0.93 0.94 0.91 0.88 0.97 0.82 0.82 1.00 0.84 0.90 0.99 0.81 0.88 0.89 0.92 0.90 0.91 0.95 0.91 0.90 0.92 0.98 0.91 0.85 0.85 0.98 0.92 0.83 0.95 0.90 0.77 0.88 0.89 0.88 0.82 0.93 0.86 0.88 0.84 0.82 0.86 0.90 0.90 0.93 0.89 0.92 0.94 0.91 0.93 0.89 0.93 0.92 0.93 0.92 0.91 0.92 0.86 0.89 0.90 0.92 0.83 0.85 0.85 0.83 0.84 0.84 0.76 0.85 0.80 0.83 0.83 0.82 0.80 0.80 0.84 0.82 0.85 0.80 0.79 0.67 0.73 0.70 0.75 0.80 0.73 0.78 0.82 0.69 0.81 0.82 0.71 0.82 0.79 0.79 0.79 0.77 0.81 0.74 0.85 0.84 0.81 0.83 0.83 0.83 0.82 0.73 0.73 0.72 0.84 0.78 0.83 0.75 0.81 0.84 0.74 0.79 0.77 0.78 0.81 0.71 0.79 0.83 0.80 0.78 0.81 0.80 0.80 0.79 0.80 0.79 0.82 0.74 0.83 0.80 0.82 0.83 0.84 0.81 0.83 0.79 0.81 0.82 0.83 0.83 0.83 0.84 0.83 0.81 0.83 0.83 0.81 0.84 0.83 0.84 0.83 0.84 0.78 0.83 0.83 0.83 0.82 0.77 0.84 0.84 0.85 0.96 0.81 0.81 0.82 0.83 0.82 0.82 0.82 0.82 0.81 0.80 0.83 0.83 0.81 0.80 0.83 0.85 0.82 0.79 0.83 0.72 0.75 0.84 1.00 0.81 0.84 0.73 0.80 0.80 0.95 0.82 0.82 0.81 0.82 0.81 0.79 0.84 0.82 0.78 0.77 0.85 0.81 0.77 0.85 0.82 0.70 0.79 0.81 0.76 0.74 0.79 0.78 0.80 0.78 0.74 0.78 0.74 0.82 0.85 0.81 0.84 0.84 0.78 0.82 0.81 0.84 0.84 0.84 0.84 0.82 0.83 0.79 0.82 0.80 0.84 0.77 0.76 0.74 0.88 0.88 0.90 0.83 0.91 0.86 0.89 0.88 0.89 0.85 0.85 0.89 0.88 0.90 0.84 0.86 0.71 0.78 0.73 0.80 0.85 0.79 0.83 0.87 0.75 0.86 0.89 0.76 0.88 0.85 0.85 0.84 0.81 0.87 0.79 0.91 0.90 0.86 0.89 0.88 0.89 0.88 0.79 0.78 0.76 0.90 0.83 0.89 0.80 0.87 0.89 0.79 0.85 0.82 0.84 0.88 0.77 0.85 0.90 0.86 0.83 0.87 0.86 0.85 0.85 0.87 0.85 0.89 0.80 0.89 0.86 0.88 0.90 0.90 0.87 0.87 0.86 0.87 0.88 0.89 0.89 0.89 0.88 0.89 0.87 0.89 0.89 0.87 0.97 0.89 0.89 0.90 0.90 0.82 0.89 0.88 0.89 0.88 0.83 0.89 0.90 0.88 0.89 0.88 0.88 0.89 0.91 0.88 0.89 0.90 0.89 0.88 0.86 0.90 0.90 0.84 0.85 0.91 0.90 0.89 0.86 0.90 0.79 0.79 0.90 0.81 1.00 0.90 0.97 0.87 0.99 0.88 0.87 0.89 0.87 0.88 0.88 0.86 0.90 0.89 0.81 0.84 0.91 0.86 0.82 0.90 0.88 0.75 0.84 0.98 0.81 0.79 0.84 0.84 0.86 0.83 0.79 0.83 0.80 0.86 0.89 0.86 0.89 0.90 0.83 0.88 0.87 0.89 0.88 0.89 0.90 0.87 0.89 0.84 0.87 0.87 0.90 0.83 0.83 0.82 0.91 0.93 0.93 0.85 0.93 0.88 0.92 0.90 0.91 0.88 0.94 0.93 0.91 0.96 0.89 0.88 0.74 0.81 0.77 0.82 0.94 0.81 0.87 0.89 0.77 0.89 0.91 0.78 0.91 0.87 0.88 0.87 0.90 0.89 0.82 0.94 0.93 0.89 0.92 0.91 0.91 0.90 0.81 0.80 0.87 0.92 0.87 0.92 0.85 0.91 0.92 0.81 0.87 0.84 0.87 0.90 0.79 0.87 0.92 0.88 0.86 0.90 0.88 0.88 0.87 0.89 0.87 0.91 0.84 0.92 0.90 0.92 0.93 0.94 0.90 0.92 0.88 0.91 0.92 0.91 0.92 0.93 0.92 0.93 0.90 0.92 0.93 0.90 0.93 0.93 0.93 0.94 0.93 0.86 0.92 0.92 0.92 0.91 0.86 0.93 0.93 0.92 0.91 0.91 0.92 0.91 0.93 0.91 0.95 0.92 0.91 0.90 0.90 0.92 0.97 0.89 0.89 0.93 0.95 0.92 0.89 0.96 0.81 0.82 0.99 0.84 0.90 1.00 0.81 0.89 0.89 0.91 0.90 0.92 0.96 0.91 0.91 0.91 0.97 0.91 0.85 0.86 0.99 0.90 0.84 0.94 0.91 0.77 0.88 0.88 0.87 0.82 0.92 0.86 0.88 0.85 0.82 0.86 0.88 0.90 0.93 0.89 0.92 0.94 0.89 0.92 0.89 0.93 0.92 0.93 0.92 0.91 0.92 0.87 0.89 0.89 0.92 0.86 0.84 0.84 0.77 0.78 0.80 0.72 0.81 0.75 0.78 0.78 0.78 0.75 0.76 0.80 0.78 0.81 0.75 0.76 0.64 0.70 0.66 0.71 0.75 0.70 0.74 0.78 0.67 0.76 0.79 0.69 0.79 0.76 0.75 0.75 0.73 0.78 0.71 0.81 0.80 0.77 0.78 0.79 0.79 0.79 0.70 0.69 0.68 0.80 0.73 0.79 0.72 0.78 0.78 0.71 0.75 0.73 0.75 0.77 0.67 0.75 0.79 0.76 0.74 0.78 0.76 0.76 0.75 0.77 0.75 0.78 0.72 0.79 0.77 0.77 0.79 0.80 0.77 0.79 0.76 0.78 0.78 0.79 0.79 0.79 0.79 0.78 0.77 0.78 0.80 0.77 0.91 0.79 0.78 0.80 0.80 0.73 0.79 0.78 0.78 0.78 0.75 0.80 0.80 0.79 0.78 0.78 0.77 0.78 0.80 0.78 0.79 0.80 0.79 0.76 0.76 0.79 0.82 0.76 0.76 0.79 0.80 0.79 0.76 0.80 0.72 0.72 0.81 0.73 0.97 0.81 1.00 0.78 0.97 0.78 0.78 0.78 0.77 0.78 0.78 0.75 0.80 0.78 0.74 0.76 0.81 0.77 0.72 0.81 0.77 0.66 0.75 0.96 0.70 0.71 0.74 0.74 0.76 0.73 0.70 0.74 0.70 0.78 0.80 0.77 0.80 0.80 0.74 0.78 0.77 0.80 0.78 0.79 0.80 0.77 0.79 0.75 0.78 0.77 0.80 0.73 0.73 0.72 0.87 0.89 0.89 0.80 0.90 0.85 0.88 0.88 0.88 0.86 0.84 0.88 0.87 0.89 0.85 0.84 0.71 0.78 0.74 0.79 0.86 0.78 0.83 0.86 0.74 0.84 0.88 0.76 0.87 0.84 0.85 0.84 0.81 0.86 0.79 0.91 0.89 0.86 0.90 0.88 0.88 0.88 0.79 0.78 0.76 0.90 0.84 0.89 0.80 0.87 0.88 0.78 0.85 0.82 0.84 0.86 0.77 0.84 0.90 0.86 0.83 0.86 0.84 0.85 0.85 0.86 0.85 0.88 0.79 0.89 0.87 0.87 0.89 0.89 0.87 0.87 0.86 0.88 0.88 0.88 0.88 0.89 0.87 0.88 0.86 0.88 0.87 0.87 0.90 0.90 0.90 0.90 0.90 0.83 0.91 0.89 0.89 0.89 0.82 0.88 0.90 0.89 0.87 0.87 0.86 0.87 0.88 0.88 0.87 0.89 0.98 0.88 0.85 0.87 0.88 0.83 0.84 0.89 0.90 0.87 0.84 0.89 0.78 0.77 0.88 0.80 0.87 0.89 0.78 1.00 0.87 0.87 0.87 0.88 0.86 0.88 0.87 0.84 0.89 0.88 0.81 0.83 0.88 0.85 0.83 0.90 0.89 0.75 0.83 0.85 0.81 0.79 0.83 0.83 0.87 0.84 0.81 0.83 0.79 0.85 0.89 0.86 0.88 0.89 0.83 0.86 0.86 0.88 0.89 0.89 0.89 0.88 0.88 0.84 0.86 0.87 0.89 0.82 0.81 0.80 0.87 0.88 0.90 0.82 0.91 0.85 0.88 0.87 0.87 0.86 0.85 0.89 0.88 0.89 0.83 0.85 0.72 0.78 0.73 0.79 0.85 0.78 0.83 0.87 0.75 0.85 0.88 0.76 0.88 0.85 0.85 0.84 0.81 0.87 0.79 0.91 0.89 0.86 0.89 0.88 0.89 0.88 0.79 0.78 0.76 0.90 0.83 0.89 0.80 0.87 0.89 0.79 0.84 0.82 0.83 0.87 0.77 0.85 0.90 0.86 0.83 0.87 0.85 0.85 0.85 0.87 0.85 0.89 0.80 0.90 0.87 0.88 0.89 0.89 0.87 0.87 0.86 0.87 0.88 0.88 0.88 0.89 0.87 0.89 0.87 0.89 0.89 0.87 0.98 0.89 0.88 0.90 0.90 0.82 0.89 0.88 0.88 0.88 0.83 0.90 0.90 0.87 0.88 0.88 0.88 0.88 0.90 0.88 0.88 0.90 0.88 0.88 0.85 0.89 0.89 0.84 0.85 0.91 0.90 0.88 0.85 0.90 0.79 0.78 0.89 0.80 0.99 0.89 0.97 0.87 1.00 0.88 0.87 0.89 0.87 0.88 0.88 0.86 0.89 0.89 0.81 0.84 0.90 0.85 0.83 0.90 0.88 0.75 0.84 0.99 0.81 0.78 0.84 0.83 0.86 0.82 0.79 0.83 0.80 0.85 0.89 0.87 0.89 0.89 0.83 0.87 0.87 0.89 0.89 0.90 0.89 0.88 0.89 0.84 0.87 0.87 0.90 0.83 0.81 0.82 0.91 0.92 0.93 0.87 0.93 0.90 0.91 0.90 0.91 0.89 0.88 0.91 0.90 0.91 0.88 0.87 0.73 0.79 0.76 0.83 0.89 0.79 0.86 0.89 0.77 0.88 0.90 0.77 0.90 0.87 0.87 0.86 0.84 0.88 0.81 0.93 0.92 0.88 0.93 0.90 0.90 0.90 0.81 0.80 0.79 0.91 0.87 0.91 0.85 0.90 0.93 0.81 0.86 0.84 0.85 0.90 0.79 0.88 0.93 0.88 0.85 0.89 0.88 0.88 0.87 0.89 0.87 0.91 0.83 0.93 0.91 0.92 0.93 0.92 0.90 0.90 0.88 0.91 0.92 0.91 0.90 0.93 0.91 0.93 0.90 0.93 0.91 0.89 0.92 0.91 0.92 0.93 0.91 0.84 0.91 0.90 0.91 0.91 0.84 0.93 0.93 0.90 0.99 0.90 0.92 0.90 0.94 0.91 0.91 0.92 0.91 0.92 0.87 0.93 0.91 0.85 0.86 0.94 0.93 0.90 0.87 0.94 0.79 0.79 0.92 0.95 0.88 0.91 0.78 0.87 0.88 1.00 0.89 0.91 0.88 0.91 0.90 0.92 0.93 0.91 0.83 0.85 0.92 0.90 0.87 0.93 0.91 0.76 0.86 0.88 0.88 0.80 0.88 0.86 0.89 0.85 0.83 0.85 0.86 0.88 0.91 0.89 0.91 0.92 0.88 0.91 0.89 0.91 0.92 0.92 0.92 0.91 0.91 0.85 0.88 0.89 0.92 0.85 0.84 0.84 0.88 0.91 0.91 0.81 0.91 0.86 0.88 0.88 0.89 0.86 0.85 0.89 0.88 0.89 0.84 0.86 0.72 0.78 0.74 0.80 0.86 0.79 0.83 0.88 0.74 0.86 0.89 0.76 0.88 0.85 0.85 0.84 0.82 0.87 0.80 0.91 0.90 0.86 0.90 0.89 0.89 0.88 0.80 0.79 0.77 0.90 0.83 0.89 0.81 0.87 0.90 0.80 0.87 0.83 0.85 0.87 0.78 0.85 0.91 0.87 0.85 0.86 0.86 0.86 0.86 0.87 0.85 0.90 0.80 0.89 0.86 0.87 0.89 0.90 0.88 0.86 0.87 0.86 0.88 0.89 0.89 0.89 0.87 0.91 0.89 0.89 0.90 0.87 0.91 0.90 0.90 0.90 0.91 0.83 0.89 0.88 0.89 0.88 0.82 0.89 0.90 0.89 0.89 0.88 0.86 0.88 0.90 0.89 0.88 0.98 0.89 0.88 0.89 0.89 0.90 0.84 0.84 0.91 0.92 0.88 0.87 0.90 0.79 0.79 0.90 0.82 0.87 0.90 0.78 0.87 0.87 0.89 1.00 0.99 0.87 0.89 0.88 0.85 0.90 0.99 0.82 0.85 0.90 0.86 0.84 0.90 0.90 0.75 0.85 0.86 0.80 0.79 0.84 0.84 0.88 0.85 0.82 0.84 0.79 0.86 0.90 0.88 0.90 0.89 0.83 0.87 0.88 0.90 0.91 0.91 0.91 0.89 0.89 0.86 0.89 0.89 0.91 0.83 0.82 0.81 0.91 0.92 0.93 0.86 0.93 0.89 0.91 0.90 0.91 0.88 0.87 0.91 0.90 0.91 0.85 0.87 0.73 0.80 0.75 0.82 0.88 0.80 0.86 0.89 0.78 0.87 0.90 0.77 0.90 0.87 0.87 0.86 0.84 0.88 0.81 0.93 0.92 0.88 0.93 0.90 0.91 0.90 0.81 0.81 0.79 0.92 0.86 0.92 0.81 0.88 0.93 0.81 0.88 0.84 0.87 0.90 0.81 0.88 0.93 0.89 0.86 0.88 0.87 0.89 0.88 0.89 0.88 0.92 0.81 0.92 0.89 0.90 0.92 0.92 0.90 0.87 0.89 0.88 0.90 0.90 0.91 0.91 0.89 0.93 0.91 0.91 0.92 0.89 0.93 0.92 0.91 0.92 0.92 0.83 0.91 0.89 0.91 0.90 0.83 0.91 0.92 0.90 0.91 0.90 0.90 0.90 0.93 0.91 0.91 1.00 0.91 0.91 0.90 0.92 0.92 0.85 0.86 0.94 0.94 0.90 0.88 0.93 0.80 0.80 0.91 0.82 0.89 0.92 0.78 0.88 0.89 0.91 0.99 1.00 0.89 0.91 0.90 0.89 0.92 0.99 0.84 0.86 0.92 0.87 0.86 0.92 0.92 0.77 0.86 0.87 0.84 0.80 0.87 0.86 0.90 0.86 0.83 0.85 0.82 0.87 0.91 0.89 0.91 0.91 0.84 0.90 0.89 0.91 0.92 0.93 0.93 0.91 0.91 0.87 0.89 0.90 0.93 0.86 0.86 0.84 0.88 0.91 0.91 0.82 0.91 0.86 0.88 0.87 0.88 0.85 0.87 0.90 0.88 0.90 0.84 0.86 0.71 0.79 0.75 0.80 0.87 0.78 0.84 0.87 0.75 0.86 0.88 0.76 0.89 0.84 0.86 0.84 0.82 0.86 0.80 0.91 0.90 0.86 0.90 0.88 0.88 0.88 0.79 0.79 0.78 0.90 0.85 0.89 0.80 0.87 0.89 0.79 0.85 0.83 0.86 0.86 0.78 0.85 0.90 0.87 0.85 0.87 0.85 0.85 0.86 0.85 0.85 0.89 0.81 0.90 0.87 0.89 0.89 0.90 0.87 0.87 0.86 0.88 0.89 0.88 0.89 0.90 0.88 0.91 0.89 0.88 0.91 0.88 0.90 0.90 0.90 0.90 0.90 0.84 0.90 0.89 0.89 0.88 0.83 0.90 0.90 0.88 0.88 0.88 0.88 0.88 0.91 0.89 0.89 0.89 0.88 0.88 0.87 0.89 0.92 0.85 0.86 0.91 0.92 0.88 0.86 0.96 0.80 0.77 0.95 0.81 0.87 0.96 0.77 0.86 0.87 0.88 0.87 0.89 1.00 0.89 0.88 0.87 0.92 0.88 0.82 0.83 0.97 0.86 0.83 0.91 0.89 0.75 0.85 0.86 0.83 0.79 0.85 0.83 0.86 0.83 0.80 0.83 0.81 0.86 0.89 0.87 0.89 0.89 0.84 0.88 0.87 0.89 0.90 0.90 0.90 0.89 0.89 0.84 0.87 0.88 0.90 0.82 0.82 0.82 0.91 0.91 0.92 0.84 0.91 0.88 0.89 0.88 0.90 0.88 0.87 0.90 0.89 0.90 0.85 0.87 0.72 0.79 0.75 0.81 0.88 0.78 0.85 0.88 0.76 0.86 0.89 0.77 0.89 0.87 0.86 0.85 0.83 0.88 0.80 0.92 0.91 0.87 0.92 0.89 0.90 0.89 0.80 0.79 0.79 0.91 0.85 0.90 0.80 0.88 0.91 0.80 0.86 0.83 0.85 0.89 0.79 0.87 0.92 0.87 0.84 0.88 0.87 0.86 0.86 0.89 0.87 0.90 0.81 0.91 0.88 0.89 0.91 0.90 0.88 0.87 0.87 0.88 0.89 0.89 0.89 0.90 0.88 0.90 0.88 0.90 0.90 0.87 0.91 0.90 0.90 0.91 0.90 0.82 0.90 0.89 0.90 0.90 0.82 0.90 0.91 0.89 0.90 0.90 0.89 0.89 0.92 1.00 0.90 0.92 0.90 0.90 0.88 0.91 0.91 0.85 0.87 0.92 0.93 0.90 0.87 0.92 0.80 0.79 0.91 0.82 0.88 0.91 0.78 0.88 0.88 0.91 0.89 0.91 0.89 1.00 0.89 0.88 0.92 0.90 0.86 0.85 0.92 0.87 0.84 0.92 0.91 0.76 0.85 0.86 0.84 0.79 0.86 0.84 0.87 0.84 0.82 0.84 0.82 0.86 0.90 0.88 0.90 0.90 0.84 0.89 0.88 0.91 0.91 0.91 0.91 0.90 0.91 0.86 0.89 0.89 0.92 0.84 0.83 0.83 0.90 0.90 0.92 0.85 0.91 0.88 0.89 0.88 0.90 0.87 0.85 0.90 0.89 0.89 0.85 0.86 0.72 0.79 0.75 0.81 0.87 0.79 0.85 0.87 0.76 0.85 0.89 0.77 0.90 0.86 0.87 0.85 0.83 0.88 0.82 0.92 0.90 0.88 0.92 0.89 0.91 0.88 0.80 0.80 0.79 0.91 0.85 0.90 0.81 0.88 0.91 0.80 0.85 0.82 0.84 0.89 0.79 0.86 0.92 0.88 0.84 0.88 0.87 0.87 0.86 0.87 0.86 0.91 0.81 0.91 0.88 0.89 0.91 0.91 0.89 0.88 0.88 0.89 0.90 0.89 0.90 0.91 0.88 0.92 0.90 0.90 0.93 0.88 0.92 0.91 0.91 0.92 0.91 0.83 0.91 0.91 0.90 0.90 0.83 0.91 0.92 0.89 0.89 0.99 0.88 1.00 0.92 0.89 0.89 0.91 0.90 0.90 0.87 0.91 0.91 0.84 0.85 0.93 0.93 1.00 0.86 0.92 0.79 0.78 0.90 0.81 0.88 0.91 0.78 0.87 0.88 0.90 0.88 0.90 0.88 0.89 1.00 0.88 0.91 0.90 0.81 0.85 0.92 0.87 0.85 0.91 0.90 0.75 0.85 0.86 0.84 0.79 0.85 0.85 0.88 0.84 0.81 0.85 0.82 0.86 0.90 0.87 0.89 0.90 0.84 0.89 0.87 0.89 0.90 0.91 0.91 0.89 0.90 0.86 0.87 0.89 0.92 0.85 0.84 0.83 0.89 0.89 0.90 0.91 0.91 0.91 0.92 0.87 0.90 0.87 0.87 0.89 0.89 0.89 0.90 0.85 0.72 0.77 0.73 0.82 0.90 0.77 0.87 0.85 0.76 0.87 0.88 0.75 0.88 0.87 0.86 0.84 0.82 0.86 0.81 0.93 0.91 0.86 0.92 0.88 0.88 0.87 0.80 0.77 0.77 0.89 0.89 0.90 0.91 0.92 0.91 0.78 0.83 0.80 0.82 0.90 0.79 0.88 0.91 0.86 0.82 0.89 0.86 0.88 0.85 0.88 0.86 0.89 0.85 0.94 0.95 0.95 0.95 0.92 0.89 0.92 0.87 0.92 0.95 0.89 0.89 0.94 0.90 0.91 0.88 0.92 0.90 0.89 0.90 0.89 0.90 0.95 0.89 0.83 0.89 0.89 0.91 0.90 0.85 0.92 0.92 0.86 0.91 0.89 0.99 0.88 0.96 0.89 0.91 0.90 0.90 0.92 0.85 0.95 0.91 0.83 0.84 0.96 0.92 0.88 0.86 0.96 0.77 0.77 0.92 0.79 0.86 0.91 0.75 0.84 0.86 0.92 0.85 0.89 0.87 0.88 0.88 1.00 0.93 0.89 0.80 0.83 0.91 0.93 0.84 0.93 0.89 0.74 0.84 0.87 0.95 0.78 0.86 0.85 0.87 0.83 0.82 0.83 0.92 0.86 0.89 0.89 0.89 0.92 0.93 0.95 0.87 0.90 0.89 0.91 0.90 0.89 0.89 0.83 0.84 0.88 0.88 0.82 0.82 0.84 0.91 0.93 0.93 0.86 0.94 0.89 0.92 0.90 0.93 0.89 0.96 0.93 0.92 0.96 0.90 0.89 0.75 0.81 0.77 0.83 0.96 0.81 0.87 0.90 0.77 0.90 0.92 0.78 0.91 0.88 0.88 0.87 0.94 0.90 0.83 0.94 0.93 0.90 0.93 0.91 0.92 0.90 0.82 0.81 0.91 0.93 0.88 0.93 0.86 0.92 0.92 0.82 0.88 0.84 0.87 0.92 0.81 0.90 0.93 0.89 0.86 0.90 0.88 0.88 0.88 0.91 0.89 0.91 0.85 0.94 0.92 0.93 0.93 0.95 0.90 0.92 0.89 0.91 0.93 0.92 0.92 0.94 0.92 0.95 0.92 0.93 0.95 0.91 0.93 0.93 0.94 0.95 0.94 0.86 0.93 0.92 0.93 0.92 0.86 0.94 0.94 0.92 0.92 0.91 0.94 0.91 0.95 0.92 0.98 0.93 0.92 0.92 0.91 0.93 0.99 0.89 0.89 0.95 0.95 0.92 0.89 0.95 0.83 0.82 0.98 0.84 0.90 0.97 0.80 0.89 0.89 0.93 0.90 0.92 0.92 0.92 0.91 0.93 1.00 0.92 0.86 0.86 0.97 0.91 0.85 0.94 0.93 0.78 0.88 0.89 0.87 0.82 0.95 0.87 0.90 0.86 0.83 0.87 0.92 0.90 0.93 0.91 0.93 0.94 0.89 0.93 0.90 0.93 0.93 0.93 0.93 0.92 0.93 0.88 0.90 0.91 0.93 0.86 0.85 0.85 0.90 0.92 0.93 0.85 0.92 0.89 0.89 0.89 0.91 0.88 0.87 0.91 0.90 0.90 0.85 0.87 0.73 0.80 0.75 0.82 0.88 0.80 0.85 0.89 0.76 0.87 0.90 0.77 0.90 0.87 0.87 0.85 0.84 0.88 0.81 0.93 0.91 0.88 0.93 0.90 0.91 0.90 0.81 0.80 0.79 0.92 0.86 0.91 0.82 0.88 0.92 0.81 0.88 0.84 0.86 0.89 0.80 0.87 0.93 0.89 0.86 0.88 0.87 0.88 0.88 0.89 0.87 0.92 0.81 0.91 0.89 0.90 0.91 0.91 0.89 0.88 0.88 0.89 0.91 0.90 0.90 0.91 0.89 0.93 0.91 0.91 0.91 0.89 0.93 0.91 0.91 0.92 0.92 0.83 0.91 0.89 0.90 0.90 0.83 0.91 0.92 0.89 0.90 0.90 0.89 0.90 0.93 0.91 0.90 0.99 0.91 0.92 0.89 0.92 0.91 0.85 0.86 0.94 0.93 0.90 0.87 0.92 0.80 0.79 0.91 0.82 0.89 0.91 0.78 0.88 0.89 0.91 0.99 0.99 0.88 0.90 0.90 0.89 0.92 1.00 0.82 0.86 0.91 0.87 0.85 0.91 0.92 0.77 0.86 0.88 0.84 0.80 0.87 0.86 0.89 0.86 0.82 0.85 0.83 0.86 0.90 0.89 0.90 0.90 0.84 0.89 0.89 0.90 0.92 0.92 0.92 0.91 0.90 0.86 0.89 0.90 0.92 0.85 0.85 0.83 0.83 0.84 0.83 0.75 0.85 0.80 0.82 0.82 0.82 0.80 0.81 0.84 0.82 0.85 0.80 0.80 0.67 0.74 0.69 0.74 0.82 0.73 0.78 0.81 0.69 0.81 0.83 0.70 0.82 0.79 0.78 0.77 0.77 0.81 0.73 0.84 0.84 0.80 0.83 0.83 0.83 0.82 0.73 0.72 0.72 0.83 0.77 0.83 0.75 0.80 0.83 0.73 0.80 0.76 0.78 0.82 0.72 0.79 0.83 0.80 0.77 0.81 0.79 0.78 0.78 0.81 0.80 0.82 0.74 0.84 0.81 0.82 0.83 0.85 0.81 0.81 0.80 0.81 0.82 0.83 0.83 0.83 0.84 0.83 0.80 0.83 0.83 0.80 0.84 0.84 0.84 0.84 0.85 0.77 0.83 0.83 0.83 0.82 0.76 0.83 0.85 0.84 0.82 0.82 0.82 0.81 0.83 0.85 0.84 0.84 0.84 0.80 0.81 0.83 0.86 0.80 0.79 0.84 0.84 0.82 0.80 0.85 0.75 0.74 0.85 0.78 0.81 0.85 0.74 0.81 0.81 0.83 0.82 0.84 0.82 0.86 0.81 0.80 0.86 0.82 1.00 0.78 0.87 0.81 0.76 0.86 0.83 0.70 0.78 0.79 0.76 0.75 0.79 0.78 0.80 0.77 0.74 0.79 0.74 0.81 0.84 0.81 0.84 0.84 0.78 0.82 0.81 0.84 0.84 0.84 0.85 0.82 0.84 0.80 0.83 0.81 0.86 0.76 0.76 0.75 0.84 0.85 0.86 0.78 0.86 0.82 0.84 0.84 0.84 0.82 0.82 0.85 0.83 0.84 0.79 0.82 0.70 0.75 0.71 0.77 0.83 0.75 0.81 0.84 0.71 0.81 0.84 0.73 0.85 0.82 0.82 0.80 0.78 0.82 0.78 0.87 0.85 0.83 0.87 0.85 0.85 0.84 0.77 0.75 0.74 0.86 0.80 0.86 0.76 0.82 0.87 0.77 0.83 0.80 0.83 0.82 0.76 0.81 0.91 0.84 0.81 0.84 0.81 0.83 0.82 0.82 0.81 0.91 0.78 0.87 0.84 0.85 0.86 0.86 0.83 0.83 0.83 0.84 0.85 0.84 0.85 0.86 0.82 0.86 0.85 0.85 0.85 0.83 0.87 0.86 0.86 0.86 0.87 0.79 0.85 0.85 0.85 0.85 0.79 0.85 0.87 0.83 0.85 0.85 0.84 0.85 0.87 0.86 0.85 0.87 0.85 0.87 0.83 0.86 0.85 0.80 0.81 0.87 0.89 0.85 0.82 0.86 0.76 0.75 0.85 0.77 0.84 0.86 0.76 0.83 0.84 0.85 0.85 0.86 0.83 0.85 0.85 0.83 0.86 0.86 0.78 1.00 0.86 0.82 0.81 0.85 0.87 0.73 0.81 0.83 0.78 0.74 0.81 0.81 0.84 0.81 0.78 0.82 0.78 0.81 0.85 0.83 0.85 0.84 0.79 0.84 0.83 0.85 0.86 0.87 0.86 0.86 0.85 0.80 0.83 0.84 0.87 0.79 0.78 0.78 0.92 0.94 0.95 0.87 0.95 0.90 0.92 0.90 0.92 0.88 0.92 0.93 0.92 0.95 0.89 0.90 0.74 0.81 0.77 0.83 0.92 0.81 0.87 0.90 0.77 0.89 0.91 0.78 0.92 0.88 0.89 0.87 0.87 0.89 0.83 0.94 0.93 0.90 0.93 0.92 0.92 0.91 0.82 0.81 0.83 0.93 0.87 0.92 0.84 0.91 0.93 0.81 0.87 0.84 0.87 0.91 0.80 0.87 0.92 0.88 0.86 0.90 0.89 0.88 0.87 0.89 0.88 0.91 0.84 0.93 0.91 0.92 0.93 0.94 0.90 0.92 0.88 0.92 0.93 0.92 0.92 0.93 0.93 0.94 0.91 0.93 0.94 0.90 0.93 0.93 0.93 0.94 0.94 0.87 0.93 0.93 0.93 0.91 0.86 0.94 0.93 0.92 0.92 0.92 0.93 0.92 0.94 0.92 0.94 0.93 0.92 0.90 0.90 0.93 0.96 0.90 0.90 0.94 0.95 0.92 0.89 0.98 0.82 0.81 0.98 0.85 0.91 0.99 0.81 0.88 0.90 0.92 0.90 0.92 0.97 0.92 0.92 0.91 0.97 0.91 0.87 0.86 1.00 0.91 0.86 0.94 0.91 0.77 0.88 0.89 0.87 0.82 0.90 0.87 0.89 0.85 0.82 0.86 0.86 0.90 0.93 0.89 0.92 0.94 0.88 0.92 0.90 0.93 0.94 0.94 0.93 0.93 0.92 0.87 0.90 0.90 0.94 0.86 0.85 0.84 0.88 0.89 0.88 0.86 0.91 0.87 0.90 0.86 0.88 0.86 0.86 0.88 0.88 0.90 0.90 0.84 0.72 0.77 0.73 0.81 0.90 0.77 0.85 0.85 0.76 0.87 0.88 0.74 0.87 0.85 0.85 0.83 0.81 0.85 0.80 0.92 0.90 0.86 0.90 0.87 0.87 0.87 0.80 0.76 0.76 0.88 0.87 0.89 0.97 0.92 0.88 0.77 0.83 0.80 0.82 0.88 0.76 0.86 0.89 0.84 0.81 0.87 0.85 0.85 0.84 0.86 0.84 0.87 0.83 0.92 0.91 0.93 0.92 0.91 0.88 0.92 0.85 0.90 0.93 0.90 0.90 0.93 0.90 0.89 0.86 0.90 0.90 0.88 0.89 0.89 0.90 0.94 0.90 0.83 0.89 0.89 0.90 0.89 0.84 0.92 0.92 0.87 0.89 0.88 0.93 0.87 0.91 0.87 0.89 0.89 0.89 0.87 0.86 0.91 0.90 0.85 0.84 0.92 0.90 0.88 0.86 0.92 0.79 0.79 0.92 0.81 0.86 0.90 0.77 0.85 0.85 0.90 0.86 0.87 0.86 0.87 0.87 0.93 0.91 0.87 0.81 0.82 0.91 1.00 0.82 0.93 0.89 0.74 0.84 0.87 0.91 0.79 0.85 0.84 0.94 0.93 0.92 0.83 0.88 0.88 0.93 0.90 0.91 0.93 0.98 0.92 0.87 0.91 0.88 0.90 0.90 0.88 0.90 0.83 0.86 0.87 0.88 0.81 0.81 0.83 0.84 0.86 0.86 0.78 0.85 0.82 0.83 0.82 0.84 0.82 0.81 0.86 0.83 0.85 0.80 0.81 0.67 0.74 0.70 0.75 0.82 0.74 0.79 0.82 0.72 0.80 0.84 0.72 0.84 0.81 0.81 0.79 0.77 0.83 0.76 0.86 0.84 0.81 0.86 0.83 0.84 0.83 0.75 0.75 0.73 0.85 0.79 0.84 0.76 0.84 0.97 0.75 0.80 0.78 0.79 0.83 0.75 0.81 0.87 0.82 0.80 0.83 0.82 0.82 0.81 0.82 0.81 0.86 0.76 0.86 0.83 0.84 0.86 0.84 0.83 0.82 0.83 0.84 0.85 0.83 0.84 0.86 0.83 0.86 0.84 0.85 0.84 0.82 0.86 0.86 0.85 0.87 0.86 0.78 0.85 0.84 0.85 0.86 0.77 0.86 0.87 0.82 0.86 0.85 0.84 0.85 0.86 0.85 0.83 0.86 0.86 0.86 0.82 0.86 0.83 0.79 0.80 0.88 0.88 0.85 0.81 0.87 0.75 0.73 0.83 0.77 0.82 0.84 0.72 0.83 0.83 0.87 0.84 0.86 0.83 0.84 0.85 0.84 0.85 0.85 0.76 0.81 0.86 0.82 1.00 0.86 0.87 0.71 0.79 0.83 0.79 0.75 0.81 0.79 0.83 0.81 0.78 0.80 0.76 0.80 0.85 0.83 0.85 0.84 0.78 0.82 0.81 0.84 0.86 0.87 0.86 0.85 0.86 0.81 0.82 0.84 0.87 0.78 0.79 0.78 0.90 0.93 0.92 0.86 0.94 0.89 0.92 0.90 0.91 0.90 0.89 0.93 0.92 0.94 0.90 0.88 0.75 0.81 0.77 0.84 0.92 0.81 0.88 0.89 0.80 0.90 0.92 0.79 0.91 0.88 0.88 0.87 0.84 0.90 0.83 0.95 0.93 0.90 0.94 0.91 0.91 0.91 0.83 0.81 0.79 0.93 0.88 0.93 0.89 0.92 0.93 0.81 0.87 0.84 0.87 0.91 0.80 0.89 0.93 0.89 0.86 0.90 0.89 0.89 0.88 0.90 0.89 0.91 0.84 0.94 0.92 0.94 0.98 0.93 0.90 0.93 0.88 0.92 0.94 0.93 0.92 0.94 0.93 0.93 0.90 0.94 0.92 0.91 0.94 0.93 0.94 0.96 0.93 0.87 0.93 0.93 0.93 0.93 0.87 0.95 0.95 0.91 0.93 0.92 0.94 0.91 0.93 0.93 0.91 0.93 0.93 0.92 0.89 0.94 0.93 0.89 0.89 0.95 0.95 0.91 0.89 0.95 0.83 0.81 0.95 0.85 0.90 0.94 0.81 0.90 0.90 0.93 0.90 0.92 0.91 0.92 0.91 0.93 0.94 0.91 0.86 0.85 0.94 0.93 0.86 1.00 0.92 0.78 0.87 0.90 0.90 0.83 0.88 0.87 0.89 0.86 0.83 0.86 0.86 0.91 0.94 0.91 0.93 0.94 0.92 0.93 0.90 0.93 0.93 0.94 0.93 0.92 0.93 0.88 0.90 0.91 0.93 0.85 0.85 0.86 0.91 0.92 0.92 0.85 0.92 0.89 0.90 0.88 0.91 0.88 0.88 0.90 0.90 0.90 0.85 0.88 0.73 0.80 0.75 0.81 0.90 0.81 0.86 0.89 0.78 0.88 0.90 0.78 0.90 0.87 0.87 0.86 0.86 0.89 0.81 0.93 0.91 0.88 0.93 0.90 0.91 0.90 0.81 0.80 0.81 0.92 0.86 0.92 0.83 0.88 0.93 0.81 0.87 0.85 0.86 0.88 0.81 0.88 0.94 0.89 0.86 0.88 0.87 0.88 0.88 0.88 0.87 0.92 0.83 0.93 0.88 0.91 0.92 0.92 0.90 0.88 0.89 0.90 0.92 0.90 0.91 0.92 0.88 0.92 0.90 0.92 0.91 0.89 0.93 0.93 0.92 0.93 0.93 0.84 0.92 0.91 0.91 0.93 0.84 0.91 0.94 0.89 0.91 0.90 0.90 0.91 0.92 0.91 0.90 0.93 0.92 0.93 0.89 0.92 0.91 0.84 0.86 0.93 0.93 0.90 0.88 0.92 0.81 0.79 0.90 0.82 0.88 0.91 0.77 0.89 0.88 0.91 0.90 0.92 0.89 0.91 0.90 0.89 0.93 0.92 0.83 0.87 0.91 0.89 0.87 0.92 1.00 0.77 0.86 0.88 0.84 0.81 0.88 0.86 0.90 0.86 0.83 0.86 0.84 0.87 0.92 0.98 0.98 0.91 0.85 0.89 0.97 0.97 0.93 0.93 0.93 0.92 0.93 0.88 0.90 0.92 0.93 0.85 0.86 0.85 0.76 0.78 0.78 0.70 0.78 0.74 0.76 0.76 0.76 0.74 0.74 0.77 0.76 0.77 0.73 0.74 0.65 0.68 0.65 0.69 0.76 0.69 0.72 0.75 0.67 0.75 0.76 0.67 0.77 0.75 0.74 0.73 0.73 0.75 0.71 0.78 0.92 0.74 0.78 0.94 0.77 0.76 0.71 0.68 0.68 0.77 0.72 0.77 0.70 0.74 0.77 0.70 0.74 0.72 0.73 0.75 0.68 0.74 0.78 0.75 0.73 0.75 0.73 0.74 0.75 0.75 0.74 0.78 0.70 0.76 0.74 0.75 0.76 0.77 0.74 0.75 0.74 0.75 0.76 0.76 0.76 0.76 0.75 0.77 0.75 0.76 0.77 0.75 0.77 0.78 0.78 0.78 0.78 0.71 0.77 0.77 0.76 0.76 0.71 0.77 0.78 0.76 0.76 0.76 0.75 0.76 0.77 0.76 0.76 0.77 0.76 0.77 0.75 0.77 0.77 0.72 0.73 0.78 0.79 0.76 0.73 0.77 0.69 0.67 0.77 0.70 0.75 0.77 0.66 0.75 0.75 0.76 0.75 0.77 0.75 0.76 0.75 0.74 0.78 0.77 0.70 0.73 0.77 0.74 0.71 0.78 0.77 1.00 0.97 0.74 0.70 0.69 0.75 0.73 0.75 0.72 0.69 0.72 0.70 0.73 0.77 0.75 0.77 0.77 0.71 0.76 0.75 0.76 0.78 0.78 0.78 0.77 0.77 0.73 0.75 0.76 0.78 0.71 0.72 0.71 0.86 0.87 0.88 0.80 0.88 0.84 0.86 0.85 0.85 0.83 0.84 0.87 0.86 0.88 0.82 0.82 0.71 0.77 0.73 0.78 0.86 0.77 0.83 0.85 0.74 0.84 0.87 0.75 0.86 0.83 0.84 0.82 0.82 0.85 0.79 0.88 0.97 0.84 0.88 0.98 0.87 0.86 0.78 0.77 0.77 0.87 0.82 0.86 0.79 0.84 0.86 0.78 0.83 0.80 0.82 0.85 0.75 0.83 0.87 0.84 0.82 0.85 0.82 0.83 0.83 0.84 0.83 0.86 0.79 0.86 0.84 0.85 0.87 0.88 0.84 0.85 0.83 0.85 0.86 0.86 0.86 0.86 0.86 0.87 0.85 0.86 0.87 0.85 0.87 0.87 0.87 0.88 0.87 0.80 0.87 0.86 0.86 0.85 0.80 0.87 0.87 0.85 0.86 0.85 0.85 0.85 0.87 0.85 0.86 0.87 0.86 0.86 0.84 0.87 0.87 0.82 0.83 0.88 0.89 0.85 0.83 0.87 0.77 0.77 0.88 0.79 0.84 0.88 0.75 0.83 0.84 0.86 0.85 0.86 0.85 0.85 0.85 0.84 0.88 0.86 0.78 0.81 0.88 0.84 0.79 0.87 0.86 0.97 1.00 0.83 0.80 0.77 0.84 0.82 0.84 0.80 0.77 0.82 0.79 0.83 0.87 0.84 0.86 0.87 0.81 0.86 0.84 0.86 0.87 0.87 0.87 0.86 0.87 0.82 0.85 0.85 0.88 0.80 0.80 0.80 0.86 0.87 0.89 0.82 0.90 0.85 0.88 0.87 0.88 0.85 0.85 0.88 0.88 0.89 0.85 0.83 0.71 0.77 0.72 0.79 0.85 0.78 0.83 0.86 0.75 0.85 0.88 0.75 0.87 0.84 0.84 0.83 0.81 0.86 0.79 0.90 0.88 0.85 0.88 0.87 0.88 0.87 0.78 0.77 0.76 0.88 0.83 0.88 0.82 0.88 0.89 0.79 0.84 0.81 0.83 0.86 0.75 0.84 0.89 0.85 0.83 0.86 0.85 0.84 0.84 0.86 0.83 0.88 0.81 0.90 0.87 0.88 0.89 0.89 0.86 0.88 0.85 0.87 0.89 0.88 0.88 0.89 0.88 0.89 0.87 0.89 0.89 0.87 0.97 0.88 0.88 0.90 0.89 0.82 0.88 0.87 0.88 0.88 0.83 0.90 0.90 0.87 0.88 0.86 0.88 0.87 0.90 0.87 0.88 0.89 0.87 0.87 0.84 0.89 0.89 0.84 0.85 0.91 0.89 0.87 0.84 0.89 0.79 0.78 0.89 0.81 0.98 0.88 0.96 0.85 0.99 0.88 0.86 0.87 0.86 0.86 0.86 0.87 0.89 0.88 0.79 0.83 0.89 0.87 0.83 0.90 0.88 0.74 0.83 1.00 0.82 0.78 0.84 0.83 0.85 0.82 0.79 0.83 0.82 0.86 0.90 0.87 0.90 0.90 0.85 0.88 0.86 0.89 0.88 0.89 0.90 0.87 0.89 0.83 0.85 0.86 0.88 0.82 0.81 0.82 0.91 0.87 0.88 0.94 0.89 0.94 0.88 0.83 0.85 0.84 0.83 0.86 0.85 0.87 0.89 0.80 0.70 0.75 0.72 0.79 0.87 0.74 0.84 0.81 0.74 0.84 0.85 0.72 0.84 0.82 0.83 0.81 0.77 0.82 0.77 0.89 0.87 0.84 0.88 0.84 0.83 0.84 0.78 0.74 0.72 0.85 0.85 0.86 0.90 0.90 0.86 0.74 0.79 0.77 0.78 0.87 0.74 0.85 0.87 0.82 0.78 0.85 0.83 0.83 0.81 0.84 0.82 0.85 0.81 0.90 0.92 0.92 0.91 0.87 0.84 0.89 0.81 0.88 0.91 0.86 0.85 0.91 0.88 0.86 0.83 0.89 0.86 0.85 0.85 0.85 0.87 0.92 0.86 0.80 0.85 0.86 0.88 0.87 0.82 0.89 0.90 0.81 0.87 0.84 0.94 0.84 0.90 0.84 0.86 0.85 0.87 0.87 0.81 0.91 0.85 0.80 0.79 0.90 0.87 0.84 0.82 0.91 0.73 0.72 0.88 0.76 0.81 0.87 0.70 0.81 0.81 0.88 0.80 0.84 0.83 0.84 0.84 0.95 0.87 0.84 0.76 0.78 0.87 0.91 0.79 0.90 0.84 0.70 0.80 0.82 1.00 0.75 0.81 0.81 0.82 0.78 0.77 0.79 0.88 0.84 0.86 0.86 0.85 0.88 0.92 0.91 0.83 0.87 0.85 0.87 0.87 0.84 0.85 0.79 0.81 0.83 0.84 0.76 0.78 0.79 0.79 0.81 0.80 0.73 0.81 0.77 0.80 0.79 0.79 0.78 0.79 0.81 0.91 0.84 0.78 0.78 0.66 0.72 0.69 0.73 0.80 0.98 0.76 0.96 0.69 0.98 0.80 0.72 0.80 0.77 0.79 0.78 0.76 0.79 0.73 0.83 0.81 0.79 0.82 0.80 0.80 0.80 0.72 0.72 0.72 0.82 0.76 0.81 0.75 0.80 0.81 0.73 0.77 0.75 0.77 0.79 0.70 0.77 0.81 0.78 0.76 0.80 0.77 0.77 0.77 0.78 0.77 0.80 0.74 0.81 0.79 0.80 0.81 0.82 0.79 0.80 0.77 0.79 0.80 0.81 0.80 0.81 0.81 0.80 0.78 0.81 0.80 0.79 0.82 0.82 0.82 0.82 0.82 0.76 0.82 0.81 0.81 0.80 0.75 0.82 0.82 0.79 0.80 0.80 0.79 0.79 0.80 0.79 0.81 0.81 0.81 0.80 0.79 0.80 0.81 0.77 0.77 0.81 0.82 0.79 0.78 0.82 0.72 0.72 0.82 0.74 0.79 0.82 0.71 0.79 0.78 0.80 0.79 0.80 0.79 0.79 0.79 0.78 0.82 0.80 0.75 0.74 0.82 0.79 0.75 0.83 0.81 0.69 0.77 0.78 0.75 1.00 0.80 0.98 0.78 0.75 0.72 0.75 0.73 0.79 0.82 0.79 0.82 0.81 0.78 0.93 0.79 0.82 0.81 0.81 0.82 0.79 0.82 0.78 0.80 0.80 0.82 0.74 0.74 0.75 0.85 0.87 0.87 0.79 0.88 0.83 0.86 0.85 0.87 0.84 0.97 0.86 0.86 0.90 0.84 0.83 0.72 0.77 0.74 0.79 0.98 0.79 0.82 0.86 0.77 0.86 0.86 0.76 0.85 0.83 0.84 0.82 0.98 0.84 0.78 0.88 0.87 0.84 0.88 0.86 0.86 0.85 0.78 0.77 0.97 0.87 0.82 0.87 0.80 0.85 0.88 0.78 0.83 0.80 0.82 0.85 0.76 0.83 0.89 0.84 0.82 0.85 0.83 0.82 0.84 0.84 0.82 0.87 0.79 0.88 0.85 0.86 0.88 0.88 0.85 0.85 0.84 0.86 0.87 0.87 0.86 0.88 0.85 0.89 0.85 0.88 0.88 0.85 0.87 0.87 0.88 0.89 0.88 0.80 0.87 0.87 0.87 0.88 0.81 0.87 0.89 0.85 0.87 0.85 0.86 0.85 0.88 0.87 0.97 0.88 0.87 0.88 0.84 0.88 0.95 0.81 0.82 0.88 0.89 0.86 0.83 0.89 0.76 0.76 0.93 0.79 0.84 0.92 0.74 0.83 0.84 0.88 0.84 0.87 0.85 0.86 0.85 0.86 0.95 0.87 0.79 0.81 0.90 0.85 0.81 0.88 0.88 0.75 0.84 0.84 0.81 0.80 1.00 0.84 0.85 0.81 0.79 0.82 0.94 0.83 0.87 0.85 0.87 0.87 0.82 0.87 0.85 0.87 0.87 0.88 0.88 0.86 0.87 0.82 0.83 0.87 0.87 0.80 0.84 0.81 0.85 0.86 0.87 0.81 0.86 0.84 0.86 0.84 0.85 0.83 0.82 0.86 0.94 0.86 0.81 0.82 0.70 0.76 0.72 0.78 0.84 0.98 0.81 0.98 0.73 0.99 0.85 0.75 0.85 0.82 0.83 0.82 0.80 0.83 0.78 0.89 0.87 0.84 0.88 0.86 0.86 0.85 0.77 0.76 0.76 0.87 0.82 0.86 0.79 0.84 0.86 0.77 0.82 0.79 0.82 0.84 0.76 0.82 0.88 0.84 0.81 0.84 0.81 0.83 0.83 0.83 0.81 0.86 0.78 0.86 0.84 0.86 0.87 0.87 0.84 0.84 0.83 0.85 0.86 0.85 0.85 0.87 0.84 0.86 0.84 0.86 0.86 0.84 0.87 0.87 0.87 0.88 0.87 0.80 0.86 0.86 0.86 0.85 0.80 0.86 0.87 0.84 0.86 0.85 0.86 0.85 0.87 0.85 0.86 0.87 0.86 0.87 0.82 0.86 0.86 0.80 0.81 0.88 0.88 0.85 0.81 0.87 0.75 0.75 0.86 0.78 0.84 0.86 0.74 0.83 0.83 0.86 0.84 0.86 0.83 0.84 0.85 0.85 0.87 0.86 0.78 0.81 0.87 0.84 0.79 0.87 0.86 0.73 0.82 0.83 0.81 0.98 0.84 1.00 0.84 0.80 0.78 0.81 0.80 0.83 0.86 0.83 0.85 0.86 0.82 0.96 0.83 0.85 0.86 0.87 0.87 0.85 0.85 0.80 0.82 0.84 0.86 0.79 0.79 0.79 0.88 0.89 0.90 0.83 0.90 0.87 0.88 0.86 0.88 0.86 0.85 0.87 0.87 0.87 0.84 0.84 0.72 0.79 0.74 0.80 0.87 0.78 0.84 0.87 0.75 0.85 0.88 0.76 0.88 0.86 0.85 0.83 0.83 0.86 0.80 0.91 0.88 0.86 0.91 0.87 0.88 0.87 0.80 0.79 0.79 0.90 0.84 0.90 0.89 0.86 0.90 0.79 0.86 0.83 0.84 0.86 0.80 0.86 0.92 0.87 0.84 0.86 0.85 0.86 0.87 0.87 0.85 0.90 0.81 0.90 0.86 0.88 0.89 0.89 0.87 0.86 0.86 0.87 0.88 0.88 0.88 0.90 0.86 0.90 0.88 0.89 0.89 0.87 0.90 0.90 0.90 0.91 0.90 0.82 0.90 0.89 0.89 0.89 0.82 0.88 0.91 0.87 0.89 0.88 0.87 0.89 0.90 0.88 0.88 0.90 0.89 0.91 0.87 0.89 0.88 0.82 0.84 0.92 0.91 0.88 0.85 0.89 0.80 0.76 0.88 0.80 0.86 0.88 0.76 0.87 0.86 0.89 0.88 0.90 0.86 0.87 0.88 0.87 0.90 0.89 0.80 0.84 0.89 0.94 0.83 0.89 0.90 0.75 0.84 0.85 0.82 0.78 0.85 0.84 1.00 0.99 0.98 0.83 0.81 0.84 0.91 0.87 0.89 0.88 0.89 0.88 0.86 0.88 0.90 0.90 0.90 0.89 0.89 0.85 0.86 0.89 0.90 0.82 0.82 0.83 0.84 0.86 0.87 0.78 0.87 0.83 0.84 0.82 0.84 0.82 0.81 0.84 0.83 0.84 0.80 0.82 0.69 0.75 0.70 0.76 0.83 0.75 0.80 0.83 0.71 0.81 0.84 0.73 0.84 0.82 0.81 0.80 0.78 0.82 0.76 0.87 0.85 0.82 0.87 0.83 0.85 0.84 0.76 0.75 0.74 0.86 0.80 0.86 0.88 0.83 0.86 0.76 0.82 0.80 0.81 0.82 0.75 0.81 0.88 0.84 0.81 0.82 0.81 0.82 0.83 0.83 0.81 0.86 0.77 0.86 0.83 0.84 0.85 0.85 0.83 0.83 0.82 0.83 0.85 0.84 0.84 0.86 0.82 0.86 0.84 0.85 0.85 0.83 0.86 0.87 0.87 0.87 0.87 0.79 0.87 0.86 0.86 0.85 0.79 0.85 0.87 0.83 0.86 0.84 0.84 0.85 0.86 0.84 0.84 0.87 0.86 0.87 0.83 0.85 0.84 0.79 0.81 0.88 0.87 0.84 0.82 0.86 0.76 0.73 0.84 0.78 0.83 0.85 0.73 0.84 0.82 0.85 0.85 0.86 0.83 0.84 0.84 0.83 0.86 0.86 0.77 0.81 0.85 0.93 0.81 0.86 0.86 0.72 0.80 0.82 0.78 0.75 0.81 0.80 0.99 1.00 0.99 0.80 0.77 0.81 0.88 0.84 0.85 0.85 0.88 0.84 0.83 0.84 0.86 0.86 0.86 0.85 0.85 0.81 0.83 0.84 0.86 0.78 0.78 0.79 0.81 0.83 0.84 0.76 0.84 0.80 0.81 0.80 0.81 0.80 0.79 0.81 0.80 0.81 0.79 0.78 0.67 0.73 0.68 0.74 0.81 0.72 0.78 0.80 0.69 0.79 0.81 0.70 0.81 0.79 0.78 0.77 0.76 0.80 0.74 0.85 0.82 0.79 0.85 0.80 0.81 0.81 0.74 0.72 0.72 0.83 0.78 0.83 0.89 0.80 0.84 0.73 0.79 0.77 0.77 0.80 0.74 0.79 0.85 0.81 0.78 0.80 0.78 0.79 0.80 0.80 0.79 0.83 0.75 0.83 0.81 0.82 0.83 0.82 0.80 0.80 0.79 0.81 0.82 0.81 0.81 0.83 0.80 0.83 0.81 0.82 0.82 0.80 0.83 0.83 0.83 0.84 0.83 0.76 0.83 0.82 0.83 0.83 0.77 0.82 0.84 0.80 0.83 0.81 0.82 0.82 0.84 0.82 0.81 0.83 0.83 0.85 0.81 0.83 0.81 0.76 0.77 0.85 0.85 0.81 0.79 0.83 0.73 0.70 0.82 0.74 0.79 0.82 0.70 0.81 0.79 0.83 0.82 0.83 0.80 0.82 0.81 0.82 0.83 0.82 0.74 0.78 0.82 0.92 0.78 0.83 0.83 0.69 0.77 0.79 0.77 0.72 0.79 0.78 0.98 0.99 1.00 0.77 0.76 0.78 0.85 0.81 0.82 0.82 0.88 0.82 0.80 0.81 0.84 0.84 0.83 0.83 0.82 0.78 0.79 0.82 0.83 0.75 0.75 0.76 0.83 0.85 0.85 0.78 0.86 0.81 0.84 0.83 0.83 0.82 0.82 0.85 0.85 0.87 0.80 0.82 0.70 0.74 0.71 0.77 0.85 0.75 0.80 0.84 0.71 0.82 0.84 0.73 0.86 0.82 0.81 0.80 0.80 0.82 0.78 0.88 0.85 0.83 0.86 0.85 0.97 0.84 0.78 0.75 0.75 0.86 0.81 0.85 0.77 0.83 0.86 0.76 0.82 0.78 0.81 0.84 0.74 0.82 0.87 0.83 0.80 0.83 0.81 0.82 0.81 0.83 0.82 0.86 0.77 0.86 0.83 0.85 0.86 0.96 0.83 0.85 0.82 0.84 0.85 0.85 0.85 0.86 0.84 0.86 0.84 0.85 0.86 0.83 0.86 0.86 0.86 0.87 0.87 0.79 0.86 0.85 0.84 0.85 0.78 0.86 0.87 0.83 0.85 0.86 0.84 0.85 0.86 0.84 0.85 0.86 0.85 0.85 0.82 0.85 0.86 0.81 0.81 0.87 0.88 0.85 0.81 0.86 0.75 0.74 0.86 0.78 0.83 0.86 0.74 0.83 0.83 0.85 0.84 0.85 0.83 0.84 0.85 0.83 0.87 0.85 0.79 0.82 0.86 0.83 0.80 0.86 0.86 0.72 0.82 0.83 0.79 0.75 0.82 0.81 0.83 0.80 0.77 1.00 0.80 0.83 0.95 0.83 0.86 0.85 0.81 0.84 0.82 0.85 0.86 0.86 0.87 0.85 0.86 0.80 0.84 0.83 0.86 0.79 0.78 0.79 0.82 0.83 0.83 0.83 0.85 0.83 0.86 0.81 0.84 0.82 0.94 0.82 0.82 0.86 0.84 0.78 0.69 0.73 0.70 0.76 0.95 0.72 0.81 0.79 0.70 0.81 0.82 0.71 0.81 0.80 0.80 0.79 0.92 0.80 0.76 0.86 0.85 0.80 0.86 0.82 0.82 0.81 0.75 0.72 0.91 0.83 0.84 0.84 0.86 0.86 0.84 0.73 0.77 0.75 0.76 0.83 0.73 0.81 0.85 0.81 0.77 0.83 0.79 0.81 0.80 0.81 0.79 0.83 0.79 0.88 0.89 0.89 0.89 0.86 0.82 0.86 0.80 0.87 0.89 0.84 0.83 0.88 0.83 0.86 0.81 0.85 0.85 0.83 0.83 0.82 0.84 0.89 0.83 0.77 0.82 0.84 0.84 0.85 0.79 0.85 0.87 0.80 0.85 0.82 0.91 0.81 0.89 0.83 0.94 0.84 0.84 0.87 0.79 0.88 0.91 0.79 0.79 0.88 0.86 0.82 0.79 0.88 0.71 0.71 0.90 0.74 0.80 0.88 0.70 0.79 0.80 0.86 0.79 0.82 0.81 0.82 0.82 0.92 0.92 0.83 0.74 0.78 0.86 0.88 0.76 0.86 0.84 0.70 0.79 0.82 0.88 0.73 0.94 0.80 0.81 0.77 0.76 0.80 1.00 0.80 0.83 0.84 0.83 0.85 0.87 0.88 0.82 0.84 0.83 0.84 0.84 0.82 0.83 0.76 0.77 0.82 0.81 0.76 0.76 0.78 0.86 0.88 0.88 0.81 0.89 0.84 0.87 0.86 0.86 0.84 0.84 0.88 0.88 0.90 0.86 0.84 0.71 0.77 0.73 0.79 0.87 0.77 0.82 0.86 0.75 0.86 0.87 0.74 0.86 0.83 0.84 0.82 0.80 0.85 0.78 0.90 0.89 0.85 0.89 0.87 0.88 0.87 0.78 0.77 0.75 0.88 0.83 0.87 0.84 0.88 0.87 0.77 0.83 0.80 0.83 0.86 0.74 0.83 0.87 0.84 0.81 0.85 0.84 0.83 0.83 0.84 0.83 0.86 0.80 0.88 0.86 0.95 0.88 0.89 0.85 0.88 0.83 0.86 0.88 0.88 0.98 0.96 0.89 0.88 0.86 0.89 0.89 0.86 0.89 0.88 0.89 0.90 0.89 0.82 0.88 0.87 0.88 0.86 0.82 0.90 0.90 0.87 0.87 0.87 0.87 0.86 0.88 0.86 0.86 0.88 0.88 0.86 0.85 0.88 0.89 0.84 0.83 0.89 0.90 0.87 0.84 0.89 0.79 0.78 0.90 0.82 0.86 0.90 0.78 0.85 0.85 0.88 0.86 0.87 0.86 0.86 0.86 0.86 0.90 0.86 0.81 0.81 0.90 0.88 0.80 0.91 0.87 0.73 0.83 0.86 0.84 0.79 0.83 0.83 0.84 0.81 0.78 0.83 0.80 1.00 0.91 0.87 0.89 0.89 0.87 0.88 0.86 0.90 0.88 0.88 0.89 0.86 0.88 0.83 0.86 0.86 0.88 0.81 0.81 0.80 0.91 0.92 0.92 0.85 0.93 0.88 0.91 0.89 0.91 0.88 0.88 0.92 0.91 0.94 0.89 0.88 0.74 0.80 0.76 0.82 0.91 0.80 0.86 0.90 0.78 0.89 0.91 0.78 0.91 0.87 0.87 0.86 0.84 0.89 0.82 0.94 0.92 0.89 0.92 0.91 0.97 0.90 0.82 0.80 0.79 0.92 0.86 0.92 0.88 0.91 0.91 0.80 0.87 0.83 0.86 0.90 0.78 0.87 0.91 0.87 0.85 0.88 0.87 0.87 0.86 0.89 0.88 0.90 0.83 0.93 0.89 0.92 0.92 0.98 0.90 0.92 0.88 0.90 0.91 0.92 0.92 0.92 0.92 0.92 0.90 0.92 0.93 0.90 0.92 0.93 0.93 0.94 0.94 0.85 0.93 0.92 0.92 0.92 0.85 0.94 0.94 0.91 0.91 0.91 0.90 0.90 0.91 0.90 0.90 0.92 0.92 0.88 0.89 0.90 0.92 0.88 0.88 0.92 0.93 0.91 0.88 0.92 0.82 0.82 0.93 0.85 0.89 0.93 0.80 0.89 0.89 0.91 0.90 0.91 0.89 0.90 0.90 0.89 0.93 0.90 0.84 0.85 0.93 0.93 0.85 0.94 0.92 0.77 0.87 0.90 0.86 0.82 0.87 0.86 0.91 0.88 0.85 0.95 0.83 0.91 1.00 0.91 0.94 0.94 0.91 0.91 0.89 0.94 0.92 0.93 0.93 0.91 0.93 0.87 0.90 0.90 0.93 0.85 0.84 0.85 0.88 0.90 0.89 0.83 0.91 0.86 0.89 0.86 0.88 0.86 0.86 0.88 0.87 0.89 0.87 0.85 0.72 0.78 0.74 0.79 0.88 0.78 0.84 0.86 0.76 0.87 0.88 0.75 0.87 0.85 0.85 0.84 0.83 0.87 0.80 0.91 0.90 0.86 0.91 0.88 0.88 0.88 0.80 0.77 0.78 0.89 0.84 0.90 0.86 0.88 0.89 0.78 0.84 0.81 0.83 0.87 0.78 0.86 0.90 0.86 0.83 0.86 0.84 0.85 0.85 0.86 0.85 0.88 0.82 0.91 0.89 0.90 0.90 0.90 0.88 0.89 0.86 0.89 0.91 0.89 0.89 0.91 0.88 0.90 0.87 0.90 0.89 0.87 0.90 0.90 0.90 0.92 0.90 0.83 0.90 0.90 0.90 0.90 0.84 0.90 0.92 0.87 0.89 0.87 0.90 0.87 0.90 0.88 0.88 0.90 0.89 0.90 0.86 0.91 0.88 0.83 0.84 0.91 0.90 0.88 0.85 0.91 0.78 0.77 0.89 0.81 0.86 0.89 0.77 0.86 0.87 0.89 0.88 0.89 0.87 0.88 0.87 0.89 0.91 0.89 0.81 0.83 0.89 0.90 0.83 0.91 0.98 0.75 0.84 0.87 0.86 0.79 0.85 0.83 0.87 0.84 0.81 0.83 0.84 0.87 0.91 1.00 0.99 0.90 0.88 0.89 0.98 0.98 0.90 0.91 0.90 0.89 0.89 0.85 0.87 0.88 0.90 0.82 0.82 0.82 0.90 0.92 0.92 0.84 0.93 0.87 0.91 0.89 0.91 0.88 0.88 0.91 0.90 0.92 0.88 0.88 0.73 0.81 0.76 0.81 0.90 0.80 0.86 0.89 0.78 0.89 0.91 0.78 0.90 0.86 0.87 0.87 0.85 0.89 0.81 0.93 0.92 0.88 0.92 0.90 0.91 0.90 0.81 0.80 0.80 0.92 0.85 0.92 0.86 0.90 0.91 0.81 0.87 0.83 0.86 0.89 0.79 0.87 0.92 0.87 0.85 0.88 0.87 0.87 0.87 0.88 0.86 0.90 0.84 0.92 0.89 0.91 0.92 0.93 0.90 0.91 0.88 0.90 0.92 0.91 0.92 0.92 0.91 0.92 0.89 0.92 0.92 0.89 0.93 0.92 0.92 0.93 0.93 0.85 0.92 0.91 0.91 0.92 0.85 0.92 0.94 0.91 0.91 0.89 0.90 0.89 0.91 0.91 0.90 0.92 0.91 0.89 0.89 0.91 0.91 0.86 0.86 0.92 0.92 0.90 0.88 0.92 0.81 0.81 0.92 0.84 0.89 0.92 0.80 0.88 0.89 0.91 0.90 0.91 0.89 0.90 0.89 0.89 0.93 0.90 0.84 0.85 0.92 0.91 0.85 0.93 0.98 0.77 0.86 0.90 0.85 0.82 0.87 0.85 0.89 0.85 0.82 0.86 0.83 0.89 0.94 0.99 1.00 0.93 0.89 0.90 0.98 0.99 0.92 0.93 0.93 0.91 0.93 0.87 0.90 0.91 0.92 0.84 0.85 0.85 0.91 0.92 0.92 0.86 0.93 0.88 0.91 0.89 0.97 0.89 0.89 0.91 0.91 0.94 0.96 0.86 0.74 0.80 0.76 0.83 0.90 0.80 0.87 0.88 0.77 0.89 0.91 0.78 0.90 0.87 0.88 0.87 0.83 0.89 0.82 0.94 0.93 0.89 0.93 0.91 0.91 0.90 0.82 0.80 0.78 0.92 0.87 0.92 0.88 0.93 0.91 0.80 0.86 0.83 0.85 0.91 0.78 0.88 0.91 0.87 0.84 0.90 0.88 0.87 0.86 0.89 0.88 0.90 0.84 0.94 0.92 0.93 0.93 0.93 0.90 0.92 0.87 0.91 0.93 0.93 0.92 0.93 0.92 0.92 0.89 0.93 0.93 0.91 0.93 0.93 0.93 0.95 0.93 0.86 0.92 0.92 0.93 0.92 0.86 0.94 0.94 0.98 0.91 0.91 0.93 0.90 0.95 0.90 0.91 0.91 0.92 0.89 0.88 0.93 0.92 0.89 0.88 0.93 0.93 0.91 0.88 0.94 0.81 0.83 0.94 0.84 0.90 0.94 0.80 0.89 0.89 0.92 0.89 0.91 0.89 0.90 0.90 0.92 0.94 0.90 0.84 0.84 0.94 0.93 0.84 0.94 0.91 0.77 0.87 0.90 0.88 0.81 0.87 0.86 0.88 0.85 0.82 0.85 0.85 0.89 0.94 0.90 0.93 1.00 0.91 0.92 0.89 0.93 0.92 0.93 0.93 0.91 0.93 0.87 0.90 0.90 0.93 0.85 0.83 0.84 0.85 0.87 0.85 0.84 0.88 0.85 0.89 0.85 0.85 0.84 0.84 0.86 0.86 0.89 0.90 0.81 0.70 0.75 0.72 0.79 0.89 0.75 0.84 0.82 0.74 0.86 0.86 0.73 0.85 0.83 0.83 0.81 0.78 0.83 0.78 0.90 0.88 0.84 0.88 0.86 0.85 0.85 0.78 0.74 0.74 0.86 0.86 0.87 0.97 0.92 0.85 0.75 0.80 0.77 0.79 0.86 0.74 0.83 0.85 0.81 0.79 0.85 0.83 0.83 0.81 0.84 0.82 0.83 0.82 0.91 0.92 0.92 0.91 0.89 0.86 0.91 0.82 0.89 0.92 0.88 0.87 0.91 0.90 0.86 0.83 0.89 0.87 0.86 0.87 0.87 0.88 0.92 0.87 0.82 0.87 0.88 0.89 0.87 0.84 0.91 0.90 0.85 0.87 0.85 0.93 0.83 0.89 0.84 0.86 0.86 0.88 0.85 0.83 0.89 0.88 0.83 0.82 0.89 0.88 0.85 0.82 0.91 0.75 0.77 0.91 0.78 0.83 0.89 0.74 0.83 0.83 0.88 0.83 0.84 0.84 0.84 0.84 0.93 0.89 0.84 0.78 0.79 0.88 0.98 0.78 0.92 0.85 0.71 0.81 0.85 0.92 0.78 0.82 0.82 0.89 0.88 0.88 0.81 0.87 0.87 0.91 0.88 0.89 0.91 1.00 0.91 0.85 0.90 0.86 0.87 0.87 0.85 0.87 0.80 0.82 0.84 0.84 0.77 0.78 0.81 0.89 0.90 0.90 0.88 0.92 0.89 0.91 0.88 0.90 0.87 0.88 0.91 0.95 0.92 0.89 0.86 0.74 0.79 0.76 0.82 0.90 0.92 0.87 0.95 0.77 0.97 0.89 0.78 0.89 0.87 0.87 0.86 0.83 0.87 0.82 0.94 0.91 0.88 0.93 0.90 0.89 0.89 0.81 0.79 0.79 0.90 0.87 0.91 0.88 0.91 0.90 0.80 0.85 0.82 0.85 0.90 0.80 0.88 0.91 0.87 0.84 0.89 0.86 0.88 0.86 0.89 0.87 0.90 0.85 0.93 0.93 0.93 0.93 0.93 0.89 0.92 0.87 0.92 0.94 0.90 0.90 0.93 0.91 0.91 0.88 0.91 0.91 0.89 0.91 0.91 0.92 0.94 0.91 0.85 0.90 0.90 0.91 0.90 0.86 0.92 0.93 0.88 0.91 0.89 0.95 0.89 0.94 0.90 0.91 0.91 0.90 0.91 0.87 0.93 0.91 0.86 0.86 0.94 0.93 0.89 0.87 0.94 0.80 0.80 0.93 0.82 0.88 0.92 0.78 0.86 0.87 0.91 0.87 0.90 0.88 0.89 0.89 0.95 0.93 0.89 0.82 0.84 0.92 0.92 0.82 0.93 0.89 0.76 0.86 0.88 0.91 0.93 0.87 0.96 0.88 0.84 0.82 0.84 0.88 0.88 0.91 0.89 0.90 0.92 0.91 1.00 0.88 0.91 0.90 0.92 0.91 0.89 0.90 0.84 0.87 0.88 0.90 0.83 0.83 0.83 0.88 0.90 0.90 0.82 0.91 0.86 0.88 0.87 0.88 0.86 0.86 0.88 0.87 0.88 0.84 0.85 0.72 0.78 0.74 0.80 0.87 0.79 0.84 0.87 0.75 0.86 0.88 0.76 0.87 0.85 0.85 0.84 0.83 0.87 0.80 0.91 0.90 0.86 0.91 0.88 0.89 0.88 0.79 0.77 0.78 0.89 0.83 0.90 0.82 0.86 0.89 0.80 0.85 0.83 0.84 0.87 0.78 0.86 0.90 0.87 0.84 0.86 0.85 0.85 0.86 0.87 0.85 0.89 0.81 0.90 0.87 0.89 0.89 0.90 0.90 0.88 0.89 0.88 0.89 0.89 0.89 0.89 0.87 0.89 0.86 0.89 0.89 0.87 0.90 0.89 0.89 0.90 0.89 0.82 0.89 0.89 0.89 0.88 0.83 0.89 0.90 0.87 0.89 0.87 0.88 0.88 0.90 0.88 0.88 0.90 0.89 0.89 0.88 0.90 0.88 0.82 0.83 0.91 0.90 0.88 0.87 0.90 0.78 0.77 0.89 0.81 0.87 0.89 0.77 0.86 0.87 0.89 0.88 0.89 0.87 0.88 0.87 0.87 0.90 0.89 0.81 0.83 0.90 0.87 0.81 0.90 0.97 0.75 0.84 0.86 0.83 0.79 0.85 0.83 0.86 0.83 0.80 0.82 0.82 0.86 0.89 0.98 0.98 0.89 0.85 0.88 1.00 0.98 0.90 0.90 0.90 0.89 0.90 0.86 0.88 0.89 0.91 0.83 0.82 0.83 0.90 0.92 0.91 0.85 0.93 0.87 0.91 0.89 0.91 0.88 0.88 0.91 0.90 0.93 0.89 0.88 0.74 0.80 0.75 0.81 0.90 0.80 0.86 0.89 0.78 0.89 0.91 0.77 0.89 0.87 0.87 0.86 0.85 0.89 0.81 0.93 0.92 0.88 0.92 0.90 0.90 0.90 0.81 0.79 0.79 0.91 0.86 0.92 0.87 0.91 0.91 0.81 0.86 0.83 0.85 0.90 0.78 0.88 0.91 0.87 0.85 0.88 0.87 0.87 0.87 0.89 0.87 0.89 0.83 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_52"><head></head><label></label><figDesc>0.85 0.97 0.89 0.89 0.88 0.91 0.88 0.87 0.90 0.89 0.90 0.87 0.87 0.73 0.80 0.76 0.81 0.88 0.79 0.85 0.88 0.77 0.86 0.90 0.77 0.89 0.87 0.86 0.85 0.83 0.88 0.81 0.93 0.91 0.88 0.93 0.89 0.90 0.90 0.81 0.80 0.79 0.91 0.85 0.91 0.83 0.89 0.92 0.80 0.87 0.84 0.86 0.89 0.80 0.88 0.93 0.88 0.86 0.88 0.87 0.88 0.87 0.89 0.88 0.92 0.82 0.92 0.88 0.90 0.91 0.91 0.88 0.88 0.87 0.89 0.91 0.90 0.90 0.91 0.89 0.91 0.89 0.92 0.90 0.89 0.92 0.92 0.92 0.92 0.92 0.84 0.91 0.91 0.91 0.91 0.83 0.92 0.93 0.89 0.90 0.89 0.90 0.89 0.91 0.90 0.90 0.92 0.92 0.92 0.89 0.91 0.90 0.85 0.86 0.92 0.93 0.89 0.87 0.93 0.79 0.79 0.91 0.82 0.87 0.91 0.77 0.88 0.88 0.91 0.89 0.91 0.89 0.90 0.89 0.89 0.92 0.91 0.82 0.86 0.93 0.88 0.85 0.92 0.92 0.77 0.86 0.87 0.84 0.79 0.86 0.85 0.89 0.85 0.83 0.85 0.82 0.86 0.91 0.89 0.91 0.91 0.85 0.89 0.89 0.91 1.00 0.99 0.93 1.00 0.91 0.88 0.90 0.90 0.98 0.84 0.84 0.83 0.91 0.92 0.92 0.85 0.93 0.89 0.91 0.89 0.91 0.88 0.88 0.91 0.91 0.93 0.87 0.87 0.73 0.80 0.75 0.82 0.90 0.81 0.86 0.89 0.78 0.88 0.91 0.78 0.90 0.87 0.87 0.86 0.84 0.89 0.81 0.94 0.91 0.88 0.93 0.90 0.91 0.90 0.81 0.80 0.80 0.92 0.87 0.92 0.83 0.90 0.93 0.81 0.87 0.84 0.86 0.90 0.80 0.87 0.93 0.88 0.85 0.89 0.88 0.88 0.88 0.88 0.87 0.92 0.83 0.93 0.90 0.91 0.92 0.93 0.90 0.89 0.89 0.90 0.91 0.92 0.92 0.92 0.90 0.91 0.88 0.91 0.91 0.90 0.92 0.92 0.92 0.94 0.93 0.84 0.92 0.91 0.92 0.92 0.83 0.93 0.94 0.90 0.90 0.91 0.90 0.91 0.92 0.91 0.91 0.92 0.91 0.90 0.95 0.91 0.92 0.87 0.88 0.93 0.94 0.91 0.95 0.93 0.81 0.82 0.92 0.83 0.89 0.92 0.79 0.88 0.89 0.91 0.89 0.91 0.89 0.91 0.90 0.89 0.93 0.90 0.84 0.85 0.92 0.90 0.86 0.93 0.93 0.77 0.87 0.89 0.85 0.82 0.87 0.85 0.89 0.85 0.82 0.86 0.83 0.88 0.93 0.89 0.93 0.93 0.87 0.90 0.90 0.92 0.92 0.93 0.94 0.91 1.00 0.94 0.96 0.97 0.95 0.85 0.85 0.93 0.86 0.88 0.88 0.79 0.88 0.83 0.86 0.85 0.86 0.83 0.84 0.85 0.85 0.88 0.82 0.83 0.71 0.77 0.72 0.77 0.84 0.78 0.81 0.84 0.73 0.83 0.86 0.74 0.85 0.83 0.83 0.81 0.79 0.84 0.77 0.88 0.86 0.83 0.88 0.84 0.85 0.85 0.78 0.77 0.75 0.87 0.81 0.87 0.79 0.84 0.87 0.77 0.83 0.80 0.81 0.83 0.76 0.81 0.87 0.83 0.81 0.84 0.82 0.82 0.83 0.83 0.82 0.87 0.83 0.87 0.84 0.85 0.86 0.86 0.84 0.83 0.84 0.84 0.85 0.86 0.87 0.87 0.84 0.85 0.82 0.86 0.84 0.85 0.87 0.88 0.87 0.88 0.88 0.80 0.87 0.86 0.87 0.86 0.79 0.87 0.88 0.86 0.85 0.86 0.84 0.86 0.86 0.86 0.85 0.87 0.87 0.86 0.98 0.86 0.86 0.81 0.82 0.87 0.89 0.86 0.98 0.87 0.77 0.77 0.86 0.79 0.84 0.87 0.75 0.84 0.84 0.85 0.86 0.87 0.84 0.86 0.86 0.83 0.88 0.86 0.80 0.80 0.87 0.83 0.81 0.88 0.88 0.73 0.82 0.83 0.79 0.78 0.82 0.80 0.85 0.81 0.78 0.80 0.76 0.83 0.87 0.85 0.87 0.87 0.80 0.84 0.86 0.88 0.89 0.89 0.88 0.88 0.94 1.00 0.99 0.98 0.93 0.80 0.80 0.78 0.88 0.90 0.90 0.81 0.90 0.86 0.88 0.87 0.88 0.85 0.85 0.88 0.88 0.91 0.84 0.85 0.72 0.78 0.73 0.79 0.85 0.79 0.83 0.87 0.74 0.85 0.88 0.75 0.88 0.84 0.84 0.83 0.82 0.86 0.78 0.91 0.89 0.86 0.90 0.87 0.89 0.87 0.79 0.78 0.77 0.89 0.83 0.89 0.80 0.86 0.89 0.79 0.86 0.83 0.84 0.87 0.77 0.85 0.90 0.86 0.83 0.86 0.85 0.84 0.85 0.86 0.85 0.89 0.82 0.89 0.85 0.87 0.88 0.90 0.86 0.86 0.86 0.86 0.87 0.88 0.89 0.89 0.87 0.87 0.85 0.88 0.87 0.87 0.89 0.90 0.90 0.90 0.91 0.82 0.90 0.88 0.89 0.88 0.81 0.90 0.91 0.89 0.88 0.88 0.86 0.88 0.88 0.89 0.87 0.90 0.88 0.87 0.98 0.88 0.89 0.84 0.85 0.89 0.91 0.88 0.98 0.89 0.79 0.80 0.89 0.82 0.87 0.89 0.78 0.86 0.87 0.88 0.89 0.89 0.87 0.89 0.87 0.84 0.90 0.89 0.83 0.83 0.90 0.86 0.82 0.90 0.90 0.75 0.85 0.85 0.81 0.80 0.83 0.82 0.86 0.83 0.79 0.84 0.77 0.86 0.90 0.87 0.90 0.90 0.82 0.87 0.88 0.90 0.91 0.91 0.91 0.90 0.96 0.99 1.00 0.98 0.95 0.83 0.82 0.81 0.89 0.91 0.91 0.83 0.91 0.87 0.90 0.88 0.90 0.87 0.87 0.89 0.88 0.89 0.85 0.85 0.72 0.79 0.74 0.81 0.89 0.80 0.84 0.87 0.76 0.86 0.89 0.76 0.88 0.86 0.86 0.85 0.84 0.87 0.79 0.92 0.89 0.86 0.92 0.88 0.89 0.88 0.80 0.79 0.79 0.90 0.85 0.90 0.81 0.88 0.91 0.79 0.85 0.83 0.85 0.87 0.79 0.86 0.92 0.87 0.84 0.87 0.86 0.87 0.87 0.87 0.85 0.91 0.81 0.91 0.88 0.90 0.91 0.90 0.88 0.86 0.88 0.88 0.90 0.89 0.90 0.91 0.87 0.90 0.87 0.90 0.89 0.88 0.90 0.91 0.90 0.92 0.91 0.82 0.90 0.89 0.90 0.90 0.81 0.91 0.92 0.88 0.89 0.89 0.89 0.89 0.90 0.89 0.90 0.91 0.90 0.91 0.98 0.90 0.90 0.83 0.85 0.91 0.92 0.89 0.98 0.91 0.79 0.79 0.90 0.80 0.87 0.89 0.77 0.87 0.87 0.89 0.89 0.90 0.88 0.89 0.89 0.88 0.91 0.90 0.81 0.84 0.90 0.87 0.84 0.91 0.92 0.76 0.85 0.86 0.83 0.80 0.87 0.84 0.89 0.84 0.82 0.83 0.82 0.86 0.90 0.88 0.91 0.90 0.84 0.88 0.89 0.91 0.91 0.92 0.92 0.90 0.97 0.98 0.98 1.00 0.95 0.83 0.84 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_53"><head>Figure 45 :</head><label>45</label><figDesc>1.00 0.98 0.95 0.99 0.99 0.99 0.75 0.69 0.58 0.81 0.52 0.76 0.79 0.63 0.75 0.61 0.78 0.76 0.68 0.76 0.78 0.73 0.80 0.58 0.76 0.71 0.71 0.48 0.76 0.66 0.66 0.78 0.41 0.66 0.69 0.82 0.81 0.78 0.80 0.81 0.78 0.82 0.77 0.66 0.72 0.80 0.79 0.81 0.71 0.78 0.75 0.78 0.61 0.55 0.42 0.70 0.64 0.71 0.75 0.66 0.69 0.63 0.73 0.70 0.65 0.71 0.68 0.72 0.60 0.74 0.75 0.77 0.78 0.71 0.75 0.79 0.76 0.84 0.81 0.73 0.67 0.73 0.77 0.73 0.60 0.78 0.60 0.69 0.69 0.70 0.74 0.78 0.68 0.65 0.73 0.72 0.70 0.77 0.74 0.75 0.74 0.57 0.76 0.69 0.63 0.77 0.78 0.74 0.67 0.77 0.74 0.83 0.73 0.79 0.64 0.73 0.71 0.77 0.58 0.77 0.71 0.78 0.39 0.69 0.73 0.66 0.63 0.74 0.44 0.62 0.61 0.78 0.73 0.77 0.69 0.71 0.76 0.77 0.65 0.75 0.67 0.66 0.70 0.78 0.54 0.78 0.55 0.73 0.76 0.64 0.86 0.40 0.73 0.56 0.76 0.73 0.70 0.73 0.69 0.64 0.66 0.42 0.45 0.76 0.77 0.68 0.73 0.78 0.73 0.69 0.70 0.73 0.79 0.75 0.59 0.78 0.72 0.71 0.74 0.75 0.98 1.00 0.98 0.96 0.97 0.98 0.68 0.63 0.53 0.74 0.44 0.67 0.71 0.56 0.67 0.56 0.70 0.69 0.63 0.69 0.69 0.69 0.74 0.54 0.67 0.62 0.64 0.49 0.72 0.63 0.62 0.71 0.36 0.61 0.64 0.76 0.74 0.73 0.74 0.74 0.71 0.74 0.71 0.62 0.66 0.75 0.70 0.75 0.63 0.68 0.67 0.69 0.58 0.52 0.40 0.64 0.60 0.66 0.70 0.63 0.65 0.56 0.63 0.61 0.62 0.65 0.61 0.68 0.55 0.68 0.67 0.69 0.71 0.63 0.69 0.69 0.70 0.76 0.73 0.66 0.62 0.67 0.69 0.66 0.57 0.70 0.58 0.61 0.63 0.67 0.70 0.71 0.65 0.60 0.70 0.68 0.66 0.71 0.67 0.66 0.67 0.53 0.69 0.64 0.59 0.74 0.70 0.67 0.60 0.72 0.65 0.77 0.67 0.69 0.61 0.64 0.61 0.71 0.53 0.73 0.64 0.69 0.35 0.63 0.68 0.61 0.58 0.69 0.38 0.54 0.54 0.69 0.66 0.71 0.65 0.67 0.74 0.68 0.62 0.69 0.62 0.58 0.66 0.69 0.50 0.68 0.50 0.65 0.69 0.57 0.80 0.36 0.65 0.51 0.69 0.68 0.62 0.65 0.61 0.59 0.63 0.38 0.42 0.68 0.67 0.64 0.66 0.71 0.67 0.64 0.65 0.67 0.69 0.67 0.55 0.70 0.64 0.63 0.66 00.53 0.50 0.59 0.57 0.55 0.82 0.69 0.50 0.68 0.38 0.58 0.64 0.50 0.56 0.51 0.62 0.62 0.56 0.66 0.61 0.61 0.69 0.51 0.60 0.53 0.61 0.41 0.69 0.61 0.61 0.67 0.96 0.57 0.63 0.71 0.68 0.68 0.68 0.68 0.65 0.65 0.67 0.59 0.63 0.70 0.66 0.71 0.56 0.59 0.56 0.59 0.54 0.46 0.37 0.61 0.51 0.61 0.65 0.60 0.59 0.52 0.55 0.54 0.58 0.59 0.55 0.63 0.49 0.62 0.60 0.63 0.64 0.57 0.62 0.57 0.66 0.67 0.66 0.63 0.61 0.63 0.63 0.64 0.54 0.62 0.56 0.59 0.57 0.64 0.67 0.64 0.65 0.56 0.66 0.61 0.63 0.65 0.62 0.60 0.61 0.52 0.62 0.59 0.56 0.61 0.64 0.63 0.55 0.65 0.56 0.70 0.58 0.60 0.52 0.60 0.55 0.63 1.00 0.60 0.57 0.58 0.35 0.59 0.58 0.59 0.54 0.64 0.34 0.49 0.50 0.61 0.55 0.63 0.56 0.63 0.59 0.58 0.55 0.63 0.56 0.53 0.60 0.63 0.48 0.61 0.47 0.58 0.67 0.54 0.68 0.34 0.59 0.49 0.63 0.64 0.56 0.62 0.57 0.61 0.65 0.36 0.41 0.63 0.60 0.57 0.59 0.60 0.62 0.60 0.61 0.62 0.61 0.60 0.54 0.65 0.58 0.58 0.58 0.58 0.77 0.73 0.72 0.79 0.78 0.77 0.76 0.73 0.59 0.81 0.47 0.71 0.77 0.61 0.67 0.60 0.77 0.76 0.70 0.77 0.75 0.74 0.82 0.59 0.74 0.67 0.72 0.52 0.79 0.70 0.69 0.79 0.42 0.67 0.73 0.83 0.81 0.80 0.81 0.81 0.77 0.81 0.79 0.68 0.73 0.82 0.77 0.83 0.74 0.72 0.69 0.74 0.64 0.56 0.43 0.70 0.64 0.72 0.76 0.69 0.70 0.63 0.68 0.67 0.67 0.70 0.67 0.74 0.61 0.74 0.74 0.76 0.78 0.68 0.71 0.69 0.75 0.79 0.77 0.73 0.69 0.74 0.74 0.73 0.62 0.75 0.63 0.68 0.68 0.75 0.77 0.77 0.72 0.67 0.77 0.76 0.71 0.77 0.73 0.74 0.75 0.60 0.73 0.69 0.63 1.00 0.75 0.80 0.64 0.76 0.70 0.82 0.70 0.74 0.62 0.72 0.68 0.77 0.60 1.00 0.68 0.72 0.37 0.69 0.72 0.66 0.62 0.74 0.44 0.59 0.60 0.74 0.68 0.76 0.68 0.81 0.99 0.72 0.65 0.73 0.75 0.65 0.70 0.75 0.56 0.73 0.54 0.71 0.77 0.62 0.83 0.41 0.71 0.57 0.75 0.73 0.68 0.74 0.68 0.67 0.70 0.44 0.49 0.74 0.72 0.69 0.68 0.71 0.73 0.69 0.71 0.72 0.75 0.74 0.62 0.76 0.70 0.68 0.70 0.70 0.71 0.64 0.56 0.73 0.70 0.70 0.78 0.73 0.59 0.82 0.53 0.77 0.82 0.63 0.73 0.61 0.82 0.78 0.71 0.81 0.83 0.75 0.83 0.61 0.80 0.76 0.72 0.52 0.75 0.67 0.66 0.81 0.41 0.67 0.71 0.82 0.83 0.80 0.80 0.83 0.77 0.85 0.80 0.66 0.74 0.80 0.85 0.82 0.74 0.81 0.74 0.81 0.60 0.53 0.41 0.70 0.66 0.73 0.75 0.64 0.67 0.65 0.76 0.74 0.65 0.71 0.69 0.71 0.65 0.76 0.79 0.81 0.80 0.74 0.75 0.77 0.78 0.82 0.82 0.76 0.70 0.76 0.81 0.75 0.61 0.81 0.60 0.73 0.71 0.71 0.75 0.80 0.68 0.64 0.72 0.73 0.71 0.79 0.77 0.81 0.75 0.61 0.75 0.71 0.64 0.69 0.80 0.75 0.69 0.76 0.76 0.80 1.00 0.80 0.61 0.78 0.76 0.78 0.57 0.68 1.00 0.80 0.37 0.71 0.76 0.65 0.65 0.73 0.47 0.64 0.63 0.79 0.76 0.75 0.67 0.71 0.67 0.79 0.64 0.75 0.67 0.69 0.68 0.83 0.55 0.82 0.54 0.76 0.79 0.69 0.81 0.44 0.76 0.57 0.79 0.76 0.75 0.77 0.72 0.66 0.69 0.44 0.48 0.81 0.81 0.72 0.72 0.76 0.75 0.70 0.73 0.76 0.82 0.77 0.60 0.80 0.75 0.72 0.76 0.78 0.78 0.69 0.59 0.81 0.80 0.78 0.83 0.75 0.61 0.88 0.62 0.88 0.90 0.70 0.89 0.65 0.89 0.82 0.71 0.85 0.90 0.75 0.86 0.60 0.89 0.85 0.76 0.53 0.75 0.67 0.64 0.83 0.43 0.70 0.71 0.86 0.89 0.81 0.84 0.88 0.83 0.91 0.83 0.67 0.77 0.81 0.92 0.85 0.84 0.93 0.84 0.91 0.61 0.56 0.44 0.74 0.72 0.75 0.80 0.65 0.70 0.71 0.88 0.85 0.68 0.76 0.75 0.73 0.70 0.82 0.87 0.87 0.86 0.81 0.84 0.87 0.81 0.87 0.89 0.79 0.70 0.80 0.86 0.78 0.60 0.89 0.59 0.76 0.77 0.69 0.75 0.87 0.66 0.66 0.71 0.74 0.72 0.83 0.82 0.90 0.80 0.60 0.83 0.76 0.69 0.72 0.90 0.78 0.76 0.81 0.87 0.87 0.83 0.93 0.77 0.85 0.85 0.85 0.58 0.72 0.80 1.00 0.40 0.76 0.90 0.68 0.69 0.90 0.50 0.72 0.67 0.89 0.83 0.82 0.86 0.74 0.69 0.93 0.71 0.81 0.71 0.78 0.85 0.92 0.57 0.92 0.57 0.83 0.83 0.71 0.85 0.45 0.82 0.57 0.84 0.76 0.83 0.83 0.79 0.67 0.64 0.45 0.48 0.86 0.92 0.77 0.77 0.81 0.79 0.73 0.75 0.79 0.92 0.85 0.58 0.85 0.82 0.79 0.85 0.91 0.39 0.35 0.34 0.40 0.38 0.35 0.44 0.43 0.32 0.44 0.24 0.39 0.42 0.33 0.38 0.33 0.42 0.43 0.37 0.44 0.40 0.41 0.48 0.32 0.40 0.35 0.39 0.31 0.46 0.40 0.41 0.44 0.25 0.37 0.42 0.46 0.43 0.44 0.45 0.45 0.42 0.42 0.47 0.42 0.41 0.47 0.44 0.46 0.38 0.44 0.36 0.39 0.37 0.33 0.25 0.39 0.34 0.40 0.43 0.39 0.41 0.34 0.36 0.36 0.40 0.37 0.37 0.41 0.31 0.41 0.41 0.41 0.42 0.37 0.41 0.36 0.43 0.45 0.43 0.39 0.40 0.43 0.42 0.41 0.36 0.41 0.36 0.39 0.37 0.43 0.44 0.43 0.44 0.39 0.43 0.42 0.41 0.44 0.41 0.39 0.40 0.36 0.39 0.38 0.43 0.38 0.46 0.40 0.35 0.42 0.36 0.48 0.38 0.37 0.46 0.36 0.35 0.46 0.35 0.37 0.37 0.40 1.00 0.38 0.44 0.36 0.34 0.48 0.22 0.29 0.33 0.38 0.35 0.40 0.42 0.40 0.38 0.38 0.43 0.40 0.36 0.35 0.48 0.43 0.31 0.42 0.31 0.37 0.42 0.36 0.50 0.22 0.38 0.32 0.42 0.43 0.37 0.39 0.36 0.41 0.42 0.25 0.30 0.41 0.40 0.39 0.37 0.39 0.41 0.40 0.40 0.40 0.38 0.39 0.35 0.41 0.36 0.37 0.36 0.37 0.69 0.63 0.56 0.71 0.69 0.67 0.77 0.73 0.61 0.81 0.51 0.75 0.80 0.64 0.72 0.60 0.79 0.77 0.69 0.79 0.79 0.73 0.82 0.60 0.77 0.71 0.72 0.51 0.76 0.68 0.68 0.79 0.43 0.68 0.72 0.82 0.81 0.79 0.79 0.81 0.77 0.82 0.79 0.67 0.73 0.80 0.81 0.82 0.72 0.77 0.71 0.77 0.62 0.54 0.43 0.71 0.65 0.72 0.75 0.66 0.69 0.64 0.73 0.70 0.67 0.71 0.69 0.72 0.62 0.75 0.76 0.77 0.78 0.98 0.75 0.74 0.77 0.80 0.79 0.74 0.69 0.74 0.78 0.75 0.61 0.78 0.61 0.72 0.70 0.72 0.75 0.78 0.70 0.65 0.73 0.72 0.71 0.77 0.75 0.77 0.74 0.61 0.74 0.71 0.63 0.69 0.77 0.75 0.68 0.76 0.73 0.82 0.73 0.77 0.63 0.74 0.71 0.77 0.59 0.69 0.71 0.76 0.38 1.00 0.74 0.67 0.65 0.74 0.45 0.62 0.62 0.77 0.72 0.76 0.67 0.73 0.67 0.75 0.66 0.75 0.67 0.66 0.69 0.79 0.55 0.78 0.54 0.72 0.77 0.66 0.80 0.43 0.74 0.57 0.76 0.74 0.71 0.74 0.70 0.67 0.90 0.43 0.48 0.77 0.78 0.70 0.70 0.73 0.74 0.70 0.72 0.74 0.78 0.75 0.62 0.78 0.72 0.70 0.73 0.75 0.73 0.68 0.58 0.76 0.74 0.72 0.81 0.75 0.62 0.85 0.56 0.84 0.85 0.67 0.81 0.63 0.84 0.79 0.71 0.82 0.84 0.75 0.85 0.61 0.82 0.78 0.74 0.81 0.77 0.68 0.67 0.81 0.42 0.68 0.73 0.85 0.86 0.81 0.83 0.85 0.81 0.87 0.81 0.69 0.76 0.82 0.86 0.84 0.79 0.86 0.78 0.85 0.66 0.60 0.48 0.73 0.69 0.74 0.81 0.70 0.72 0.69 0.81 0.78 0.72 0.74 0.72 0.76 0.78 0.79 0.82 0.83 0.83 0.76 0.79 0.81 0.80 0.84 0.85 0.77 0.70 0.78 0.83 0.78 0.63 0.84 0.63 0.74 0.75 0.72 0.76 0.84 0.69 0.66 0.74 0.75 0.72 0.81 0.79 0.84 0.78 0.61 0.80 0.75 0.69 0.72 0.87 0.76 0.73 0.82 0.81 0.87 0.78 0.86 0.75 0.80 0.78 0.85 0.58 0.72 0.76 0.90 0.44 0.74 1.00 0.71 0.67 0.87 0.46 0.69 0.63 0.84 0.80 0.82 0.81 0.75 0.69 0.86 0.72 0.81 0.69 0.71 0.85 0.86 0.56 0.86 0.55 0.78 0.80 0.69 0.84 0.44 0.79 0.57 0.82 0.76 0.77 0.77 0.75 0.68 0.67 0.45 0.49 0.84 0.85 0.94 0.74 0.78 0.76 0.71 0.74 0.76 0.85 0.80 0.61 0.82 0.77 0.76 0.82 0.83 0.66 0.61 0.56 0.68 0.65 0.63 0.73 0.72 0.57 0.76 0.46 0.74 0.74 0.61 0.66 0.55 0.74 0.71 0.67 0.76 0.75 0.72 0.79 0.61 0.70 0.65 0.68 0.50 0.76 0.66 0.68 0.77 0.44 0.64 0.72 0.78 0.76 0.75 0.76 0.78 0.73 0.77 0.75 0.66 0.70 0.77 0.77 0.77 0.67 0.72 0.64 0.72 0.72 0.66 0.54 0.66 0.59 0.67 0.78 0.74 0.73 0.62 0.65 0.64 0.75 0.66 0.62 0.77 0.58 0.70 0.70 0.73 0.73 0.65 0.71 0.68 0.73 0.75 0.75 0.70 0.69 0.71 0.75 0.71 0.58 0.72 0.59 0.68 0.64 0.70 0.73 0.72 0.69 0.65 0.72 0.70 0.69 0.72 0.71 0.71 0.70 0.60 0.92 0.66 0.60 0.66 0.71 0.69 0.63 0.74 0.64 0.79 0.67 0.69 0.62 0.69 0.64 0.71 0.59 0.66 0.65 0.68 0.36 0.67 0.71 1.00 0.59 0.74 0.42 0.57 0.59 0.89 0.67 0.73 0.68 0.70 0.64 0.66 0.62 0.72 0.65 0.59 0.69 0.75 0.53 0.72 0.53 0.66 0.74 0.63 0.80 0.41 0.70 0.56 0.72 0.72 0.65 0.69 0.64 0.69 0.71 0.41 0.48 0.73 0.72 0.66 0.67 0.69 0.70 0.67 0.68 0.69 0.70 0.69 0.61 0.72 0.67 0.71 0.73 0.67 0.63 0.58 0.52 0.65 0.63 0.62 0.71 0.66 0.83 0.75 0.46 0.69 0.73 0.58 0.66 0.55 0.72 0.70 0.61 0.73 0.72 0.67 0.75 0.56 0.71 0.66 0.64 0.47 0.71 0.62 0.61 0.72 0.39 0.60 0.65 0.75 0.75 0.73 0.74 0.75 0.70 0.76 0.72 0.62 0.66 0.73 0.74 0.75 0.65 0.71 0.65 0.71 0.55 0.49 0.38 0.64 0.60 0.65 0.70 0.61 0.63 0.59 0.66 0.64 0.62 0.66 0.62 0.67 0.56 0.69 0.69 0.71 0.72 0.65 0.68 0.68 0.71 0.74 0.73 0.68 0.62 0.68 0.71 0.67 0.56 0.71 0.56 0.64 0.65 0.66 0.69 0.72 0.64 0.60 0.67 0.66 0.65 0.71 0.69 0.70 0.68 0.90 0.68 0.66 0.56 0.63 0.71 0.67 0.63 0.69 0.66 0.76 0.67 0.70 0.56 0.68 0.65 0.70 0.54 0.62 0.65 0.69 0.34 0.65 0.67 0.59 1.00 0.66 0.43 0.56 0.58 0.70 0.66 0.69 0.61 0.66 0.60 0.68 0.59 0.68 0.60 0.61 0.62 0.73 0.50 0.72 0.50 0.67 0.71 0.60 0.73 0.39 0.69 0.52 0.71 0.70 0.66 0.69 0.64 0.61 0.64 0.38 0.43 0.71 0.71 0.63 0.64 0.68 0.68 0.65 0.66 0.68 0.72 0.69 0.57 0.71 0.66 0.63 0.67 0.68 0.74 0.69 0.63 0.76 0.73 0.71 0.82 0.78 0.63 0.86 0.53 0.81 0.83 0.67 0.84 0.64 0.82 0.81 0.71 0.83 0.81 0.78 0.87 0.64 0.78 0.72 0.77 0.55 0.83 0.73 0.73 0.84 0.46 0.72 0.78 0.87 0.86 0.83 0.84 0.86 0.81 0.86 0.83 0.72 0.78 0.85 0.84 0.87 0.75 0.80 0.73 0.80 0.74 0.67 0.54 0.74 0.69 0.75 0.83 0.76 0.78 0.66 0.74 0.73 0.77 0.73 0.71 0.81 0.65 0.78 0.80 0.81 0.82 0.73 0.84 0.75 0.80 0.84 0.83 0.78 0.75 0.79 0.81 0.78 0.64 0.81 0.65 0.75 0.72 0.77 0.81 0.82 0.75 0.70 0.79 0.77 0.76 0.81 0.78 0.79 0.78 0.64 0.79 0.72 0.71 0.75 0.83 0.77 0.67 0.82 0.76 0.86 0.75 0.79 0.85 0.77 0.72 0.82 0.64 0.74 0.73 0.90 0.48 0.74 0.87 0.74 0.66 1.00 0.43 0.65 0.61 0.80 0.74 0.81 0.97 0.76 0.72 0.79 0.76 0.80 0.71 0.68 0.98 0.82 0.59 0.81 0.59 0.75 0.81 0.68 0.88 0.43 0.77 0.59 0.80 0.78 0.73 0.78 0.73 0.73 0.74 0.46 0.52 0.80 0.79 0.74 0.72 0.76 0.77 0.73 0.76 0.77 0.79 0.78 0.64 0.80 0.74 0.77 0.80 0.81 0.44 0.38 0.34 0.46 0.45 0.44 0.50 0.46 0.36 0.51 0.34 0.49 0.53 0.40 0.46 0.38 0.51 0.48 0.44 0.50 0.51 0.49 0.51 0.36 0.50 0.47 0.44 0.31 0.46 0.41 0.41 0.50 0.25 0.41 0.43 0.51 0.51 0.49 0.50 0.50 0.48 0.52 0.48 0.41 0.45 0.50 0.51 0.51 0.46 0.52 0.47 0.51 0.36 0.33 0.25 0.44 0.42 0.44 0.46 0.38 0.40 0.42 0.48 0.47 0.41 0.44 0.42 0.43 0.38 0.47 0.50 0.49 0.50 0.46 0.49 0.50 0.47 0.51 0.52 0.47 0.43 0.47 0.52 0.48 0.39 0.51 0.37 0.46 0.47 0.44 0.46 0.50 0.42 0.40 0.44 0.45 0.43 0.50 0.51 0.52 0.47 0.37 0.48 0.45 0.39 0.44 0.52 0.47 0.43 0.48 0.49 0.51 0.48 0.51 0.40 0.47 0.48 0.48 0.34 0.44 0.47 0.50 0.22 0.45 0.46 0.42 0.43 0.43 1.00 0.40 0.96 0.50 0.46 0.46 0.40 0.44 0.42 0.49 0.39 0.47 0.42 0.43 0.38 0.53 0.34 0.52 0.34 0.47 0.47 0.44 0.46 0.26 0.48 0.34 0.50 0.47 0.47 0.48 0.45 0.40 0.43 0.27 0.30 0.47 0.51 0.43 0.43 0.47 0.46 0.43 0.46 0.47 0.52 0.49 0.37 0.50 0.47 0.46 0.49 0.49 0.62 0.54 0.46 0.64 0.63 0.61 0.68 0.61 0.77 0.71 0.48 0.69 0.73 0.57 0.67 0.53 0.72 0.68 0.58 0.68 0.73 0.63 0.71 0.49 0.70 0.67 0.63 0.44 0.63 0.56 0.54 0.69 0.36 0.58 0.61 0.72 0.72 0.67 0.69 0.72 0.68 0.74 0.68 0.55 0.65 0.68 0.73 0.70 0.66 0.73 0.66 0.72 0.50 0.44 0.36 0.61 0.58 0.62 0.64 0.54 0.57 0.57 0.69 0.66 0.56 0.62 0.60 0.59 0.57 0.67 0.70 0.70 0.70 0.66 0.65 0.69 0.67 0.71 0.72 0.65 0.58 0.66 0.70 0.65 0.49 0.71 0.50 0.63 0.62 0.59 0.63 0.71 0.58 0.56 0.61 0.62 0.61 0.68 0.67 0.72 0.66 0.53 0.66 0.62 0.55 0.59 0.71 0.65 0.61 0.65 0.82 0.71 0.66 0.74 0.53 0.68 0.67 0.68 0.49 0.59 0.64 0.72 0.29 0.62 0.69 0.57 0.56 0.65 0.40 1.00 0.55 0.71 0.66 0.66 0.58 0.61 0.57 0.72 0.56 0.66 0.58 0.61 0.59 0.74 0.46 0.73 0.46 0.68 0.69 0.58 0.69 0.35 0.69 0.47 0.69 0.63 0.66 0.68 0.63 0.55 0.56 0.37 0.39 0.71 0.73 0.61 0.62 0.67 0.64 0.59 0.62 0.65 0.73 0.70 0.50 0.70 0.65 0.64 0.68 0.71 0.61 0.54 0.49 0.63 0.62 0.60 0.69 0.64 0.51 0.71 0.44 0.66 0.71 0.55 0.62 0.53 0.70 0.66 0.60 0.69 0.69 0.66 0.71 0.52 0.68 0.63 0.62 0.44 0.65 0.59 0.59 0.69 0.37 0.58 0.61 0.71 0.70 0.69 0.69 0.70 0.67 0.72 0.67 0.57 0.63 0.70 0.70 0.71 0.62 0.69 0.63 0.68 0.52 0.46 0.36 0.61 0.58 0.62 0.64 0.56 0.58 0.58 0.64 0.63 0.57 0.61 0.59 0.61 0.53 0.65 0.67 0.67 0.68 0.63 0.66 0.66 0.66 0.70 0.70 0.65 0.60 0.65 0.70 0.66 0.55 0.69 0.53 0.63 0.64 0.62 0.65 0.68 0.60 0.56 0.63 0.63 0.60 0.68 0.69 0.69 0.65 0.52 0.66 0.62 0.54 0.61 0.70 0.65 0.59 0.67 0.65 0.71 0.65 0.68 0.55 0.65 0.64 0.66 0.50 0.60 0.63 0.67 0.33 0.62 0.63 0.59 0.58 0.61 0.96 0.55 1.00 0.68 0.63 0.65 0.56 0.62 0.58 0.66 0.56 0.65 0.58 0.58 0.56 0.71 0.48 0.69 0.47 0.64 0.66 0.59 0.66 0.36 0.66 0.49 0.68 0.65 0.63 0.66 0.62 0.57 0.61 0.38 0.43 0.66 0.68 0.60 0.60 0.64 0.65 0.61 0.63 0.65 0.69 0.66 0.53 0.68 0.63 0.62 0.66 0.66 0.78 0.69 0.60 0.81 0.79 0.77 0.84 0.78 0.63 0.88 0.60 0.88 0.90 0.71 0.82 0.65 0.89 0.82 0.74 0.87 0.91 0.78 0.88 0.64 0.88 0.83 0.77 0.54 0.79 0.69 0.68 0.85 0.45 0.71 0.75 0.87 0.89 0.84 0.86 0.89 0.84 0.91 0.85 0.69 0.79 0.83 0.92 0.86 0.83 0.91 0.82 0.91 0.67 0.61 0.48 0.75 0.72 0.77 0.83 0.71 0.74 0.72 0.85 0.83 0.73 0.77 0.74 0.78 0.69 0.82 0.86 0.87 0.86 0.80 0.81 0.86 0.83 0.87 0.89 0.80 0.73 0.82 0.87 0.80 0.63 0.88 0.61 0.78 0.77 0.73 0.78 0.87 0.70 0.69 0.75 0.76 0.74 0.84 0.83 0.89 0.80 0.64 0.98 0.77 0.67 0.74 0.87 0.80 0.76 0.82 0.84 0.89 0.82 0.90 0.68 0.85 0.83 0.84 0.61 0.74 0.79 0.89 0.38 0.77 0.84 0.89 0.70 0.80 0.50 0.71 0.68 1.00 0.82 0.82 0.73 0.76 0.71 0.88 0.71 0.82 0.73 0.76 0.73 0.93 0.58 0.90 0.59 0.83 0.84 0.73 0.88 0.46 0.84 0.59 0.85 0.80 0.82 0.83 0.79 0.72 0.70 0.47 0.51 0.87 0.90 0.77 0.78 0.82 0.80 0.75 0.77 0.80 0.90 0.84 0.62 0.86 0.81 0.80 0.85 0.86 0.73 0.66 0.57 0.76 0.75 0.72 0.77 0.73 0.59 0.82 0.56 0.82 0.85 0.66 0.78 0.61 0.85 0.77 0.70 0.81 0.86 0.74 0.83 0.60 0.83 0.78 0.71 0.50 0.73 0.65 0.65 0.80 0.41 0.66 0.69 0.82 0.83 0.78 0.80 0.83 0.78 0.86 0.81 0.65 0.73 0.78 0.87 0.81 0.79 0.86 0.76 0.84 0.62 0.56 0.44 0.78 0.66 0.73 0.78 0.66 0.69 0.67 0.85 0.78 0.68 0.80 0.79 0.73 0.64 0.77 0.79 0.81 0.81 0.75 0.77 0.81 0.78 0.82 0.84 0.75 0.67 0.75 0.82 0.75 0.61 0.83 0.59 0.71 0.73 0.69 0.74 0.82 0.67 0.66 0.70 0.71 0.69 0.79 0.79 0.84 0.75 0.60 0.77 0.73 0.62 0.69 0.81 0.74 0.72 0.95 0.79 0.84 0.79 0.84 0.62 0.80 0.78 0.79 0.55 0.68 0.76 0.83 0.35 0.72 0.80 0.67 0.66 0.74 0.46 0.66 0.63 0.82 1.00 0.97 0.67 0.71 0.66 0.82 0.64 0.97 0.69 0.72 0.68 0.85 0.54 0.85 0.55 0.78 0.80 0.67 0.81 0.43 0.79 0.55 0.80 0.74 0.77 0.78 0.74 0.65 0.67 0.44 0.48 0.83 0.86 0.71 0.74 0.77 0.75 0.71 0.72 0.76 0.85 0.80 0.59 0.80 0.76 0.74 0.79 0.80 0.77 0.71 0.64 0.79 0.77 0.75 0.82 0.79 0.64 0.87 0.53 0.82 0.86 0.68 0.78 0.65 0.86 0.82 0.74 0.85 0.85 0.79 0.88 0.64 0.83 0.75 0.77 0.56 0.83 0.74 0.74 0.85 0.45 0.72 0.77 0.88 0.87 0.85 0.86 0.88 0.83 0.88 0.86 0.72 0.78 0.87 0.87 0.88 0.78 0.83 0.75 0.83 0.72 0.64 0.51 0.83 0.69 0.78 0.85 0.75 0.78 0.70 0.83 0.77 0.77 0.84 0.82 0.81 0.67 0.80 0.81 0.83 0.84 0.76 0.80 0.79 0.83 0.86 0.86 0.80 0.75 0.80 0.84 0.81 0.67 0.84 0.67 0.76 0.75 0.78 0.82 0.85 0.76 0.72 0.79 0.78 0.76 0.83 0.82 0.83 0.80 0.66 0.80 0.76 0.67 0.76 0.83 0.80 0.73 1.00 0.78 0.90 0.78 0.83 0.68 0.81 0.77 0.83 0.63 0.76 0.75 0.82 0.40 0.76 0.82 0.73 0.69 0.81 0.46 0.66 0.65 0.82 0.97 1.00 0.74 0.79 0.74 0.80 0.70 1.00 0.74 0.72 0.75 0.85 0.59 0.84 0.60 0.79 0.84 0.70 0.87 0.44 0.80 0.61 0.82 0.80 0.77 0.81 0.77 0.74 0.76 0.47 0.52 0.84 0.83 0.76 0.76 0.79 0.80 0.76 0.77 0.80 0.83 0.81 0.67 0.83 0.77 0.78 0.81 0.80 0.69 0.65 0.59 0.70 0.67 0.66 0.74 0.71 0.57 0.79 0.46 0.74 0.74 0.59 0.80 0.57 0.74 0.74 0.65 0.75 0.73 0.71 0.80 0.56 0.70 0.64 0.70 0.50 0.77 0.68 0.67 0.77 0.39 0.66 0.71 0.80 0.79 0.76 0.77 0.79 0.74 0.78 0.77 0.67 0.71 0.78 0.77 0.80 0.68 0.72 0.66 0.71 0.69 0.63 0.51 0.67 0.63 0.69 0.77 0.72 0.73 0.61 0.66 0.65 0.72 0.66 0.64 0.75 0.59 0.72 0.71 0.73 0.75 0.65 0.80 0.68 0.74 0.76 0.75 0.70 0.69 0.72 0.74 0.71 0.61 0.73 0.61 0.67 0.67 0.71 0.75 0.75 0.69 0.65 0.74 0.72 0.70 0.74 0.71 0.71 0.71 0.59 0.73 0.66 0.63 0.69 0.74 0.70 0.62 0.75 0.67 0.79 0.69 0.71 0.81 0.68 0.64 0.74 0.56 0.68 0.67 0.86 0.42 0.67 0.81 0.68 0.61 0.97 0.40 0.58 0.56 0.73 0.67 0.74 1.00 0.70 0.66 0.69 0.67 0.73 0.64 0.61 0.97 0.74 0.54 0.73 0.53 0.68 0.74 0.60 0.80 0.40 0.70 0.54 0.73 0.72 0.66 0.70 0.65 0.68 0.68 0.42 0.48 0.72 0.71 0.66 0.66 0.69 0.71 0.67 0.69 0.70 0.71 0.70 0.59 0.73 0.68 0.70 0.72 0.75 0.71 0.67 0.62 0.73 0.71 0.69 0.80 0.78 0.62 0.84 0.48 0.74 0.79 0.63 0.71 0.62 0.80 0.80 0.71 0.81 0.79 0.77 0.85 0.64 0.76 0.70 0.74 0.55 0.83 0.74 0.74 0.84 0.44 0.70 0.78 0.86 0.84 0.83 0.84 0.84 0.79 0.84 0.83 0.72 0.77 0.86 0.81 0.86 0.75 0.75 0.70 0.75 0.67 0.58 0.46 0.74 0.68 0.76 0.79 0.72 0.73 0.65 0.70 0.68 0.71 0.74 0.70 0.77 0.64 0.77 0.76 0.79 0.80 0.71 0.77 0.72 0.81 0.82 0.81 0.77 0.74 0.77 0.79 0.77 0.66 0.78 0.67 0.71 0.71 0.79 0.81 0.80 0.77 0.69 0.80 0.78 0.76 0.80 0.77 0.76 0.77 0.65 0.76 0.95 0.66 0.81 0.78 0.81 0.92 0.80 0.71 0.85 0.74 0.75 0.66 0.76 0.70 0.79 0.63 0.81 0.71 0.74 0.40 0.73 0.75 0.70 0.66 0.76 0.44 0.61 0.62 0.76 0.71 0.79 0.70 1.00 0.77 0.73 0.70 0.77 0.75 0.66 0.72 0.79 0.59 0.77 0.58 0.73 0.80 0.67 0.83 0.42 0.77 0.61 0.78 0.78 0.71 0.76 0.71 0.72 0.77 0.45 0.51 0.79 0.76 0.73 0.72 0.75 0.77 0.73 0.74 0.76 0.76 0.76 0.67 0.79 0.72 0.71 0.73 0.73 0.76 0.74 0.74 0.79 0.78 0.77 0.74 0.70 0.57 0.78 0.44 0.68 0.74 0.59 0.65 0.58 0.74 0.74 0.68 0.74 0.72 0.73 0.79 0.57 0.71 0.64 0.69 0.50 0.78 0.69 0.68 0.77 0.42 0.65 0.71 0.81 0.78 0.78 0.79 0.79 0.75 0.78 0.77 0.67 0.70 0.80 0.74 0.81 0.70 0.69 0.65 0.71 0.63 0.55 0.43 0.68 0.62 0.69 0.74 0.68 0.69 0.61 0.65 0.64 0.65 0.68 0.65 0.73 0.58 0.71 0.71 0.73 0.75 0.66 0.69 0.66 0.72 0.76 0.74 0.71 0.68 0.72 0.71 0.71 0.60 0.72 0.61 0.66 0.65 0.73 0.76 0.75 0.71 0.65 0.76 0.74 0.70 0.75 0.71 0.70 0.73 0.58 0.71 0.66 0.61 0.99 0.73 0.76 0.62 0.74 0.67 0.80 0.68 0.70 0.61 0.69 0.65 0.74 0.59 0.99 0.67 0.69 0.38 0.67 0.69 0.64 0.60 0.72 0.42 0.57 0.58 0.71 0.66 0.74 0.66 0.77 1.00 0.69 0.63 0.72 0.71 0.62 0.69 0.73 0.54 0.70 0.53 0.68 0.74 0.60 0.82 0.37 0.69 0.56 0.72 0.72 0.65 0.71 0.66 0.66 0.69 0.42 0.47 0.71 0.69 0.66 0.66 0.69 0.71 0.67 0.69 0.70 0.71 0.71 0.60 0.74 0.67 0.66 0.68 0.68 0.77 0.68 0.58 0.81 0.79 0.77 0.81 0.73 0.60 0.86 0.62 0.87 0.90 0.70 0.83 0.65 0.87 0.81 0.70 0.84 0.90 0.74 0.85 0.60 0.89 0.85 0.75 0.53 0.73 0.65 0.62 0.82 0.44 0.69 0.70 0.85 0.88 0.81 0.83 0.87 0.82 0.90 0.81 0.65 0.75 0.80 0.91 0.83 0.85 0.95 0.84 0.91 0.57 0.51 0.40 0.74 0.71 0.75 0.77 0.62 0.67 0.71 0.88 0.85 0.64 0.76 0.74 0.70 0.72 0.81 0.87 0.86 0.84 0.81 0.79 0.87 0.79 0.86 0.88 0.79 0.68 0.79 0.85 0.78 0.60 0.89 0.58 0.76 0.76 0.67 0.73 0.86 0.64 0.65 0.69 0.72 0.76 0.82 0.81 0.92 0.79 0.60 0.81 0.75 0.83 0.72 0.93 0.78 0.75 0.79 0.87 0.87 0.82 0.93 0.68 0.85 0.85 0.93 0.58 0.72 0.79 0.93 0.38 0.75 0.86 0.66 0.68 0.79 0.49 0.72 0.66 0.88 0.82 0.80 0.69 0.73 0.69 1.00 0.70 0.79 0.72 0.78 0.72 0.91 0.56 0.92 0.57 0.83 0.81 0.71 0.85 0.45 0.81 0.56 0.84 0.75 0.84 0.83 0.79 0.66 0.62 0.44 0.46 0.88 0.91 0.77 0.76 0.81 0.78 0.72 0.74 0.79 0.92 0.84 0.57 0.85 0.81 0.77 0.84 0.87 0.65 0.62 0.54 0.67 0.64 0.63 0.71 0.68 0.57 0.76 0.44 0.69 0.72 0.59 0.65 0.56 0.71 0.71 0.64 0.73 0.72 0.70 0.76 0.57 0.69 0.62 0.66 0.47 0.73 0.64 0.64 0.72 0.38 0.63 0.69 0.77 0.75 0.75 0.74 0.76 0.71 0.76 0.72 0.64 0.67 0.75 0.73 0.77 0.65 0.71 0.64 0.68 0.61 0.54 0.44 0.66 0.91 0.67 0.72 0.65 0.65 0.58 0.65 0.63 0.64 0.66 0.62 0.69 0.57 0.69 0.70 0.72 0.72 0.65 0.69 0.66 0.73 0.73 0.74 0.70 0.66 0.69 0.71 0.70 0.61 0.72 0.62 0.66 0.65 0.69 0.72 0.72 0.68 0.62 0.70 0.69 0.66 0.72 0.68 0.69 0.69 0.58 0.68 0.67 0.63 0.66 0.74 0.69 0.62 0.71 0.66 0.77 0.67 0.69 0.91 0.68 0.63 0.74 0.55 0.65 0.64 0.71 0.43 0.66 0.72 0.62 0.59 0.76 0.39 0.56 0.56 0.71 0.64 0.70 0.67 0.70 0.63 0.70 1.00 0.70 0.67 0.60 0.71 0.72 0.51 0.72 0.63 0.65 0.70 0.58 0.76 0.39 0.68 0.54 0.70 0.68 0.63 0.67 0.63 0.65 0.67 0.41 0.46 0.69 0.71 0.64 0.64 0.68 0.68 0.64 0.68 0.68 0.71 0.69 0.57 0.72 0.65 0.67 0.68 0.67 0.75 0.69 0.62 0.78 0.75 0.73 0.82 0.78 0.63 0.86 0.54 0.82 0.85 0.68 0.77 0.64 0.85 0.81 0.73 0.84 0.84 0.78 0.87 0.63 0.82 0.75 0.76 0.56 0.81 0.72 0.73 0.84 0.46 0.71 0.76 0.87 0.86 0.84 0.85 0.87 0.82 0.87 0.84 0.71 0.77 0.85 0.87 0.87 0.78 0.83 0.75 0.82 0.71 0.64 0.51 0.84 0.69 0.77 0.84 0.75 0.77 0.69 0.84 0.76 0.76 0.85 0.83 0.80 0.67 0.80 0.80 0.82 0.83 0.76 0.80 0.78 0.82 0.85 0.85 0.79 0.74 0.80 0.83 0.80 0.66 0.84 0.65 0.75 0.74 0.77 0.81 0.84 0.75 0.70 0.78 0.77 0.75 0.82 0.81 0.82 0.78 0.65 0.79 0.75 0.66 0.74 0.82 0.78 0.73 0.99 0.77 0.88 0.77 0.83 0.67 0.80 0.76 0.82 0.63 0.73 0.75 0.81 0.40 0.75 0.81 0.72 0.68 0.80 0.47 0.66 0.65 0.82 0.97 1.00 0.73 0.77 0.72 0.79 0.70 1.00 0.72 0.71 0.74 0.84 0.58 0.84 0.58 0.77 0.83 0.70 0.86 0.46 0.79 0.60 0.82 0.79 0.77 0.80 0.75 0.73 0.75 0.46 0.51 0.84 0.83 0.75 0.75 0.79 0.79 0.75 0.76 0.79 0.83 0.80 0.65 0.82 0.76 0.77 0.81 0.79 0.67 0.62 0.54 0.67 0.65 0.64 0.73 0.70 0.55 0.78 0.47 0.70 0.75 0.60 0.68 0.57 0.75 0.74 0.68 0.74 0.76 0.73 0.79 0.56 0.72 0.66 0.68 0.48 0.74 0.67 0.65 0.76 0.39 0.64 0.69 0.79 0.78 0.77 0.77 0.78 0.74 0.79 0.76 0.65 0.70 0.78 0.76 0.79 0.72 0.71 0.67 0.73 0.60 0.54 0.42 0.67 0.60 0.68 0.73 0.64 0.67 0.60 0.67 0.65 0.64 0.67 0.64 0.70 0.58 0.71 0.72 0.73 0.75 0.67 0.70 0.70 0.74 0.77 0.76 0.70 0.66 0.71 0.73 0.71 0.58 0.74 0.60 0.66 0.67 0.71 0.74 0.75 0.69 0.64 0.73 0.72 0.69 0.74 0.71 0.71 0.72 0.57 0.71 0.67 0.61 0.75 0.73 0.74 0.64 0.74 0.68 0.77 0.70 0.72 0.58 0.70 0.64 0.73 0.56 0.75 0.67 0.71 0.36 0.67 0.69 0.65 0.60 0.71 0.42 0.58 0.58 0.73 0.69 0.74 0.64 0.75 0.71 0.72 0.67 0.72 1.00 0.61 0.68 0.74 0.53 0.73 0.75 0.69 0.74 0.60 0.78 0.41 0.70 0.55 0.73 0.71 0.67 0.70 0.64 0.64 0.67 0.41 0.46 0.73 0.73 0.66 0.68 0.70 0.70 0.67 0.69 0.70 0.74 0.72 0.61 0.74 0.69 0.66 0.69 0.69 0.66 0.58 0.51 0.68 0.67 0.66 0.71 0.66 0.54 0.76 0.51 0.73 0.77 0.58 0.70 0.57 0.75 0.72 0.62 0.72 0.77 0.66 0.75 0.53 0.75 0.71 0.65 0.45 0.67 0.59 0.57 0.72 0.39 0.60 0.63 0.76 0.77 0.72 0.74 0.76 0.72 0.78 0.72 0.58 0.66 0.72 0.77 0.75 0.71 0.77 0.71 0.77 0.53 0.47 0.36 0.65 0.62 0.66 0.69 0.58 0.60 0.60 0.73 0.70 0.59 0.68 0.65 0.64 0.58 0.70 0.74 0.74 0.74 0.69 0.69 0.73 0.71 0.76 0.77 0.68 0.60 0.69 0.72 0.68 0.54 0.76 0.52 0.64 0.66 0.62 0.67 0.75 0.60 0.59 0.63 0.65 0.64 0.73 0.71 0.74 0.70 0.53 0.72 0.66 0.57 0.64 0.77 0.69 0.66 0.72 0.74 0.77 0.71 0.79 0.58 0.97 0.98 0.72 0.53 0.65 0.69 0.78 0.35 0.66 0.71 0.59 0.61 0.68 0.43 0.61 0.58 0.76 0.72 0.72 0.61 0.66 0.62 0.78 0.60 0.71 0.61 1.00 0.62 0.77 0.49 0.76 0.51 0.71 0.71 0.61 0.73 0.38 0.71 0.50 0.74 0.68 0.71 0.97 0.98 0.59 0.58 0.40 0.42 0.73 0.76 0.64 0.65 0.70 0.69 0.63 0.65 0.69 0.78 0.73 0.52 0.74 0.70 0.67 0.71 0.74 0.70 0.66 0.61 0.71 0.68 0.67 0.76 0.73 0.59 0.80 0.47 0.73 0.75 0.61 0.82 0.59 0.75 0.76 0.67 0.77 0.74 0.73 0.82 0.58 0.72 0.65 0.72 0.57 0.80 0.70 0.70 0.79 0.42 0.68 0.73 0.82 0.81 0.79 0.79 0.81 0.77 0.80 0.78 0.69 0.73 0.81 0.78 0.82 0.69 0.73 0.68 0.72 0.67 0.60 0.49 0.70 0.63 0.71 0.77 0.71 0.73 0.62 0.67 0.66 0.70 0.68 0.67 0.75 0.63 0.73 0.74 0.75 0.76 0.67 0.83 0.69 0.75 0.78 0.77 0.73 0.70 0.73 0.75 0.74 0.62 0.75 0.64 0.70 0.68 0.74 0.77 0.77 0.72 0.66 0.76 0.74 0.72 0.76 0.73 0.72 0.73 0.61 0.73 0.67 0.68 0.71 0.78 0.73 0.62 0.76 0.68 0.81 0.70 0.72 0.84 0.70 0.65 0.78 0.60 0.70 0.68 0.85 0.48 0.69 0.85 0.69 0.62 0.98 0.38 0.59 0.56 0.73 0.68 0.75 0.97 0.72 0.69 0.72 0.71 0.74 0.68 0.62 1.00 0.76 0.57 0.75 0.55 0.69 0.76 0.63 0.83 0.41 0.72 0.57 0.74 0.74 0.67 0.71 0.66 0.68 0.70 0.44 0.50 0.74 0.72 0.73 0.68 0.70 0.73 0.70 0.71 0.73 0.72 0.71 0.61 0.75 0.69 0.69 0.71 0.74 0.78 0.69 0.60 0.82 0.80 0.77 0.86 0.81 0.67 0.91 0.62 0.88 0.92 0.73 0.85 0.67 0.92 0.86 0.76 0.89 0.93 0.80 0.91 0.66 0.90 0.87 0.80 0.56 0.81 0.72 0.71 0.88 0.48 0.74 0.78 0.90 0.92 0.87 0.88 0.91 0.86 0.94 0.88 0.71 0.81 0.86 0.95 0.89 0.86 0.93 0.84 0.92 0.64 0.57 0.45 0.78 0.75 0.79 0.82 0.69 0.73 0.72 0.88 0.85 0.71 0.79 0.77 0.76 0.71 0.84 0.88 0.89 0.89 0.82 0.84 0.89 0.85 0.91 0.92 0.83 0.73 0.82 0.90 0.83 0.66 0.91 0.65 0.81 0.80 0.79 0.84 0.92 0.78 0.77 0.81 0.81 0.81 0.90 0.89 0.92 0.83 0.67 0.87 0.80 0.70 0.76 0.90 0.83 0.79 0.84 0.87 0.90 0.86 0.92 0.70 0.86 0.85 0.87 0.63 0.75 0.83 0.92 0.43 0.79 0.86 0.75 0.73 0.82 0.53 0.74 0.71 0.93 0.85 0.85 0.74 0.79 0.73 0.91 0.72 0.84 0.74 0.77 0.76 1.00 0.59 0.93 0.60 0.85 0.87 0.75 0.90 0.45 0.85 0.62 0.88 0.82 0.84 0.84 0.80 0.70 0.73 0.48 0.52 0.90 0.96 0.79 0.80 0.84 0.82 0.76 0.79 0.82 0.93 0.87 0.64 0.88 0.83 0.80 0.86 0.89 0.54 0.50 0.46 0.55 0.53 0.51 0.61 0.57 0.48 0.63 0.36 0.57 0.60 0.48 0.55 0.48 0.60 0.61 0.56 0.62 0.60 0.58 0.64 0.48 0.57 0.53 0.57 0.41 0.63 0.56 0.57 0.62 0.33 0.55 0.59 0.65 0.64 0.63 0.62 0.63 0.61 0.64 0.63 0.56 0.59 0.65 0.61 0.65 0.53 0.57 0.53 0.58 0.50 0.43 0.35 0.57 0.50 0.57 0.59 0.53 0.56 0.49 0.54 0.53 0.54 0.54 0.53 0.58 0.47 0.58 0.56 0.58 0.59 0.54 0.58 0.54 0.60 0.62 0.61 0.92 0.55 0.57 0.60 0.58 0.50 0.60 0.52 0.53 0.55 0.58 0.61 0.61 0.57 0.52 0.60 0.59 0.57 0.60 0.57 0.57 0.57 0.49 0.57 0.56 0.49 0.56 0.60 0.59 0.52 0.60 0.54 0.61 0.57 0.57 0.50 0.57 0.53 0.59 0.48 0.56 0.55 0.57 0.31 0.55 0.56 0.53 0.50 0.59 0.34 0.46 0.48 0.58 0.54 0.59 0.54 0.59 0.54 0.56 0.51 0.58 0.53 0.49 0.57 0.59 1.00 0.58 0.40 0.56 0.61 0.50 0.62 0.34 0.58 0.44 0.58 0.58 0.54 0.58 0.53 0.54 0.57 0.33 0.38 0.59 0.57 0.54 0.55 0.58 0.58 0.55 0.56 0.58 0.59 0.58 0.50 0.60 0.55 0.55 0.56 0.56 0.78 0.68 0.59 0.81 0.79 0.77 0.85 0.77 0.64 0.89 0.62 0.88 0.92 0.72 0.86 0.66 0.90 0.84 0.74 0.87 0.92 0.78 0.89 0.64 0.89 0.86 0.78 0.55 0.78 0.70 0.68 0.86 0.47 0.72 0.76 0.88 0.91 0.85 0.86 0.91 0.85 0.92 0.85 0.70 0.79 0.84 0.93 0.87 0.84 0.93 0.89 0.91 0.63 0.56 0.44 0.77 0.74 0.78 0.81 0.67 0.71 0.72 0.88 0.85 0.69 0.79 0.77 0.74 0.70 0.82 0.88 0.89 0.88 0.82 0.82 0.89 0.84 0.89 0.91 0.81 0.74 0.82 0.88 0.82 0.66 0.90 0.64 0.78 0.80 0.73 0.78 0.88 0.71 0.70 0.74 0.77 0.75 0.85 0.84 0.90 0.82 0.64 0.84 0.79 0.70 0.73 0.90 0.81 0.78 0.83 0.87 0.89 0.85 0.92 0.71 0.85 0.84 0.86 0.61 0.73 0.82 0.92 0.42 0.78 0.86 0.72 0.72 0.81 0.52 0.73 0.69 0.90 0.85 0.84 0.73 0.77 0.70 0.92 0.72 0.84 0.73 0.76 0.75 0.93 0.58 1.00 0.58 0.84 0.85 0.73 0.88 0.47 0.84 0.60 0.86 0.79 0.83 0.84 0.79 0.71 0.69 0.44 0.48 0.89 0.93 0.78 0.78 0.82 0.80 0.74 0.77 0.80 0.92 0.86 0.62 0.86 0.83 0.80 0.85 0.89 0.55 0.50 0.45 0.55 0.54 0.53 0.59 0.56 0.46 0.63 0.37 0.56 0.61 0.48 0.55 0.47 0.61 0.60 0.55 0.61 0.61 0.60 0.64 0.47 0.59 0.53 0.55 0.38 0.61 0.55 0.55 0.61 0.34 0.52 0.56 0.65 0.63 0.63 0.63 0.64 0.61 0.63 0.61 0.53 0.58 0.64 0.62 0.64 0.54 0.57 0.54 0.58 0.50 0.45 0.35 0.55 0.50 0.56 0.61 0.54 0.55 0.50 0.54 0.53 0.54 0.56 0.53 0.58 0.47 0.57 0.56 0.58 0.59 0.54 0.57 0.56 0.61 0.61 0.62 0.57 0.53 0.56 0.60 0.58 0.48 0.60 0.49 0.54 0.54 0.57 0.61 0.61 0.57 0.53 0.60 0.58 0.56 0.60 0.58 0.56 0.57 0.49 0.58 0.55 0.49 0.55 0.60 0.58 0.52 0.60 0.54 0.65 0.56 0.58 0.48 0.57 0.52 0.59 0.47 0.54 0.54 0.57 0.31 0.54 0.55 0.53 0.50 0.59 0.34 0.46 0.47 0.59 0.55 0.60 0.53 0.58 0.53 0.57 0.63 0.58 0.75 0.51 0.55 0.60 0.40 0.58 1.00 0.56 0.60 0.48 0.64 0.32 0.57 0.44 0.59 0.58 0.54 0.57 0.53 0.53 0.56 0.32 0.36 0.59 0.58 0.53 0.55 0.57 0.58 0.55 0.57 0.57 0.59 0.58 0.51 0.60 0.55 0.55 0.56 0.56 0.73 0.65 0.57 0.75 0.74 0.72 0.79 0.73 0.62 0.83 0.55 0.80 0.84 0.65 0.76 0.62 0.97 0.79 0.70 0.82 0.96 0.74 0.83 0.61 0.83 0.78 0.73 0.50 0.75 0.67 0.66 0.81 0.43 0.68 0.72 0.83 0.84 0.80 0.82 0.84 0.79 0.87 0.95 0.66 0.75 0.81 0.86 0.83 0.77 0.84 0.77 0.84 0.59 0.53 0.40 0.72 0.67 0.73 0.77 0.64 0.68 0.66 0.79 0.76 0.66 0.73 0.71 0.71 0.64 0.77 0.80 0.81 0.81 0.75 0.76 0.80 0.79 0.83 0.84 0.77 0.69 0.76 0.81 0.76 0.61 0.83 0.61 0.72 0.74 0.70 0.75 0.82 0.68 0.65 0.72 0.72 0.71 0.80 0.78 0.82 0.76 0.60 0.78 0.73 0.64 0.71 0.83 0.77 0.71 0.78 0.79 0.84 0.79 0.84 0.63 0.79 0.77 0.80 0.58 0.71 0.76 0.83 0.37 0.72 0.78 0.66 0.67 0.75 0.47 0.68 0.64 0.83 0.78 0.79 0.68 0.73 0.68 0.83 0.65 0.77 0.69 0.71 0.69 0.85 0.56 0.84 0.56 1.00 0.97 0.67 0.81 0.43 0.99 0.56 0.81 0.76 0.76 0.78 0.73 0.65 0.67 0.44 0.48 0.81 0.84 0.71 0.73 0.77 0.76 0.70 0.73 0.76 0.84 0.80 0.60 0.82 0.76 0.73 0.78 0.81 0.76 0.69 0.62 0.78 0.76 0.74 0.85 0.80 0.67 0.89 0.55 0.81 0.87 0.68 0.78 0.66 0.98 0.84 0.75 0.87 0.95 0.80 0.90 0.66 0.84 0.77 0.79 0.53 0.85 0.76 0.75 0.88 0.51 0.74 0.80 0.90 0.89 0.87 0.87 0.90 0.84 0.90 0.99 0.74 0.81 0.89 0.89 0.90 0.77 0.84 0.78 0.84 0.67 0.59 0.46 0.79 0.69 0.79 0.83 0.72 0.75 0.69 0.79 0.76 0.73 0.78 0.76 0.78 0.65 0.82 0.83 0.85 0.86 0.77 0.81 0.80 0.85 0.87 0.87 0.82 0.76 0.81 0.85 0.82 0.68 0.85 0.69 0.78 0.77 0.79 0.83 0.86 0.78 0.72 0.81 0.80 0.78 0.84 0.82 0.83 0.80 0.67 0.82 0.78 0.69 0.77 0.85 0.82 0.74 0.85 0.79 0.89 0.81 0.84 0.67 0.81 0.77 0.84 0.67 0.77 0.79 0.83 0.42 0.77 0.80 0.74 0.71 0.81 0.47 0.69 0.66 0.84 0.80 0.84 0.74 0.80 0.74 0.81 0.70 0.83 0.74 0.71 0.76 0.87 0.61 0.85 0.60 0.97 1.00 0.72 0.88 0.44 0.98 0.62 0.84 0.82 0.77 0.81 0.76 0.74 0.77 0.47 0.53 0.84 0.84 0.75 0.77 0.80 0.81 0.76 0.78 0.81 0.85 0.82 0.68 0.85 0.78 0.77 0.80 0.82 0.64 0.57 0.51 0.66 0.63 0.62 0.72 0.69 0.55 0.75 0.48 0.70 0.74 0.59 0.67 0.57 0.74 0.72 0.63 0.75 0.73 0.67 0.76 0.56 0.72 0.68 0.67 0.48 0.71 0.63 0.63 0.74 0.40 0.63 0.68 0.75 0.76 0.74 0.72 0.77 0.72 0.77 0.75 0.61 0.68 0.74 0.77 0.75 0.68 0.72 0.67 0.72 0.55 0.47 0.39 0.66 0.59 0.67 0.67 0.59 0.62 0.59 0.69 0.67 0.60 0.66 0.63 0.64 0.57 0.70 0.72 0.73 0.73 0.67 0.68 0.69 0.71 0.75 0.74 0.69 0.64 0.69 0.78 0.76 0.61 0.76 0.62 0.85 0.69 0.65 0.69 0.73 0.64 0.60 0.66 0.65 0.66 0.71 0.72 0.71 0.68 0.57 0.71 0.66 0.58 0.62 0.72 0.69 0.64 0.71 0.67 0.74 0.70 0.72 0.58 0.69 0.67 0.70 0.54 0.62 0.69 0.71 0.36 0.66 0.69 0.63 0.60 0.68 0.44 0.58 0.59 0.73 0.67 0.70 0.60 0.67 0.60 0.71 0.58 0.70 0.60 0.61 0.63 0.75 0.50 0.73 0.48 0.67 0.72 1.00 0.76 0.40 0.69 0.52 0.72 0.69 0.66 0.68 0.64 0.61 0.65 0.39 0.44 0.72 0.72 0.67 0.65 0.69 0.73 0.71 0.72 0.73 0.74 0.70 0.56 0.72 0.67 0.65 0.69 0.70 0.86 0.80 0.75 0.88 0.85 0.84 0.87 0.84 0.68 0.93 0.59 0.85 0.90 0.75 0.84 0.70 0.89 0.88 0.80 0.89 0.89 0.85 0.94 0.68 0.86 0.79 0.83 0.58 0.90 0.79 0.80 0.91 0.49 0.78 0.84 0.94 0.93 0.92 0.91 0.93 0.89 0.93 0.90 0.79 0.83 0.93 0.92 0.94 0.81 0.88 0.80 0.86 0.73 0.64 0.52 0.81 0.73 0.81 0.87 0.78 0.80 0.71 0.82 0.80 0.77 0.81 0.78 0.83 0.68 0.86 0.87 0.88 0.89 0.79 0.85 0.85 0.88 0.93 0.92 0.84 0.78 0.83 0.90 0.87 0.73 0.88 0.74 0.82 0.81 0.84 0.88 0.89 0.82 0.78 0.85 0.84 0.83 0.88 0.86 0.86 0.85 0.70 0.86 0.80 0.75 0.84 0.89 0.84 0.76 0.88 0.83 0.94 0.84 0.86 0.74 0.83 0.78 0.88 0.68 0.83 0.81 0.85 0.50 0.80 0.84 0.80 0.73 0.88 0.46 0.69 0.66 0.88 0.81 0.87 0.80 0.83 0.82 0.85 0.76 0.86 0.78 0.73 0.83 0.90 0.62 0.88 0.64 0.81 0.88 0.76 1.00 0.47 0.83 0.67 0.87 0.85 0.79 0.84 0.78 0.76 0.81 0.49 0.55 0.88 0.87 0.78 0.81 0.85 0.84 0.80 0.83 0.84 0.87 0.84 0.71 0.87 0.81 0.80 0.82 0.84 0.40 0.36 0.31 0.42 0.39 0.39 0.48 0.42 0.35 0.50 0.30 0.43 0.48 0.41 0.42 0.38 0.49 0.47 0.42 0.50 0.50 0.47 0.49 0.32 0.46 0.42 0.42 0.29 0.43 0.38 0.43 0.45 0.23 0.40 0.44 0.50 0.45 0.49 0.48 0.49 0.45 0.50 0.48 0.38 0.41 0.48 0.48 0.48 0.41 0.46 0.42 0.46 0.36 0.33 0.23 0.44 0.36 0.43 0.44 0.41 0.42 0.37 0.43 0.41 0.39 0.42 0.41 0.41 0.37 0.44 0.46 0.47 0.46 0.43 0.44 0.42 0.46 0.49 0.46 0.43 0.41 0.44 0.45 0.43 0.36 0.47 0.38 0.43 0.43 0.41 0.45 0.46 0.42 0.39 0.43 0.48 0.42 0.44 0.45 0.46 0.44 0.38 0.44 0.43 0.41 0.40 0.42 0.44 0.39 0.44 0.43 0.48 0.44 0.45 0.38 0.47 0.44 0.47 0.34 0.41 0.44 0.45 0.22 0.43 0.44 0.41 0.39 0.43 0.26 0.35 0.36 0.46 0.43 0.44 0.40 0.42 0.37 0.45 0.39 0.46 0.41 0.38 0.41 0.45 0.34 0.47 0.32 0.43 0.44 0.40 0.47 1.00 0.47 0.34 0.46 0.44 0.43 0.46 0.42 0.38 0.41 0.26 0.29 0.48 0.46 0.43 0.44 0.45 0.44 0.43 0.44 0.43 0.50 0.43 0.35 0.44 0.43 0.39 0.44 0.44 0.73 0.65 0.57 0.75 0.73 0.71 0.81 0.76 0.65 0.86 0.55 0.80 0.85 0.67 0.77 0.64 0.98 0.81 0.72 0.84 0.96 0.77 0.86 0.64 0.83 0.78 0.74 0.52 0.79 0.70 0.70 0.83 0.43 0.69 0.75 0.86 0.86 0.83 0.84 0.86 0.81 0.88 0.97 0.69 0.78 0.84 0.87 0.85 0.77 0.84 0.77 0.84 0.63 0.55 0.43 0.73 0.69 0.75 0.79 0.68 0.71 0.67 0.78 0.75 0.69 0.74 0.71 0.74 0.65 0.79 0.81 0.82 0.83 0.76 0.78 0.80 0.81 0.84 0.85 0.79 0.71 0.78 0.83 0.79 0.64 0.83 0.64 0.75 0.75 0.74 0.78 0.83 0.72 0.68 0.75 0.76 0.75 0.81 0.80 0.83 0.77 0.64 0.80 0.77 0.66 0.72 0.83 0.78 0.75 0.80 0.79 0.86 0.79 0.83 0.66 0.80 0.77 0.81 0.59 0.71 0.76 0.82 0.38 0.74 0.79 0.70 0.69 0.77 0.48 0.69 0.66 0.84 0.79 0.80 0.70 0.77 0.69 0.81 0.68 0.79 0.70 0.71 0.72 0.85 0.58 0.84 0.57 0.99 0.98 0.69 0.83 0.47 1.00 0.60 0.82 0.78 0.77 0.79 0.74 0.68 0.72 0.44 0.49 0.83 0.84 0.73 0.75 0.78 0.78 0.73 0.75 0.78 0.84 0.81 0.63 0.83 0.77 0.75 0.79 0.82 0.56 0.51 0.48 0.57 0.54 0.53 0.62 0.60 0.49 0.64 0.37 0.58 0.63 0.49 0.54 0.49 0.61 0.60 0.55 0.62 0.60 0.60 0.66 0.49 0.58 0.53 0.57 0.41 0.63 0.56 0.57 0.64 0.35 0.53 0.60 0.65 0.65 0.64 0.63 0.66 0.61 0.64 0.64 0.55 0.59 0.65 0.63 0.66 0.55 0.58 0.54 0.58 0.51 0.44 0.35 0.58 0.50 0.58 0.59 0.55 0.56 0.49 0.54 0.53 0.54 0.56 0.54 0.58 0.48 0.59 0.59 0.61 0.61 0.55 0.59 0.56 0.62 0.62 0.63 0.59 0.58 0.60 0.63 0.61 0.51 0.61 0.53 0.56 0.55 0.60 0.63 0.61 0.59 0.54 0.60 0.59 0.57 0.60 0.59 0.59 0.58 0.51 0.58 0.57 0.51 0.58 0.60 0.60 0.54 0.63 0.55 0.64 0.57 0.58 0.51 0.58 0.53 0.60 0.49 0.57 0.57 0.57 0.32 0.57 0.57 0.56 0.52 0.59 0.34 0.47 0.49 0.59 0.55 0.61 0.54 0.61 0.56 0.56 0.54 0.60 0.55 0.50 0.57 0.62 0.44 0.60 0.44 0.56 0.62 0.52 0.67 0.34 0.60 1.00 0.60 0.60 0.54 0.59 0.53 0.58 0.61 0.34 0.40 0.60 0.59 0.55 0.55 0.57 0.58 0.56 0.58 0.58 0.58 0.58 0.52 0.60 0.56 0.55 0.57 0.56 0.76 0.69 0.61 0.78 0.77 0.74 0.83 0.78 0.65 0.88 0.56 0.83 0.87 0.69 0.79 0.66 0.87 0.83 0.75 0.85 0.87 0.80 0.89 0.66 0.85 0.79 0.78 0.56 0.82 0.73 0.72 0.86 0.46 0.73 0.77 0.89 0.89 0.85 0.87 0.89 0.84 0.90 0.85 0.72 0.79 0.86 0.89 0.88 0.80 0.85 0.79 0.86 0.66 0.58 0.46 0.76 0.72 0.78 0.82 0.71 0.73 0.69 0.81 0.79 0.71 0.78 0.74 0.77 0.69 0.81 0.83 0.85 0.85 0.78 0.80 0.82 0.83 0.87 0.87 0.80 0.74 0.81 0.85 0.80 0.66 0.86 0.65 0.76 0.77 0.77 0.81 0.86 0.75 0.70 0.79 0.78 0.76 0.85 0.83 0.84 0.81 0.66 0.82 0.77 0.68 0.75 0.85 0.81 0.75 0.83 0.81 0.89 0.81 0.87 0.67 0.82 0.80 0.84 0.63 0.75 0.79 0.84 0.42 0.76 0.82 0.72 0.71 0.80 0.50 0.69 0.68 0.85 0.80 0.82 0.73 0.78 0.72 0.84 0.70 0.82 0.73 0.74 0.74 0.88 0.58 0.86 0.59 0.81 0.84 0.72 0.87 0.46 0.82 0.60 1.00 0.97 0.83 0.82 0.77 0.71 0.74 0.47 0.51 0.85 0.86 0.77 0.76 0.92 0.80 0.76 0.78 0.80 0.87 0.83 0.65 0.85 0.79 0.77 0.81 0.83 0.73 0.68 0.62 0.74 0.73 0.70 0.81 0.78 0.64 0.85 0.50 0.76 0.82 0.65 0.73 0.63 0.83 0.81 0.74 0.83 0.82 0.80 0.87 0.68 0.79 0.72 0.76 0.54 0.84 0.74 0.74 0.84 0.46 0.73 0.77 0.87 0.86 0.85 0.85 0.86 0.82 0.86 0.84 0.73 0.78 0.87 0.84 0.88 0.78 0.78 0.73 0.79 0.68 0.59 0.47 0.74 0.67 0.76 0.81 0.73 0.74 0.66 0.73 0.73 0.72 0.75 0.71 0.78 0.70 0.79 0.77 0.80 0.82 0.73 0.79 0.75 0.81 0.84 0.83 0.78 0.74 0.78 0.81 0.79 0.67 0.81 0.67 0.73 0.74 0.79 0.82 0.82 0.77 0.70 0.81 0.79 0.76 0.82 0.80 0.78 0.78 0.70 0.79 0.74 0.66 0.74 0.80 0.79 0.70 0.81 0.74 0.87 0.78 0.79 0.65 0.76 0.72 0.80 0.64 0.73 0.76 0.76 0.43 0.74 0.76 0.72 0.70 0.78 0.47 0.63 0.65 0.80 0.74 0.80 0.72 0.78 0.72 0.75 0.68 0.79 0.71 0.68 0.74 0.82 0.58 0.79 0.58 0.76 0.82 0.69 0.85 0.44 0.78 0.60 0.97 1.00 0.83 0.78 0.72 0.71 0.78 0.47 0.53 0.85 0.79 0.73 0.74 0.90 0.79 0.75 0.76 0.78 0.79 0.78 0.67 0.81 0.75 0.74 0.76 0.77 0.70 0.62 0.54 0.73 0.72 0.69 0.77 0.71 0.60 0.81 0.55 0.78 0.83 0.64 0.76 0.61 0.82 0.77 0.68 0.79 0.83 0.72 0.82 0.57 0.81 0.77 0.72 0.50 0.72 0.65 0.63 0.78 0.42 0.67 0.68 0.81 0.83 0.78 0.80 0.83 0.77 0.84 0.78 0.64 0.72 0.78 0.85 0.81 0.97 0.83 0.76 0.83 0.56 0.50 0.38 0.69 0.66 0.72 0.74 0.61 0.66 0.65 0.79 0.77 0.63 0.72 0.69 0.69 0.91 0.76 0.79 0.80 0.80 0.74 0.75 0.79 0.76 0.82 0.82 0.75 0.66 0.75 0.80 0.74 0.59 0.82 0.57 0.71 0.72 0.67 0.72 0.81 0.65 0.63 0.69 0.72 0.70 0.79 0.78 0.81 0.75 0.78 0.77 0.72 0.63 0.68 0.82 0.75 0.71 0.76 0.79 0.82 0.77 0.84 0.61 0.78 0.77 0.79 0.56 0.68 0.75 0.83 0.37 0.71 0.77 0.65 0.66 0.73 0.47 0.66 0.63 0.82 0.77 0.77 0.66 0.71 0.65 0.84 0.63 0.77 0.67 0.71 0.67 0.84 0.54 0.83 0.54 0.76 0.77 0.66 0.79 0.43 0.77 0.54 0.83 0.83 1.00 0.76 0.74 0.62 0.64 0.43 0.45 0.97 0.83 0.71 0.72 0.79 0.75 0.70 0.71 0.75 0.84 0.78 0.57 0.80 0.76 0.72 0.77 0.80 0.73 0.65 0.58 0.76 0.73 0.72 0.81 0.76 0.63 0.86 0.55 0.79 0.85 0.67 0.77 0.64 0.84 0.82 0.71 0.82 0.85 0.77 0.86 0.62 0.81 0.76 0.75 0.50 0.79 0.70 0.69 0.82 0.45 0.70 0.75 0.86 0.86 0.83 0.84 0.86 0.82 0.87 0.82 0.69 0.76 0.83 0.86 0.86 0.77 0.83 0.77 0.83 0.62 0.55 0.43 0.75 0.68 0.76 0.79 0.68 0.70 0.67 0.79 0.75 0.68 0.76 0.73 0.74 0.63 0.79 0.81 0.83 0.83 0.76 0.78 0.79 0.81 0.85 0.85 0.77 0.71 0.78 0.81 0.78 0.64 0.84 0.63 0.74 0.74 0.74 0.79 0.84 0.72 0.68 0.75 0.76 0.75 0.82 0.80 0.82 0.78 0.63 0.80 0.75 0.66 0.74 0.84 0.79 0.73 0.81 0.80 0.86 0.80 0.84 0.66 0.99 0.97 0.81 0.62 0.74 0.77 0.83 0.39 0.74 0.77 0.69 0.69 0.78 0.48 0.68 0.66 0.83 0.78 0.81 0.70 0.76 0.71 0.83 0.67 0.80 0.70 0.97 0.71 0.84 0.58 0.84 0.57 0.78 0.81 0.68 0.84 0.46 0.79 0.59 0.82 0.78 0.76 1.00 0.98 0.69 0.71 0.45 0.49 0.81 0.83 0.71 0.74 0.78 0.78 0.73 0.74 0.78 0.84 0.80 0.62 0.82 0.77 0.75 0.78 0.80 0.69 0.61 0.54 0.71 0.69 0.68 0.76 0.70 0.57 0.81 0.52 0.75 0.80 0.62 0.72 0.60 0.79 0.76 0.66 0.77 0.80 0.71 0.80 0.57 0.78 0.73 0.69 0.49 0.73 0.64 0.63 0.77 0.42 0.64 0.69 0.81 0.81 0.77 0.79 0.81 0.77 0.82 0.77 0.63 0.70 0.77 0.81 0.80 0.74 0.79 0.74 0.79 0.58 0.51 0.39 0.69 0.65 0.70 0.74 0.63 0.65 0.63 0.75 0.72 0.63 0.71 0.68 0.69 0.62 0.75 0.77 0.78 0.78 0.73 0.73 0.76 0.75 0.80 0.81 0.72 0.65 0.73 0.76 0.72 0.59 0.79 0.57 0.69 0.69 0.68 0.73 0.79 0.66 0.63 0.69 0.70 0.69 0.77 0.75 0.77 0.74 0.58 0.76 0.70 0.61 0.68 0.79 0.74 0.69 0.77 0.76 0.81 0.74 0.82 0.62 0.98 0.97 0.76 0.57 0.68 0.72 0.79 0.36 0.70 0.75 0.64 0.64 0.73 0.45 0.63 0.62 0.79 0.74 0.77 0.65 0.71 0.66 0.79 0.63 0.75 0.64 0.98 0.66 0.80 0.53 0.79 0.53 0.73 0.76 0.64 0.78 0.42 0.74 0.53 0.77 0.72 0.74 0.98 1.00 0.64 0.65 0.41 0.45 0.77 0.79 0.69 0.70 0.73 0.73 0.68 0.70 0.73 0.80 0.76 0.57 0.78 0.73 0.71 0.74 0.77 0.64 0.59 0.55 0.65 0.62 0.61 0.72 0.69 0.57 0.78 0.43 0.69 0.73 0.59 0.68 0.57 0.71 0.73 0.66 0.74 0.71 0.72 0.79 0.59 0.68 0.61 0.69 0.48 0.79 0.70 0.71 0.77 0.43 0.65 0.73 0.80 0.77 0.77 0.78 0.78 0.75 0.77 0.75 0.68 0.71 0.79 0.74 0.80 0.63 0.67 0.63 0.69 0.69 0.61 0.49 0.68 0.61 0.70 0.77 0.72 0.72 0.57 0.63 0.62 0.72 0.67 0.64 0.76 0.55 0.68 0.69 0.68 0.71 0.65 0.70 0.64 0.74 0.74 0.73 0.70 0.98 0.95 0.72 0.73 0.64 0.71 0.65 0.66 0.67 0.71 0.74 0.71 0.71 0.63 0.73 0.70 0.68 0.72 0.69 0.66 0.69 0.60 0.73 0.68 0.61 0.68 0.72 0.72 0.62 0.76 0.65 0.79 0.67 0.69 0.62 0.68 0.62 0.73 0.61 0.67 0.66 0.67 0.41 0.67 0.68 0.69 0.61 0.73 0.40 0.55 0.57 0.72 0.65 0.74 0.68 0.72 0.66 0.66 0.65 0.73 0.64 0.59 0.68 0.70 0.54 0.71 0.53 0.65 0.74 0.61 0.76 0.38 0.68 0.58 0.71 0.71 0.62 0.69 0.64 1.00 0.72 0.39 0.45 0.70 0.67 0.64 0.65 0.67 0.70 0.67 0.69 0.70 0.69 0.69 0.62 0.73 0.66 0.69 0.69 0.66 0.66 0.63 0.59 0.67 0.64 0.62 0.78 0.77 0.63 0.80 0.42 0.67 0.74 0.60 0.64 0.59 0.75 0.76 0.71 0.79 0.73 0.77 0.83 0.65 0.70 0.62 0.72 0.51 0.83 0.75 0.77 0.81 0.47 0.70 0.78 0.83 0.79 0.82 0.80 0.80 0.76 0.79 0.81 0.72 0.75 0.84 0.77 0.84 0.65 0.67 0.62 0.68 0.69 0.58 0.48 0.72 0.64 0.74 0.77 0.73 0.74 0.60 0.62 0.62 0.72 0.70 0.67 0.77 0.57 0.73 0.69 0.73 0.76 0.84 0.74 0.66 0.79 0.78 0.76 0.74 0.73 0.73 0.75 0.78 0.69 0.72 0.71 0.70 0.70 0.87 0.87 0.74 0.84 0.70 0.86 0.85 0.75 0.83 0.81 0.68 0.73 0.65 0.72 0.71 0.63 0.72 0.71 0.76 0.65 0.78 0.62 0.80 0.70 0.65 0.64 0.69 0.62 0.73 0.65 0.70 0.69 0.64 0.42 0.90 0.67 0.71 0.64 0.74 0.43 0.56 0.61 0.70 0.67 0.76 0.68 0.77 0.69 0.62 0.67 0.75 0.67 0.58 0.70 0.73 0.57 0.69 0.56 0.67 0.77 0.65 0.81 0.41 0.72 0.61 0.74 0.78 0.64 0.71 0.65 0.72 1.00 0.44 0.52 0.74 0.72 0.67 0.69 0.71 0.74 0.72 0.73 0.73 0.69 0.70 0.68 0.75 0.67 0.68 0.68 0.67 0.42 0.38 0.36 0.43 0.43 0.41 0.47 0.81 0.37 0.50 0.30 0.44 0.49 0.37 0.42 0.38 0.48 0.48 0.42 0.48 0.47 0.47 0.52 0.38 0.45 0.42 0.43 0.30 0.48 0.44 0.42 0.50 0.24 0.41 0.44 0.52 0.50 0.50 0.50 0.51 0.48 0.51 0.50 0.41 0.47 0.51 0.48 0.52 0.40 0.45 0.43 0.45 0.39 0.35 0.27 0.44 0.40 0.45 0.48 0.43 0.43 0.38 0.48 0.41 0.42 0.44 0.43 0.47 0.37 0.51 0.45 0.47 0.49 0.44 0.45 0.43 0.47 0.49 0.49 0.45 0.43 0.45 0.48 0.47 0.40 0.48 0.40 0.43 0.44 0.47 0.50 0.49 0.46 0.40 0.48 0.47 0.45 0.49 0.49 0.44 0.46 0.36 0.46 0.43 0.41 0.44 0.49 0.46 0.41 0.47 0.43 0.51 0.45 0.46 0.39 0.44 0.42 0.48 0.36 0.44 0.44 0.45 0.25 0.43 0.45 0.41 0.38 0.46 0.27 0.37 0.38 0.47 0.44 0.47 0.42 0.45 0.42 0.44 0.41 0.46 0.41 0.40 0.44 0.48 0.33 0.44 0.32 0.44 0.47 0.39 0.49 0.26 0.44 0.34 0.47 0.47 0.43 0.45 0.41 0.39 0.44 1.00 0.99 0.47 0.45 0.41 0.43 0.45 0.45 0.43 0.44 0.46 0.47 0.45 0.37 0.48 0.44 0.43 0.44 0.44 0.45 0.42 0.40 0.46 0.46 0.44 0.52 0.85 0.41 0.56 0.31 0.48 0.53 0.41 0.45 0.42 0.53 0.53 0.48 0.53 0.51 0.53 0.58 0.43 0.49 0.45 0.49 0.34 0.55 0.50 0.50 0.56 0.27 0.47 0.51 0.57 0.55 0.57 0.56 0.56 0.53 0.56 0.55 0.48 0.52 0.58 0.53 0.58 0.44 0.48 0.45 0.48 0.45 0.40 0.31 0.49 0.44 0.50 0.53 0.50 0.49 0.42 0.50 0.44 0.48 0.48 0.47 0.53 0.40 0.55 0.49 0.51 0.53 0.48 0.51 0.47 0.53 0.54 0.53 0.50 0.49 0.50 0.53 0.52 0.45 0.52 0.46 0.47 0.48 0.54 0.57 0.53 0.53 0.45 0.55 0.54 0.51 0.54 0.54 0.48 0.51 0.42 0.51 0.48 0.45 0.49 0.53 0.52 0.45 0.53 0.46 0.56 0.49 0.48 0.45 0.47 0.44 0.53 0.41 0.49 0.48 0.48 0.30 0.48 0.49 0.48 0.43 0.52 0.30 0.39 0.43 0.51 0.48 0.52 0.48 0.51 0.47 0.46 0.46 0.51 0.46 0.42 0.50 0.52 0.38 0.48 0.36 0.48 0.53 0.44 0.55 0.29 0.49 0.40 0.51 0.53 0.45 0.49 0.45 0.45 0.52 0.99 1.00 0.51 0.48 0.46 0.47 0.49 0.50 0.49 0.50 0.51 0.50 0.49 0.44 0.53 0.47 0.48 0.48 0.48 0.76 0.68 0.59 0.79 0.76 0.74 0.85 0.80 0.66 0.89 0.58 0.84 0.89 0.70 0.81 0.65 0.88 0.84 0.76 0.86 0.89 0.79 0.89 0.66 0.86 0.82 0.78 0.57 0.80 0.72 0.72 0.86 0.47 0.73 0.77 0.89 0.91 0.86 0.87 0.91 0.84 0.91 0.86 0.71 0.79 0.86 0.92 0.88 0.97 0.88 0.81 0.88 0.64 0.57 0.45 0.77 0.71 0.78 0.81 0.69 0.73 0.71 0.84 0.81 0.71 0.78 0.76 0.77 0.89 0.83 0.85 0.87 0.87 0.79 0.81 0.84 0.84 0.88 0.89 0.81 0.73 0.81 0.87 0.82 0.67 0.88 0.66 0.77 0.79 0.76 0.80 0.87 0.73 0.70 0.77 0.79 0.78 0.85 0.84 0.87 0.81 0.80 0.83 0.79 0.70 0.74 0.87 0.81 0.77 0.84 0.83 0.88 0.83 0.88 0.67 0.83 0.80 0.85 0.63 0.74 0.81 0.86 0.41 0.77 0.84 0.73 0.71 0.80 0.47 0.71 0.66 0.87 0.83 0.84 0.72 0.79 0.71 0.88 0.69 0.84 0.73 0.73 0.74 0.90 0.59 0.89 0.59 0.81 0.84 0.72 0.88 0.48 0.83 0.60 0.85 0.85 0.97 0.81 0.77 0.70 0.74 0.47 0.51 1.00 0.89 0.79 0.78 0.82 0.81 0.76 0.78 0.81 0.89 0.84 0.64 0.86 0.81 0.77 0.82 0.85 0.77 0.67 0.57 0.81 0.79 0.77 0.84 0.77 0.64 0.88 0.63 0.88 0.91 0.72 0.87 0.65 0.90 0.83 0.74 0.87 0.93 0.77 0.88 0.63 0.90 0.86 0.77 0.53 0.77 0.68 0.67 0.85 0.46 0.71 0.74 0.87 0.89 0.83 0.85 0.89 0.84 0.92 0.85 0.68 0.78 0.83 0.94 0.86 0.86 0.94 0.85 0.92 0.61 0.54 0.43 0.76 0.74 0.77 0.80 0.65 0.70 0.70 0.88 0.84 0.68 0.78 0.76 0.73 0.69 0.82 0.87 0.88 0.87 0.82 0.82 0.89 0.83 0.88 0.90 0.81 0.70 0.80 0.88 0.81 0.63 0.90 0.61 0.77 0.79 0.80 0.85 0.92 0.79 0.76 0.81 0.84 0.80 0.92 0.93 0.92 0.81 0.64 0.84 0.80 0.68 0.72 0.88 0.81 0.79 0.82 0.88 0.88 0.84 0.92 0.68 0.85 0.84 0.85 0.60 0.72 0.81 0.92 0.40 0.78 0.85 0.72 0.71 0.79 0.51 0.73 0.68 0.90 0.86 0.83 0.71 0.76 0.69 0.91 0.71 0.83 0.73 0.76 0.72 0.96 0.57 0.93 0.58 0.84 0.84 0.72 0.87 0.46 0.84 0.59 0.86 0.79 0.83 0.83 0.79 0.67 0.72 0.45 0.48 0.89 1.00 0.77 0.78 0.83 0.79 0.73 0.76 0.80 0.92 0.86 0.61 0.86 0.82 0.79 0.85 0.89 0.68 0.64 0.55 0.71 0.68 0.66 0.76 0.72 0.57 0.79 0.49 0.76 0.78 0.61 0.70 0.60 0.77 0.75 0.69 0.77 0.77 0.71 0.80 0.59 0.76 0.71 0.71 0.93 0.75 0.67 0.66 0.77 0.41 0.66 0.71 0.81 0.80 0.78 0.78 0.80 0.76 0.81 0.76 0.67 0.73 0.79 0.80 0.80 0.73 0.76 0.71 0.77 0.60 0.52 0.41 0.70 0.64 0.72 0.75 0.64 0.67 0.64 0.73 0.70 0.66 0.70 0.68 0.70 0.81 0.73 0.76 0.77 0.77 0.71 0.72 0.73 0.76 0.79 0.80 0.74 0.68 0.73 0.77 0.74 0.61 0.78 0.63 0.70 0.70 0.70 0.73 0.78 0.67 0.63 0.72 0.72 0.70 0.76 0.74 0.77 0.73 0.59 0.74 0.70 0.65 0.69 0.78 0.74 0.67 0.76 0.73 0.80 0.73 0.78 0.62 0.73 0.71 0.79 0.57 0.69 0.72 0.77 0.39 0.70 0.94 0.66 0.63 0.74 0.43 0.61 0.60 0.77 0.71 0.76 0.66 0.73 0.66 0.77 0.64 0.75 0.66 0.64 0.73 0.79 0.54 0.78 0.53 0.71 0.75 0.67 0.78 0.43 0.73 0.55 0.77 0.73 0.71 0.71 0.69 0.64 0.67 0.41 0.46 0.79 0.77 1.00 0.70 0.73 0.73 0.69 0.71 0.72 0.78 0.74 0.59 0.76 0.72 0.68 0.74 0.75 0.73 0.66 0.61 0.75 0.73 0.71 0.77 0.72 0.59 0.80 0.50 0.75 0.80 0.62 0.72 0.59 0.80 0.76 0.68 0.78 0.80 0.73 0.81 0.61 0.77 0.72 0.71 0.51 0.75 0.67 0.67 0.78 0.44 0.66 0.71 0.81 0.81 0.79 0.79 0.81 0.77 0.82 0.79 0.66 0.73 0.79 0.82 0.81 0.73 0.78 0.72 0.78 0.60 0.52 0.41 0.70 0.64 0.71 0.75 0.65 0.67 0.63 0.74 0.71 0.65 0.71 0.68 0.70 0.62 0.75 0.75 0.78 0.78 0.71 0.76 0.86 0.79 0.94 0.92 0.74 0.67 0.73 0.78 0.74 0.62 0.78 0.62 0.69 0.71 0.70 0.74 0.78 0.69 0.66 0.72 0.72 0.71 0.77 0.76 0.78 0.73 0.60 0.75 0.71 0.62 0.69 0.77 0.73 0.69 0.76 0.73 0.82 0.74 0.78 0.61 0.75 0.72 0.76 0.59 0.68 0.72 0.77 0.37 0.70 0.74 0.67 0.64 0.72 0.43 0.62 0.60 0.78 0.74 0.76 0.66 0.72 0.66 0.76 0.64 0.75 0.68 0.65 0.68 0.80 0.55 0.78 0.55 0.73 0.77 0.65 0.81 0.44 0.75 0.55 0.76 0.74 0.72 0.74 0.70 0.65 0.69 0.43 0.47 0.78 0.78 0.70 1.00 0.76 0.74 0.70 0.72 0.74 0.79 0.75 0.60 0.78 0.72 0.70 0.74 0.76 0.78 0.71 0.65 0.80 0.78 0.76 0.80 0.75 0.62 0.85 0.54 0.80 0.84 0.66 0.76 0.63 0.83 0.80 0.72 0.82 0.84 0.76 0.85 0.63 0.81 0.77 0.74 0.53 0.78 0.70 0.69 0.82 0.44 0.69 0.74 0.85 0.85 0.86 0.83 0.86 0.81 0.87 0.82 0.69 0.76 0.86 0.86 0.87 0.76 0.83 0.76 0.83 0.63 0.55 0.44 0.74 0.69 0.75 0.78 0.67 0.70 0.66 0.78 0.76 0.68 0.75 0.72 0.73 0.65 0.78 0.79 0.81 0.82 0.75 0.80 0.82 0.80 0.86 0.86 0.78 0.69 0.77 0.82 0.78 0.64 0.83 0.63 0.73 0.74 0.74 0.78 0.82 0.72 0.67 0.76 0.75 0.74 0.81 0.79 0.82 0.77 0.62 0.79 0.74 0.66 0.71 0.82 0.78 0.72 0.79 0.78 0.85 0.78 0.84 0.65 0.78 0.76 0.81 0.60 0.71 0.76 0.81 0.39 0.73 0.78 0.69 0.68 0.76 0.47 0.67 0.64 0.82 0.77 0.79 0.69 0.75 0.69 0.81 0.68 0.79 0.70 0.70 0.70 0.84 0.58 0.82 0.57 0.77 0.80 0.69 0.85 0.45 0.78 0.57 0.92 0.90 0.79 0.78 0.73 0.67 0.71 0.45 0.49 0.82 0.83 0.73 0.76 1.00 0.78 0.74 0.74 0.78 0.84 0.79 0.63 0.82 0.76 0.73 0.78 0.80 0.73 0.67 0.61 0.75 0.73 0.71 0.80 0.76 0.63 0.85 0.50 0.77 0.83 0.64 0.74 0.64 0.82 0.80 0.72 0.82 0.82 0.78 0.86 0.63 0.79 0.73 0.75 0.54 0.81 0.72 0.72 0.83 0.44 0.71 0.75 0.87 0.85 0.83 0.84 0.85 0.81 0.86 0.83 0.71 0.77 0.85 0.84 0.86 0.75 0.79 0.74 0.79 0.65 0.57 0.45 0.74 0.68 0.76 0.80 0.71 0.73 0.66 0.75 0.73 0.70 0.75 0.72 0.76 0.65 0.78 0.78 0.80 0.81 0.74 0.92 0.76 0.81 0.84 0.83 0.77 0.73 0.78 0.86 0.84 0.71 0.85 0.71 0.76 0.80 0.77 0.81 0.82 0.75 0.69 0.79 0.78 0.75 0.81 0.79 0.78 0.77 0.64 0.78 0.74 0.66 0.73 0.81 0.78 0.71 0.81 0.75 0.87 0.77 0.80 0.65 0.77 0.73 0.81 0.62 0.73 0.75 0.79 0.41 0.74 0.76 0.70 0.68 0.77 0.46 0.64 0.65 0.80 0.75 0.80 0.71 0.77 0.71 0.78 0.68 0.79 0.70 0.69 0.73 0.82 0.58 0.80 0.58 0.76 0.81 0.73 0.84 0.44 0.78 0.58 0.80 0.79 0.75 0.78 0.73 0.70 0.74 0.45 0.50 0.81 0.79 0.73 0.74 0.78 1.00 0.99 0.83 1.00 0.80 0.78 0.64 0.81 0.74 0.74 0.76 0.78 0.69 0.64 0.59 0.70 0.68 0.67 0.76 0.72 0.61 0.80 0.47 0.71 0.77 0.60 0.69 0.60 0.77 0.76 0.69 0.78 0.76 0.74 0.82 0.60 0.74 0.67 0.71 0.52 0.78 0.69 0.69 0.78 0.43 0.67 0.72 0.83 0.80 0.79 0.80 0.81 0.77 0.81 0.78 0.68 0.73 0.81 0.78 0.82 0.69 0.73 0.68 0.74 0.63 0.55 0.44 0.70 0.63 0.72 0.76 0.69 0.70 0.61 0.68 0.68 0.68 0.70 0.67 0.73 0.61 0.73 0.73 0.75 0.77 0.69 0.89 0.70 0.76 0.79 0.78 0.73 0.69 0.73 0.82 0.81 0.69 0.81 0.69 0.73 0.76 0.74 0.78 0.77 0.72 0.66 0.76 0.74 0.72 0.77 0.74 0.72 0.73 0.62 0.74 0.69 0.62 0.70 0.76 0.74 0.66 0.77 0.69 0.83 0.71 0.74 0.61 0.72 0.68 0.76 0.60 0.69 0.70 0.73 0.40 0.70 0.71 0.67 0.65 0.73 0.43 0.59 0.61 0.75 0.71 0.76 0.67 0.73 0.67 0.72 0.64 0.75 0.67 0.63 0.70 0.76 0.55 0.74 0.55 0.70 0.76 0.71 0.80 0.43 0.73 0.56 0.76 0.75 0.70 0.73 0.68 0.67 0.72 0.43 0.49 0.76 0.73 0.69 0.70 0.74 0.99 1.00 0.82 0.99 0.75 0.73 0.62 0.76 0.69 0.69 0.71 0.72 0.70 0.65 0.59 0.72 0.69 0.68 0.78 0.75 0.61 0.82 0.49 0.75 0.80 0.63 0.71 0.62 0.79 0.78 0.70 0.80 0.79 0.76 0.83 0.62 0.77 0.70 0.73 0.53 0.80 0.71 0.71 0.81 0.43 0.68 0.74 0.84 0.82 0.82 0.81 0.82 0.78 0.83 0.80 0.69 0.75 0.83 0.81 0.84 0.72 0.76 0.70 0.77 0.65 0.56 0.46 0.71 0.67 0.73 0.78 0.69 0.71 0.64 0.72 0.70 0.70 0.71 0.68 0.75 0.63 0.76 0.76 0.78 0.79 0.71 0.75 0.74 0.79 0.81 0.81 0.75 0.71 0.75 0.95 0.82 0.69 0.83 0.69 0.75 0.77 0.75 0.78 0.79 0.74 0.68 0.77 0.75 0.73 0.79 0.76 0.76 0.76 0.63 0.76 0.71 0.64 0.71 0.77 0.76 0.67 0.78 0.72 0.83 0.75 0.76 0.65 0.74 0.70 0.77 0.61 0.71 0.73 0.75 0.40 0.72 0.74 0.68 0.66 0.76 0.46 0.62 0.63 0.77 0.72 0.77 0.69 0.74 0.69 0.74 0.68 0.76 0.69 0.65 0.71 0.79 0.56 0.77 0.57 0.73 0.78 0.72 0.83 0.44 0.75 0.58 0.78 0.76 0.71 0.74 0.70 0.69 0.73 0.44 0.50 0.78 0.76 0.71 0.72 0.74 0.83 0.82 1.00 0.84 0.78 0.75 0.64 0.78 0.72 0.71 0.74 0.74 0.73 0.67 0.61 0.75 0.73 0.71 0.80 0.76 0.63 0.85 0.51 0.77 0.83 0.64 0.74 0.65 0.82 0.80 0.72 0.82 0.82 0.77 0.85 0.63 0.80 0.73 0.75 0.53 0.80 0.71 0.71 0.82 0.45 0.71 0.75 0.86 0.85 0.83 0.84 0.85 0.81 0.86 0.83 0.70 0.77 0.85 0.84 0.86 0.74 0.79 0.74 0.80 0.64 0.57 0.45 0.74 0.68 0.75 0.80 0.70 0.72 0.66 0.75 0.73 0.70 0.74 0.72 0.76 0.64 0.78 0.78 0.80 0.81 0.74 0.91 0.76 0.80 0.83 0.83 0.77 0.73 0.78 0.86 0.83 0.70 0.86 0.69 0.76 0.79 0.76 0.80 0.82 0.74 0.68 0.78 0.77 0.75 0.81 0.79 0.79 0.77 0.64 0.78 0.74 0.66 0.73 0.81 0.77 0.71 0.80 0.76 0.87 0.77 0.80 0.65 0.78 0.74 0.81 0.62 0.72 0.76 0.79 0.40 0.74 0.76 0.69 0.68 0.77 0.47 0.65 0.65 0.80 0.76 0.80 0.70 0.76 0.70 0.79 0.68 0.79 0.70 0.69 0.73 0.82 0.58 0.80 0.57 0.76 0.81 0.73 0.84 0.43 0.78 0.58 0.80 0.78 0.75 0.78 0.73 0.70 0.73 0.46 0.51 0.81 0.80 0.72 0.74 0.78 1.00 0.99 0.84 1.00 0.81 0.78 0.63 0.81 0.74 0.74 0.77 0.78 0.79 0.69 0.60 0.82 0.80 0.78 0.85 0.77 0.64 0.89 0.63 0.88 0.92 0.72 0.84 0.66 0.90 0.84 0.74 0.87 0.92 0.78 0.89 0.64 0.90 0.86 0.78 0.54 0.78 0.69 0.67 0.85 0.46 0.72 0.74 0.88 0.90 0.84 0.87 0.90 0.85 0.93 0.85 0.69 0.79 0.84 0.93 0.87 0.85 0.93 0.85 0.93 0.62 0.56 0.43 0.77 0.74 0.78 0.81 0.67 0.71 0.73 0.88 0.85 0.69 0.79 0.77 0.75 0.70 0.83 0.88 0.88 0.87 0.82 0.81 0.88 0.83 0.89 0.91 0.82 0.72 0.82 0.88 0.81 0.63 0.90 0.62 0.78 0.79 0.72 0.78 0.88 0.70 0.69 0.74 0.76 0.75 0.85 0.84 0.91 0.82 0.64 0.84 0.78 0.68 0.74 0.89 0.81 0.78 0.82 0.88 0.90 0.85 0.93 0.68 0.87 0.86 0.86 0.61 0.75 0.82 0.92 0.38 0.78 0.85 0.70 0.72 0.79 0.52 0.73 0.69 0.90 0.85 0.83 0.71 0.76 0.71 0.92 0.71 0.83 0.74 0.78 0.72 0.93 0.59 0.92 0.59 0.84 0.85 0.74 0.87 0.50 0.84 0.58 0.87 0.79 0.84 0.84 0.80 0.69 0.69 0.47 0.50 0.89 0.92 0.78 0.79 0.84 0.80 0.75 0.78 0.81 1.00 0.97 0.78 0.97 0.83 0.79 0.85 0.89 0.75 0.67 0.59 0.77 0.76 0.74 0.81 0.76 0.64 0.86 0.58 0.83 0.87 0.69 0.79 0.64 0.86 0.81 0.72 0.84 0.87 0.77 0.86 0.63 0.85 0.79 0.76 0.52 0.78 0.69 0.68 0.83 0.44 0.70 0.74 0.86 0.87 0.83 0.84 0.86 0.81 0.89 0.84 0.69 0.77 0.83 0.88 0.85 0.79 0.86 0.79 0.86 0.63 0.56 0.44 0.74 0.71 0.75 0.80 0.68 0.71 0.69 0.81 0.79 0.69 0.75 0.73 0.75 0.66 0.79 0.83 0.84 0.83 0.77 0.79 0.82 0.81 0.85 0.86 0.79 0.72 0.79 0.83 0.79 0.63 0.85 0.62 0.75 0.76 0.73 0.78 0.84 0.71 0.68 0.75 0.75 0.73 0.82 0.81 0.85 0.79 0.63 0.80 0.76 0.66 0.74 0.84 0.79 0.75 0.80 0.81 0.87 0.80 0.86 0.66 0.81 0.79 0.82 0.60 0.74 0.77 0.85 0.39 0.75 0.80 0.69 0.69 0.78 0.49 0.70 0.66 0.84 0.80 0.81 0.70 0.76 0.71 0.84 0.69 0.80 0.72 0.73 0.71 0.87 0.58 0.86 0.58 0.80 0.82 0.70 0.84 0.43 0.81 0.58 0.83 0.78 0.78 0.80 0.76 0.69 0.70 0.45 0.49 0.84 0.86 0.74 0.75 0.79 0.78 0.73 0.75 0.78 0.97 1.00 0.89 0.99 0.79 0.76 0.80 0.83 0.59 0.55 0.52 0.60 0.57 0.56 0.67 0.65 0.54 0.70 0.40 0.60 0.66 0.55 0.57 0.51 0.66 0.66 0.61 0.67 0.65 0.66 0.72 0.55 0.62 0.55 0.63 0.45 0.72 0.63 0.65 0.70 0.38 0.60 0.66 0.72 0.69 0.71 0.70 0.70 0.67 0.69 0.70 0.62 0.64 0.73 0.67 0.73 0.57 0.60 0.56 0.61 0.60 0.52 0.41 0.62 0.53 0.63 0.69 0.64 0.64 0.53 0.56 0.55 0.62 0.61 0.58 0.67 0.50 0.63 0.61 0.65 0.66 0.58 0.64 0.59 0.67 0.68 0.67 0.65 0.63 0.64 0.66 0.66 0.58 0.64 0.60 0.60 0.61 0.67 0.69 0.65 0.67 0.60 0.69 0.66 0.64 0.66 0.64 0.60 0.64 0.56 0.63 0.62 0.55 0.63 0.64 0.66 0.57 0.68 0.57 0.73 0.61 0.60 0.54 0.61 0.55 0.65 0.54 0.62 0.60 0.58 0.35 0.62 0.61 0.61 0.57 0.64 0.37 0.50 0.53 0.62 0.59 0.67 0.59 0.67 0.60 0.57 0.57 0.65 0.61 0.52 0.61 0.64 0.50 0.62 0.51 0.60 0.68 0.56 0.71 0.35 0.63 0.52 0.65 0.67 0.57 0.62 0.57 0.62 0.68 0.37 0.44 0.64 0.61 0.59 0.60 0.63 0.64 0.62 0.64 0.63 0.78 0.89 1.00 0.90 0.59 0.60 0.60 0.59 0.78 0.70 0.63 0.80 0.78 0.76 0.85 0.80 0.66 0.89 0.57 0.83 0.88 0.70 0.80 0.66 0.88 0.84 0.75 0.87 0.88 0.80 0.89 0.66 0.86 0.79 0.78 0.55 0.83 0.74 0.73 0.86 0.48 0.73 0.78 0.90 0.90 0.86 0.87 0.89 0.84 0.91 0.87 0.73 0.80 0.87 0.90 0.89 0.80 0.86 0.80 0.86 0.67 0.59 0.46 0.77 0.73 0.79 0.83 0.72 0.75 0.70 0.81 0.79 0.72 0.78 0.76 0.79 0.69 0.82 0.84 0.86 0.86 0.79 0.81 0.82 0.84 0.88 0.88 0.81 0.75 0.82 0.85 0.81 0.66 0.87 0.66 0.78 0.77 0.78 0.82 0.86 0.76 0.71 0.79 0.79 0.77 0.85 0.83 0.86 0.81 0.66 0.82 0.78 0.68 0.77 0.86 0.82 0.75 0.83 0.82 0.90 0.82 0.87 0.69 0.83 0.80 0.84 0.65 0.76 0.80 0.85 0.41 0.78 0.82 0.72 0.71 0.80 0.50 0.70 0.68 0.86 0.80 0.83 0.73 0.79 0.74 0.85 0.72 0.82 0.74 0.74 0.75 0.88 0.60 0.86 0.60 0.82 0.85 0.72 0.87 0.44 0.83 0.60 0.85 0.81 0.80 0.82 0.78 0.73 0.75 0.48 0.53 0.86 0.86 0.76 0.78 0.82 0.81 0.76 0.78 0.81 0.97 0.99 0.90 1.00 0.80 0.78 0.82 0Cosine similarity scores between Flores-200 languages in NLLB-200 at different layers of the encoder-decoder architecture. The similarity is measured w.r.t. the gating decisions (expert choice) per language (source-side in the encoder and target-side in the decoder)Appendix F. Model Card -NLLB-200Model Details a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_54"><head></head><label></label><figDesc>? Citation Information NLLB Team et al, No Language Left Behind: Scaling Human-Centered Machine Translation, Arxiv, 2022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We depict the technical components of No Language Left Behind and how they fit together. We display the interaction between data, how data is utilized in the models we develop (orange), and how models are evaluated. Datasets shown in blue are novel datasets created in No Language Left Behind.</figDesc><table><row><cell>Primary Bitexts</cell><cell>NLLB Seed Public Bitext</cell><cell>LASER3</cell><cell>Mined</cell><cell>NLLB-200</cell><cell>FLORES-200 Toxicity-200</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Bitext</cell><cell>Model</cell><cell></cell></row><row><cell></cell><cell>Monolingual</cell><cell></cell><cell></cell><cell></cell><cell>Human</cell></row><row><cell></cell><cell>Data</cell><cell></cell><cell></cell><cell></cell><cell>Evaluation</cell></row><row><cell></cell><cell></cell><cell cols="2">Language Identification</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>&amp; Cleaning</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mixture of Experts</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Curriculum Learning</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Self-Supervised Training</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Backtranslation</cell><cell></cell></row><row><cell>Model</cell><cell>Data set</cell><cell></cell><cell></cell><cell>Incorporating NLLB-Seed</cell><cell></cell></row></table><note>Figure 2: How the Pieces Fit Together, a Bird's-Eye View:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Translation Models covering 202 languages NLLB</head><label></label><figDesc>Seed training data in 39 languages NLLB-MD: Seed data in different domains in 6 languages to assess generalization Toxicity-200: wordlists to detect toxicity in 200 languages Language Identification for more than 200 languages LASER3: sentence encoders for identifying aligned bitext for 148 languages stopes: a data mining library that can be used to process and clean monolingual data, then create aligned bitext Training data recreation: Scripts that recreate our training data ? -200: A 54.5B Sparsely Gated Mixture-of-Experts model 3.3B and 1.3B Dense Transformer models 1.3B and 600M Dense transformer models distilled from NLLB-200</figDesc><table><row><cell>Human-Translated Datasets</cell></row><row><cell>Flores-200: Evaluation dataset in 204 languages</cell></row><row><cell>NLLB-Seed:</cell></row></table><note>? Tools to Create Large Scale Bitext Datasets</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>We display the language Code, language name, Script, and language Family. The symbol indicates machine translation support by Google and/or Microsoft, whereas indicates support by neither. Res. indicates if we classify the language as high or low-resource. Specification contains, if available, additional information on the language variant collected in Flores-200. The superscript new indicates new languages added to Flores-200 compared to Flores-101.</figDesc><table /><note>204 Languages of No Language Left Behind:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>FLORES at a Glance. (left) FLORES is divided into three evaluation splits, totaling 3001 sentences. (right) Summary of Quality Control based on the statistics of 73 languages that implemented the new Flores-200 workflow.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Central Kanuri (Arabic script)</cell></row><row><cell></cell><cell></cell><cell>Central Aymara</cell><cell>Kabiy?</cell></row><row><cell></cell><cell>Crimean Tatar</cell><cell>Bemba</cell></row><row><cell></cell><cell>Limburgish</cell><cell></cell></row><row><cell>Quality score</cell><cell>Chokwe</cell><cell></cell><cell>90% Quality Standard</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Filter Label Filtered Sentence Histogram</head><label></label><figDesc>urd_Arab ??????M M ? ? ?? ???????? ????????? : dan_Latn ???????? ???? ?? ??? , urdu: ????? ?????? ????? ) er et distrikt i den Script jpn_Jpan 4.0, CUDA ???????? 40W?Quadro FX 380 ?? 450MHz zho_Hant ???? 2009 ? 2 ? 10 ?). Satellite map ???? English tur_Latn A module is said to be semisimple if it is the sum of simple submodules. nld_Latn Line drawing and design: From the book Brazil and the Brazilians, 1857</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>Examples of Sentences Filtered from our LID training dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Noisy Web Data.</head><label></label><figDesc>Despite strong results on Flores-200, we expect a sizable gap in performance when applying our LID model to our target web data. 15. https://github.com/google/cld3 16. https://github.com/saffsd/langid.py 17. https://pypi.org/project/langdetect/ # Supported Flores-200 ? CLD3 ? LangId ? LangDetect Flores-200 ? CLD3 ? LangId Flores-200 ? CLD3</figDesc><table><row><cell></cell><cell>Languages</cell><cell></cell><cell>51 Labels</cell><cell></cell><cell>78 Labels</cell><cell></cell><cell>95 Labels</cell></row><row><cell></cell><cell></cell><cell>F1</cell><cell>FPR</cell><cell>F1</cell><cell>FPR</cell><cell>F1</cell><cell>FPR</cell></row><row><cell>LangDetect</cell><cell>55</cell><cell>97.3</cell><cell>0.0526</cell><cell>64.4</cell><cell>0.4503</cell><cell>53.1</cell><cell>0.4881</cell></row><row><cell>LangId</cell><cell>97</cell><cell>98.6</cell><cell>0.0200</cell><cell>92.0</cell><cell>0.0874</cell><cell>75.8</cell><cell>0.2196</cell></row><row><cell>CLD3</cell><cell>107</cell><cell>98.2</cell><cell>0.0225</cell><cell>97.7</cell><cell>0.0238</cell><cell>97.0</cell><cell>0.0283</cell></row><row><cell>Ours</cell><cell>218</cell><cell>99.4</cell><cell>0.0084</cell><cell>98.8</cell><cell>0.0133</cell><cell>98.5</cell><cell>0.0134</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 : Comparison of Open-Source Language Identification Models with various intersections of labelsF1 Macro F1 Macro Precision Macro Recall Macro FPR Micro FPR</head><label>4</label><figDesc>. F1 is the micro F1 score and FPR is the micro False Positive Rate.</figDesc><table><row><cell cols="2">Micro Low-Resource 95.63</cell><cell>95.9</cell><cell>97.6</cell><cell>95.4</cell><cell>0.01213</cell><cell>0.0235</cell></row><row><cell>All Flores-200</cell><cell>95.85</cell><cell>95.5</cell><cell>94.0</cell><cell>95.7</cell><cell>0.02110</cell><cell>0.0210</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Performance of our LID system on FLORES-200. Arabic languoids and Akan/Twi have been merged after linguistic analysis.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Language Ours F1 CLD3 F1 Ours FPR CLD3 FPR</head><label></label><figDesc></figDesc><table><row><cell>Micro</cell><cell>79.14</cell><cell>64.41</cell><cell>0.79</cell><cell>1.12</cell></row><row><cell>Macro</cell><cell>74.16</cell><cell>60.13</cell><cell>0.77</cell><cell>1.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 :</head><label>6</label><figDesc>Only the average performance of languages supported by both CLD3 and our model is shown. Full table inAppendix Table 50.</figDesc><table><row><cell>True label</cell><cell>bjn_Latn min_Latn ind_Latn sot_Latn tsn_Latn kam_Latn swh_Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pes_Arab prs_Arab kab_Latn taq_Latn arb_Arab knc_Arab</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>yue_Hant zho_Hant bam_Latn dyu_Latn hrv_Latn bos_Latn isl_Latn fao_Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>bjn_Latn</cell><cell>min_Latn</cell><cell>ind_Latn</cell><cell>sot_Latn</cell><cell>tsn_Latn</cell><cell>kam_Latn</cell><cell>swh_Latn</cell><cell>pes_Arab</cell><cell>prs_Arab</cell><cell>kab_Latn</cell><cell>taq_Latn</cell><cell>arb_Arab</cell><cell>knc_Arab</cell><cell>yue_Hant</cell><cell>zho_Hant</cell><cell>bam_Latn</cell><cell>dyu_Latn</cell><cell>hrv_Latn</cell><cell>bos_Latn</cell><cell>isl_Latn</cell><cell>fao_Latn</cell></row><row><cell></cell><cell></cell><cell cols="5">Prediction</cell><cell></cell><cell></cell><cell cols="4">Prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Comparison of CLD3 and Our Model on a Challenge Set built from Human Annotations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Examples of Identical Sentences in Akan and Twi, two similar languages.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Low LID threshold Internet Plus ? 58,50 eng_Latn at 0.19 LID score LID mismatch Best v?to ever! doc. LID French, sent. LID Czech Numbers Vol.180 Sep. (2011) exceeded numbers ratio Punctuation . * sApEvAte cHe... ? (Previous page) exceeded punctuation ratio</figDesc><table><row><cell>Emoji</cell><cell>#gymgirl</cell><cell>exceeded emoji ratio</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 8 :</head><label>8</label><figDesc>Examples of Filtered Sentences in our monolingual pipeline.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table /><note>Statistics and Mining Performance for European minority languages. BLEU scores are foreign into English.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 11 :</head><label>11</label><figDesc>Statistics and MiningPerformance for Creole languages.</figDesc><table><row><cell>Lang.</cell><cell>kab</cell><cell>taq</cell><cell>taq</cell><cell>tzm</cell></row><row><cell>Script</cell><cell cols="4">Latn Latn Tfng Tfng</cell></row><row><cell>bitexts</cell><cell cols="2">72k 10.2k</cell><cell cols="2">4k 6.2k</cell></row><row><cell>BLEU</cell><cell>1.2</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>xsim [%]</cell><cell cols="3">0.99 24.11 35.57</cell><cell>3.66</cell></row><row><cell cols="2">Monolingual 3.4M</cell><cell>23k</cell><cell>5k</cell><cell>59k</cell></row><row><cell>Mined</cell><cell cols="2">3.1M 240k</cell><cell cols="2">-111k</cell></row><row><cell>BLEU</cell><cell>6.2</cell><cell>1.2</cell><cell>-</cell><cell>3.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 12 :</head><label>12</label><figDesc></figDesc><table /><note>Statistics and Mining Performance for Berber languages.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 13 :</head><label>13</label><figDesc></figDesc><table /><note>Statistics and Mining Performance for Malayo-Polynesian languages.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 14 :</head><label>14</label><figDesc></figDesc><table /><note>xsim Error Rates on FLORES devtest for Amharic and Tigrinya and different training strategies. The specified amount of training data excludes 4M sentences of English for our models.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Modeling Contributions of No Language Left Behind:</head><label></label><figDesc>As highlighted, we describe several modeling techniques to enable coverage of hundreds of languages in one model. We focus on effectively scaling model capacity while mitigating overfitting, as well as how to improve backtranslation for low-resource languages and incorporate NLLB-Seed.</figDesc><table><row><cell>Primary Bitexts</cell><cell>LASER3</cell><cell>Mined</cell><cell>NLLB-200</cell><cell>FLORES-200 Toxicity-200</cell></row><row><cell></cell><cell></cell><cell>Bitext</cell><cell>Model</cell><cell></cell></row><row><cell></cell><cell>Monolingual</cell><cell></cell><cell></cell><cell>Human</cell></row><row><cell></cell><cell>Data</cell><cell></cell><cell></cell><cell>Evaluation</cell></row><row><cell></cell><cell cols="2">Language Identification</cell><cell></cell><cell></cell></row><row><cell></cell><cell>&amp; Cleaning</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Mixture of Experts</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Curriculum Learning</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Self-Supervised Training</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Backtranslation</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Incorporating NLLB-Seed</cell><cell></cell></row><row><cell>Model</cell><cell>Data set</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Figure 14:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 51</head><label>51</label><figDesc>There are a total of 864M examples in this benchmark. The highest resource language pair has 186M examples and the lowest resource language pair has 40K examples, thus representing the extreme skew characteristic of the final dataset with 202 languages.</figDesc><table /><note>). Out of 70 low-resource directions, 22 are very low-resource, i.e., have less than 100K training examples. The dataset is composed of publicly available bitext in all 110 language directions (see Section 8.1.2) and large scale mined data (see Section 5.3) in English-centric directions.26</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head>Table 15 :</head><label>15</label><figDesc></figDesc><table /><note>Vanilla Sparsely Gated MoE with and without overall dropout (validation set chrF ++ ). We report averages in each set of directions: eng_Latn-xx, xx-eng_Latn and xx-yy as all. For eng_Latn-xx and xx-eng_Latn we breakdown the pairs by resource level: high-resource (high), low-resource (low) and very low resource (v.low) -We see that a vanilla MoE model does not outperform the corresponding 1.3B dense model on the ablation benchmark. On adding overall dropout, we see a significant improvement in all directions on MoE models. At smaller computational cost per update (615M), MoE with overall dropout shows larger gains.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head></head><label></label><figDesc>MoE =2). Each MoE sublayer has 64 experts (close to the number of languages in the benchmark, i.e., 53) and routes input tokens to the top-2 expert FFN sublayers in the MoE layer as in</figDesc><table /><note>Experimental Setup. We train a baseline dense Transformer encoder-decoder model with 1.3B parameters with model dimension 1024, FFN dimension 8192, 16 attention heads, 24 encoder layers and 24 decoder layers. Next, we train a corresponding Sparsely Gated MoE model by replacing the dense FFN sublayer with an MoE sublayer in every alternate Transformer layer of the model (f</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head>Table 15</head><label>15</label><figDesc></figDesc><table /><note>, we see that the Sparsely Gated MoE model with 64 experts (MoE-64), while computationally similar, shows good improvements when compared to the dense 615M model. We see 1+ chrF ++ score improvements on all subsets except for very low resource pairs (v.low) and non-English pairs (xx-yy). There are worse trends when scaling up the computational cost per update -for the MoE-64 model (computationally similar to the dense 1.3B model), we see neutral or worse performance compared to the dense 1.3B model.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_41"><head>Table 16 :</head><label>16</label><figDesc></figDesc><table /><note>Comparison of Various Regularization Strategies. We find that EOM and CMR strategies perform similarly, and both outperform the baseline MoE with overall dropout. ?: best of sweep.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_43"><head>Table 17 :</head><label>17</label><figDesc>Curriculum Learning Results demonstrate that for vanilla MoE, training on a curriculum reduces overfitting, particularly for eng_Latn-xx low and very low resource pairs. For MoE EOM, a curriculum does not help.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_45"><head>?</head><label></label><figDesc>Multitask training on SSL and MMT (DAE+MMT). We combine the training examples for SSL and MMT and train the model for 200K updates on a combination of both tasks. ? Multitask training on SSL and MMT, followed by finetuning on MMT (DAE+MMT ? MMT). We first perform multitask training on SSL and MMT for 150K updates, followed by finetuning on MMT only for 50K updates</figDesc><table><row><cell></cell><cell></cell><cell>eng_Latn-xx</cell><cell></cell><cell>xx-eng_Latn</cell><cell>xx-yy</cell></row><row><cell></cell><cell>all</cell><cell>high low v.low</cell><cell>all</cell><cell>high low v.low</cell><cell>all</cell></row><row><cell>MMT</cell><cell cols="4">43.3 55.4 38.4 31.6 53.5 63.6 49.4 46.5</cell><cell>41.3</cell></row><row><cell>DAE?MMT</cell><cell cols="4">42.6 55.0 37.6 30.8 52.3 62.2 48.3 45.4</cell><cell>40.4</cell></row><row><cell>DAE+MMT</cell><cell cols="4">43.5 55.2 38.8 32.7 54.4 63.6 50.7 48.4</cell><cell>42.4</cell></row><row><cell cols="5">DAE+MMT?MMT 43.4 55.4 38.5 32.2 54.3 63.6 50.5 48.0</cell><cell>42.2</cell></row><row><cell cols="6">Table 18: Effect of SSL Curriculum. We find DAE+MMT training jointly to be the</cell></row><row><cell>most effective strategy.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Results. First, we see that pretraining on SSL, followed by finetuning on MMT (DAE</cell></row><row><cell cols="6">? MMT) hurts performance (-0.7 chrF ++ on eng_Latn-xx, -1.2 chrF ++ on xx-eng_Latn)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_47"><head>Table 19 : Effect of Different SSL Objectives. We</head><label>19</label><figDesc>find training MMT+DAE the most effective compared to adding the LM task.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_48"><head>?We train all Source Human Aligned? Noisy? Limited Size? Model-Dependent? Models Used</head><label></label><figDesc>SmtBT: backtranslations obtained via a series of bilingual MOSES models(Koehn  et al., 2007)  trained on Primary and Mined data. The optimal model hyperparameters were chosen via Flores-200 validation data. First, we study the effect of using MmtBT in addition to Primary and Mined data. We use a dense 1.3B model and train on two data setups, namely Primary and Primary+Mined. We then use the model trained on the Primary+Mined dataset and generate backtranslation data for all the English-centric pairs in the dataset. This backtranslated data becomes our MmtBT. In the next experiment, we use the exact same model setup and train on a dataset comprising Primary+Mined+MmtBT. Our objective here is to observe the benefits of backtranslation over Primary+Mined.</figDesc><table><row><cell>PublicBitext Mined MmtBT SmtBT Experimental Setup. NLLB-Seed Ideal Data</cell><cell>--Sentence Encoders Multilingual Bilingual MOSES -</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_49"><head>Table 20 :44.2 55.5 39.6 34.0 55.9 64.9 52.2 50.9 41.1</head><label>20</label><figDesc>Dataset Characteristics of the sources we compare in this section. Of these datasets, NLLB-Seed is by far the smallest. For low-resource languages, PublicBitext is often extremely limited. Mined, MmtBT, and SmtBT are limited only by the amount of available monolingual data and the quality of the models used to produce them.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">eng_Latn-xx</cell><cell></cell><cell></cell><cell cols="2">xx-eng_Latn</cell><cell>xx-yy</cell></row><row><cell></cell><cell>all</cell><cell>high</cell><cell cols="2">low v.low</cell><cell>all</cell><cell>high</cell><cell>low v.low</cell><cell>all</cell></row><row><cell>Primary</cell><cell cols="3">41.0 52.8 36.3</cell><cell>28.1</cell><cell cols="3">47.4 60.5 42.1</cell><cell>36.7</cell><cell>39.2</cell></row><row><cell>+Mined</cell><cell cols="7">43.8 55.2 39.2 34.0 53.9 64.4 49.6</cell><cell>46.1</cell><cell>40.9</cell></row><row><cell>+MmtBT</cell><cell cols="7">44.0 55.1 39.5 34.0 55.7 64.8 52.0</cell><cell>50.8</cell><cell>40.6</cell></row><row><cell>+SmtBT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_50"><head>Table 21 : Comparison of aggregate Model Performance trained on Different Data Combinations, evaluated</head><label>21</label><figDesc>on Flores-200 dev for ablation dataset directions. We observe that adding SmtBT data improves over the +Mined+MmtBT and overall gives the best performance across all language directions and resource level types.the models for 200,000 updates and compare the best checkpoints on the Flores-200 development set using chrF ++ .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_52"><head>Table 22 : Average Performance of Teacher Backtranslation Models, evaluated</head><label>22</label><figDesc>on Flores-200 dev for the subset of backtranslated directions where both methods were used.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_53"><head>44.2 55.5 39.6 34.0 55.9 64.9 52.2 50.9 41.1</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">eng_Latn-xx</cell><cell></cell><cell></cell><cell cols="2">xx-eng_Latn</cell><cell>xx-yy</cell></row><row><cell></cell><cell>all</cell><cell>high</cell><cell cols="2">low v.low</cell><cell>all</cell><cell>high</cell><cell>low v.low</cell><cell>all</cell></row><row><cell>No Tags</cell><cell cols="3">42.8 54.5 38.0</cell><cell>31.9</cell><cell cols="3">54.8 64.2 50.9</cell><cell>48.4</cell><cell>40.8</cell></row><row><cell>Single Tag</cell><cell cols="7">44.0 55.2 39.4 34.2 55.5 64.6 51.8</cell><cell>50.5</cell><cell>40.7</cell></row><row><cell>Finegrained Tags</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_54"><head>Table 23 :</head><label>23</label><figDesc>Comparing Different Tagging Schemes on Flores-200 ablation dev set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_56"><head>Table 24 : Training Set Size and FLORES-200 Validation Performance of low-resource bilingual models. For</head><label>24</label><figDesc>each direction we report figures for the model trained on PublicBitext, on NLLB-Seed, and on their combination.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_57"><head>Table 25 : Training Set Size and FLORES-200 Validation Performance of base low-resource bilingual models and their backtranslation-augmented versions. We</head><label>25</label><figDesc>report figures for the model trained on publicly available parallel data and on the diminished public data concatenated with NLLB-Seed. Each experiment is repeated three times and we report averages, with standard deviation in brackets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_62"><head></head><label></label><figDesc>Dropout<ref type="bibr" target="#b147">(Srivastava et al., 2014)</ref> is a strong regularization technique to reduce overfitting in deep neural networks. In Section 6.2.2, we expand and improve upon overall dropout and design specific regularization techniques for Mixture of Expert (MoE) layers. Based on the results and analyses of the variants we tried, we choose the regularization strategy of EOM, which provides the best trade-off between performance/accuracy and efficiency. EOM requires 23% fewer FLOPs compared to CMR. Although CMR is slightly better in terms of performance, we optimize for training efficiency as opposed to marginal performance gains.</figDesc><table><row><cell></cell><cell cols="2">eng_Latn-xx</cell><cell></cell><cell></cell><cell cols="2">xx-eng_Latn</cell><cell></cell><cell>xx-yy Avg.</cell></row><row><cell>all</cell><cell>high</cell><cell cols="2">low v.low</cell><cell>all</cell><cell>high</cell><cell cols="2">low v.low</cell><cell>all</cell><cell>all</cell></row><row><cell cols="3">f MoE =2 44.6 54.3 41.2</cell><cell>38.8</cell><cell cols="3">56.0 63.8 53.2</cell><cell>52.1</cell><cell>41.4</cell><cell>47.3</cell></row><row><cell cols="8">f MoE =4 44.8 54.2 41.4 39.1 56.3 63.8 53.5 52.8</cell><cell>42.0 47.7</cell></row><row><cell cols="3">f MoE =6 44.6 54.0 41.2</cell><cell>39.0</cell><cell cols="3">56.1 63.7 53.4</cell><cell>52.7</cell><cell>41.7</cell><cell>47.5</cell></row></table><note>Strategy.Experimental Setup. We compare MoE-128 models in three setups varying the frequency of placement of MoE layers: f MoE ? {2, 4, 6}. We use the EOM dropout strategy for all</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_63"><head>Table 28 :</head><label>28</label><figDesc></figDesc><table /><note>Effect of Frequency of Insertion of MoE Layers. We report chrF ++ scores on Flores-200 dev set on different types of language pairs. For eng_Latn-xx and xx-eng_Latn we include all 201 pairs each. For xx-yy we randomly choose 200 directions. We observe that placing MoE layers with a frequency 4 provides the best performance across all types of pairs and overall average.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_65"><head>Table 29 :</head><label>29</label><figDesc></figDesc><table /><note>Improvements from different Curriculum Learning (CL) Strategies. We report chrF ++ scores on Flores-200 dev set on different types of language pairs. For eng_Latn-xx and xx-eng_Latn we include all 201 pairs each. For xx-yy we randomly choose 200 directions. We observe that on average, the variant (3) 4-phase CL, performs best.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_66"><head></head><label></label><figDesc>and M2M-100<ref type="bibr" target="#b151">(Fan et al., 2020)</ref>, which report scores on a subset of 87 languages of Flores-101. On this smaller subset, NLLB-200 again outperforms by +6.9 spBLEU on average. Overall, the results show that NLLB-200 improves upon</figDesc><table><row><cell></cell><cell cols="2">eng_Latn-xx xx-eng_Latn</cell><cell>xx-yy</cell><cell>Avg.</cell></row><row><cell>87 languages</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>M2M-100</cell><cell>-/-</cell><cell>-/-</cell><cell>-/-</cell><cell>13.6/-</cell></row><row><cell>Deepnet</cell><cell>-/-</cell><cell>-/-</cell><cell>-/-</cell><cell>18.6/-</cell></row><row><cell>NLLB-200</cell><cell>35.4/52.1</cell><cell>42.4/62.1</cell><cell cols="2">25.2/43.2 25.5/43.5</cell></row><row><cell>101 languages</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeltaLM</cell><cell>26.6/-</cell><cell>33.2/-</cell><cell>16.4/-</cell><cell>16.7/-</cell></row><row><cell>NLLB-200</cell><cell>34.0/50.6</cell><cell>41.2/60.9</cell><cell cols="2">23.7/41.4 24.0/41.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_67"><head>Table 30 :</head><label>30</label><figDesc>Comparison on FLORES-101 devtest. We evaluate over full Flores-101 10k directions. We report both spBLEU/chrF ++ where available. All spBLEU numbers are computed with Flores-101 SPM tokenizer. Scores for DeltaLM are taken from Flores-101 leaderboard. M2M-100 and Deepnet average is only over 87 languages that overlap with Flores-101, we also show NLLB-200 performance on that subset of languages. NLLB-200 outperforms previous state of the art models by a significant margin, even after supporting twice as many languages.</figDesc><table><row><cell></cell><cell></cell><cell>eng_Latn-xx</cell><cell></cell><cell></cell><cell>xx-eng_Latn</cell><cell></cell></row><row><cell></cell><cell cols="6">MMTAfrica M2M-100* NLLB-200 MMTAfrica M2M-100* NLLB-200</cell></row><row><cell>hau_Latn</cell><cell>-/-</cell><cell>4.0/-</cell><cell>33.6/53.5</cell><cell>-/-</cell><cell>16.3/-</cell><cell>38.5/57.3</cell></row><row><cell>ibo_Latn</cell><cell>21.4/-</cell><cell>19.9/-</cell><cell>25.8/41.4</cell><cell>15.4/-</cell><cell>12.0/-</cell><cell>35.5/54.4</cell></row><row><cell>lug_Latn</cell><cell>-/-</cell><cell>7.6/-</cell><cell>16.8/39.8</cell><cell>-/-</cell><cell>7.7/-</cell><cell>27.4/46.7</cell></row><row><cell>luo_Latn</cell><cell>-/-</cell><cell>13.7/-</cell><cell>18.0/38.5</cell><cell>-/-</cell><cell>11.8/-</cell><cell>24.5/43.7</cell></row><row><cell>swh_Latn</cell><cell>40.1/-</cell><cell>27.1/-</cell><cell>37.9/58.6</cell><cell>28.4/-</cell><cell>25.8/-</cell><cell>48.1/66.1</cell></row><row><cell>wol_Latn</cell><cell>-/-</cell><cell>8.2/-</cell><cell>11.5/29.7</cell><cell>-/-</cell><cell>7.5/-</cell><cell>22.4/41.2</cell></row><row><cell>xho_Latn</cell><cell>27.1/-</cell><cell>-/-</cell><cell>29.5/48.6</cell><cell>21.7/-</cell><cell>-/-</cell><cell>41.9/59.9</cell></row><row><cell>yor_Latn</cell><cell>12.0/-</cell><cell>13.4/-</cell><cell>13.8/25.5</cell><cell>9.0/-</cell><cell>9.3/-</cell><cell>26.6/46.3</cell></row><row><cell>zul_Latn</cell><cell>-/-</cell><cell>19.2/-</cell><cell>36.3/53.3</cell><cell>-/-</cell><cell>19.2/-</cell><cell>43.4/61.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_68"><head>Table 31 : Comparison on FLORES-101 devtest on African Languages.</head><label>31</label><figDesc>We compare to two recent works MMTAfrica and M2M-100* finetuned on MAFAND-MT dataset. We report spBLEU/chrF ++ and bold the best score. NLLB-200 outperforms previous state-of-the-art by significant margins on most translation directions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_69"><head>Table 32 : Comparison on FLORES-101 devtest on Indian Languages.</head><label>32</label><figDesc>We report BLEU (with default 13a Moses tokenizer)/BLEU (with IndicNLP tokenizer)/chrF ++ where available, and bold the best score.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_71"><head>Table 33 :</head><label>33</label><figDesc>Performance We report both chrF ++ and spBLEU scores. All spBLEU scores are computed with our newly trained SPM tokenizer in Section 8.1.1.200.We categorize our results first into high-resource, low-resource, and very low-resource directions, and then we further subdivide each of these into out of English (eng_Latn-xx), into English (xx-eng_Latn), and non-English-centric directions (xx-yy). We evaluate NLLB-200 on 201 eng_Latn-xx directions, 201 xx-eng_Latn directions, and 40,200 xx-yy directions -for a total of 40602 directions. There are 2,862 high-resource pairs and 37,740 low-resource 39 pairs, out of which 26,796 are also very low-resource. 40 Out of these, our training set contains parallel data for 2,440 directions, meaning 38,162 directions were never seen by the model during training (zero-shot). We present the results in</figDesc><table><row><cell>Evaluation Setup.</cell></row></table><note>of NLLB-200 on FLORES-200 devtest set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_72"><head></head><label></label><figDesc>The non-English side of Flores-200 are usually human translated, and might therefore not have as good fluency as naturally occurring text due to the effect of translationese (Zhang and Toral, 2019). FromTable 33, we observe that high-resource eng_Latn-xx directions on average are +13.0 chrF ++ better than low-resource ones. For xx-eng_Latn, the gap is smaller, with high-resource only +9.0 chrF ++ better on average. This also demonstrates that into English directions perform better for low resource languages. One benefit of a model having better into English performance is that it can be used to improve out of English performance using backtranslation (Section 8.1.3).We further break down the English-centric performance into Flores-101 languages and new languages added to Flores-200. On Flores-101 directions, NLLB-200 achieves 50.7 39. A direction is defined as low-resource if any one language in the pair is low-resource, otherwise it is considered high-resource. 40. A language is defined as very low-resource if it has fewer than 100k samples across all pairings with any other language in our dataset.</figDesc><table><row><cell></cell><cell cols="2">eng_Latn-xx</cell><cell cols="2">xx-eng_Latn</cell><cell cols="2">Average</cell></row><row><cell></cell><cell>low</cell><cell>v.low</cell><cell>low</cell><cell>v.low</cell><cell>low</cell><cell>v.low</cell></row><row><cell cols="4">Google Translate 32.3/50.3 27.0/46.5 35.9/57.1</cell><cell>35.8/57.0</cell><cell>34.1/53.7</cell><cell>31.3/51.7</cell></row><row><cell>NLLB-200</cell><cell>30.3/48.2</cell><cell cols="5">25.7/45.0 41.3/60.4 41.1/60.3 35.8/54.3 33.4/52.6</cell></row></table><note>(3)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_73"><head>Table 34 : Comparison on 102 Low-Resource Directions on FLORES-200 devtest against commercial translation systems.</head><label>34</label><figDesc>We evaluate on all English-centric low-resource directions that overlap between Flores-200 and Google's Translation API as of this writing. We report both spBLEU/chrF ++ and bold the best score. We observe that NLLB-200 outperforms significantly on xx-eng_Latn and overall average. chrF ++ on eng_Latn-xx and 60.9 chrF ++ on xx-eng_Latn directions. On the subset of the new languages (Flores-200 -Flores-101), NLLB-200 achieves 39.9 chrF ++ on eng_Latnxx and 52.6 chrF ++ on xx-eng_Latn directions. The new languages in Flores-200 are all low-resource, and on average have significantly worse performance than the languages in Flores-101 -overall, translation systems still have lot of room for improvement. We next focus on the non-Englishcentric performance of NLLB-200. The Flores-200 evaluation dataset has complete many-to-many support, meaning the majority of directions it covers are non-English-centric. Compared to models evaluated on Flores-101, Flores-200 supports evaluation on ?30k more non-English-centric directions. We evaluate on 40,200 xx-yy directions, where NLLB-200 achieves on average 35.6 chrF ++ . This is significantly lower than performance compared to xx-eng_Latn(-21.2 chrF ++ ) or eng_Latn-xx(-9.7 chrF ++ ) because most directions are unsupervised and the majority of the pairs have languages which are not lexically or syntactically similar, making translation more challenging.As previously mentioned, 38,162 directions in total in Flores-200 are never seen by NLLB-200. On these zero-shot pairs, the model achieves 35.4 chrF ++ .</figDesc><table /><note>Non-English-Centric and Zero-Shot Performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_74"><head>Table 34</head><label>34</label><figDesc>demonstrates that NLLB-200 significantly outperforms on xx-eng_Latn directions with +3.3 chrF ++ for both low-resource and very low-resource directions. On eng_Latn-xx directions, NLLB-200 lags behind by -2.1 chrF ++ on low-resource and -1.5 chrF ++ on very low-resource directions. Overall, NLLB-200 outperforms by +0.6 chrF ++ on low and +0.9 chrF ++ on very low-resource pairs despite covering 202 languages. We provide performance comparison on high-resource pairs and additional analysis in Appendix E.2.2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_75"><head></head><label></label><figDesc>TICO: sampled from a variety of public sources containing COVID-19 related content<ref type="bibr" target="#b14">(Anastasopoulos et al., 2020)</ref>, this dataset comes from different domains(medical, news,  conversational, etc.)  and covers 36 languages. We pair 28 languages with English for a total of 56 directions.Mafand-MT:an African news corpus that covers 16 languages(Adelani et al., 2022). 8 languages are paired with English (hau_Latn, ibo_Latn, lug_Latn, luo_Latn, swh_Latn, tsn_Latn, yor_Latn, zul_Latn ) and 5 other languages are paired with French (bam_Latn, ewe_Latn, fon_Latn, mos_Latn and wol_Latn ) for a total of 26 directions. : an evaluation set for machine translation of South African languages (McKellar, 2017), it consists of 500 sentences from South African governmental data, translated separately by four different professional human translators for each of the 11 official South African languages. 9 of these languages are covered by NLLB-200: afr_Latn, eng_Latn, nso_Latn, sot_Latn, ssw_Latn, tsn_Latn, tso_Latn, xho_Latn and zul_Latn. There is no standard valid/test split, so we use the first half (250 sentences yielding 1000 pairs) for validation and the second half for testing following Fan et al. (2020). MADAR: created by translating select sentences from the Basic Traveling Expression Corpus (BTEC) (Bouamor et al., 2018). This corpus covers dialects from 25 Arabicspeaking cities, in addition to English, French and Modern Standard Arabic (MDA). We map 16 out of these dialects to the 8 Arabic dialects in NLLB-200 (aeb_Arab, acm_Arab, acq_Arab, ajp_Arab, apc_Arab, ars_Arab, ary_Arab and arz_Arab ) and pair each with Modern Standard Arabic (arb_Arab). We use the MADAR shared task test split (corpus_6_test_corpus_26_test) for evaluation (Bouamor et al., 2019).</figDesc><table><row><cell>xx-eng Published NLLB-200 Published NLLB-200 khm (b) 5.9/-0.4/27.4 (b) 10.7/-16.8/36.5 npi (c) 7.4/-10.4/39.0 (c) 14.5/-29.3/54.8 pbt (b) 9.3/-10.5/34.3 (b) 15.7/-22.0/46.8 sin (c) 3.3/-11.6/40.9 (c) 13.7/-23.7/49.8 Autshumatoeng-xx (a) Flores(v1)</cell><cell>eng-xx Published NLLB-200 Published NLLB-200 xx-eng hin (l) 22.1/-27.2/51.5 (l) 32.9/-37.4/61.9 khm (l) 43.9/-45.8/42.3 (l) 27.5/-39.1/61.1 mya (c) 39.2/-23.5/31.5 (c) 34.9/-32.7/57.9 (b) WAT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_76"><head>Table 35 : Comparison to State-of-the-Art on FLORES (v1) (devtest) and WAT's Test Sets.</head><label>35</label><figDesc>We report BLEU/chrF ++ scores where available and bold the best score. Low-resource languages are underlined. In each direction, we display the best performing model from published work: (b), (c), and (l) Nakazawa et al.(2019)    </figDesc><table><row><cell cols="2">eng-xx</cell><cell cols="2">xx-eng</cell><cell cols="2">eng-xx</cell><cell>xx-eng</cell></row><row><cell cols="4">Published NLLB-200 Published NLLB-200</cell><cell cols="3">Published NLLB-200 Published NLLB-200</cell></row><row><cell>ces (b) 26.5/-</cell><cell cols="2">25.2/50.6 (d) 35.3/-</cell><cell>33.6/56.8</cell><cell>arb (b) 22.0/-</cell><cell cols="2">25/47.2 (b) 44.5/-</cell><cell>44.7/63.7</cell></row><row><cell>deu (a) 44.9/-</cell><cell cols="2">33.0/59.2 (a) 42.6/-</cell><cell>37.7/60.5</cell><cell>deu (k) 25.5/-</cell><cell cols="2">31.6/57.8 (k) 28.0/-</cell><cell>36.5/57.5</cell></row><row><cell>est (a) 26.5/-</cell><cell cols="2">27.0/55.7 (a) 38.6/-</cell><cell>34.7/59.1</cell><cell>fra (g) 40.0/-</cell><cell cols="2">43.0/65.6 (g) 39.4/-</cell><cell>45.8/64.8</cell></row><row><cell>fin (a) 32.1/-</cell><cell cols="2">27.7/57.7 (a) 40.5/-</cell><cell>28.8/53.7</cell><cell>ita (b) 38.1/-</cell><cell cols="2">42.5/64.4 (b) 43.3/-</cell><cell>48.2/66.5</cell></row><row><cell>fra (a) 46.7/-</cell><cell cols="2">44.2/65.7 (a) 43.9/-</cell><cell>41.9/63.9</cell><cell>jpn (c) 19.4/-</cell><cell>19.5/21.5 (c) 19.1/-</cell><cell>22.6/46.1</cell></row><row><cell>guj (d) 17.8/-</cell><cell>17.6/46.6 (f) 25.1/-</cell><cell></cell><cell>31.2/56.5</cell><cell>kor (c) 22.6/-</cell><cell>22.5/27.9 (c) 24.6/-</cell><cell>25.4/48.0</cell></row><row><cell>hin (f) 25.5/-</cell><cell>26.0/51.5 (f) 29.7/-</cell><cell></cell><cell>37.4/61.9</cell><cell>nld (c) 34.8/-</cell><cell cols="2">34.9/60.2 (c) 43.3/-</cell><cell>41.0/60.9</cell></row><row><cell>kaz (i) 15.5/-</cell><cell cols="2">34.8/61.5 (i) 30.5/-</cell><cell>30.2/56.0</cell><cell>pes (j) 06.5/-</cell><cell>15.5/39.2 (j) 18.4/-</cell><cell>42.3/61.3</cell></row><row><cell>lit (a) 17.0/-</cell><cell cols="2">37.0/63.9 (a) 36.8/-</cell><cell>29.7/56.4</cell><cell>pol (j) 16.1/-</cell><cell>21.1/48.3 (j) 18.3/-</cell><cell>27.1/48.2</cell></row><row><cell>lvs (a) 25.0/-</cell><cell cols="2">21.3/50.8 (a) 28.6/-</cell><cell>24.8/50.8</cell><cell>ron (k) 25.2/-</cell><cell cols="2">29.4/55.5 (k) 31.8/-</cell><cell>42.0/62.0</cell></row><row><cell>ron (a) 41.2/-</cell><cell cols="2">41.5/58.0 (h) 43.8/-</cell><cell>43.4/64.7</cell><cell>rus (j) 11.2/-</cell><cell>24.0/47.0 (j) 19.3/-</cell><cell>30.1/51.3</cell></row><row><cell>rus (a) 31.7/-</cell><cell cols="2">44.8/65.1 (a) 39.8/-</cell><cell>39.9/61.9</cell><cell>vie (c) 35.4/-</cell><cell>34.8/53.7 (c) 36.1/-</cell><cell>36.6/57.1</cell></row><row><cell>spa (e) 33.5/-tur (a) 32.7/-</cell><cell cols="2">37.2/59.3 (e) 34.5/-23.3/54.2 (a) 35.0/-</cell><cell>37.6/59.9 34.3/58.3</cell><cell>(b) IWSLT</cell><cell></cell></row><row><cell>zho (b) 35.1/-</cell><cell cols="2">33.9/22.7 (a) 28.9/-</cell><cell>28.5/53.9</cell><cell></cell><cell></cell></row><row><cell>(a) WMT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_77"><head>Table 36 : Comparison to State-of-the-Art on WMT &amp; IWSLT Test Sets. We</head><label>36</label><figDesc>report BLEU/chrF ++ scores where available and bold the best score. Low-resource languages are underlined. In each direction, we display the best performing model from published work:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_78"><head></head><label></label><figDesc>on all 8 directions but 1 (khm_Khmr-eng_Latn) WAT.Next, in  Table 35b, we evaluate NLLB-200 on 6 directions from WAT. The mya_Mymr and khm_Khmr test sets are part of the ALT corpus (news articles) and the hin_Deva test set comes from the news domain as well (IITB: Newswire). NLLB-200 outperforms the state of the art when translating into English with an average of 3.5 BLEU points. Translating from English is better by 3.5 BLEU points on average, excluding the drop on eng_Latn-mya_Latn which puts the average at -3 BLEU.WMT. Similar to WAT in domain, WMT's test sets cover news articles. On this particular benchmark (seeTable 36a),Kim et al. (2021)  achieved accuracies that outperform the previous state of the art by a large margin. Compared toKim et al. (2021), NLLB-200 improves on eng_Latn-xx directions by approximately 1.9 BLEU point. However, when translating into English, NLLB-200 scores worse on average by 0.6 BLEU points.IWSLT.On the 12 high-resource languages paired with English in IWSLT(Table 36b), NLLB-200 outperforms state of the art on 21 out of 24 directions. Translating from English improves on average by 3.5 BLEU points and translating into English by 6.3 BLEU points.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_79"><head></head><label></label><figDesc>the performance of NLLB-200 on the MAFAND-MT test set to that of the best M2M-100 model finetuned on the news domain with MAFAND data, as reported inAdelani et al. (2022). NLLB-200 outperforms the previous state-ofthe-art on 14 out of the 26 tested directions, proving that the model can generalize well to other domains. Translating into English sees an average improvement of +4.0 BLEU points. However, translating into French (high-resource) is worse on average by -0.9 BLEU, with a significant drop in the performance of the bam_Latn-fra_Latn direction. Translating from English is worse on average by -0.6 BLEU and translating from French is worse on average by -4.4 BLEU. Additionally, we evaluate NLLB-200 on datasets with non-English-centric pairs; MADAR for Arabic dialects and Autshumato for African languages, in</figDesc><table /><note>Non-English-Centric Evaluation on Autshumato and MADAR.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_81"><head>Table 37 :</head><label>37</label><figDesc>Comparison on TICO. We report BLEU/chrF ++ scores, where available, from<ref type="bibr" target="#b14">Anastasopoulos et al. (2020)</ref> and bold the best score. Low-resource languages are underlined.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_82"><head></head><label></label><figDesc>Adelani et al. (2022) NLLB-200 Adelani et al. (2022) NLLB-200</figDesc><table><row><cell></cell><cell>eng-xx</cell><cell></cell><cell>xx-eng</cell><cell></cell></row><row><cell>hau_Latn</cell><cell>15.9/42.1</cell><cell>8.2/34.8</cell><cell>18.2/40.2</cell><cell>13.5/37.9</cell></row><row><cell>ibo_Latn</cell><cell>26.0/51.3</cell><cell>23.9/50.4</cell><cell>21.9/48.0</cell><cell>21.9/46.1</cell></row><row><cell>lug_Latn</cell><cell cols="2">15.7/46.9 25.8/55.2</cell><cell cols="2">22.4/48.5 30.9/54.4</cell></row><row><cell>luo_Latn</cell><cell cols="2">12.0/39.4 14.0/40.4</cell><cell cols="2">14.3/38.3 15.9/38.4</cell></row><row><cell>swh_Latn</cell><cell>27.7/57.2</cell><cell>30.7/56.0</cell><cell cols="2">30.6/55.8 39.3/60.8</cell></row><row><cell>tsn_Latn</cell><cell>31.9/59.5</cell><cell>28.5/55.6</cell><cell cols="2">27.8/54.0 37.3/60.2</cell></row><row><cell>yor_Latn</cell><cell>13.9/37.4</cell><cell>14.4/36.3</cell><cell cols="2">18.0/41.0 24.4/46.7</cell></row><row><cell>zul_Latn</cell><cell>22.9/56.3</cell><cell>16.1/47.3</cell><cell>38.1/57.7</cell><cell>40.3/59.7</cell></row><row><cell></cell><cell>fra-xx</cell><cell></cell><cell>xx-fra</cell><cell></cell></row><row><cell></cell><cell cols="4">Adelani et al. (2022) NLLB-200 Adelani et al. (2022) NLLB-200</cell></row><row><cell>bam_Latn</cell><cell>24.7/49.9</cell><cell>7.7/29.9</cell><cell>25.8/49.0</cell><cell>14.6/37.5</cell></row><row><cell>ewe_Latn</cell><cell>8.9/37.5</cell><cell>8.3/36.4</cell><cell cols="2">11.6/37.2 19.4/42.6</cell></row><row><cell>fon_Latn</cell><cell>7.4/28.5</cell><cell>3.4/21.8</cell><cell>9.9/28.9</cell><cell>8.9/28.7</cell></row><row><cell>mos_Latn</cell><cell>2.2/16.8</cell><cell>5.4/27.6</cell><cell>4.1/18.8</cell><cell>6.1/23.5</cell></row><row><cell>wol_Latn</cell><cell>12.7/35.8</cell><cell>9.1/29.9</cell><cell>11.5/35.3</cell><cell>9.5/30.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_83"><head>Table 38 :</head><label>38</label><figDesc>Comparison</figDesc><table /><note>to State-of-the-Art on MAFAND-MT's Test Set. We report BLEU/chrF ++ for NLLB-200 and the best BLEU and chrF ++ from Adelani et al. (2022). The best scores in each direction are bolded. Low-resource languages are underlined.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_85"><head></head><label></label><figDesc>Bilingual Multilingual Dense Multilingual MoE Bilingual Multilingual Dense Multilingual MoE chrF</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>eng_Latn-xx</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>xx-eng_Latn</cell><cell></cell><cell></cell></row><row><cell></cell><cell>++</cell><cell>chrF ++</cell><cell>?</cell><cell>chrF ++</cell><cell>?</cell><cell>chrF ++</cell><cell>chrF ++</cell><cell>?</cell><cell>chrF ++</cell><cell>?</cell></row><row><cell>aka_latn</cell><cell>16.8</cell><cell>35.6</cell><cell>18.8</cell><cell>36.2</cell><cell>19.4</cell><cell>36.6</cell><cell>45.6</cell><cell>9.0</cell><cell>46.7</cell><cell>10.1</cell></row><row><cell>smo_Latn</cell><cell>49.4</cell><cell>50.5</cell><cell>1.1</cell><cell>50.2</cell><cell>0.8</cell><cell>49.7</cell><cell>57.1</cell><cell>7.4</cell><cell>58.1</cell><cell>8.4</cell></row><row><cell>sot_Latn</cell><cell>44.4</cell><cell>46.4</cell><cell>2.0</cell><cell>46</cell><cell>1.6</cell><cell>52.3</cell><cell>61.9</cell><cell>9.6</cell><cell>62.5</cell><cell>10.2</cell></row><row><cell>twi_Latn</cell><cell>36.7</cell><cell>38.7</cell><cell>2.0</cell><cell>39.1</cell><cell>2.4</cell><cell>39.8</cell><cell>46.6</cell><cell>6.8</cell><cell>48</cell><cell>8.2</cell></row><row><cell>umb_Latn</cell><cell>23.3</cell><cell>23.8</cell><cell>0.5</cell><cell>24.1</cell><cell>0.8</cell><cell>27.2</cell><cell>31.6</cell><cell>4.4</cell><cell>32.7</cell><cell>5.5</cell></row><row><cell>vec_Latn</cell><cell>40.6</cell><cell>47.1</cell><cell>6.5</cell><cell>48.9</cell><cell>8.3</cell><cell>56.3</cell><cell>67.9</cell><cell>11.6</cell><cell>63.7</cell><cell>7.4</cell></row><row><cell>guj_Gujr</cell><cell>51.7</cell><cell>50.6</cell><cell>-1.1</cell><cell>51.4</cell><cell>-0.3</cell><cell>57.4</cell><cell>65.3</cell><cell>7.9</cell><cell>66.4</cell><cell>9.0</cell></row><row><cell>mya_Mymr</cell><cell>27.1</cell><cell>32.7</cell><cell>5.6</cell><cell>32.6</cell><cell>5.5</cell><cell>43.5</cell><cell>56</cell><cell>12.5</cell><cell>57.2</cell><cell>13.7</cell></row><row><cell>npi_Deva</cell><cell>32.4</cell><cell>47.9</cell><cell>15.5</cell><cell>48.6</cell><cell>16.2</cell><cell>56.2</cell><cell>67.3</cell><cell>11.1</cell><cell>67.6</cell><cell>11.4</cell></row><row><cell>pbt_Arab</cell><cell>37.1</cell><cell>35.3</cell><cell>-1.8</cell><cell>36.5</cell><cell>-0.6</cell><cell>47.2</cell><cell>56.6</cell><cell>9.4</cell><cell>56.9</cell><cell>9.7</cell></row><row><cell>sin_Sinh</cell><cell>41.8</cell><cell>43.1</cell><cell>1.3</cell><cell>44.1</cell><cell>2.3</cell><cell>51.0</cell><cell>61.7</cell><cell>10.7</cell><cell>63.0</cell><cell>12.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_86"><head>Table 39 : Comparison of FLORES-200 devtest Performance between Bilingual and Multilingual dense and MoE models.</head><label>39</label><figDesc>We see that most low-resource languages benefit from multilingual transfer, leading to significant chrF ++ increase. Both dense and MoE models perform better and the gains are more consistent when translating into eng_Latn.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_88"><head>Table 40 : Performance of Offline and Online Distillation in the Wikipedia Setting.</head><label>40</label><figDesc>Weshow the average Flores-200 devtest chrf++ performance for English, French, and Spanish source for teacher, fine-tuned teacher, and student models. A full list of results is available inTable 59or (3) had high article deletion rates. Additionally, we include four other directions of interest to Wikipedia: por_Latn-oci_Latn, cat_Latn-oci_Latn, zho_Hans-yue_Hant, and rus_Cyrl-bak_Cyrl. We train a 1.3B parameter dense teacher model on the ablation dataset (see Section 6.4), for all the language pairs listed above. The teacher model is smaller than our full 54B NLLB-200 model and is dense rather than sparse, reducing the time required to generate the student model training dataset for offline distillation. We further fine-tune this model on Wikipedia domain bitexts, which include Wikipedia translation content edits. We filter the fine-tuning data using LID and length-based filtering, similar to the filtering described in Section 8.1.4. Using offline sequence-level distillation (Kim and Rush, 2016), we distill the same fine-tuned teacher model into a smaller 500M parameter dense model. We prepare the distilled model's training dataset by generating translations for monolingual Wikipedia corpus data, using the fine-tuned teacher model. To generate this training dataset for the student model, we run beam search with a beam size of 4 with the teacher model. The source sentences come from monolingual Wikipedia data dumps 43 for eng_Latn, fra_Latn, spa_Latn, rus_Cyrl, por_Latn, cat_Latn. Finally, we train the student model on this distilled training dataset. Note that while the teacher model is trained on both directions, the distilled student model is only trained on the eng_Latn-xx, spa_Latn-xx, fra_Latn-xx, por_Latn-oci_Latn, cat_Latn-oci_Latn, zho_Hans-yue_Hant, and rus_Cyrl-bak_Cyrl directions.</figDesc><table /><note>Teacher Training and Fine-Tuning.Applying Offline Distillation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_90"><head>Table 41 :</head><label>41</label><figDesc>Distillation of NLLB-200. We report chrF ++ scores on Flores-200 devtest set for the full NLLB-200, dense baselines, and dense distilled models. For eng_Latn-xx and xx-eng_Latn we include all 201 pairs each. For xx-yy we randomly choose 200 directions. We observe that distilled models perform better than dense baseline models trained from scratch without distillation. very low-resource languages. For example, for the 1.3B model setting, the distilled model performs better than the dense baseline by +0.6 chrF ++ . However, on average, a gap of -1.4 chrF ++ for 1.3B and -3.7 chrF ++ for 615M remains between the student model and NLLB-200 performance.</figDesc><table /><note>8.6.4 Conclusion.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_91"><head></head><label></label><figDesc>Ejawi 45 and an ALA-LC transliterator for the Tifinagh script 46 ? Our own NLLB-200 translation model, which is capable of translating between any pair of 200 languages ? A custom neural transliteration model. The model is trained on NLLB-Seed and is based on a standard transformer architecture, with 4-layer encoder and decoder, 445. https://www.ejawi.net/ We are aware that Acehnese and Banjar use two different varieties of Arabic script (Jawi and Pegon, respectively), and that Acehnese is not part of the Malay group of languages, while eJawi is optimized for Malay languages written in Jawi. We use this approach to get a reasonably approximated transliteration. 46. https://www.translitteration.com/transliteration/en/tamazight/ala-lc/</figDesc><table><row><cell></cell><cell cols="3">ace_Arab-ace_Latn bjn_Arab-bjn_Latn taq_Tfng-taq_Latn</cell></row><row><cell>uroman</cell><cell>0.47</cell><cell>0.40</cell><cell>0.23</cell></row><row><cell>Ejawi/ALA-LC</cell><cell>0.66</cell><cell>0.72</cell><cell>0.24</cell></row><row><cell>Ejawi/ALA-LC + rules</cell><cell>0.37</cell><cell>0.35</cell><cell>0.22</cell></row><row><cell>Custom Model</cell><cell>0.32</cell><cell>0.31</cell><cell>0.29</cell></row><row><cell>Rules + Custom Model</cell><cell>0.25</cell><cell>0.20</cell><cell>0.22</cell></row><row><cell>NLLB-200 translation model</cell><cell>0.54</cell><cell>0.41</cell><cell>0.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_92"><head>Table 42 :</head><label>42</label><figDesc>Transliteration Performance of Various Approaches, measured by Character Error Rate (CER, lower is better) on Flores-200 devtest set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_94"><head>Table 43 :</head><label>43</label><figDesc>Quantification of Differences between six Arabic Languoids in Flores-200devtest, measured via sentence-and corpus-level DL, spBLEU and chrF ++ of each languoid's reference against the MSA (arb_Arab) reference.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_97"><head>Table 44 :</head><label>44</label><figDesc>Comparison of Translations Generated by NLLB-200 from English Flores-200devtest data. We report the translation performance for each direction. We further compare against the MSA reference as well as the translation of English into MSA.</figDesc><table><row><cell></cell><cell cols="2">Transl. perf.</cell><cell></cell><cell cols="2">against generated MSA</cell><cell></cell></row><row><cell>Direction</cell><cell cols="2">spBLEU chrF ++</cell><cell>sDL</cell><cell>cDL</cell><cell cols="2">spBLEU chrF ++</cell></row><row><cell>fra_Latn-acm_Arab</cell><cell>3.7</cell><cell>18.4</cell><cell>65.80%</cell><cell>8.47%</cell><cell>7.3</cell><cell>25.4</cell></row><row><cell>fra_Latn-acq_Arab</cell><cell>19.1</cell><cell>34.3</cell><cell cols="2">33.06% 17.54%</cell><cell>37.1</cell><cell>49.4</cell></row><row><cell>fra_Latn-aeb_Arab</cell><cell>11.5</cell><cell>30.2</cell><cell cols="2">49.89% 10.49%</cell><cell>18.2</cell><cell>37.9</cell></row><row><cell>fra_Latn-ars_Arab</cell><cell>25.8</cell><cell>40.5</cell><cell cols="2">25.71% 11.16%</cell><cell>50.3</cell><cell>60.6</cell></row><row><cell>fra_Latn-ajp_Arab</cell><cell>27.0</cell><cell>42.8</cell><cell cols="2">29.41% 10.41%</cell><cell>49.0</cell><cell>59.2</cell></row><row><cell>fra_Latn-arz_Arab</cell><cell>25.3</cell><cell>41.2</cell><cell>29.86%</cell><cell>9.26%</cell><cell>49.8</cell><cell>62.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_98"><head>Table 45 :</head><label>45</label><figDesc>Comparison</figDesc><table /><note>of Translations Generated by NLLB-200 from French. Flores-200 devtest data, analogously to Table 44.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_99"><head></head><label></label><figDesc>into eng_Latn into fra_Latn Language spBLEU chrF ++ spBLEU chrF ++</figDesc><table><row><cell>acm_Arab</cell><cell>43.3</cell><cell>63.1</cell><cell>39.2</cell><cell>57.6</cell></row><row><cell>acq_Arab</cell><cell>45.1</cell><cell>64.9</cell><cell>40.2</cell><cell>58.5</cell></row><row><cell>aeb_Arab</cell><cell>38.6</cell><cell>59.5</cell><cell>36.0</cell><cell>55.1</cell></row><row><cell>ars_Arab</cell><cell>46.8</cell><cell>66.0</cell><cell>41.6</cell><cell>59.5</cell></row><row><cell>ajp_Arab</cell><cell>48.2</cell><cell>67.4</cell><cell>41.8</cell><cell>59.9</cell></row><row><cell>arz_Arab</cell><cell>40.7</cell><cell>60.8</cell><cell>37.8</cell><cell>56.1</cell></row><row><cell>arb_Arab</cell><cell>48.3</cell><cell>66.9</cell><cell>42.3</cell><cell>59.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_100"><head>Table 46 :</head><label>46</label><figDesc>Languoid Performance on FLORES-200 devtest. We evaluate translation into English and French for seven Arabic languoids, including MSA (bottom).</figDesc><table /><note>NLLB-200 Performance. We next shift our focus to how NLLB-200 generates Arabic languoids. Tables 44 and 45 look at the model's translation from English and French respectively. The first two columns measure the model's translation performance against each languoid's Flores-200 reference.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_102"><head>Table 47 :</head><label>47</label><figDesc>Comparison</figDesc><table /><note>of a Custom Arabic model and NLLB-200 on FLORES-200 devtest.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_104"><head>Table 48 : Total Carbon Footprint for No Language Left Behind.</head><label>48</label><figDesc>We provide detailed estimates for all the steps that use GPUs for computation. Here we list factors that went into computation: Time in hours -total GPU time required for the step. Power Consumption -power consumption per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by the providers. such models with careful considerations can help keep power consumption and carbon emissions lower for large workloads. It is a paramount responsibility for machine learning researchers to measure and report carbon emission impact of their work thoroughly, optimize their workloads towards energy efficient architectures and training paradigms, and keep in mind the negative environmental impact of inefficient workloads. Large scale machine learning workloads often come at a cost to the environment and if such works are not open sourced, similar efforts will have to be duplicated across multiple research groups which further results in more carbon emissions. To help reduce the need for any duplication of similar workloads, we open source the NLLB-200 and other smaller dense models, distilled models, our optimized training/inference code, evaluation results and benchmarks, and metadata for mined bitext data. All materials and code are accessible at https: //github.com/facebookresearch/fairseq/tree/nllb.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_105"><head></head><label></label><figDesc>Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.417. URL https://aclanthology.org/2020.acl-main.417. Ankur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, Theresa Breiner, Vera Axelrod, Jason Riesa, Yuan Cao, Mia Xu Chen, Klaus Macherey, Maxim Krikun, Pidong Wang, Alexander Gutkin, Apurva Shah, Yanping Huang, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. Building machine translation systems for the next thousand languages, 2022. URL https://arxiv.org/abs/2205.03983. Emily Bender. The #benderrule: On naming the languages we study and why it matters. Lin Blodgett, Solon Barocas, Hal Daum? III, and Hanna Wallach. Language (technology) is power: A critical survey of "bias" in NLP. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454-5476, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.485. URL https://aclanthology.org/2020.acl-main.485. Foreseeing and mitigating harms. In CHI Conference on Human Factors in Computing Systems Extended Abstracts, pages 1-3, 2022. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135-146, 2017. ISSN 2307-387X. Distinction: A social critique of the judgement of taste. Harvard University Press, 1987. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In EMNLP, pages 632-642, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10. 18653/v1/D15-1075. URL https://aclanthology.org/D15-1075. Non-linear mapping for improved identification of 1300+ languages. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 627-632, 2014. Taina Bucher. Want to be on the top? algorithmic power and the threat of invisibility on facebook. New media &amp; society, 14(7):1164-1180, 2012. Cristian Bucilu?, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535-541, 2006. Guanhua Chen, Shuming Ma, Yun Chen, Dongdong Zhang, Jia Pan, Wenping Wang, and Furu Wei. Towards making the most of multilingual pretraining for zero-shot neural machine translation. CoRR, abs/2110.08547, 2021. URL https://arxiv.org/abs/2110. 08547. understanding of pretrained multilingual models in truly low-resource languages. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6279-6299, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.435. URL https:// aclanthology.org/2022.acl-long.435. Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale. In Proc. of EMNLP, 2018. Ahmed El-Kishky, Vishrav Chaudhary, Francisco Guzman, and Philipp Koehn. A massive collection of cross-lingual web-document pairs. In EMNLP, pages 5960-5969, 2020. The Journal of Machine Learning Research, 2020. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):1-39, 2022. URL http://jmlr.org/papers/v23/21-0998.html. Goutte, Serge L?ger, Shervin Malmasi, and Marcos Zampieri. Discriminating similar languages: Evaluations and explorations. CoRR, abs/1610.00031, 2016. URL http: //arxiv.org/abs/1610.00031. Thamme Gowda, Zhao Zhang, Chris Mattmann, and Jonathan May. Many-to-English machine translation tools, data, and pretrained models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 306-316, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. acl-demo.37. URL https://aclanthology.org/2021.acl-demo.37. Brian Strope, and Ray Kurzweil. Effective parallel corpus mining using bilingual sentence embeddings, 2018. URL https: //arxiv.org/abs/1807.11906. Jauhiainen, Krister Lind?n, and Heidi Jauhiainen. Evaluation of language identification methods using 285 languages. In Proceedings of the 21st Nordic Conference on Computational Linguistics, pages 183-191, 2017. Tommi Jauhiainen, Marco Lui, Marcos Zampieri, Timothy Baldwin, and Krister Lind?n. Automatic language identification in texts: A survey. Journal of Artificial Intelligence Research, 65:675-782, 2019. Isaac Johnson and Emily Lescak. Considerations for multilingual wikipedia research. arXiv preprint arXiv:2204.02483, 04 2022. Khonji, Youssef Iraqi, and Andrew Jones. Phishing detection: a literature survey. IEEE Communications Surveys &amp; Tutorials, 15(4):2091-2121, 2013. Elaine C Khoong and Jorge A Rodriguez. A research agenda for using machine translation in clinical medicine. Journal of General Internal Medicine, 37(5):1275-1277, 2022. Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. In EMNLP, 2016. Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andr?s Felipe Cruz-Salinas, Liyang Lu, Amr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany Hassan Awadalla. Scalable and efficient moe training for multitask multilingual models. CoRR, abs/2109.10465, 2021. URL https://arxiv.org/abs/2109.10465. Philipp Koehn, Francisco Guzm?n, Vishrav Chaudhary, and Juan Pino. Findings of the WMT 2019 shared task on parallel corpus filtering for low-resource conditions. In Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2), 2019. Philipp Koehn, Vishrav Chaudhary, Ahmed El-Kishky, Naman Goyal, Peng-Jen Chen, and Francisco Guzm?n. Findings of the WMT 2020 shared task on parallel corpus filtering and alignment. In Proceedings of the Fifth Conference on Machine Translation, pages 726-742, Online, November 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.wmt-1.78. Aman Kumar, Himani Shrotriya, Prachi Sahu, Raj Dabre, Ratish Puduppully, Anoop Kunchukuttan, Amogh Mishra, Mitesh M Khapra, and Pratyush Kumar. Indicnlg suite: Multilingual datasets for diverse nlg tasks in indic languages. arXiv preprint arXiv:2203.05437, 2022. Sachin Kumar, Antonios Anastasopoulos, Shuly Wintner, and Yulia Tsvetkov. Machine translation into low-resource language varieties. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 110-121, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. acl-short.16. URL https://aclanthology.org/2021.acl-short.16. May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.6. URL https://aclanthology.org/2022.findings-acl.6. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. CoRR, 2021. URL https://arxiv.org/abs/2107.06499. Lees, Daniel Borkan, Ian Kivlichan, Jorge Nario, and Tesh Goyal. Capturing covertly toxic speech via crowdsourcing. In Proceedings of the First Workshop on Bridging Human-Computer Interaction and Natural Language Processing, Online, April 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.hcinlp-1.3. Heather Lent, Kelechi Ogueji, Miryam de Lhoneux, Orevaoghene Ahia, and Anders S?gaard. What a creole wants, what a creole needs. arXiv preprint arXiv:2206.00437, 2022. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. CoRR, abs/2006.16668, 2020. URL https://arxiv.org/abs/2006.16668. Shuming Shi, and Zhaopeng Tu. On the complementarity between pre-training and back-translation for neural machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2900-2907, Punta Cana, Dominican Republic, November 2021a. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.247. URL https://aclanthology.org/2021.findings-emnlp.247. Computing Machinery. ISBN 9781450361255. doi: 10.1145/3287560.3287596. URL https://doi.org/10.1145/3287560.3287596. Aaron Mueller, Garrett Nicolai, Arya D McCarthy, Dylan Lewis, Winston Wu, and David Yarowsky. An analysis of massively multilingual neural machine translation for low-resource languages. In Proceedings of The 12th language resources and evaluation conference, pages 3710-3718, 2020. Namrata Mukhija, Monojit Choudhury, and Kalika Bali. Designing language technologies for social good: The road not taken. arXiv preprint arXiv:2110.07444, 2021. Participatory research for low-resourced machine translation: A case study in African languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2144-2160, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.195. URL https://aclanthology.org/2020.findings-emnlp.195. Hassan Sajjad, Ahmed Abdelali, Nadir Durrani, and Fahim Dalvi. AraBench: Benchmarking dialectal Arabic-English machine translation. In Proceedings of the 28th International Conference on Computational Linguistics, pages 5094-5107, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/ v1/2020.coling-main.447. URL https://aclanthology.org/2020.coling-main.447. Mohammad Salameh, Houda Bouamor, and Nizar Habash. Fine-grained Arabic dialect identification. In Coling, pages 1332-1344, Santa Fe, New Mexico, USA, August 2018. Association for Computational Linguistics. URL https://aclanthology.org/C18-1113. Sellam, Dipanjan Das, and Ankur Parikh. Bleurt: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, 2020. Rico Sennrich and Biao Zhang. Revisiting low-resource neural machine translation: A case study. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 211-221, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1021. URL https://www.aclweb.org/anthology/ P19-1021. Hatem Haddad, and Malek Naski. AI4D -african language program. arXiv preprint arXiv:2104.02516, 2021. Shuming Ma, Dongdong Zhang, and Marine Carpuat. How does distilled data complexity impact the quality and confidence of non-autoregressive machine translation? In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4392-4400, 2021b. Yinfei Yang, Gustavo Hernandez Abrego, Steve Yuan, Mandy Guo, Qinlan Shen, Daniel Cer, Yun-hsuan Sung, Brian Strope, and Ray Kurzweil. Improving multilingual sentence embedding using bi-directional dual encoder with additive margin softmax. In IJCAI, pages 5370-5378, 2019. Zhen Yang, Wei Chen, Feng Wang, and Bo Xu. Unsupervised neural machine translation with weight sharing. CoRR, abs/1804.09057, 2018. URL http://arxiv.org/abs/1804.09057. Analyzing knowledge distillation in neural machine translation. In Proceedings of the 15th International Conference on Spoken Language Translation, pages 23-30, 2018. Mike Zhang and Antonio Toral. The effect of translationese in machine translation test sets. arXiv preprint arXiv:1906.08069, 2019. Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. Transfer learning for lowresource neural machine translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1568-1575, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1163. URL https://aclanthology.org/D16-1163.</figDesc><table><row><cell>The Gradient, 2019. Su Cyril Tommi Alina Karakanta, Jon Dehdari, and Josef Genabith. Neural machine translation for low-http://www.aclweb.org/anthology/J05-4003. 2020. Oxford review of economic policy, 17(2):248-264, 2001. Exploiting Non-Parallel Corpora. Computational Linguistics, 31(4):477-504, 2005. URL (ACL), 2016a. of the 28th International Conference on Computational Linguistics, pages 3413-3421, Mahmoud Bruce Kogut and Anca Metiu. Open-source software development and distributed innovation. Fahimeh Saleh, Wray Buntine, and Gholamreza Haffari. Collective wisdom: Improving low-Thibault Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. Designing effective sparse expert models. arXiv preprint arXiv:2202.08906, 2022. Shoshana Zuboff. The age of surveillance capitalism: The fight for a human future at the Alyssa Dragos Stefan Munteanu and Daniel Marcu. Improving Machine Translation Performance by resource neural machine translation using adaptive knowledge distillation. In Proceedings models with monolingual data. Conference of the Association for Computational Linguistics new frontier of power: Barack Obama's books of 2019. Profile books, 2019.</cell></row><row><cell>resource languages without parallel corpora. Machine Translation, 32(1 -2):167 -189,</cell></row><row><cell>jun 2018. ISSN 0922-6567. doi: 10.1007/s10590-017-9203-5. URL https://doi.org/10. Nitish Singh, Kevin Lehnert, and Kathleen Bostick. Global social media usage: Insights</cell></row><row><cell>1007/s10590-017-9203-5. into reaching consumers worldwide. Thunderbird International Business Review, 54(5):</cell></row><row><cell>683-700, 2012.</cell></row></table><note>Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret Mitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610-623, 2021. Yoshua Bengio, J?r?me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41-48, 2009. Yoshua Bengio, Nicholas L?onard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. URL http://arxiv.org/abs/1308.3432. Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, and Rifat Shahriyar. Banglanlg: Benchmarks and resources for evaluating low-resource natural language generation in bangla. arXiv preprint arXiv:2205.11081, 2022. Steven Bird. Designing for language revitalisation. In Gilles Adda, Khalid Choukri, Irm- garda Kasinskaite-Buddeberg, Joseph Mariani, H?l?ne Mazo, and Sakriani Sakti, editors, Language Technologies for All (LT4All), pages 296-299. European Language Resources Association (ELRA), 2019. URL https://en.unesco.org/LT4All. International Con- ference Language Technologies for All, LT4All ; Conference date: 04-12-2019 Through 06-12-2019. Steven Bird and David Chiang. Machine translation for language preservation. In Proceedings of COLING 2012: Posters, pages 125-134, 2012.Su Lin Blodgett, Q Vera Liao, Alexandra Olteanu, Rada Mihalcea, Michael Muller, Mor- gan Klaus Scheuerman, Chenhao Tan, and Qian Yang. Responsible language technologies:Ond?ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aur?lie N?v?ol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos Zampieri. Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 131-198, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-2301. URL https://aclanthology.org/W16-2301. Marcel Bollmann. A large-scale comparison of historical text normalization systems. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3885-3898, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1389. URL https://aclanthology. org/N19-1389. Houda Bouamor and Hassan Sajjad. H2@BUCC18: Parallel Sentence Extraction from Comparable Corpora Using Multilingual Sentence Embeddings. In BUCC, May 2018. Houda Bouamor, Nizar Habash, Mohammad Salameh, Wajdi Zaghouani, Owen Rambow, Dana Abdulrahim, Ossama Obeid, Salam Khalifa, Fadhl Eryani, Alexander Erdmann, and Kemal Oflazer. The MADAR Arabic dialect corpus and lexicon. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, May 2018. European Language Resources Association (ELRA). URL https://aclanthology.org/L18-1535. Houda Bouamor, Sabit Hassan, and Nizar Habash. The MADAR shared task on Arabic fine- grained dialect identification. In Proceedings of the Fourth Arabic Natural Language Process- ing Workshop, pages 199-207, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4622. URL https://aclanthology.org/W19-4622. Pierre Bourdieu.Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 1993. Ralf D Brown.Christian Buck and Philipp Koehn. Findings of the wmt 2016 bilingual document alignment shared task. In Proceedings of the First Conference on Machine Translation, pages 554-563, Berlin, Germany, August 2016. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/W/W16/W16-2347. Lindsay Bywood, Panayota Georgakopoulou, and Thierry Etchegoyhen. Embracing the threat: machine translation as a solution for subtitling. Perspectives, 25(3):492-508, 2017. Isaac Caswell, Ciprian Chelba, and David Grangier. Tagged back-translation. In Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pages 53-63, 2019. Isaac Caswell, Theresa Breiner, Daan van Esch, and Ankur Bapna. Language ID in the wild: Unexpected challenges on the path to a thousand-language web text corpus. In Proceedings of the 28th International Conference on Computational Linguistics, pages 6588-6608, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.579. URL https://aclanthology.org/ 2020.coling-main.579. Mauro Cettolo, Jan Niehues, Sebastian St?ker, Luisa Bentivogli, and Marcello Federico. Report on the 11th IWSLT evaluation campaign. In Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign, pages 2-17, Lake Tahoe, California, December 2014. URL https://aclanthology.org/2014.iwslt-evaluation. 1. Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian St?ker, Katsuhito Sudoh, Koichiro Yoshino, and Christian Federmann. Overview of the IWSLT 2017 evaluation campaign. In Proceedings of the 14th International Conference on Spoken Language Translation, pages 2-14, Tokyo, Japan, December 2017. International Workshop on Spoken Language Translation. URL https://aclanthology.org/2017.iwslt-1.1.Zewen Chi, Li Dong, Shuming Ma, Shaohan Huang Xian-Ling Mao, Heyan Huang, and Furu Wei. mt6: Multilingual pretrained text-to-text transformer with translation pairs. arXiv preprint arXiv:2104.08692, 2021.languageAnca Elena-Bucea, Frederico Cruz-Jesus, Tiago Oliveira, and Pedro Sim?es Coelho. Assessing the role of age, education, gender and income on the digital divide: evidence for the european union. Information Systems Frontiers, 23(4):1007-1021, 2021. Chris Chinenye Emezue and Bonaventure F. P. Dossou. MMTAfrica: Multilingual machine translation for African languages. In Proceedings of the Sixth Conference on Machine Translation, pages 398-411, Online, November 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.wmt-1.48. Chris Chinenye Emezue and Femi Pancrace Bonaventure Dossou. Ffr v1. 1: Fon-french neural machine translation. In Proceedings of the The Fourth Widening Natural Language Processing Workshop, pages 83-87, 2020. Cristina Espa?a-Bonet, ?d?m Csaba Varga, Alberto Barr?n-Cede?o, and Josef van Genabith. An Empirical Analysis of NMT-Derived Interlingual Embeddings and their Use in Parallel Sentence Identification. IEEE Journal of Selected Topics in Signal Processing, pages 1340-1348, 2017. Thierry Etchegoyhen and Andoni Azpeitia. Set-Theoretic Alignment for Comparable Corpora. In ACL, pages 2009-2018, 2016. doi: 10.18653/v1/P16-1189. URL http: //www.aclweb.org/anthology/P16-1189. Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin. Beyond english-centric multilingual machine translation.Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. Language- agnostic bert sentence embedding, 2020. URL https://arxiv.org/abs/2007.01852. Charles A Ferguson. Diglossia. word, 15(2):325-340, 1959.Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc'Aurelio Ranzato, Francisco Guzm?n, and Angela Fan. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522-538, 2022. doi: 10.1162/tacl_a_00474. URL https://aclanthology.org/2022.tacl-1.30. Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. Continuous measurement scales in human evaluation of machine translation. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 33-41, Sofia, Bulgaria, August 2013. Association for Computational Linguistics. URL https://aclanthology.org/W13-2305. ?douard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tom?? Mikolov. Learning word vectors for 157 languages. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), 2018. Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O.K. Li. Universal neural machine translation for extremely low resource languages. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 344-354, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1032. URL https://aclanthology.org/N18-1032. Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor OK Li. Improved zero-shot neural machine translation via ignoring spurious correlations. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1258-1268, 2019. Mandy Guo, Qinlan Shen, Yinfei Yang, Heming Ge, Daniel Cer, Gustavo Hernandez Abrego, Keith Stevens, Noah Constant, Yun-Hsuan Sung,Udit Gupta, Mariam Elgamal, Gage Hills, Gu-Yeon Wei, Hsien-Hsin S Lee, David Brooks, and Carole-Jean Wu. Act: designing sustainable computer systems with an architectural carbon modeling tool. In Proceedings of the 49th Annual International Symposium on Computer Architecture, pages 784-799, 2022a.Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi?gas, Martin Wattenberg, Greg Corrado, et al. Google' s mul- tilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339-351, 2017. Pratik Joshi, Christain Barnes, Sebastin Santy, Simran Khanuja, Sanket Shah, Anirudh Srinivasan, Satwik Bhattamishra, Sunayana Sitaram, Monojit Choudhury, and Kalika Bali. Unsung challenges of building and deploying language technologies for low resource language communities. arXiv preprint arXiv:1912.03457, 2019. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The state and fate of linguistic diversity and inclusion in the nlp world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282-6293, 2020. Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 427-431. Association for Computational Linguistics, April 2017. Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700-1709, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1176. Shivani Kapania, Oliver Siy, Gabe Clapper, Azhagu Meena SP, and Nithya Sambasivan. " because ai is 100% right and safe" : User attitudes and sources of ai authority in india. In CHI Conference on Human Factors in Computing Systems, pages 1-18, 2022.Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL- HLT, pages 4171-4186, 2019.Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980. Svetlana Kiritchenko, Isar Nejadgholi, and Kathleen C Fraser. Confronting abusive language online: A survey from the ethical and human rights perspective. Journal of Artificial Intelligence Research, 71:431-478, 2021. Tom Kocmi, Dominik Mach??ek, and Ond?ej Bojar. The Reality of Multi-Lingual Machine Translation. UFAL, Prague, Czechia, 2021. Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In MT Summit, 2005. Philipp Koehn. Statistical machine translation. Cambridge University Press, 2009. Philipp Koehn and Ulrich Germann. The impact of machine translation quality on human post-editing. In Proceedings of the EACL 2014 Workshop on Humans and Computer- assisted Translation, pages 38-46, 2014. Philipp Koehn and Rebecca Knowles. Six challenges for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation, pages 28-39, Vancouver, August 2017. Association for Computational Linguistics. URL http://www.aclweb.org/ anthology/W17-3204. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions, pages 177-180. Association for Computational Linguistics, 2007. Philipp Koehn, Huda Khayrallah, Kenneth Heafield, and Mikel L. Forcada. Findings of the wmt 2018 shared task on parallel corpus filtering. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 726-739, Belgium, Brussels, October 2018. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ W18-6453.Anastasia Kozyreva, Philipp Lorenz-Spreen, Ralph Hertwig, Stephan Lewandowsky, and Stefan M Herzog. Public attitudes towards algorithmic personalization and use of personal data online: Evidence from germany, great britain, and the united states. Humanities and Social Sciences Communications, 8(1):1-11, 2021. Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Beno?t Sagot, Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey Osei, Pedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, Andre Niyongabo Rubungo, Toan Q. Nguyen, Mathias M?ller, Andr? M?ller, Shamsuddeen Hassan Muhammad, Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine ?abuk Ball?, Stella Biderman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa Adeyemi. Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets. Transactions of the Association for Computational Linguistics, 10:50-72, January 2022. ISSN 2307-387X. doi: 10.1162/tacl_a_00447. URL https://doi.org/10.1162/tacl_a_00447. Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Eduardo Blanco and Wei Lu, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: System Demonstrations, Brussels, Belgium, October 31 -November 4, 2018, pages 66-71. Association for Computational Linguistics, 2018.doi: 10.18653/v1/d18-2012. URL https://doi.org/10.18653/v1/d18-2012. Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh- Thang Luong, and Orhan Firat. Beyond distillation: Task-level mixture-of-experts for efficient inference. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3577-3599, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.304. URL https://aclanthology.org/2021.findings-emnlp.304.Anoop Kunchukuttan. The IndicNLP Library. https://github.com/anoopkunchukuttan/ indic_nlp_library/blob/master/docs/indicnlp.pdf, 2020. Keita Kurita, Anna Belova, and Antonios Anastasopoulos. Towards robust toxic content classification. CoRR, abs/1912.06872, 2019. URL http://arxiv.org/abs/1912.06872. Remy Kusters, Dusan Misevic, Hugues Berry, Antoine Cully, Yann Le Cunff, Loic Dandoy, Natalia D?az-Rodr?guez, Marion Ficher, Jonathan Grizou, Alice Othmani, et al. Interdis- ciplinary research in artificial intelligence: Challenges and opportunities. Frontiers in Big Data, page 45, 2020. Garry Kuwanto, Afra Feyza Aky?rek, Isidora Chara Tourni, Siyang Li, and Derry Wijaya. Low-resource machine translation for low-resource languages: Leveraging comparable data, code-switching and compute resources. CoRR, abs/2103.13272, 2021. URL https: //arxiv.org/abs/2103.13272. Ivana Kvapil?kov?, Mikel Artetxe, Gorka Labaka amd Eneko Agirre, and Ond?ej Bojar. Unsupervised multilingual sentence embeddings for parallel corpus mining. In ACL, 2020. Niklas Laxstr?m, Pau Giner, and Santhosh Thottingal. Content translation: Computer- assisted translation tool for wikipedia articles. CoRR, abs/1506.01914, 2015. URL http://arxiv.org/abs/1506.01914. En-Shiun Lee, Sarubi Thillainathan, Shravan Nayak, Surangika Ranathunga, David Ade- lani, Ruisi Su, and Arya McCarthy. Pre-trained multilingual sequence-to-sequence mod- els: A hope for low-resource language translation? In Findings of the Association for Computational Linguistics: ACL 2022, pages 58-67, Dublin, Ireland,Sangmin-Michelle Lee. The impact of using machine translation on efl students' writing. Computer Assisted Language Learning, 33(3):157-175, 2020.Peggy Levitt and B Nadya Jaworsky. Transnational migration studies: Past developments and future trends. Annual review of sociology, 33:129, 2007. Peggy Levitt and Deepak Lamba-Nieves. Social remittances revisited. Journal of ethnic and migration studies, 37(1):1-22, 2011. Shahar Levy, Koren Lazar, and Gabriel Stanovsky. Collecting a large-scale gender bias dataset for coreference resolution and machine translation. arXiv preprint arXiv:2109.03858, 2021.M. Paul Lewis, editor. Ethnologue: Languages of the World. SIL International, Dallas, TX, USA, sixteenth edition, 2009. Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In International Conference on Machine Learning, pages 6265-6274. PMLR, 2021. Daniel Licht, Cynthia Gao, Janice Lam, Francisco Guzman, Mona Diab, and Philipp Koehn. Consistent human evaluation of machine translation across language pairs, 2022. URL https://arxiv.org/abs/2205.08533. Pierre Lison and J?rg Tiedemann. Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles, 2016. Rui Liu, Young Jin Kim, Alexandre Muzio, Barzan Mozafari, and Hany Hassan Awadalla. Gating dropout: Communication-efficient regularization for sparsely activated transformers. arXiv preprint arXiv:2205.14336, 2022. Xuebo Liu, Longyue Wang, Derek F. Wong, Liang Ding, Lidia S. Chao,Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machineToshiaki Nakazawa, Chenchen Ding, Raj Dabre, Anoop Kunchukuttan, Nobushige Doi, Yusuke Oda, Ond?ej Bojar, Shantipriya Parida, Isao Goto, and Hidaya Mino, editors. Proceedings of the 6th Workshop on Asian Translation, Hong Kong, China, November 2019. Association for Computational Linguistics. URL https://aclanthology.org/D19-5200. Toshiaki Nakazawa, Hideki Nakayama, Chenchen Ding, Raj Dabre, Shohei Higashiyama, Hideya Mino, Isao Goto, Win Pa Pa, Anoop Kunchukuttan, Shantipriya Parida, Ond?ej Bojar, Chenhui Chu, Akiko Eriguchi, Kaori Abe, Yusuke Oda, and Sadao Kurohashi. Overview of the 8th workshop on Asian translation. In Proceedings of the 8th Workshop on Asian Translation (WAT2021), pages 1-45, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.wat-1.1. URL https://aclanthology. org/2021.wat-1.1. Wilhelmina Nekoto, Vukosi Marivate, Tshinondiwa Matsila, Timi Fasubaa, Taiwo Fagbo- hungbe, Solomon Oluwole Akinola, Shamsuddeen Muhammad, Salomon Kabongo Kabena- mualu, Salomey Osei, Freshia Sackey, Rubungo Andre Niyongabo, Ricky Macharm, Perez Ogayo, Orevaoghene Ahia, Musie Meressa Berhe, Mofetoluwa Adeyemi, Masabata Mokgesi- Selinga, Lawrence Okegbemi, Laura Martinus, Kolawole Tajudeen, Kevin Degila, Kelechi Ogueji, Kathleen Siminyu, Julia Kreutzer, Jason Webster, Jamiil Toure Ali, Jade Ab- bott, Iroro Orife, Ignatius Ezeani, Idris Abdulkadir Dangana, Herman Kamper, Hady Elsahar, Goodness Duru, Ghollah Kioko, Murhabazi Espoir, Elan van Biljon, Daniel Whitenack, Christopher Onyefuluchi, Chris Chinenye Emezue, Bonaventure F. P. Dossou, Blessing Sibanda, Blessing Bassey, Ayodele Olabiyi, Arshath Ramkilowan, Alp ?ktem, Adewale Akinfaderin, and Abdallah Bashir.Toan Q. Nguyen and David Chiang. Transfer learning across low-resource, related languages for neural machine translation. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 296-301, Taipei,Julia Sallabank. Attitudes to endangered languages: Identities and policies. Cambridge University Press, 2013. Nithya Sambasivan. Seeing like a dataset from the global south. Interactions, 28(4):76-78, 2021. Nithya Sambasivan and Jess Holbrook. Toward responsible AI for the next billion users. Interactions, 26(1):68-71, 2018. Nithya Sambasivan, Erin Arnesen, Ben Hutchinson, Tulsee Doshi, and Vinodkumar Prab- hakaran. Re-imagining algorithmic fairness in india and beyond. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 315-328, 2021. Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and A Noah Smith. The risk of racial bias in hate speech detection. In ACL, 2019. Kevin P Scannell. The Cr?bad?n Project: Corpus building for under-resourced languages. Cahiers du Cental, 5:1, 2007. Holger Schwenk. Investigations on large-scale lightly-supervised training for statistical machine translation. In Proceedings of the 5th International Workshop on Spoken Language Translation: Papers, 2008. Holger Schwenk. Filtering and mining parallel data in a joint multilingual space. In ACL, pages 228-234, 2018. Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzm?n.WikiMatrix: Mining 135m parallel sentences in 1620 language pairs from wikipedia. In ACL, pages 1351-1361, 2021a. Holger Schwenk, Guillaume Wenzek, Sergey Edunov, ?douard Grave, Armand Joulin, and Angela Fan. CCMatrix: Mining billions of high-quality parallel sentences on the web. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6490-6500, 2021b.Rico Sennrich, Barry Haddow, and Alexandra Birch. Edinburgh neural machine translation systems for wmt 16. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 371-376, 2016b. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In Proceedings of International Conference on Learning Representations (ICLR), 2017. URL https://openreview.net/pdf?id=B1ckMDqlg. Shashi Shekhar, Dilip Kumar Sharma, and MM Sufyan Beg. Language identification framework in code-mixed social media text based on quantum lstm-the word belongs to which language? Modern Physics Letters B, 34(06):2050086, 2020. Aditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Firat, Mia Xu Chen, Sneha Kudugunta, Naveen Arivazhagan, and Yonghui Wu. Leveraging monolingual data with self-supervision for multilingual neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2827-2835, 2020. Aditya Siddhant, Ankur Bapna, Orhan Firat, Yuan Cao, Mia Xu Chen, Isaac Caswell, and Xavier Garcia. Towards the next 1000 languages in multilingual machine transla- tion: Exploring the synergy between supervised and self-supervised learning. CoRR, abs/2201.03110, 2022. URL https://arxiv.org/abs/2201.03110. Kathleen Siminyu, Godson Kalipe, Davor Orlic, Jade Abbott, Vukosi Marivate, Sackey Freshia, Prateek Sibal, Bhanu Neupane, David I. Adelani, Amelia Taylor, Jamiil Toure ALI, Kevin Degila, Momboladji Balogoun, Thierno Ibrahima DIOP, Davis David, Chayma Fourati,Raivis Skadi??, M?rcis Pinnis, Andrejs Vasi?jevs, Inguna Skadi?a, and Tomas Hudik. Application of machine translation in localization into low-resourced languages. In Weijia Xu,Marcos Zampieri, Binyam Gebrekidan Gebre, Hernani Costa, and Josef Van Genabith. Comparing approaches to the identification of similar languages. In Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialects, pages 66-72, 2015a. Marcos Zampieri, Liling Tan, Nikola Ljube?i?, J?rg Tiedemann, and Preslav Nakov. Overview of the DSL shared task 2015. In Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialects, pages 1-9, 2015b. Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. SemEval-2019 task 6: Identifying and categorizing offensive language in social media (OffensEval). In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 75-86, Minneapolis, Minnesota, USA, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/S19-2010. URL https://aclanthology. org/S19-2010. Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. Improving massively multilingual neural machine translation and zero-shot translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1628-1639, 2020. Biao Zhang, Ankur Bapna, Rico Sennrich, and Orhan Firat. Share or not? learning to schedule language-specific capacity for multilingual translation. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=Wj4ODo0uyCF. Dakun Zhang, Josep M Crego, and Jean Senellart.Chunting Zhou, Graham Neubig, and Jiatao Gu. Understanding knowledge distillation in non-autoregressive machine translation. CoRR, abs/1911.02727, 2020. URL http: //arxiv.org/abs/1911.02727.Ethan Zuckerman. The polyglot internet, October 2008. URL https://ethanzuckerman. com/the-polyglot-internet/. Alp ?ktem, Muhannad Albayk Jaam, Eric DeLuca, and Grace Tang. Gamayun -language technology for humanitarian response. In 2020 IEEE Global Humanitarian Technology Conference (GHTC), pages 1-4, 2020. doi: 10.1109/GHTC46280.2020.9342939.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_106"><head>Table 50</head><label>50</label><figDesc>compares the performance of our model against the readily available CLD3 system.</figDesc><table><row><cell>Language</cell><cell cols="3">F1 Precision Recall</cell><cell>FPR Language</cell><cell cols="3">F1 Precision Recall</cell><cell>FPR</cell></row><row><cell>ace_Arab</cell><cell>97.0</cell><cell>98.5</cell><cell cols="2">95.6 0.0074 lit_Latn</cell><cell>99.9</cell><cell>99.8</cell><cell>99.9 0.0010</cell></row><row><cell>ace_Latn</cell><cell>99.5</cell><cell>99.3</cell><cell cols="2">99.7 0.0035 lmo_Latn</cell><cell>97.8</cell><cell>98.0</cell><cell>97.6 0.0099</cell></row><row><cell>afr_Latn</cell><cell>99.9</cell><cell>99.8</cell><cell cols="2">100.0 0.0010 ltg_Latn</cell><cell>99.1</cell><cell>100.0</cell><cell>98.2 0.0000</cell></row><row><cell>aka_Latn</cell><cell>99.9</cell><cell>100.0</cell><cell cols="2">99.8 0.0000 ltz_Latn</cell><cell>100.0</cell><cell>100.0</cell><cell>99.9 0.0000</cell></row><row><cell>amh_Ethi</cell><cell>99.9</cell><cell>99.8</cell><cell cols="2">100.0 0.0010 lua_Latn</cell><cell>99.6</cell><cell>99.7</cell><cell>99.5 0.0015</cell></row><row><cell>arb_Arab</cell><cell>96.9</cell><cell>94.0</cell><cell cols="2">100.0 0.2667 lug_Latn</cell><cell>99.3</cell><cell>98.7</cell><cell>99.9 0.0064</cell></row><row><cell>arb_Latn</cell><cell>99.7</cell><cell>99.9</cell><cell cols="2">99.5 0.0005 luo_Latn</cell><cell>99.9</cell><cell>99.9</cell><cell>99.9 0.0005</cell></row><row><cell>asm_Beng</cell><cell>100.0</cell><cell>100.0</cell><cell cols="2">99.9 0.0000 lus_Latn</cell><cell>99.6</cell><cell>100.0</cell><cell>99.1 0.0000</cell></row><row><cell>ast_Latn</cell><cell>99.1</cell><cell>98.8</cell><cell cols="2">99.4 0.0059 mag_Deva</cell><cell>97.1</cell><cell>97.8</cell><cell>96.3 0.0109</cell></row><row><cell>awa_Deva</cell><cell>96.5</cell><cell>98.6</cell><cell cols="2">94.5 0.0069 mai_Deva</cell><cell>99.1</cell><cell>99.9</cell><cell>98.3 0.0005</cell></row><row><cell>ayr_Latn</cell><cell>100.0</cell><cell>100.0</cell><cell cols="2">99.9 0.0000 mal_Mlym</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0 0.0000</cell></row><row><cell>azb_Arab</cell><cell>88.1</cell><cell>98.5</cell><cell cols="2">79.7 0.0059 mar_Deva</cell><cell>99.8</cell><cell>99.6</cell><cell>100.0 0.0020</cell></row><row><cell>azj_Latn</cell><cell>99.8</cell><cell>99.5</cell><cell cols="2">100.0 0.0025 min_Latn</cell><cell>54.1</cell><cell>100.0</cell><cell>37.1 0.0000</cell></row><row><cell>bak_Cyrl</cell><cell>99.9</cell><cell>99.9</cell><cell cols="2">99.9 0.0005 mkd_Cyrl</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0 0.0000</cell></row><row><cell>bam_Latn</cell><cell>61.3</cell><cell>47.4</cell><cell cols="2">86.9 0.4817 plt_Latn</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0 0.0000</cell></row><row><cell>ban_Latn</cell><cell>97.0</cell><cell>99.5</cell><cell cols="2">94.7 0.0025 mlt_Latn</cell><cell>100.0</cell><cell>99.9</cell><cell>100.0 0.0005</cell></row><row><cell>bel_Cyrl</cell><cell>100.0</cell><cell>100.0</cell><cell cols="2">100.0 0.0000 mni_Beng</cell><cell>100.0</cell><cell>100.0</cell><cell>99.9 0.0000</cell></row><row><cell>bem_Latn</cell><cell>97.5</cell><cell>95.4</cell><cell cols="2">99.7 0.0242 khk_Cyrl</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0 0.0000</cell></row><row><cell>ben_Beng</cell><cell>99.9</cell><cell>99.8</cell><cell cols="2">100.0 0.0010 mos_Latn</cell><cell>96.9</cell><cell>100.0</cell><cell>94.0 0.0000</cell></row><row><cell>bho_Deva</cell><cell>95.5</cell><cell>98.4</cell><cell cols="2">92.7 0.0074 mri_Latn</cell><cell>99.8</cell><cell>99.8</cell><cell>99.8 0.0010</cell></row><row><cell>bjn_Arab</cell><cell>95.1</cell><cell>96.9</cell><cell cols="2">93.4 0.0148 zsm_Latn</cell><cell>93.9</cell><cell>93.8</cell><cell>94.1 0.0311</cell></row><row><cell>bjn_Latn</cell><cell>83.8</cell><cell>74.7</cell><cell cols="2">95.6 0.1621 mya_Mymr</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0 0.0000</cell></row><row><cell>bod_Tibt</cell><cell>99.1</cell><cell>98.3</cell><cell cols="2">100.0 0.0089 nld_Latn</cell><cell>98.8</cell><cell>97.7</cell><cell>100.0 0.0119</cell></row><row><cell>bos_Latn</cell><cell>59.0</cell><cell>80.5</cell><cell cols="2">46.5 0.0563 nno_Latn</cell><cell>98.1</cell><cell>98.0</cell><cell>98.2 0.0099</cell></row><row><cell>bug_Latn</cell><cell>97.4</cell><cell>98.9</cell><cell cols="2">95.9 0.0054 nob_Latn</cell><cell>98.5</cell><cell>97.8</cell><cell>99.3 0.0114</cell></row><row><cell>bul_Cyrl</cell><cell>100.0</cell><cell>100.0</cell><cell cols="2">99.9 0.0000 npi_Deva</cell><cell>99.8</cell><cell>99.5</cell><cell>100.0 0.0025</cell></row><row><cell>cat_Latn</cell><cell>99.3</cell><cell>98.6</cell><cell cols="2">100.0 0.0069 nso_Latn</cell><cell>98.3</cell><cell>97.2</cell><cell>99.5 0.0143</cell></row><row><cell>ceb_Latn</cell><cell>99.9</cell><cell>99.9</cell><cell cols="2">99.8 0.0005 nus_Latn</cell><cell>99.8</cell><cell>99.7</cell><cell>99.9 0.0015</cell></row><row><cell>ces_Latn</cell><cell>99.9</cell><cell>99.8</cell><cell cols="2">100.0 0.0010 nya_Latn</cell><cell>94.9</cell><cell>97.6</cell><cell>92.3 0.0114</cell></row><row><cell>cjk_Latn</cell><cell>87.4</cell><cell>97.3</cell><cell cols="2">79.3 0.0109 oci_Latn</cell><cell>98.6</cell><cell>97.3</cell><cell>100.0 0.0138</cell></row><row><cell>ckb_Arab</cell><cell>100.0</cell><cell>100.0</cell><cell cols="2">100.0 0.0000 gaz_Latn</cell><cell>100.0</cell><cell>99.9</cell><cell>100.0 0.0005</cell></row><row><cell>crh_Latn</cell><cell>98.3</cell><cell>100.0</cell><cell cols="2">96.6 0.0000 ory_Orya</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0 0.0000</cell></row><row><cell>cym_Latn</cell><cell>100.0</cell><cell>100.0</cell><cell cols="2">100.0 0.0000 pag_Latn</cell><cell>99.5</cell><cell>99.8</cell><cell>99.2 0.0010</cell></row><row><cell>dan_Latn</cell><cell>99.7</cell><cell>99.8</cell><cell cols="2">99.5 0.0010 pan_Guru</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0 0.0000</cell></row><row><cell>deu_Latn</cell><cell>99.1</cell><cell>98.2</cell><cell cols="2">100.0 0.0094 pap_Latn</cell><cell>98.8</cell><cell>97.8</cell><cell>99.8 0.0114</cell></row><row><cell>dik_Latn</cell><cell>99.4</cell><cell>100.0</cell><cell cols="2">98.8 0.0000 pol_Latn</cell><cell>98.8</cell><cell>97.6</cell><cell>100.0 0.0124</cell></row><row><cell>diq_Latn</cell><cell>--</cell><cell>0.0</cell><cell cols="2">--0.0010 por_Latn</cell><cell>99.1</cell><cell>98.3</cell><cell>99.9 0.0089</cell></row><row><cell>dyu_Latn</cell><cell>5.6</cell><cell>33.7</cell><cell cols="2">3.1 0.0301 prs_Arab</cell><cell>54.4</cell><cell>94.8</cell><cell>38.1 0.0104</cell></row><row><cell>dzo_Tibt</cell><cell>99.8</cell><cell>100.0</cell><cell cols="2">99.6 0.0000 pbt_Arab</cell><cell>99.8</cell><cell>99.7</cell><cell>99.8 0.0015</cell></row><row><cell>ell_Grek</cell><cell>100.0</cell><cell>100.0</cell><cell cols="2">100.0 0.0000 quy_Latn</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0 0.0000</cell></row><row><cell>eng_Latn</cell><cell>97.0</cell><cell>94.2</cell><cell cols="2">100.0 0.0306 ron_Latn</cell><cell>99.8</cell><cell>99.6</cell><cell>100.0 0.0020</cell></row><row><cell>epo_Latn</cell><cell>99.7</cell><cell>99.4</cell><cell cols="2">100.0 0.0030 run_Latn</cell><cell>97.9</cell><cell>98.1</cell><cell>97.6 0.0094</cell></row><row><cell>est_Latn</cell><cell>99.9</cell><cell>99.8</cell><cell cols="2">100.0 0.0010 rus_Cyrl</cell><cell>100.0</cell><cell>99.9</cell><cell>100.0 0.0005</cell></row><row><cell>eus_Latn</cell><cell>99.9</cell><cell>99.8</cell><cell cols="2">100.0 0.0010 sag_Latn</cell><cell>99.7</cell><cell>99.9</cell><cell>99.5 0.0005</cell></row><row><cell>ewe_Latn</cell><cell>99.8</cell><cell>99.6</cell><cell cols="2">100.0 0.0020 san_Deva</cell><cell>99.6</cell><cell>99.9</cell><cell>99.2 0.0005</cell></row><row><cell>fao_Latn</cell><cell>49.1</cell><cell>100.0</cell><cell cols="2">32.5 0.0000 sat_Olck</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0 0.0000</cell></row><row><cell>pes_Arab</cell><cell>69.8</cell><cell>54.2</cell><cell cols="2">98.0 0.4140 scn_Latn</cell><cell>99.4</cell><cell>98.9</cell><cell>99.8 0.0054</cell></row><row><cell>fij_Latn</cell><cell>100.0</cell><cell>99.9</cell><cell cols="2">100.0 0.0005 shn_Mymr</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0 0.0000</cell></row><row><cell>fin_Latn</cell><cell>100.0</cell><cell>100.0</cell><cell cols="2">100.0 0.0000 sin_Sinh</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0 0.0000</cell></row><row><cell>fon_Latn</cell><cell>99.8</cell><cell>100.0</cell><cell cols="2">99.6 0.0000 slk_Latn</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0 0.0000</cell></row><row><cell>fra_Latn</cell><cell>99.8</cell><cell>99.7</cell><cell cols="2">99.9 0.0015 slv_Latn</cell><cell>99.9</cell><cell>99.7</cell><cell>100.0 0.0015</cell></row><row><cell>fur_Latn</cell><cell>99.8</cell><cell>99.9</cell><cell cols="2">99.7 0.0005 smo_Latn</cell><cell>100.0</cell><cell>100.0</cell><cell>99.9 0.0000</cell></row><row><cell>fuv_Latn</cell><cell>98.4</cell><cell>99.4</cell><cell cols="2">97.4 0.0030 sna_Latn</cell><cell>99.3</cell><cell>98.7</cell><cell>99.9 0.0064</cell></row><row><cell>gla_Latn</cell><cell>99.9</cell><cell>99.7</cell><cell cols="2">100.0 0.0015 snd_Arab</cell><cell>99.8</cell><cell>99.6</cell><cell>100.0 0.0020</cell></row><row><cell>gle_Latn</cell><cell>100.0</cell><cell>99.9</cell><cell cols="2">100.0 0.0005 som_Latn</cell><cell>100.0</cell><cell>99.9</cell><cell>100.0 0.0005</cell></row><row><cell>glg_Latn</cell><cell>99.6</cell><cell>99.6</cell><cell cols="2">99.6 0.0020 sot_Latn</cell><cell>75.4</cell><cell>100.0</cell><cell>60.6 0.0000</cell></row><row><cell>grn_Latn</cell><cell>100.0</cell><cell>100.0</cell><cell cols="2">99.9 0.0000 spa_Latn</cell><cell>99.5</cell><cell>98.9</cell><cell>100.0 0.0054</cell></row><row><cell>guj_Gujr</cell><cell>100.0</cell><cell>100.0</cell><cell cols="2">100.0 0.0000 als_Latn</cell><cell>99.9</cell><cell>99.7</cell><cell>100.0 0.0015</cell></row><row><cell>hat_Latn</cell><cell>99.9</cell><cell>99.9</cell><cell cols="2">99.9 0.0005 srd_Latn</cell><cell>98.2</cell><cell>100.0</cell><cell>96.4 0.0000</cell></row><row><cell>hau_Latn</cell><cell>99.8</cell><cell>99.6</cell><cell cols="2">100.0 0.0020 srp_Cyrl</cell><cell>100.0</cell><cell>99.9</cell><cell>100.0 0.0005</cell></row><row><cell>heb_Hebr</cell><cell>100.0</cell><cell>100.0</cell><cell cols="2">100.0 0.0000 ssw_Latn</cell><cell>99.2</cell><cell>99.8</cell><cell>98.5 0.0010</cell></row><row><cell>hin_Deva</cell><cell>89.2</cell><cell>80.6</cell><cell cols="2">99.9 0.1206 sun_Latn</cell><cell>96.4</cell><cell>95.9</cell><cell>96.9 0.0208</cell></row><row><cell>hne_Deva</cell><cell>94.4</cell><cell>98.6</cell><cell cols="2">90.6 0.0064 swe_Latn</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0 0.0000</cell></row><row><cell>hrv_Latn</cell><cell>73.3</cell><cell>62.3</cell><cell cols="2">88.9 0.2688 swh_Latn</cell><cell>88.4</cell><cell>79.2</cell><cell>100.0 0.1309</cell></row><row><cell>hun_Latn</cell><cell>99.1</cell><cell>98.3</cell><cell cols="2">100.0 0.0089 szl_Latn</cell><cell>99.0</cell><cell>99.9</cell><cell>98.1 0.0005</cell></row><row><cell>hye_Armn</cell><cell>100.0</cell><cell>100.0</cell><cell cols="2">100.0 0.0000 tah_Latn</cell><cell>--</cell><cell>0.0</cell><cell>--0.0005</cell></row><row><cell>ibo_Latn</cell><cell>100.0</cell><cell>99.9</cell><cell cols="2">100.0 0.0005 tam_Taml</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0 0.0000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_107"><head>Table 49 :</head><label>49</label><figDesc></figDesc><table><row><cell></cell><cell>F1</cell><cell></cell><cell>FPR</cell><cell></cell><cell>F1</cell><cell></cell><cell>FPR</cell></row><row><cell></cell><cell cols="4">Ours CLD3 Ours CLD3</cell><cell cols="4">Ours CLD3 Ours CLD3</cell></row><row><cell>umb_Latn</cell><cell>0.80</cell><cell>--</cell><cell>1.59</cell><cell cols="2">--diq_Latn 65.07</cell><cell>--</cell><cell>0.47</cell><cell>--</cell></row><row><cell cols="2">xho_Latn 10.64</cell><cell>6.40</cell><cell>2.27</cell><cell cols="2">1.26 kab_Latn 65.29</cell><cell>--</cell><cell>1.70</cell><cell>--</cell></row><row><cell cols="2">kam_Latn 10.88</cell><cell>--</cell><cell>1.18</cell><cell cols="2">--run_Latn 66.33</cell><cell>--</cell><cell>0.62</cell><cell>--</cell></row><row><cell cols="2">sna_Latn 14.13</cell><cell>11.73</cell><cell>2.47</cell><cell cols="2">2.60 smo_Latn 72.05</cell><cell>78.25</cell><cell>1.63</cell><cell>0.94</cell></row><row><cell cols="2">pag_Latn 15.72</cell><cell>--</cell><cell>2.43</cell><cell cols="2">--bho_Deva 73.06</cell><cell>--</cell><cell>0.03</cell><cell>--</cell></row><row><cell cols="2">tso_Latn 19.75</cell><cell>--</cell><cell>2.60</cell><cell cols="2">--scn_Latn 73.59</cell><cell>--</cell><cell>1.57</cell><cell>--</cell></row><row><cell cols="2">war_Latn 20.72</cell><cell>--</cell><cell>2.56</cell><cell cols="2">--bak_Cyrl 73.95</cell><cell>--</cell><cell>0.44</cell><cell>--</cell></row><row><cell cols="2">ast_Latn 21.24</cell><cell>--</cell><cell>2.70</cell><cell cols="2">--nno_Latn 74.08</cell><cell>--</cell><cell>1.15</cell><cell>--</cell></row><row><cell cols="2">lmo_Latn 24.79</cell><cell>--</cell><cell>3.73</cell><cell cols="2">--srd_Latn 76.64</cell><cell>--</cell><cell>0.05</cell><cell>--</cell></row><row><cell cols="2">zul_Latn 27.22</cell><cell>19.32</cell><cell>1.14</cell><cell cols="2">1.45 ceb_Latn 78.26</cell><cell>66.08</cell><cell>0.49</cell><cell>0.73</cell></row><row><cell cols="2">lim_Latn 28.71</cell><cell>--</cell><cell>3.22</cell><cell cols="2">--oci_Latn 79.59</cell><cell>--</cell><cell>0.37</cell><cell>--</cell></row><row><cell cols="2">kon_Latn 29.03</cell><cell>--</cell><cell>2.18</cell><cell cols="2">--bul_Cyrl 81.86</cell><cell>68.81</cell><cell>0.93</cell><cell>0.92</cell></row><row><cell cols="2">bem_Latn 29.42</cell><cell>--</cell><cell>2.60</cell><cell cols="2">--lus_Latn 84.60</cell><cell>--</cell><cell>0.34</cell><cell>--</cell></row><row><cell cols="2">lug_Latn 29.89</cell><cell>--</cell><cell>1.69</cell><cell cols="2">--kaz_Cyrl 85.81</cell><cell>72.79</cell><cell>0.40</cell><cell>0.66</cell></row><row><cell cols="2">mos_Latn 31.24</cell><cell>--</cell><cell>2.37</cell><cell cols="2">--hat_Latn 86.51</cell><cell>65.02</cell><cell>0.31</cell><cell>1.31</cell></row><row><cell cols="2">twi_Latn 31.97</cell><cell>--</cell><cell>2.52</cell><cell cols="2">--azb_Arab 86.63</cell><cell>--</cell><cell>0.22</cell><cell>--</cell></row><row><cell cols="2">sot_Latn 33.97</cell><cell>38.31</cell><cell>2.87</cell><cell cols="2">1.93 glg_Latn 86.70</cell><cell>68.79</cell><cell>0.28</cell><cell>0.94</cell></row><row><cell cols="2">dyu_Latn 36.32</cell><cell>--</cell><cell>0.52</cell><cell cols="2">--hrv_Latn 87.54</cell><cell>23.72</cell><cell>0.24</cell><cell>2.19</cell></row><row><cell cols="2">min_Latn 37.28</cell><cell>--</cell><cell>2.55</cell><cell cols="2">--san_Deva 88.78</cell><cell>--</cell><cell>0.21</cell><cell>--</cell></row><row><cell cols="2">hau_Latn 43.27</cell><cell>39.63</cell><cell>2.12</cell><cell cols="2">2.16 bod_Tibt 90.67</cell><cell>--</cell><cell>0.11</cell><cell>--</cell></row><row><cell cols="2">kmb_Latn 44.28</cell><cell>--</cell><cell>2.09</cell><cell cols="2">--quy_Latn 90.97</cell><cell>--</cell><cell>0.09</cell><cell>--</cell></row><row><cell cols="2">kin_Latn 46.26</cell><cell>--</cell><cell>2.00</cell><cell cols="2">--bos_Latn 92.68</cell><cell>63.60</cell><cell>0.74</cell><cell>1.43</cell></row><row><cell cols="2">cjk_Latn 47.08</cell><cell>--</cell><cell>1.91</cell><cell cols="2">--mai_Deva 92.73</cell><cell>--</cell><cell>0.15</cell><cell>--</cell></row><row><cell cols="2">ewe_Latn 49.48</cell><cell>--</cell><cell>2.26</cell><cell cols="2">--jav_Latn 92.82</cell><cell>68.95</cell><cell>0.08</cell><cell>0.67</cell></row><row><cell cols="2">lua_Latn 50.19</cell><cell>--</cell><cell>0.88</cell><cell cols="2">--kat_Geor 93.19</cell><cell>76.18</cell><cell>0.08</cell><cell>0.12</cell></row><row><cell cols="2">fuv_Latn 51.06</cell><cell>--</cell><cell>0.89</cell><cell cols="2">--lvs_Latn 93.64</cell><cell>84.49</cell><cell>0.17</cell><cell>0.28</cell></row><row><cell cols="2">ilo_Latn 51.58</cell><cell>--</cell><cell>2.12</cell><cell cols="2">--slv_Latn 93.95</cell><cell>76.94</cell><cell>0.10</cell><cell>0.32</cell></row><row><cell cols="2">tum_Latn 51.59</cell><cell>--</cell><cell>2.00</cell><cell cols="2">--kbp_Latn 94.05</cell><cell>--</cell><cell>0.20</cell><cell>--</cell></row><row><cell cols="2">vec_Latn 51.85</cell><cell>--</cell><cell>1.70</cell><cell cols="2">--kik_Latn 94.81</cell><cell>--</cell><cell>0.10</cell><cell>--</cell></row><row><cell cols="2">tsn_Latn 52.97</cell><cell>--</cell><cell>2.48</cell><cell cols="2">--sun_Latn 95.90</cell><cell>74.26</cell><cell>0.07</cell><cell>0.79</cell></row><row><cell cols="2">pap_Latn 57.64</cell><cell>--</cell><cell>2.05</cell><cell cols="2">--kmr_Latn 96.10</cell><cell>63.98</cell><cell>0.10</cell><cell>1.59</cell></row><row><cell cols="2">tgk_Cyrl 59.66</cell><cell>51.40</cell><cell>0.80</cell><cell cols="2">0.79 kir_Cyrl 96.98</cell><cell>87.37</cell><cell>0.06</cell><cell>0.37</cell></row><row><cell cols="2">eus_Latn 61.32</cell><cell>64.25</cell><cell>0.91</cell><cell cols="2">0.60 npi_Deva 97.17</cell><cell>76.76</cell><cell>0.16</cell><cell>2.61</cell></row><row><cell cols="2">fij_Latn 61.54</cell><cell>--</cell><cell>1.66</cell><cell cols="2">--khk_Cyrl 98.47</cell><cell>96.05</cell><cell>0.04</cell><cell>0.09</cell></row><row><cell cols="2">fon_Latn 62.85</cell><cell>--</cell><cell>1.81</cell><cell cols="2">--tuk_Latn 99.19</cell><cell>--</cell><cell>0.03</cell><cell>--</cell></row><row><cell cols="2">sag_Latn 63.30</cell><cell>--</cell><cell>1.33</cell><cell cols="2">--sat_Olck 99.60</cell><cell>--</cell><cell>--</cell><cell>--</cell></row></table><note>LID results on all Flores-200 languages.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_108"><head>Table 50 :</head><label>50</label><figDesc>False Positive Rates (FPR) are reported on top of F1 scores, which can be misleading when an LID system is eventually to be applied on web data with different class balances than the development set. Cells corresponding to languages unsupported by CLD3 are left blank.</figDesc><table /><note>Comparison of CLD3 and our model on a challenge set built from human annotations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_109"><head>Table 51 :Appendix E. Bringing it All Together E.1 Preparing the Data</head><label>51</label><figDesc>Language Pairs in the Ablation Dataset used in Section 6 E.1.1 Primary Dataset Composition For reference, we summarize in Table 52 some of the main datasets used in training our model NLLB-200. Our data was largely downloaded via OPUS</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_110"><head></head><label></label><figDesc>49. https://github.com/facebookresearch/fairseq/tree/nllb/data</figDesc><table><row><cell>Corpus Name</cell><cell>Citation</cell><cell cols="2"># Directions # Languages</cell></row><row><cell cols="2">AAU Ethiopian Languages Abate et al. (2018)</cell><cell>3</cell><cell>4</cell></row><row><cell>AI4D</cell><cell>Degila et al. (2020); Siminyu et al. (2021)</cell><cell>3</cell><cell>5</cell></row><row><cell>DGT</cell><cell>Tiedemann (2012)</cell><cell>94</cell><cell>24</cell></row><row><cell>ECB</cell><cell>Tiedemann (2012)</cell><cell>74</cell><cell>19</cell></row><row><cell>EMEA</cell><cell>Tiedemann (2012)</cell><cell>86</cell><cell>22</cell></row><row><cell>English-Twi</cell><cell>Azunre et al. (2021a,b)</cell><cell>2</cell><cell>1</cell></row><row><cell>EU Bookshop</cell><cell>Skadi?? et al. (2014b)</cell><cell>160</cell><cell>38</cell></row><row><cell>GlobalVoices</cell><cell>Tiedemann (2012)</cell><cell>235</cell><cell>41</cell></row><row><cell>HornMT</cell><cell>Hadgu et al. (2021)</cell><cell>10</cell><cell>. 5</cell></row><row><cell>InfoPankki v1</cell><cell>Tiedemann (2012)</cell><cell>30</cell><cell>12</cell></row><row><cell cols="2">QCRI Educational Domain Abdelali et al. (2014)</cell><cell>866</cell><cell>135</cell></row><row><cell>JHU Bible</cell><cell>McCarthy et al. (2020)</cell><cell>300</cell><cell>155</cell></row><row><cell>MADAR</cell><cell>Bouamor et al. (2019)</cell><cell>5</cell><cell>6</cell></row><row><cell>Mburisano</cell><cell>Marais et al. (2021)</cell><cell>7</cell><cell>8</cell></row><row><cell>MENYO-20k</cell><cell>Adelani et al. (2021)</cell><cell>2</cell><cell>1</cell></row><row><cell>MultiIndicMT</cell><cell>Nakazawa et al. (2021)</cell><cell>10</cell><cell>11</cell></row><row><cell>NLLB-Seed</cell><cell>This work</cell><cell>39</cell><cell>40</cell></row><row><cell>OpenSubtitles v2018</cell><cell>Lison and Tiedemann (2016)</cell><cell>370</cell><cell>53</cell></row><row><cell>Tanzil</cell><cell>Tiedemann (2012)</cell><cell>273</cell><cell>38</cell></row><row><cell>Tatoeba</cell><cell>Tiedemann (2012)</cell><cell>493</cell><cell>143</cell></row><row><cell>Tico19 v20201028</cell><cell>Anastasopoulos et al. (2020)</cell><cell>48</cell><cell>34</cell></row><row><cell>TWB-Gamayun</cell><cell>?ktem et al. (2020)</cell><cell>4</cell><cell>6</cell></row><row><cell cols="2">United Nations Resolutions Rafalovitch and Dale (2014)</cell><cell>20</cell><cell>7</cell></row><row><cell>Turkic Interlingua (TIL)</cell><cell>Mirzakhalov et al. (2021)</cell><cell>46</cell><cell>11</cell></row><row><cell>Wikimedia v20210402</cell><cell>Tiedemann (2012)</cell><cell>582</cell><cell>154</cell></row><row><cell>XhosaNavy</cell><cell>Tiedemann (2012)</cell><cell>2</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_111"><head>Table 52 :</head><label>52</label><figDesc>Summary of some of the main datasets used in training NLLB-200. Direction counts do not include reverse directions. For the different curriculum setups, here are the list of directions used:</figDesc><table><row><cell></cell><cell></cell><cell>Baseline</cell><cell>219M</cell><cell>514M</cell><cell>1.4B</cell></row><row><cell></cell><cell>18</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>17</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Avg BLEU</cell><cell>16</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>15</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2 1 9 M</cell><cell>5 1 4 M</cell><cell>7 6 6 M</cell><cell>1 . 4 B</cell><cell>1 . 9 B</cell></row></table><note>Figure 43: Importance of BT Quality on Model Scalability.umb_Latn, wol_Latn, zul_Latn. We experiment along two axes: BT-generating model size (219M and 1.4B) and final model size (219M, 514M, 776M, 1.4B). Our results in Figure 43 indicate that when training on backtranslations generated with small models, performance quickly plateaus.E.1.3 Training Directions and Curriculum Buckets. Training. Here we list of the training directions available for different data sources used for training after filtering.? Primary : https://github.com/facebookresearch/fairseq/tree/nllb/examples/nllb/ modeling/scripts/flores200/lang_pairs_primary.txt ? Mined : https://github.com/facebookresearch/fairseq/tree/nllb/examples/nllb/modeling/ scripts/flores200/lang_pairs_mine.txt ? Primary+Mined : https://github.com/facebookresearch/fairseq/tree/nllb/examples/ nllb/modeling/scripts/flores200/lang_pairs_primary_mine.txt ? Primary+Mined+MmtBT+SmtBT : https://github.com/facebookresearch/fairseq/ tree/nllb/examples/nllb/modeling/scripts/flores200/lang_pairs.txt Curriculum.(a) Step 0?170k: https://github.com/facebookresearch/fairseq/tree/nllb/examples/ nllb/modeling/scripts/flores200/final_lang_pairs_cl3.txt (b) Step 170k?230k: https://github.com/facebookresearch/fairseq/tree/nllb/examples/ nllb/modeling/scripts/flores200/final_lang_pairs_cl2.txt</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_112"><head>Table 53 :</head><label>53</label><figDesc>Step 200k?300k: https://github.com/facebookresearch/fairseq/tree/nllb/examples/ nllb/modeling/scripts/flores200/lang_pairs.txt</figDesc><table /><note>Comparison against MMTAfrica on Flores-101 devtest set. We compare non-English-centric performance in this table. We report spBLEU/chrF ++ and bold the best score. NLLB-200 outperforms on most translation directions.(c) Step 230k?270k: https://github.com/facebookresearch/fairseq/tree/nllb/examples/ nllb/modeling/scripts/flores200/final_lang_pairs_cl1.txt (d) Step 270k?300k: https://github.com/facebookresearch/fairseq/tree/nllb/examples/ nllb/modeling/scripts/flores200/lang_pairs.txt 2. Naive 2-phase curriculum : (a) Step 0?200k: https://github.com/facebookresearch/fairseq/tree/nllb/examples/ nllb/modeling/scripts/flores200/cl1_lang_pairs.txt (b)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_113"><head>Table 53</head><label>53</label><figDesc>, we compare against MMTAfrica (Emezue and Dossou, 2021) on non-Englishcentric translation performance on Flores-101 devtest.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">eng_Latn-xx</cell><cell></cell><cell></cell><cell></cell><cell>xx-eng_Latn</cell></row><row><cell></cell><cell>all</cell><cell>high</cell><cell></cell><cell>low</cell><cell>v.low</cell><cell>all</cell><cell></cell><cell>high</cell><cell>low</cell><cell>v.low</cell></row><row><cell cols="7">Google Translate 37.5/54.3 42.5/58.3 32.3/50.3 27.0/46.5 39.0/59.9</cell><cell cols="2">42.2/62.5</cell><cell>35.9/57.1</cell><cell>35.8/57.0</cell></row><row><cell>NLLB-200</cell><cell>34.5/51.6</cell><cell cols="2">38.5/55.0</cell><cell>30.3/48.2</cell><cell cols="4">25.7/45.0 43.1/62.1 45.0/63.7 41.3/60.4 41.1/60.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>all</cell><cell>high</cell><cell>low</cell><cell></cell><cell>v.low</cell></row><row><cell></cell><cell cols="7">Google Translate 38.3/57.1 42.3/60.4 34.1/53.7</cell><cell>31.3/51.7</cell></row><row><cell></cell><cell cols="2">NLLB-200</cell><cell cols="6">38.8/56.9 41.7/59.3 35.8/54.3 33.4/52.6</cell></row></table><note>E.2.2 Comparison against Google Translate Following results in Section 8.3.2, we present the complete comparison against Google Translate (GT) on 206 English-centric directions(104 high-resource and 102 low-resource) which overlap with Flores-200. While NLLB-200 performs better on xx-eng_Latn</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_114"><head>Table 54 :</head><label>54</label><figDesc>Comparison We evaluate on allEnglish-centric directions that overlap between Flores-200 and Google's Translation API. We report both spBLEU/chrF ++ and bold the best score. We observe that NLLB-200 outperforms on xx-eng_Latn and overall average.</figDesc><table /><note>on 206 Flores-200 devtest directions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_115"><head>Percentage of Flores-200 devtest lines with candidate toxic terms detected</head><label></label><figDesc>in translations with NLLB-200 both into English and out of English.</figDesc><table><row><cell>0.1% Figure 44: Corpus language 1.0% 10.0% Candidate Toxicity kon_Latn knc_Latn knc_Arab kmr_Latn kmb_Latn kir_Cyrl kin_Latn kik_Latn khm_Khmr khk_Cyrl kea_Latn kbp_Latn kaz_Cyrl kat_Geor kas_Deva kas_Arab kan_Knda kam_Latn kac_Latn kab_Latn jpn_Jpan jav_Latn ita_Latn isl_Latn ind_Latn ilo_Latn ibo_Latn hye_Armn hun_Latn hrv_Latn hne_Deva hin_Deva heb_Hebr hau_Latn hat_Latn guj_Gujr grn_Latn glg_Latn gle_Latn gla_Latn gaz_Latn fuv_Latn fur_Latn fra_Latn fon_Latn fin_Latn fij_Latn fao_Latn ewe_Latn eus_Latn est_Latn epo_Latn ell_Grek dzo_Tibt dyu_Latn dik_Latn deu_Latn dan_Latn cym_Latn crh_Latn ckb_Arab cjk_Latn ces_Latn ceb_Latn cat_Latn bul_Cyrl bug_Latn bos_Latn bod_Tibt bjn_Latn bjn_Arab bho_Deva ben_Beng bem_Latn bel_Cyrl ban_Latn bam_Latn bak_Cyrl azj_Latn azb_Arab ayr_Latn awa_Deva ast_Latn asm_Beng arz_Arab ary_Arab ars_Arab arb_Arab apc_Arab amh_Ethi als_Latn aka_Latn ajp_Arab afr_Latn aeb_Arab acq_Arab acm_Arab ace_Latn ace_Arab Mean Over All Low Resource Mean Over All High Resource Mean Over All Into English Out of English IWSLT arb IWSLT deu IWSLT fra IWSLT ita IWSLT jpn IWSLT kor IWSLT nld IWSLT pes IWSLT pol IWSLT ron IWSLT rus IWSLT vie WMT ces WMT deu WMT est WMT fin WMT fra WMT guj WMT hin WMT kaz WMT lit WMT lvs WMT ron WMT rus WMT spa WMT tur WMT zho_Hans WMT 19 zul_Latn zsm_Latn zho_Hant zho_Hans yue_Hant yor_Latn ydd_Hebr xho_Latn wol_Latn war_Latn vie_Latn vec_Latn uzn_Latn urd_Arab umb_Latn ukr_Cyrl uig_Arab tzm_Tfng twi_Latn tur_Latn tum_Latn tuk_Latn tso_Latn tsn_Latn tpi_Latn tir_Ethi tha_Thai tgl_Latn tgk_Cyrl tel_Telu tat_Cyrl taq_Tfng taq_Latn tam_Taml szl_Latn swh_Latn swe_Latn sun_Latn ssw_Latn srp_Cyrl srd_Latn spa_Latn sot_Latn som_Latn snd_Arab sna_Latn smo_Latn slv_Latn slk_Latn sin_Sinh shn_Mymr scn_Latn sat_Olck san_Deva sag_Latn rus_Cyrl run_Latn ron_Latn quy_Latn prs_Arab por_Latn pol_Latn plt_Latn pes_Arab pbt_Arab pap_Latn pan_Guru pag_Latn ory_Orya oci_Latn nya_Latn nus_Latn nso_Latn npi_Deva nob_Latn nno_Latn nld_Latn mya_Mymr mri_Latn mos_Latn mni_Beng mlt_Latn mkd_Cyrl min_Latn mar_Deva mal_Mlym mai_Deva mag_Deva lvs_Latn lus_Latn luo_Latn lug_Latn lua_Latn ltz_Latn ltg_Latn lmo_Latn lit_Latn lin_Latn lim_Latn lij_Latn lao_Laoo kor_Hang reference tst2017 tst2017.mltlng 0.1% tst2017 tst2017.mltlng tst2017 tst2017 tst2017.mltlng tst2014 tst2014 tst2017.mltlng tst2014 tst2015 WMT 18 WMT 14 WMT 18 WMT 19 WMT 14 WMT 19 WMT 14 WMT 19 WMT 19 WMT 17 WMT 16 WMT 19 WMT 13 WMT 18 WAT mya WAT 19 -ALT corpus 1.0% Candidate Toxicity WAT khm WAT 19 -ALT corpus WAT tam WAT 19 -Entam WAT hin WAT 19 -IITB</cell><cell>10.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_116"><head>Table 55 :</head><label>55</label><figDesc>Public benchmarks. For corpora with multiple editions we specify in the reference column the version we used for testing.</figDesc><table><row><cell cols="2">IWSLT arb</cell><cell>Apply QCRI arabic normalizer https://alt.qcri.org/tools/arabic-normalizer/</cell></row><row><cell cols="2">IWSLT kor</cell><cell>Segment with Mecab-Ko https://konlpy.org/en/v0.3.0/install/</cell></row><row><cell cols="2">IWSLT jpn</cell><cell>Segment with KyTea http://www.phontron.com/kytea/</cell></row><row><cell>WAT</cell><cell cols="2">khm Tokenize with WAT's kmseg Nakazawa et al. (2021)</cell></row><row><cell>WAT</cell><cell cols="2">mya Tokenize with WAT's myseg Nakazawa et al. (2021)</cell></row><row><cell>WAT</cell><cell>hin</cell><cell>Tokenize with Indic-NLP Libraray https://anoopkunchukuttan.github.io/indic_nlp_library/</cell></row><row><cell>WMT</cell><cell>ron</cell><cell>Apply special normalization and remove diacritics (Sennrich et al., 2016b) https://github.com/rsennrich/wmt16-scripts/tree/master/preprocess</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_117"><head>Table 56 :</head><label>56</label><figDesc></figDesc><table><row><cell>?</cell><cell>eng</cell><cell>afr</cell><cell>nso</cell><cell>sot</cell><cell>ssw</cell><cell>tso</cell><cell>tsn</cell><cell>xho</cell><cell>zul</cell></row><row><cell>eng</cell><cell>-</cell><cell cols="8">36.9/63.4 24.3/51.6 15.3/42.8 9.5/44.0 18.2/46.7 21.3/48.7 10.8/44.6 13.0/47.3</cell></row><row><cell cols="2">afr 39.6/62.6</cell><cell>-</cell><cell cols="7">26.7/53.2 19.3/45.9 10.2/45.2 17.4/46.0 22.6/49.3 11.0/45.0 13.2/47.4</cell></row><row><cell cols="3">nso 29.5/53.1 28.9/54.0</cell><cell>-</cell><cell cols="6">16.1/42.5 8.6/41.8 17.5/44.9 22.1/47.9 9.6/41.9 12.9/45.2</cell></row><row><cell cols="4">sot 28.0/51.7 27.3/52.9 24.6/50.2</cell><cell>-</cell><cell cols="5">8.7/42.7 17.4/45.2 21.8/47.5 9.2/41.4 11.9/44.4</cell></row><row><cell cols="5">ssw 25.7/49.3 24.2/49.5 20.9/46.3 14.8/39.6</cell><cell>-</cell><cell cols="4">14.8/41.9 18.8/43.8 8.6/38.5 11.3/43.2</cell></row><row><cell cols="6">tso 26.9/50.3 25.8/51.2 20.8/46.6 12.6/38.9 7.9/39.9</cell><cell>-</cell><cell cols="3">19.0/44.8 8.0/39.2 10.1/42.1</cell></row><row><cell cols="2">tsn 25.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Evaluation details for irregular directions in public benchmarks. For the directions listed above, we apply special pre-processing on the hypotheses and references before measuring BLEU scores.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_118"><head>Table 57 :</head><label>57</label><figDesc></figDesc><table><row><cell>MADAR</cell><cell></cell><cell>Tico</cell><cell></cell></row><row><cell>arb-xx</cell><cell>xx-arb</cell><cell>eng-xx</cell><cell>xx-eng</cell></row><row><cell cols="4">ary 17.2/43.4 26.2/47.2 amh 13.7/36.7 37.6/60.2</cell></row><row><cell cols="4">acm 15.4/42.9 26.4/47.7 ben 22.6/52.3 52.1/72.3</cell></row><row><cell cols="4">apc 20.2/46.6 26.5/48.3 ckb 13.7/46.4 40.8/61.9</cell></row><row><cell cols="4">ars 19.8/45.5 27.8/49.3 hau 29.2/53.8 41.6/60.9</cell></row><row><cell cols="4">acq 11.3/37.9 28.6/50.0 kmr 17.4/45.8 41.8/62.5</cell></row><row><cell cols="2">ajp 17.6/43.5 27.5/49.1 mya</cell><cell cols="2">7.0/42.1 37.9/61.4</cell></row><row><cell cols="4">aeb 14.5/40.9 21.0/42.1 npi 23.1/55.2 54.8/74.3</cell></row><row><cell cols="4">arz 17.5/45.2 27.4/49.5 pbt 26.2/49.9 45.7/66.5</cell></row><row><cell></cell><cell>som</cell><cell cols="2">9.6/31.7 19.5/36.4</cell></row><row><cell></cell><cell cols="3">tgl 49.7/70.5 65.0/79.5</cell></row><row><cell></cell><cell>tir</cell><cell cols="2">9.6/30.0 33.7/56.1</cell></row></table><note>Scores of NLLB-200 on Autshumato's test set. We report BLEU/chrF ++ . Low-resource languages are underlined.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_119"><head>Table 58 :</head><label>58</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">eng_Latn-xx fra_Latn-xx spa_Latn-xx</cell></row><row><cell>asm_Beng</cell><cell>34.9</cell><cell>31</cell><cell>29.6</cell></row><row><cell>ast_Latn</cell><cell>56</cell><cell>50.6</cell><cell>47.1</cell></row><row><cell>ayr_Latn</cell><cell>29.2</cell><cell>29.1</cell><cell>28.4</cell></row><row><cell>bak_Cyrl</cell><cell>49.1</cell><cell>44.5</cell><cell>41.5</cell></row><row><cell>bem_Latn</cell><cell>41.7</cell><cell>38.6</cell><cell>37</cell></row><row><cell>ckb_Arab</cell><cell>45.1</cell><cell>37.4</cell><cell>36.6</cell></row><row><cell>hau_Latn</cell><cell>52.9</cell><cell>46.8</cell><cell>43.8</cell></row><row><cell>ibo_Latn</cell><cell>43.6</cell><cell>39.3</cell><cell>37.4</cell></row><row><cell>ilo_Latn</cell><cell>55.6</cell><cell>50.1</cell><cell>47.1</cell></row><row><cell>isl_Latn</cell><cell>51.3</cell><cell>46.3</cell><cell>42.2</cell></row><row><cell>kon_Latn</cell><cell>49.4</cell><cell>47.5</cell><cell>45.4</cell></row><row><cell>lin_Latn</cell><cell>51.9</cell><cell>49.8</cell><cell>47.3</cell></row><row><cell>lug_Latn</cell><cell>39.6</cell><cell>36.4</cell><cell>34.7</cell></row><row><cell>nso_Latn</cell><cell>54.6</cell><cell>47.1</cell><cell>44.6</cell></row><row><cell>oci_Latn</cell><cell>60.6</cell><cell>54.2</cell><cell>45.9</cell></row><row><cell>orm_Latn</cell><cell>37.5</cell><cell>34.8</cell><cell>33.7</cell></row><row><cell>quy_Latn</cell><cell>29.8</cell><cell>29.4</cell><cell>29.2</cell></row><row><cell>ssw_Latn</cell><cell>46.1</cell><cell>42</cell><cell>39.8</cell></row><row><cell>tir_Ethi</cell><cell>24.6</cell><cell>22.1</cell><cell>21.2</cell></row><row><cell>tsn_Latn</cell><cell>49.3</cell><cell>45.8</cell><cell>43.8</cell></row><row><cell>tso_Latn</cell><cell>51.4</cell><cell>47.3</cell><cell>44.5</cell></row><row><cell>wol_Latn</cell><cell>31.8</cell><cell>30.3</cell><cell>28.5</cell></row><row><cell>yue_Hant</cell><cell>21.3</cell><cell>19.8</cell><cell>17.8</cell></row><row><cell>zho_Hans</cell><cell>24.2</cell><cell>21.8</cell><cell>19.7</cell></row><row><cell>zul_Latn</cell><cell>54.6</cell><cell>48.1</cell><cell>44.5</cell></row><row><cell cols="2">por_Latn-oci_Latn</cell><cell>54.2</cell><cell></cell></row><row><cell cols="2">cat_Latn-oci_Latn</cell><cell>52.9</cell><cell></cell></row><row><cell cols="2">zho_Hans-yue_Hant</cell><cell>23.3</cell><cell></cell></row><row><cell cols="2">rus_Cyrl-bak_Cyrl</cell><cell>47.8</cell><cell></cell></row></table><note>Scores of NLLB-200 on additional directions from MADAR and Tico. We report BLEU/chrF ++ . Low-resource languages are underlined. Tico scores, where available, come from Anastasopoulos et al. (2020).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_120"><head>Table 59 :</head><label>59</label><figDesc>Flores-200 devtest chrf++ performance for offline 1.3B parameter Wikipedia student model by language pair. .00 1.00 1.00 1.00 1.00 0.94 0.89 0.97 0.94 0.70 0.94 0.88 0.86 0.91 0.80 0.93 0.92 0.83 0.86 0.93 0.69 0.94 0.88 0.82 0.93 0.94 0.90 0.94 0.95 0.90 0.90 0.82 0.94 0.77 0.94 0.89 0.93 0.95 0.90 0.94 0.95 0.92 0.89 0.77 0.94 0.93 0.94 0.92 0.93 0.93 0.71 0.81 0.86 0.74 0.89 0.90 0.88 0.95 0.90 0.88 0.83 0.92 0.93 0.91 0.89 0.91 0.93 0.86 0.94 0.93 0.93 0.94 0.92 0.91 0.91 0.93 0.93 0.94 0.93 0.93 0.94 0.93 0.94 0.93 0.94 0.93 0.87 0.93 0.94 0.94 0.95 0.94 0.91 0.94 0.94 0.94 0.94 0.94 0.93 0.94 0.92 0.93 0.89 0.95 0.93 0.94 0.93 0.89 0.94 0.93 0.91 0.88 0.94 0.90 0.91 0.89 0.95 0.89 0.93 0.90 0.94 0.92 0.77 0.89 0.88 0.92 0.90 0.90 0.89 0.93 0.94 0.92 0.94 0.92 0.93 0.93 0.94 0.89 0.94 0.91 0.91 0.92 0.93 0.93 0.93 0.94 0.83 0.82 0.83 0.95 0.88 0.75 0.92 0.94 0.94 0.93 0.89 0.87 0.92 0.94 0.94 0.93 0.93 0.92 0.82 0.91 0.94 0.94 0.94 0.94 0.94 0.92 0.89 0.90 0.93 0.92 0.90 0.91 0.92 1.00 1.00 1.00 0.99 1.00 1.00 0.93 0.90 0.96 0.94 0.70 0.94 0.88 0.86 0.91 0.80 0.92 0.92 0.83 0.86 0.93 0.69 0.94 0.87 0.82 0.92 0.94 0.90 0.94 0.94 0.90 0.90 0.82 0.93 0.77 0.94 0.88 0.93 0.95 0.89 0.94 0.95 0.92 0.89 0.77 0.94 0.92 0.94 0.92 0.92 0.93 0.70 0.81 0.85 0.73 0.88 0.89 0.87 0.94 0.89 0.87 0.82 0.92 0.92 0.90 0.88 0.91 0.92 0.87 0.94 0.93 0.93 0.94 0.92 0.91 0.91 0.93 0.93 0.94 0.93 0.93 0.93 0.93 0.93 0.93 0.94 0.93 0.88 0.93 0.94 0.94 0.94 0.94 0.91 0.94 0.94 0.93 0.94 0.93 0.92 0.94 0.92 0.92 0.89 0.95 0.93 0.94 0.93 0.88 0.94 0.93 0.90 0.88 0.93 0.90 0.90 0.88 0.95 0.89 0.93 0.89 0.94 0.92 0.77 0.90 0.87 0.91 0.90 0.90 0.88 0.93 0.93 0.92 0.94 0.92 0.93 0.93 0.94 0.89 0.93 0.91 0.91 0.92 0.92 0.93 0.92 0.94 0.83 0.81 0.83 0.95 0.88 0.75 0.92 0.93 0.93 0.93 0.88 0.86 0.91 0.93 0.93 0.93 0.93 0.92 0.82 0.90 0.93 0.94 0.93 0.93 0.94 0.91 0.88 0.89 0.92 0.91 0.89 0.91 0.92 1.00 1.00 1.00 1.00 1.00 1.00 0.92 0.88 0.98 0.93 0.69 0.93 0.87 0.85 0.90 0.79 0.91 0.91 0.82 0.85 0.92 0.68 0.93 0.86 0.81 0.91 0.93 0.89 0.93 0.93 0.88 0.89 0.81 0.92 0.76 0.93 0.87 0.92 0.94 0.88 0.93 0.93 0.91 0.88 0.76 0.93 0.91 0.93 0.90 0.91 0.92 0.70 0.80 0.84 0.72 0.88 0.89 0.87 0.93 0.88 0.86 0.82 0.91 0.91 0.90 0.87 0.90 0.91 0.85 0.93 0.92 0.92 0.93 0.91 0.90 0.90 0.92 0.91 0.92 0.92 0.92 0.92 0.91 0.92 0.92 0.93 0.92 0.86 0.92 0.93 0.93 0.93 0.93 0.90 0.93 0.93 0.92 0.93 0.92 0.91 0.93 0.91 0.91 0.88 0.94 0.92 0.93 0.92 0.88 0.93 0.92 0.90 0.87 0.92 0.89 0.89 0.87 0.94 0.88 0.92 0.88 0.93 0.91 0.76 0.88 0.86 0.91 0.89 0.89 0.87 0.92 0.92 0.91 0.93 0.90 0.92 0.92 0.93 0.88 0.92 0.90 0.90 0.91 0.91 0.92 0.91 0.93 0.82 0.81 0.82 0.94 0.87 0.74 0.91 0.92 0.92 0.92 0.87 0.85 0.90 0.92 0.92 0.91 0.92 0.90 0.81 0.89 0.92 0.93 0.92 0.92 0.93 0.90 0.87 0.88 0.91 0.90 0.89 0.90 0.91 1.00 0.99 1.00 1.00 1.00 1.00 0.94 0.90 0.97 0.95 0.70 0.95 0.88 0.86 0.92 0.80 0.93 0.93 0.84 0.86 0.94 0.69 0.95 0.88 0.82 0.93 0.95 0.91 0.95 0.95 0.90 0.90 0.82 0.94 0.77 0.95 0.89 0.93 0.96 0.90 0.95 0.95 0.93 0.89 0.77 0.94 0.94 0.95 0.92 0.94 0.94 0.71 0.82 0.86 0.74 0.89 0.90 0.89 0.95 0.91 0.88 0.83 0.93 0.94 0.92 0.89 0.92 0.94 0.86 0.95 0.94 0.94 0.95 0.93 0.92 0.92 0.94 0.93 0.95 0.94 0.94 0.95 0.93 0.94 0.94 0.95 0.94 0.88 0.94 0.94 0.95 0.95 0.95 0.91 0.95 0.95 0.94 0.95 0.94 0.94 0.95 0.93 0.94 0.90 0.95 0.94 0.94 0.94 0.90 0.95 0.94 0.92 0.88 0.94 0.90 0.92 0.90 0.95 0.89 0.94 0.90 0.95 0.93 0.77 0.90 0.88 0.92 0.90 0.90 0.89 0.93 0.95 0.92 0.95 0.92 0.94 0.94 0.95 0.89 0.94 0.91 0.91 0.92 0.93 0.94 0.93 0.95 0.83 0.82 0.84 0.95 0.88 0.75 0.92 0.94 0.94 0.94 0.90 0.88 0.92 0.94 0.94 0.93 0.94 0.92 0.82 0.91 0.94 0.95 0.94 0.94 0.95 0.92 0.89 0.90 0.93 0.92 0.91 0.92 0.93 1.00 1.00 1.00 1.00 1.00 1.00 0.92 0.88 0.98 0.93 0.70 0.93 0.87 0.85 0.90 0.79 0.92 0.92 0.83 0.85 0.92 0.68 0.93 0.87 0.81 0.92 0.93 0.89 0.93 0.94 0.89 0.89 0.81 0.93 0.76 0.93 0.88 0.92 0.94 0.89 0.93 0.94 0.91 0.88 0.76 0.93 0.92 0.93 0.91 0.92 0.92 0.70 0.80 0.85 0.73 0.88 0.89 0.87 0.94 0.89 0.87 0.82 0.91 0.92 0.90 0.88 0.90 0.92 0.85 0.93 0.92 0.92 0.93 0.91 0.90 0.90 0.92 0.92 0.93 0.92 0.92 0.93 0.92 0.93 0.92 0.93 0.92 0.86 0.92 0.93 0.93 0.94 0.93 0.90 0.93 0.93 0.93 0.93 0.92 0.92 0.93 0.91 0.92 0.88 0.94 0.92 0.93 0.92 0.88 0.93 0.92 0.90 0.87 0.93 0.89 0.89 0.88 0.94 0.88 0.92 0.89 0.93 0.91 0.76 0.88 0.87 0.91 0.89 0.89 0.88 0.92 0.93 0.91 0.93 0.90 0.92 0.92 0.93 0.88 0.93 0.90 0.90 0.91 0.91 0.92 0.92 0.93 0.82 0.81 0.82 0.94 0.87 0.75 0.91 0.93 0.92 0.92 0.88 0.86 0.90 0.92 0.93 0.92 0.92 0.90 0.81 0.90 0.93 0.93 0.93 0.92 0.93 0.91 0.87 0.89 0.92 0.91 0.89 0.90 0.91 1.00 1.00 1.00 1.00 1.00 1.00 0.94 0.90 0.97 0.95 0.71 0.94 0.88 0.86 0.92 0.81 0.93 0.93 0.83 0.86 0.94 0.69 0.94 0.88 0.83 0.93 0.94 0.90 0.94 0.95 0.90 0.90 0.83 0.94 0.77 0.94 0.89 0.93 0.95 0.90 0.95 0.95 0.93 0.89 0.77 0.94 0.93 0.94 0.92 0.93 0.94 0.71 0.82 0.86 0.74 0.90 0.90 0.89 0.95 0.90 0.88 0.84 0.93 0.93 0.91 0.89 0.92 0.93 0.87 0.94 0.93 0.93 0.95 0.93 0.92 0.92 0.94 0.93 0.94 0.94 0.94 0.94 0.93 0.94 0.94 0.95 0.93 0.88 0.93 0.94 0.95 0.95 0.95 0.91 0.95 0.95 0.94 0.95 0.94 0.93 0.94 0.93 0.93 0.90 0.95 0.94 0.94 0.94 0.90 0.94 0.93 0.91 0.88 0.94 0.90 0.91 0.90 0.95 0.89 0.94 0.90 0.95 0.92 0.77 0.90 0.88 0.92 0.90 0.90 0.89 0.93 0.94 0.92 0.94 0.92 0.94 0.93 0.95 0.90 0.94 0.91 0.91 0.92 0.93 0.94 0.93 0.95 0.83 0.82 0.83 0.95 0.89 0.76 0.92 0.94 0.94 0.94 0.89 0.87 0.92 0.94 0.94 0.93 0.94 0.92 0.82 0.91 0.94 0.94 0.94 0.94 0.95 0.92 0.89 0.90 0.93 0.92 0.90 0.91 0.92 0.94 0.93 0.92 0.94 0.92 0.94 1.00 0.97 0.85 0.97 0.73 0.97 0.92 0.89 0.97 0.81 0.95 0.95 0.85 0.90 0.97 0.73 0.97 0.89 0.87 0.95 0.97 0.93 0.97 0.97 0.92 0.92 0.86 0.96 0.81 0.97 0.91 0.95 0.97 0.92 0.97 0.97 0.95 0.91 0.81 0.97 0.96 0.97 0.97 0.98 0.98 0.71 0.82 0.86 0.74 0.92 0.92 0.91 0.97 0.91 0.89 0.85 0.95 0.96 0.93 0.92 0.94 0.95 0.91 0.99 0.99 0.98 0.99 0.95 0.98 0.97 0.98 0.98 0.99 0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.95 0.98 0.99 0.99 0.99 0.99 0.97 0.98 0.99 0.99 0.99 0.98 0.98 0.99 0.96 0.96 0.95 0.98 0.98 0.97 0.98 0.92 0.98 0.98 0.96 0.93 0.98 0.95 0.94 0.92 0.98 0.94 0.97 0.94 0.98 0.96 0.86 0.94 0.92 0.96 0.94 0.95 0.93 0.97 0.98 0.95 0.97 0.96 0.98 0.97 0.98 0.94 0.98 0.96 0.95 0.96 0.98 0.97 0.99 0.99 0.87 0.85 0.89 0.97 0.93 0.80 0.94 0.98 0.98 0.98 0.92 0.90 0.97 0.98 0.98 0.99 0.98 0.97 0.86 0.93 0.97 0.97 0.96 0.97 0.98 0.95 0.91 0.92 0.95 0.94 0.93 0.94 0.96 0.89 0.90 0.88 0.90 0.88 0.90 0.97 1.00 0.81 0.93 0.71 0.93 0.90 0.86 0.97 0.77 0.92 0.91 0.82 0.87 0.93 0.71 0.93 0.85 0.83 0.91 0.93 0.90 0.93 0.93 0.88 0.88 0.82 0.91 0.77 0.93 0.86 0.92 0.93 0.88 0.93 0.93 0.90 0.88 0.79 0.93 0.92 0.93 0.94 0.94 0.94 0.68 0.79 0.83 0.71 0.88 0.88 0.87 0.93 0.88 0.85 0.82 0.91 0.91 0.89 0.88 0.90 0.91 0.87 0.95 0.94 0.94 0.94 0.90 0.98 0.94 0.96 0.94 0.94 0.95 0.94 0.94 0.94 0.94 0.95 0.94 0.94 0.98 0.94 0.97 0.96 0.95 0.96 0.95 0.95 0.95 0.94 0.94 0.94 0.93 0.95 0.92 0.92 0.91 0.94 0.94 0.94 0.94 0.88 0.94 0.94 0.92 0.90 0.93 0.92 0.90 0.89 0.94 0.91 0.94 0.89 0.93 0.93 0.94 0.91 0.89 0.92 0.91 0.92 0.90 0.93 0.94 0.91 0.94 0.93 0.94 0.94 0.94 0.91 0.93 0.93 0.91 0.92 0.93 0.93 0.95 0.95 0.83 0.82 0.88 0.94 0.90 0.77 0.91 0.94 0.94 0.93 0.88 0.85 0.94 0.94 0.94 0.96 0.94 0.94 0.83 0.88 0.92 0.93 0.92 0.92 0.93 0.900.87 0.88 0.91 0.89 0.89 0.89 0.92 0.97 0.96 0.98 0.97 0.98 0.97 0.85 0.81 1.00 0.86 0.64 0.86 0.80 0.77 0.83 0.72 0.84 0.84 0.75 0.78 0.84 0.62 0.86 0.80 0.73 0.84 0.86 0.82 0.86 0.87 0.82 0.81 0.73 0.86 0.69 0.85 0.81 0.84 0.87 0.82 0.86 0.86 0.84 0.80 0.70 0.85 0.84 0.86 0.83 0.84 0.85 0.64 0.74 0.78 0.66 0.81 0.81 0.80 0.87 0.82 0.80 0.75 0.84 0.85 0.84 0.81 0.83 0.85 0.77 0.85 0.84 0.85 0.86 0.84 0.83 0.82 0.85 0.84 0.85 0.84 0.84 0.85 0.83 0.85 0.84 0.85 0.84 0.79 0.84 0.85 0.85 0.86 0.85 0.82 0.85 0.85 0.85 0.86 0.85 0.84 0.86 0.84 0.85 0.80 0.87 0.84 0.86 0.84 0.81 0.86 0.84 0.85 0.79 0.85 0.81 0.82 0.81 0.87 0.80 0.84 0.81 0.86 0.83 0.69 0.80 0.79 0.83 0.81 0.81 0.80 0.84 0.85 0.83 0.86 0.83 0.84 0.85 0.87 0.81 0.85 0.82 0.82 0.83 0.84 0.84 0.84 0.86 0.75 0.74 0.75 0.88 0.78 0.68 0.84 0.85 0.85 0.85 0.81 0.79 0.82 0.85 0.85 0.83 0.85 0.82 0.74 0.82 0.85 0.85 0.85 0.84 0.85 0.83 0.80 0.80 0.84 0.83 0.82 0.83 0.83 0.94 0.94 0.93 0.95 0.93 0.95 0.97 0.93 0.86 1.00 0.72 0.97 0.91 0.88 0.94 0.82 0.95 0.95 0.85 0.89 0.97 0.72 0.99 0.88 0.85 0.94 0.97 0.92 0.96 0.97 0.92 0.92 0.85 0.96 0.80 0.96 0.90 0.95 0.97 0.91 0.96 0.97 0.94 0.91 0.80 0.96 0.99 0.96 0.96 0.97 0.98 0.71 0.83 0.88 0.74 0.91 0.92 0.91 0.98 0.94 0.92 0.84 0.99 0.98 0.96 0.92 0.94 0.97 0.90 0.98 0.97 0.97 0.98 0.95 0.95 0.95 0.97 0.97 0.97 0.97 0.97 0.98 0.97 0.97 0.97 0.98 0.96 0.91 0.97 0.97 0.97 0.98 0.97 0.95 0.97 0.98 0.97 0.98 0.97 0.97 0.98 0.96 0.96 0.94 0.98 0.97 0.97 0.97 0.97 0.97 0.97 0.95 0.93 0.98 0.94 0.98 0.97 0.98 0.94 0.97 0.93 0.98 0.96 0.81 0.94 0.91 0.95 0.94 0.93 0.93 0.96 0.97 0.96 0.97 0.96 0.97 0.97 0.98 0.94 0.97 0.95 0.95 0.96 0.97 0.96 0.97 0.98 0.85 0.84 0.87 0.96 0.92 0.78 0.94 0.97 0.97 0.97 0.93 0.91 0.95 0.97 0.97 0.96 0.97 0.96 0.84 0.92 0.96 0.97 0.96 0.96 0.97 0.94 0.90 0.91 0.95 0.94 0.93 0.94 0.96 0.70 0.70 0.69 0.70 0.70 0.71 0.73 0.71 0.64 0.72 1.00 0.70 0.89 0.68 0.71 0.61 0.73 0.71 0.63 0.95 0.78 0.99 0.72 0.65 0.65 0.70 0.70 0.67 0.70 0.71 0.67 0.68 0.65 0.70 0.59 0.71 0.69 0.72 0.70 0.67 0.71 0.72 0.70 0.65 0.98 0.71 0.71 0.72 0.71 0.73 0.70 0.57 0.63 0.67 0.57 0.69 0.69 0.69 0.71 0.69 0.68 0.64 0.71 0.71 0.71 0.69 0.70 0.70 0.66 0.73 0.70 0.72 0.72 0.71 0.70 0.69 0.70 0.71 0.72 0.73 0.73 0.72 0.72 0.70 0.70 0.71 0.71 0.68 0.70 0.73 0.72 0.74 0.71 0.68 0.71 0.71 0.73 0.73 0.74 0.70 0.73 0.71 0.73 0.68 0.73 0.70 0.71 0.70 0.69 0.70 0.70 0.71 0.69 0.70 0.69 0.69 0.69 0.74 0.69 0.70 0.68 0.70 0.71 0.61 0.68 0.69 0.69 0.67 0.70 0.70 0.71 0.71 0.70 0.71 0.71 0.70 0.70 0.70 0.70 0.70 0.69 0.69 0.70 0.70 0.70 0.73 0.71 0.96 0.97 0.64 0.71 0.68 0.99 0.70 0.71 0.72 0.71 0.70 0.70 0.70 0.73 0.73 0.72 0.72 0.70 0.97 0.70 0.73 0.74 0.73 0.72 0.72 0.71 0.71 0.69 0.70 0.71 0.68 0.69 0.690.94 0.94 0.93 0.95 0.93 0.94 0.97 0.93 0.86 0.97 0.70 1.00 0.91 0.88 0.94 0.80 0.94 0.94 0.84 0.87 0.96 0.70 0.97 0.88 0.86 0.94 0.96 0.92 0.96 0.96 0.91 0.91 0.85 0.95 0.80 0.96 0.89 0.95 0.96 0.90 0.96 0.96 0.94 0.90 0.78 0.96 0.95 0.96 0.96 0.96 0.98 0.71 0.82 0.86 0.73 0.89 0.90 0.89 0.97 0.91 0.89 0.82 0.95 0.95 0.92 0.89 0.92 0.95 0.89 0.97 0.97 0.96 0.97 0.94 0.96 0.96 0.98 0.97 0.97 0.97 0.97 0.97 0.97 0.98 0.98 0.98 0.97 0.92 0.97 0.97 0.97 0.97 0.97 0.96 0.97 0.97 0.97 0.97 0.96 0.97 0.97 0.95 0.95 0.95 0.98 0.97 0.98 0.97 0.92 0.97 0.97 0.95 0.92 0.98 0.97 0.94 0.92 0.98 0.96 0.97 0.92 0.98 0.98 0.82 0.97 0.90 0.95 0.97 0.94 0.92 0.96 0.96 0.95 0.97 0.98 0.97 0.97 0.98 0.97 0.97 0.96 0.94 0.98 0.96 0.96 0.97 0.97 0.84 0.82 0.88 0.95 0.91 0.76 0.93 0.97 0.96 0.96 0.91 0.89 0.95 0.96 0.96 0.96 0.97 0.96 0.83 0.91 0.95 0.96 0.95 0.95 0.96 0.93 0.89 0.90 0.94 0.92 0.92 0.92 0.95 0.88 0.88 0.87 0.88 0.87 0.88 0.92 0.90 0.80 0.91 0.89 0.91 1.00 0.85 0.90 0.77 0.90 0.89 0.80 0.97 0.93 0.89 0.91 0.82 0.81 0.89 0.90 0.87 0.90 0.90 0.86 0.86 0.81 0.89 0.75 0.90 0.86 0.90 0.90 0.86 0.91 0.91 0.88 0.85 0.94 0.90 0.90 0.91 0.91 0.91 0.91 0.69 0.79 0.82 0.70 0.85 0.86 0.85 0.91 0.86 0.85 0.79 0.89 0.89 0.88 0.85 0.87 0.89 0.86 0.92 0.90 0.91 0.92 0.90 0.91 0.90 0.91 0.91 0.92 0.92 0.92 0.92 0.92 0.91 0.92 0.92 0.92 0.88 0.91 0.93 0.92 0.92 0.92 0.89 0.92 0.91 0.92 0.92 0.92 0.90 0.92 0.90 0.90 0.88 0.92 0.91 0.91 0.91 0.87 0.90 0.90 0.89 0.88 0.90 0.89 0.88 0.87 0.93 0.88 0.91 0.87 0.90 0.91 0.79 0.88 0.86 0.89 0.88 0.90 0.88 0.91 0.91 0.89 0.91 0.90 0.91 0.91 0.90 0.88 0.90 0.90 0.88 0.90 0.90 0.90 0.92 0.91 0.96 0.96 0.90 0.92 0.88 0.93 0.88 0.91 0.91 0.90 0.88 0.86 0.90 0.91 0.91 0.92 0.910.89 0.96 0.87 0.91 0.92 0.91 0.91 0.91 0.89 0.86 0.87 0.89 0.88 0.87 0.87 0.89 0.86 0.86 0.85 0.86 0.85 0.86 0.89 0.86 0.77 0.88 0.68 0.88 0.85 1.00 0.88 0.73 0.86 0.86 0.77 0.83 0.90 0.68 0.88 0.80 0.99 0.87 0.87 0.85 0.89 0.88 0.83 0.83 0.98 0.86 0.97 0.88 0.82 0.87 0.87 0.83 0.88 0.88 0.86 0.83 0.75 0.87 0.87 0.88 0.88 0.87 0.87 0.67 0.76 0.79 0.69 0.83 0.84 0.83 0.88 0.83 0.81 0.78 0.85 0.86 0.84 0.83 0.85 0.87 0.84 0.89 0.87 0.87 0.89 0.86 0.87 0.88 0.88 0.88 0.88 0.88 0.88 0.89 0.87 0.89 0.89 0.88 0.88 0.85 0.88 0.89 0.89 0.89 0.90 0.89 0.89 0.89 0.88 0.88 0.88 0.87 0.89 0.87 0.87 0.95 0.88 0.89 0.87 0.88 0.83 0.89 0.88 0.86 0.86 0.89 0.87 0.85 0.84 0.88 0.94 0.88 0.84 0.88 0.87 0.75 0.86 0.84 0.87 0.86 0.89 0.85 0.88 0.88 0.88 0.89 0.88 0.88 0.87 0.88 0.86 0.89 0.88 0.97 0.88 0.86 0.88 0.89 0.90 0.80 0.78 0.80 0.88 0.96 0.74 0.87 0.89 0.88 0.88 0.83 0.81 0.91 0.88 0.88 0.88 0.87 0.88 0.78 0.85 0.88 0.88 0.88 0.88 0.89 0.87 0.84 0.85 0.87 0.86 0.84 0.84 0.87 0.91 0.91 0.90 0.92 0.90 0.92 0.97 0.97 0.83 0.94 0.71 0.94 0.90 0.88 1.00 0.79 0.94 0.93 0.84 0.87 0.95 0.71 0.95 0.87 0.86 0.93 0.94 0.91 0.94 0.95 0.90 0.91 0.85 0.94 0.80 0.95 0.89 0.94 0.94 0.90 0.95 0.95 0.92 0.90 0.79 0.95 0.94 0.95 0.96 0.95 0.95 0.70 0.80 0.84 0.72 0.91 0.91 0.90 0.94 0.89 0.87 0.85 0.92 0.93 0.90 0.90 0.93 0.93 0.89 0.95 0.95 0.95 0.95 0.92 0.97 0.94 0.96 0.94 0.95 0.96 0.95 0.95 0.95 0.95 0.96 0.95 0.95 0.97 0.95 0.97 0.97 0.96 0.96 0.95 0.96 0.96 0.95 0.95 0.95 0.95 0.95 0.95 0.94 0.93 0.95 0.95 0.95 0.95 0.89 0.95 0.95 0.94 0.91 0.95 0.93 0.91 0.89 0.96 0.92 0.95 0.91 0.95 0.94 0.91 0.91 0.90 0.93 0.91 0.93 0.92 0.94 0.95 0.93 0.95 0.93 0.95 0.95 0.95 0.92 0.95 0.94 0.94 0.94 0.94 0.93 0.96 0.96 0.84 0.82 0.85 0.95 0.91 0.77 0.92 0.94 0.94 0.94 0.90 0.87 0.95 0.95 0.95 0.96 0.95 0.96 0.83 0.91 0.94 0.95 0.94 0.94 0.95 0.93 0.90 0.91 0.93 0.92 0.91 0.91 0.94 0.80 0.80 0.79 0.80 0.79 0.81 0.81 0.77 0.72 0.82 0.61 0.80 0.77 0.73 0.79 1.00 0.81 0.82 0.72 0.76 0.82 0.62 0.82 0.76 0.71 0.80 0.83 0.78 0.82 0.82 0.79 0.79 0.71 0.83 0.68 0.82 0.78 0.80 0.83 0.79 0.83 0.83 0.81 0.76 0.67 0.82 0.81 0.82 0.81 0.82 0.81 0.61 0.70 0.73 0.63 0.77 0.79 0.78 0.81 0.77 0.76 0.72 0.81 0.81 0.79 0.76 0.80 0.80 0.74 0.83 0.82 0.82 0.83 0.81 0.80 0.79 0.81 0.81 0.83 0.82 0.84 0.83 0.82 0.82 0.80 0.83 0.80 0.76 0.82 0.81 0.82 0.82 0.81 0.78 0.81 0.82 0.83 0.83 0.83 0.81 0.82 0.81 0.81 0.78 0.82 0.81 0.79 0.81 0.77 0.80 0.81 0.78 0.76 0.81 0.76 0.79 0.77 0.81 0.79 0.80 0.78 0.81 0.80 0.68 0.77 0.76 0.80 0.77 0.77 0.78 0.80 0.81 0.80 0.80 0.81 0.80 0.80 0.80 0.77 0.80 0.79 0.79 0.80 0.82 0.81 0.83 0.82 0.74 0.73 0.72 0.80 0.75 0.66 0.78 0.82 0.83 0.82 0.79 0.76 0.80 0.84 0.83 0.81 0.82 0.80 0.71 0.79 0.83 0.82 0.81 0.80 0.82 0.81 0.77 0.78 0.80 0.81 0.77 0.78 0.790.93 0.92 0.91 0.93 0.92 0.93 0.95 0.92 0.84 0.95 0.73 0.94 0.90 0.86 0.94 0.81 1.00 0.98 0.84 0.90 0.96 0.73 0.95 0.94 0.84 0.97 0.98 0.91 0.97 0.97 0.95 0.90 0.84 0.98 0.78 0.95 0.89 0.93 0.97 0.90 0.95 0.98 1.00 0.88 0.81 0.94 0.94 0.95 0.94 0.96 0.94 0.69 0.80 0.84 0.72 0.88 0.89 0.88 0.94 0.89 0.87 0.82 0.93 0.93 0.91 0.88 0.91 0.93 0.87 0.96 0.95 0.95 0.95 0.94 0.94 0.93 0.95 0.94 0.95 0.95 0.96 0.95 0.94 0.95 0.95 0.95 0.94 0.91 0.94 0.96 0.96 0.96 0.96 0.92 0.95 0.96 0.96 0.95 0.95 0.95 0.95 0.95 0.95 0.91 0.95 0.95 0.94 0.95 0.90 0.95 0.95 0.92 0.91 0.95 0.90 0.92 0.90 0.95 0.90 0.95 0.92 0.94 0.93 0.81 0.91 0.90 0.93 0.90 0.92 0.92 0.94 0.95 0.94 0.95 0.92 0.95 0.94 0.94 0.90 0.95 0.93 0.92 0.92 0.95 0.94 0.95 0.95 0.87 0.85 0.84 0.95 0.90 0.79 0.92 0.95 0.95 0.95 0.91 0.89 0.93 0.96 0.96 0.95 0.95 0.94 0.85 0.92 0.95 0.96 0.95 0.95 0.96 0.94 0.90 0.91 0.94 0.93 0.92 0.92 0.94 0.92 0.92 0.91 0.93 0.92 0.93 0.95 0.91 0.84 0.95 0.71 0.94 0.89 0.86 0.93 0.82 0.98 1.00 0.87 0.88 0.96 0.71 0.95 0.97 0.84 0.99 0.99 0.94 0.98 0.99 0.97 0.93 0.84 0.99 0.79 0.96 0.88 0.93 0.98 0.89 0.95 0.99 0.98 0.93 0.79 0.95 0.94 0.95 0.95 0.95 0.95 0.69 0.80 0.84 0.72 0.88 0.90 0.88 0.94 0.89 0.87 0.82 0.93 0.93 0.90 0.88 0.91 0.93 0.88 0.96 0.95 0.96 0.96 0.94 0.94 0.94 0.95 0.95 0.96 0.96 0.96 0.96 0.95 0.96 0.95 0.96 0.95 0.90 0.95 0.96 0.96 0.96 0.96 0.94 0.96 0.96 0.96 0.96 0.95 0.95 0.96 0.95 0.94 0.92 0.95 0.95 0.94 0.95 0.89 0.95 0.96 0.93 0.91 0.96 0.92 0.92 0.90 0.95 0.91 0.95 0.92 0.95 0.94 0.80 0.92 0.90 0.94 0.92 0.93 0.92 0.95 0.95 0.94 0.95 0.94 0.95 0.95 0.95 0.92 0.95 0.94 0.93 0.94 0.95 0.94 0.96 0.96 0.85 0.84 0.85 0.95 0.91 0.77 0.95 0.95 0.95 0.95 0.92 0.89 0.94 0.96 0.96 0.95 0.95 0.95 0.83 0.91 0.95 0.95 0.94 0.94 0.96 0.93 0.89 0.90 0.93 0.92 0.91 0.92 0.940.83 0.83 0.82 0.84 0.83 0.83 0.85 0.82 0.75 0.85 0.63 0.84 0.80 0.77 0.84 0.72 0.84 0.87 1.00 0.78 0.87 0.63 0.85 0.84 0.75 0.87 0.86 0.94 0.89 0.86 0.91 0.93 0.75 0.85 0.70 0.91 0.80 0.91 0.85 0.81 0.91 0.87 0.83 0.91 0.72 0.91 0.86 0.90 0.86 0.87 0.85 0.62 0.72 0.75 0.65 0.79 0.80 0.79 0.84 0.80 0.78 0.75 0.83 0.83 0.81 0.80 0.82 0.83 0.79 0.86 0.85 0.85 0.86 0.84 0.84 0.85 0.85 0.85 0.86 0.86 0.86 0.85 0.85 0.86 0.85 0.86 0.85 0.80 0.85 0.85 0.86 0.86 0.86 0.84 0.86 0.86 0.85 0.85 0.85 0.86 0.86 0.86 0.84 0.81 0.86 0.85 0.85 0.85 0.81 0.85 0.85 0.82 0.80 0.85 0.81 0.83 0.81 0.85 0.81 0.85 0.81 0.85 0.84 0.71 0.82 0.80 0.83 0.82 0.82 0.82 0.85 0.85 0.82 0.84 0.84 0.85 0.85 0.85 0.81 0.84 0.84 0.82 0.84 0.85 0.84 0.85 0.85 0.75 0.75 0.76 0.89 0.81 0.69 0.88 0.85 0.85 0.85 0.82 0.80 0.85 0.86 0.86 0.86 0.86 0.86 0.74 0.82 0.85 0.85 0.84 0.84 0.85 0.83 0.79 0.81 0.83 0.83 0.81 0.82 0.84 0.86 0.86 0.85 0.86 0.85 0.86 0.90 0.87 0.78 0.89 0.95 0.87 0.97 0.83 0.87 0.76 0.90 0.88 0.78 1.00 0.93 0.94 0.89 0.81 0.80 0.87 0.88 0.84 0.88 0.88 0.84 0.84 0.80 0.88 0.74 0.88 0.84 0.88 0.88 0.84 0.89 0.89 0.88 0.82 0.98 0.88 0.88 0.89 0.88 0.90 0.88 0.67 0.76 0.80 0.68 0.84 0.85 0.85 0.88 0.84 0.82 0.79 0.87 0.88 0.86 0.84 0.86 0.87 0.82 0.90 0.88 0.89 0.89 0.87 0.88 0.86 0.88 0.89 0.89 0.90 0.90 0.89 0.89 0.88 0.88 0.89 0.89 0.84 0.88 0.90 0.90 0.90 0.89 0.86 0.89 0.89 0.90 0.90 0.91 0.88 0.90 0.88 0.89 0.85 0.90 0.88 0.88 0.88 0.84 0.88 0.88 0.87 0.86 0.87 0.85 0.86 0.84 0.91 0.86 0.88 0.86 0.88 0.88 0.75 0.85 0.85 0.87 0.84 0.87 0.86 0.89 0.88 0.87 0.89 0.87 0.88 0.88 0.87 0.86 0.88 0.87 0.86 0.87 0.88 0.88 0.90 0.89 0.99 0.99 0.80 0.88 0.84 0.97 0.86 0.89 0.89 0.88 0.85 0.84 0.87 0.90 0.90 0.89 0.89 0.87 0.99 0.86 0.89 0.91 0.89 0.88 0.89 0.87 0.85 0.85 0.87 0.87 0.86 0.86 0.88 0.93 0.93 0.92 0.94 0.92 0.94 0.97 0.93 0.84 0.97 0.78 0.96 0.93 0.90 0.95 0.82 0.96 0.96 0.87 0.93 1.00 0.77 0.97 0.89 0.88 0.95 0.97 0.94 0.97 0.97 0.93 0.92 0.87 0.96 0.82 0.97 0.91 0.95 0.97 0.92 0.97 0.98 0.95 0.92 0.85 0.96 0.97 0.97 0.96 0.98 0.97 0.71 0.82 0.86 0.73 0.91 0.92 0.91 0.96 0.91 0.89 0.85 0.95 0.96 0.93 0.91 0.94 0.95 0.89 0.98 0.97 0.97 0.98 0.96 0.96 0.95 0.97 0.97 0.97 0.98 0.98 0.98 0.97 0.98 0.97 0.98 0.96 0.92 0.97 0.97 0.98 0.98 0.97 0.95 0.97 0.98 0.98 0.98 0.98 0.97 0.98 0.97 0.96 0.95 0.97 0.97 0.96 0.97 0.92 0.97 0.98 0.94 0.93 0.97 0.93 0.95 0.93 0.97 0.94 0.97 0.93 0.97 0.96 0.82 0.93 0.92 0.95 0.93 0.94 0.94 0.96 0.97 0.95 0.97 0.95 0.97 0.96 0.96 0.93 0.97 0.95 0.95 0.95 0.97 0.96 0.98 0.97 0.90 0.88 0.86 0.96 0.93 0.83 0.94 0.97 0.97 0.97 0.93 0.91 0.96 0.98 0.98 0.97 0.98 0.96 0.88 0.95 0.98 0.98 0.97 0.97 0.98 0.96 0.93 0.93 0.96 0.95 0.93 0.94 0.96 0.69 0.69 0.68 0.69 0.68 0.69 0.73 0.71 0.62 0.72 0.99 0.70 0.89 0.68 0.71 0.62 0.73 0.71 0.63 0.94 0.77 1.00 0.72 0.64 0.65 0.70 0.70 0.68 0.70 0.70 0.67 0.68 0.65 0.70 0.59 0.71 0.69 0.71 0.70 0.68 0.71 0.72 0.70 0.66 0.98 0.71 0.72 0.71 0.72 0.73 0.71 0.57 0.63 0.67 0.56 0.69 0.69 0.70 0.71 0.69 0.68 0.65 0.71 0.71 0.71 0.69 0.71 0.70 0.67 0.73 0.70 0.72 0.72 0.69 0.71 0.70 0.71 0.72 0.73 0.73 0.73 0.72 0.72 0.70 0.71 0.71 0.71 0.69 0.71 0.74 0.73 0.74 0.72 0.70 0.72 0.72 0.73 0.73 0.74 0.70 0.73 0.72 0.73 0.68 0.73 0.71 0.71 0.70 0.69 0.71 0.71 0.72 0.72 0.70 0.71 0.69 0.69 0.75 0.70 0.70 0.68 0.70 0.71 0.62 0.69 0.69 0.69 0.69 0.71 0.71 0.71 0.71 0.71 0.72 0.72 0.70 0.70 0.70 0.71 0.70 0.70 0.70 0.72 0.71 0.70 0.73 0.71 0.96 0.97 0.65 0.71 0.70 0.99 0.71 0.71 0.72 0.71 0.69 0.68 0.71 0.73 0.73 0.72 0.72 0.71 0.97 0.70 0.73 0.74 0.73 0.71 0.72 0.71 0.70 0.69 0.70 0.71 0.68 0.68 0.69 0.94 0.94 0.93 0.95 0.93 0.94 0.97 0.93 0.86 0.99 0.72 0.97 0.91 0.88 0.95 0.82 0.95 0.95 0.85 0.89 0.97 0.72 1.00 0.88 0.86 0.95 0.97 0.92 0.96 0.97 0.92 0.92 0.86 0.96 0.80 0.97 0.91 0.95 0.97 0.92 0.97 0.97 0.95 0.91 0.80 0.96 0.99 0.97 0.96 0.97 0.98 0.72 0.83 0.88 0.75 0.91 0.91 0.90 0.98 0.94 0.92 0.84 0.99 0.98 0.96 0.92 0.94 0.97 0.90 0.98 0.97 0.97 0.98 0.95 0.96 0.95 0.97 0.97 0.97 0.97 0.98 0.98 0.97 0.98 0.97 0.98 0.97 0.92 0.97 0.97 0.98 0.98 0.98 0.95 0.98 0.98 0.98 0.98 0.97 0.97 0.98 0.96 0.96 0.94 0.97 0.97 0.97 0.98 0.97 0.98 0.97 0.95 0.93 0.98 0.94 0.98 0.97 0.98 0.94 0.97 0.94 0.98 0.96 0.81 0.94 0.91 0.95 0.94 0.94 0.93 0.96 0.97 0.96 0.98 0.96 0.98 0.97 0.97 0.94 0.97 0.96 0.95 0.96 0.97 0.96 0.97 0.98 0.86 0.84 0.86 0.96 0.92 0.79 0.94 0.97 0.97 0.97 0.93 0.90 0.96 0.97 0.97 0.97 0.98 0.97 0.84 0.92 0.96 0.97 0.96 0.96 0.97 0.94 0.90 0.91 0.95 0.94 0.93 0.94 0.96 0.88 0.87 0.86 0.88 0.87 0.88 0.89 0.85 0.80 0.88 0.65 0.88 0.82 0.80 0.87 0.76 0.94 0.97 0.84 0.81 0.89 0.64 0.88 1.00 0.79 0.98 0.95 0.90 0.95 0.95 0.95 0.90 0.78 0.96 0.73 0.90 0.82 0.87 0.95 0.83 0.89 0.94 0.95 0.91 0.72 0.88 0.87 0.88 0.89 0.89 0.88 0.65 0.76 0.79 0.69 0.82 0.83 0.82 0.88 0.83 0.81 0.76 0.86 0.86 0.84 0.82 0.84 0.86 0.81 0.89 0.88 0.89 0.89 0.87 0.87 0.88 0.88 0.88 0.89 0.89 0.89 0.89 0.89 0.89 0.88 0.90 0.88 0.83 0.89 0.89 0.89 0.89 0.89 0.87 0.89 0.89 0.89 0.89 0.88 0.89 0.89 0.88 0.88 0.85 0.89 0.88 0.88 0.89 0.83 0.89 0.89 0.86 0.84 0.89 0.84 0.85 0.83 0.88 0.84 0.88 0.85 0.89 0.87 0.74 0.85 0.83 0.87 0.84 0.86 0.84 0.88 0.89 0.87 0.88 0.86 0.89 0.88 0.88 0.84 0.88 0.87 0.85 0.87 0.88 0.87 0.89 0.89 0.78 0.78 0.78 0.90 0.83 0.70 0.94 0.88 0.88 0.88 0.84 0.81 0.87 0.89 0.89 0.88 0.89 0.88 0.76 0.85 0.88 0.89 0.88 0.88 0.89 0.86 0.83 0.84 0.87 0.86 0.85 0.86 0.87 0.82 0.82 0.81 0.82 0.81 0.83 0.87 0.83 0.73 0.85 0.65 0.86 0.81 0.99 0.86 0.71 0.84 0.84 0.75 0.80 0.88 0.65 0.86 0.79 1.00 0.85 0.84 0.83 0.87 0.86 0.81 0.80 0.99 0.84 0.98 0.85 0.79 0.84 0.85 0.80 0.85 0.86 0.84 0.81 0.72 0.84 0.85 0.85 0.85 0.85 0.85 0.62 0.72 0.75 0.65 0.80 0.81 0.80 0.85 0.79 0.77 0.75 0.83 0.83 0.81 0.80 0.82 0.83 0.80 0.87 0.85 0.85 0.86 0.83 0.84 0.85 0.86 0.85 0.86 0.86 0.85 0.86 0.85 0.87 0.87 0.86 0.85 0.82 0.85 0.86 0.86 0.86 0.87 0.86 0.86 0.86 0.86 0.85 0.85 0.85 0.86 0.84 0.84 0.94 0.85 0.86 0.84 0.86 0.81 0.86 0.86 0.84 0.84 0.86 0.84 0.82 0.81 0.86 0.93 0.86 0.83 0.85 0.86 0.73 0.83 0.81 0.85 0.83 0.87 0.82 0.87 0.85 0.85 0.87 0.85 0.86 0.85 0.85 0.83 0.87 0.85 0.96 0.85 0.85 0.85 0.86 0.87 0.77 0.75 0.76 0.84 0.94 0.71 0.84 0.86 0.86 0.86 0.81 0.78 0.89 0.85 0.86 0.86 0.85 0.86 0.75 0.81 0.85 0.85 0.85 0.85 0.86 0.84 0.80 0.81 0.84 0.82 0.83 0.83 0.850.93 0.92 0.91 0.93 0.92 0.93 0.95 0.91 0.84 0.94 0.70 0.94 0.89 0.87 0.93 0.80 0.97 0.99 0.87 0.87 0.95 0.70 0.95 0.98 0.85 1.00 0.99 0.94 0.98 0.98 0.96 0.94 0.85 0.99 0.79 0.96 0.89 0.94 0.98 0.90 0.95 0.98 0.97 0.95 0.78 0.95 0.94 0.95 0.94 0.95 0.94 0.71 0.81 0.85 0.74 0.88 0.89 0.88 0.94 0.89 0.87 0.82 0.92 0.93 0.91 0.88 0.91 0.93 0.88 0.95 0.94 0.95 0.95 0.93 0.93 0.94 0.94 0.94 0.95 0.95 0.95 0.95 0.95 0.96 0.95 0.96 0.95 0.90 0.95 0.96 0.96 0.96 0.96 0.93 0.96 0.95 0.95 0.95 0.94 0.95 0.95 0.94 0.93 0.91 0.95 0.95 0.94 0.95 0.89 0.95 0.95 0.92 0.91 0.95 0.91 0.91 0.89 0.95 0.91 0.95 0.91 0.95 0.93 0.80 0.91 0.89 0.94 0.91 0.92 0.91 0.95 0.95 0.93 0.95 0.93 0.95 0.95 0.94 0.91 0.95 0.93 0.92 0.93 0.94 0.94 0.95 0.95 0.84 0.83 0.84 0.95 0.91 0.76 0.97 0.94 0.94 0.94 0.90 0.87 0.94 0.95 0.95 0.95 0.95 0.94 0.82 0.91 0.94 0.95 0.94 0.94 0.95 0.93 0.89 0.90 0.93 0.92 0.91 0.92 0.94 0.94 0.94 0.93 0.95 0.93 0.94 0.97 0.93 0.86 0.97 0.70 0.96 0.90 0.87 0.94 0.83 0.98 0.99 0.86 0.88 0.97 0.70 0.97 0.95 0.84 0.99 1.00 0.93 0.98 0.99 0.95 0.94 0.84 1.00 0.78 0.97 0.90 0.95 0.99 0.92 0.97 0.99 0.98 0.92 0.78 0.96 0.96 0.97 0.96 0.97 0.97 0.70 0.82 0.86 0.73 0.90 0.91 0.90 0.96 0.91 0.88 0.83 0.95 0.95 0.92 0.90 0.93 0.94 0.89 0.97 0.97 0.97 0.97 0.95 0.95 0.96 0.96 0.97 0.97 0.97 0.98 0.97 0.97 0.97 0.96 0.98 0.96 0.91 0.97 0.97 0.97 0.97 0.97 0.94 0.97 0.97 0.97 0.97 0.97 0.97 0.97 0.96 0.96 0.93 0.97 0.96 0.96 0.97 0.91 0.97 0.97 0.94 0.92 0.97 0.93 0.94 0.92 0.97 0.92 0.96 0.93 0.97 0.95 0.81 0.93 0.91 0.95 0.93 0.93 0.93 0.96 0.97 0.95 0.96 0.95 0.97 0.96 0.97 0.93 0.96 0.95 0.93 0.95 0.97 0.96 0.97 0.97 0.85 0.84 0.86 0.96 0.90 0.77 0.95 0.96 0.96 0.96 0.93 0.90 0.95 0.97 0.97 0.96 0.97 0.96 0.83 0.93 0.96 0.97 0.96 0.96 0.97 0.95 0.91 0.92 0.95 0.94 0.93 0.94 0.96 0.90 0.90 0.89 0.91 0.89 0.90 0.93 0.90 0.82 0.92 0.67 0.92 0.87 0.85 0.91 0.78 0.91 0.94 0.94 0.84 0.94 0.68 0.92 0.90 0.83 0.94 0.93 1.00 0.97 0.93 0.97 0.90 0.83 0.92 0.77 0.93 0.86 0.92 0.93 0.87 0.93 0.95 0.90 0.97 0.77 0.92 0.92 0.92 0.92 0.93 0.92 0.67 0.78 0.81 0.70 0.86 0.87 0.85 0.92 0.86 0.84 0.81 0.90 0.90 0.87 0.86 0.88 0.90 0.88 0.93 0.93 0.92 0.93 0.91 0.91 0.92 0.92 0.93 0.93 0.93 0.93 0.93 0.93 0.94 0.93 0.94 0.92 0.88 0.93 0.93 0.94 0.93 0.93 0.92 0.93 0.93 0.93 0.93 0.92 0.93 0.93 0.92 0.91 0.89 0.93 0.92 0.92 0.93 0.87 0.92 0.93 0.89 0.88 0.92 0.90 0.89 0.87 0.93 0.89 0.92 0.88 0.93 0.92 0.78 0.90 0.87 0.91 0.89 0.90 0.88 0.92 0.92 0.90 0.92 0.91 0.93 0.92 0.93 0.89 0.92 0.91 0.91 0.91 0.92 0.92 0.92 0.93 0.81 0.81 0.83 0.93 0.89 0.74 0.96 0.93 0.92 0.92 0.89 0.86 0.92 0.93 0.92 0.93 0.92 0.92 0.80 0.88 0.92 0.92 0.91 0.92 0.93 0.90 0.86 0.88 0.90 0.89 0.88 0.89 0.91 0.94 0.94 0.93 0.95 0.93 0.94 0.97 0.93 0.86 0.96 0.70 0.96 0.90 0.89 0.94 0.82 0.97 0.98 0.89 0.88 0.97 0.70 0.96 0.95 0.87 0.98 0.98 0.97 1.00 0.99 0.97 0.93 0.87 0.98 0.82 0.96 0.90 0.95 0.98 0.91 0.97 0.99 0.97 0.95 0.79 0.96 0.95 0.96 0.96 0.96 0.96 0.71 0.82 0.86 0.74 0.89 0.90 0.89 0.96 0.90 0.88 0.83 0.94 0.94 0.92 0.89 0.92 0.94 0.90 0.97 0.97 0.97 0.97 0.94 0.95 0.96 0.97 0.960.97 0.97 0.97 0.97 0.96 0.97 0.97 0.97 0.96 0.91 0.97 0.97 0.97 0.97 0.97 0.95 0.97 0.97 0.97 0.97 0.96 0.97 0.97 0.95 0.95 0.94 0.97 0.97 0.96 0.97 0.91 0.97 0.97 0.94 0.92 0.97 0.93 0.93 0.91 0.97 0.94 0.97 0.93 0.97 0.96 0.81 0.94 0.91 0.95 0.93 0.94 0.93 0.96 0.97 0.95 0.97 0.95 0.97 0.96 0.97 0.93 0.97 0.95 0.95 0.95 0.96 0.96 0.96 0.97 0.85 0.83 0.86 0.96 0.92 0.77 0.97 0.97 0.97 0.96 0.92 0.89 0.96 0.97 0.96 0.97 0.97 0.96 0.83 0.92 0.96 0.96 0.95 0.96 0.97 0.94 0.90 0.92 0.94 0.93 0.92 0.93 0.95 0.95 0.94 0.93 0.95 0.94 0.95 0.97 0.93 0.87 0.97 0.71 0.96 0.90 0.88 0.95 0.82 0.97 0.99 0.86 0.88 0.97 0.70 0.97 0.95 0.86 0.98 0.99 0.93 0.99 1.00 0.95 0.93 0.86 0.99 0.81 0.97 0.90 0.95 0.99 0.91 0.97 0.99 0.98 0.93 0.79 0.96 0.95 0.96 0.96 0.97 0.97 0.71 0.82 0.86 0.74 0.90 0.91 0.90 0.96 0.91 0.89 0.84 0.95 0.95 0.92 0.90 0.93 0.95 0.89 0.98 0.97 0.97 0.98 0.95 0.95 0.96 0.97 0.97 0.97 0.97 0.98 0.98 0.97 0.98 0.97 0.98 0.97 0.91 0.97 0.97 0.97 0.98 0.97 0.95 0.97 0.98 0.98 0.98 0.97 0.97 0.98 0.96 0.96 0.94 0.97 0.97 0.97 0.97 0.91 0.97 0.97 0.95 0.92 0.98 0.93 0.93 0.91 0.97 0.93 0.97 0.94 0.97 0.96 0.81 0.93 0.91 0.96 0.93 0.94 0.93 0.96 0.97 0.95 0.97 0.95 0.97 0.97 0.97 0.93 0.97 0.96 0.94 0.95 0.97 0.96 0.97 0.97 0.85 0.84 0.86 0.97 0.91 0.77 0.96 0.97 0.97 0.97 0.92 0.89 0.96 0.97 0.97 0.97 0.97 0.96 0.83 0.92 0.96 0.96 0.96 0.96 0.97 0.94 0.90 0.91 0.95 0.93 0.93 0.94 0.96 0.90 0.90 0.88 0.90 0.89 0.90 0.92 0.88 0.82 0.92 0.67 0.91 0.86 0.83 0.90 0.79 0.95 0.97 0.91 0.84 0.93 0.67 0.92 0.95 0.81 0.96 0.95 0.97 0.97 0.95 1.00 0.89 0.81 0.95 0.76 0.92 0.86 0.90 0.94 0.87 0.92 0.95 0.95 0.92 0.75 0.91 0.91 0.91 0.92 0.93 0.92 0.66 0.77 0.81 0.69 0.84 0.86 0.84 0.91 0.86 0.84 0.79 0.89 0.90 0.87 0.84 0.87 0.89 0.84 0.93 0.92 0.93 0.93 0.90 0.90 0.91 0.92 0.92 0.93 0.92 0.93 0.93 0.92 0.93 0.92 0.93 0.91 0.86 0.92 0.92 0.93 0.93 0.92 0.90 0.93 0.93 0.93 0.93 0.92 0.92 0.93 0.91 0.91 0.88 0.92 0.91 0.91 0.92 0.86 0.92 0.92 0.90 0.87 0.92 0.88 0.88 0.87 0.92 0.88 0.91 0.88 0.92 0.91 0.77 0.89 0.86 0.90 0.88 0.88 0.88 0.91 0.92 0.89 0.91 0.90 0.92 0.92 0.92 0.88 0.91 0.90 0.89 0.90 0.92 0.91 0.92 0.92 0.81 0.80 0.82 0.93 0.87 0.74 0.93 0.92 0.92 0.92 0.89 0.86 0.90 0.93 0.92 0.92 0.92 0.91 0.79 0.88 0.91 0.92 0.91 0.91 0.92 0.90 0.85 0.87 0.90 0.89 0.88 0.89 0.90</figDesc><table><row><cell></cell><cell>acm Arab</cell><cell>aeb Arab</cell><cell>apc Arab</cell><cell>arb Arab</cell><cell>ary Arab</cell><cell>arz Arab</cell><cell>amh Ethi</cell><cell>tir Ethi</cell><cell>heb Hebr</cell><cell>hau Latn</cell><cell>kab Latn</cell><cell>mlt Latn</cell><cell>som Latn</cell><cell>taq Latn</cell><cell>tzm Tfng</cell><cell>epo Latn</cell><cell>aka Latn</cell><cell>bem Latn</cell><cell>cjk Latn</cell><cell>ewe Latn</cell><cell>fon Latn</cell><cell>fuv Latn</cell><cell>ibo Latn</cell><cell>kam Latn</cell><cell>kbp Latn</cell><cell>kik Latn</cell><cell>kin Latn</cell><cell>kmb Latn</cell><cell>kon Latn</cell><cell>lin Latn</cell><cell>lua Latn</cell><cell>lug Latn</cell><cell>mos Latn</cell><cell>run Latn</cell><cell>sag Latn</cell><cell>sna Latn</cell><cell>sot Latn</cell><cell>ssw Latn</cell><cell>swh Latn</cell><cell>tsn Latn</cell><cell>tso Latn</cell><cell>tum Latn</cell><cell>twi Latn</cell><cell>umb Latn</cell><cell>wol Latn</cell><cell>xho Latn</cell><cell>yor Latn</cell><cell>zul Latn</cell><cell>sat Beng</cell><cell>khm Khmr</cell><cell>vie Latn</cell><cell>ace Latn</cell><cell>ban Latn</cell><cell>bjn Latn</cell><cell>bug Latn</cell><cell>ceb Latn</cell><cell>fij Latn</cell><cell>ilo Latn</cell><cell>ind Latn</cell><cell>jav Latn</cell><cell>min Latn</cell><cell>mri Latn</cell><cell>plt Latn</cell><cell>smo Latn</cell><cell>sun Latn</cell><cell>tgl Latn</cell><cell>war Latn</cell><cell>zsm Latn</cell><cell>ayr Latn</cell><cell>kan Knda</cell><cell>mal Mlym</cell><cell>tam Taml</cell><cell>tel Telu</cell><cell>eus Latn</cell><cell>ckb Arab</cell><cell>kas Arab</cell><cell>pes Arab</cell><cell>snd Arab</cell><cell>urd Arab</cell><cell>hye Armn</cell><cell>asm Beng</cell><cell>ben Beng</cell><cell>bel Cyrl</cell><cell>bul Cyrl</cell><cell>mkd Cyrl</cell><cell>rus Cyrl</cell><cell>srp Cyrl</cell><cell>tgk Cyrl</cell><cell>ukr Cyrl</cell><cell>awa Deva</cell><cell>bho Deva</cell><cell>hin Deva</cell><cell>hne Deva</cell><cell>kas Deva</cell><cell>mag Deva</cell><cell>mai Deva</cell><cell>mar Deva</cell><cell>npi Deva</cell><cell>san Deva</cell><cell>ell Grek</cell><cell>guj Gujr</cell><cell>ydd Hebr</cell><cell>afr Latn</cell><cell>als Latn</cell><cell>ast Latn</cell><cell>bos Latn</cell><cell>cat Latn</cell><cell>ces Latn</cell><cell>cym Latn</cell><cell>dan Latn</cell><cell>deu Latn</cell><cell>eng Latn</cell><cell>fao Latn</cell><cell>fra Latn</cell><cell>fur Latn</cell><cell>gla Latn</cell><cell>gle Latn</cell><cell>glg Latn</cell><cell>hat Latn</cell><cell>hrv Latn</cell><cell>isl Latn</cell><cell>ita Latn</cell><cell>kea Latn</cell><cell>kmr Latn</cell><cell>lij Latn</cell><cell>lim Latn</cell><cell>lit Latn</cell><cell>lmo Latn</cell><cell>ltg Latn</cell><cell>ltz Latn</cell><cell>lvs Latn</cell><cell>nld Latn</cell><cell>nno Latn</cell><cell>nob Latn</cell><cell>scn Latn</cell><cell>slk Latn</cell><cell>slv Latn</cell><cell>spa Latn</cell><cell>srd Latn</cell><cell>swe Latn</cell><cell>szl Latn</cell><cell>tpi Latn</cell><cell>vec Latn</cell><cell>sin Sinh</cell><cell>jpn Jpan</cell><cell>kat Geor</cell><cell>kor Hang</cell><cell>bam Latn</cell><cell>dyu Latn</cell><cell>khk Cyrl</cell><cell>knc Arab</cell><cell>knc Latn</cell><cell>dik Latn</cell><cell>luo Latn</cell><cell>zho Hans</cell><cell>yue Hant</cell><cell>zho Hant</cell><cell>kac Latn</cell><cell>lus Latn</cell><cell>mni Beng</cell><cell>mya Mymr</cell><cell>bod Tibt</cell><cell>dzo Tibt</cell><cell>lao Laoo</cell><cell>shn Mymr</cell><cell>grn Latn</cell><cell>azb Arab</cell><cell>uig Arab</cell><cell>bak Cyrl</cell><cell>kaz Cyrl</cell><cell>kir Cyrl</cell><cell>tat Cyrl</cell><cell>azj Latn</cell><cell>crh Latn</cell><cell>tuk Latn</cell><cell>tur Latn</cell><cell>uzn Latn</cell><cell>est Latn</cell><cell>fin Latn</cell><cell>hun Latn</cell></row><row><cell>acm Arab aeb Arab apc Arab arb Arab ary Arab arz Arab amh Ethi tir Ethi heb Hebr hau Latn kab Latn mlt Latn som Latn taq Latn tzm Tfng epo Latn aka Latn bem Latn cjk Latn ewe Latn fon Latn fuv Latn ibo Latn kam Latn kbp Latn kik Latn kin Latn kmb Latn kon Latn lin Latn lua Latn lug Latn</cell><cell cols="109">1.00 10.90 0.90 0.89 0.90 0.89 0.90 0.92 0.88 0.81 0.92 0.68 0.91 0.86 0.83 0.91 0.79 0.90 0.93 0.93 0.84 0.92 0.68 0.92 0.90 0.80 0.94 0.94 0.90 0.93 0.93 0.89 1.00 0.80 0.94 0.74 0.97 0.86 0.97 0.94 0.88 0.97 0.94 0.90 0.92 0.76 0.97 0.92 0.97 0.92 0.93 0.91 0.67 0.78 0.82 0.70 0.86 0.87 0.85 0.91 0.86 0.84 0.80 0.90 0.90 0.88 0.86 0.88 0.90 0.85 0.92 0.92 0.92 0.93 0.90 0.91 0.91 0.92 0.92 0.92 0.92 0.92 0.92 0.92 0.92 0.92 0.93 0.91 0.87 0.92 0.92 0.93 0.93 0.92 0.90 0.92 0.92 0.92 0.92 0.92 0.92 0.92 0.92 0.90 0.88 0.92 0.92 0.91 0.92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mos Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>run Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sag Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sna Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sot Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ssw Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>swh Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tsn Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tso Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tum Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>twi Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>umb Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>wol Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>xho Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>yor Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>zul Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sat Beng</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>khm Khmr</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>vie Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ace Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ban Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bjn Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bug Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ceb Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>fij Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ilo Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ind Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>jav Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>min Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mri Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>plt Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>smo Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sun Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tgl Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>war Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>zsm Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ayr Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>kan Knda</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mal Mlym</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tam Taml</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tel Telu</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>eus Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ckb Arab</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>kas Arab</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pes Arab</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>snd Arab</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>urd Arab</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hye Armn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>asm Beng</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ben Beng</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bel Cyrl</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bul Cyrl</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mkd Cyrl</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>rus Cyrl</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>srp Cyrl</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tgk Cyrl</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ukr Cyrl</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>awa Deva</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bho Deva</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hin Deva</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hne Deva</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>kas Deva</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mag Deva</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mai Deva</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mar Deva</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>npi Deva</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>san Deva</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ell Grek</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>guj Gujr</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ydd Hebr</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>afr Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>als Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ast Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bos Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>cat Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ces Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>cym Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dan Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>deu Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>eng Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>fao Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>fra Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>fur Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>gla Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>gle Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>glg Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hat Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hrv Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>isl Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ita Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>kea Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>kmr Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>lij Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>lim Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>lit Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>lmo Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ltg Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ltz Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>lvs Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>nld Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>nno Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>nob Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>scn Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>slk Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>slv Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>spa Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>srd Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>swe Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>szl Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tpi Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>vec Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sin Sinh</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>jpn Jpan</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>kat Geor</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>kor Hang</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bam Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dyu Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>khk Cyrl</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>knc Arab</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>knc Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dik Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>luo Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>zho Hans</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>yue Hant</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>zho Hant</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>kac Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>lus Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mni Beng</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mya Mymr</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bod Tibt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dzo Tibt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>lao Laoo</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>shn Mymr</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>grn Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>azb Arab</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>uig Arab</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bak Cyrl</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>kaz Cyrl</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>kir Cyrl</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tat Cyrl</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>azj Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>crh Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tuk Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tur Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>uzn Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>est Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>fin Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hun Latn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_133"><head></head><label></label><figDesc>? Person or organization developing model: Developed by Meta AI Research ? Model date: June 30th, 2022 ? Model version: NLLB-200 ? Model type: Transformer Mixture-of-Experts machine translation model. -Information about training algorithms, parameters, fairness constraints or other applied approaches, and features The exact training algorithm, data and the strategies to handle data imbalances for high and low resource languages that were used to train NLLB-200 is described in the paper. -Paper or other resource for more information NLLB Team et al, No Language Left Behind: Scaling Human-Centered Machine Translation, Arxiv, 2022 -License: CC-BY-NC b -Where to send questions or comments about the model: https://github.com/facebookresearch/fairseq/issues NLLB-200 is a machine translation model primarily intended for research in machine translation, especially for low-resource languages. It allows for single sentence translation among 200 languages. Information on how to use the model can be found in Fairseq code repository along with the training code and references to evaluation and training data. ? Primary intended users: Primary users are researchers and machine translation research community. ? Out-of-scope use cases: NLLB-200 is a research model and is not released for production deployment. NLLB-200 is trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation. The model was trained with input lengths not exceeding 512 tokens, therefore translating longer sequences might result in quality degradation. NLLB-200 translations can not be used as certified translations.</figDesc><table><row><cell>Intended Use</cell></row><row><cell>? Primary intended uses:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_134"><head>Appendix G. Data Card for NLLB-Seed Data</head><label></label><figDesc>For this card, we use the template from<ref type="bibr" target="#b66">Mitchell et al. (2019)</ref>. b. https://creativecommons.org/licenses/by-nc/4.0/legalcode Seed data is a collection of human translated data sampled from Wikimedia' s List of articles every Wikipedia should have b , a collection of 10,000 Wikidata IDs corresponding to notable topics in different fields of knowledge and human activity. It contains bitext from English to other 43 languages for 6193 sentences. The motivation of this data was to provide a starter set of clean data on variety of topics in those languages. ? How to use the data You can access links to the data in the README at https://github.com/ facebookresearch/fairseq/tree/nllb ? Supported Tasks and Leaderboards NLLB model uses this data to boost the performance of low-resource languages.</figDesc><table><row><cell>Dataset Description a</cell></row><row><cell>? Dataset Summary</cell></row><row><cell>The NLLB-</cell></row></table><note>a.? Languages NLLB-Seed contains 43 language pairs with English.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_135"><head></head><label></label><figDesc>We use a template for this data card https://huggingface.co/docs/datasets/v1.12.0/dataset_ card.html b. https://meta.wikimedia.org/wiki/List_of_articles_every_Wikipedia_should_have/ Expanded c. https://creativecommons.org/licenses/by-sa/4.0/ -Who are the annotators?</figDesc><table><row><cell>are releasing translations based on source sentences from Wikipedia under the terms of</cell></row><row><cell>CC-BY-SA c</cell></row><row><cell>? Citation Information</cell></row><row><cell>NLLB Team et al, No Language Left Behind: Scaling Human-Centered Machine Translation,</cell></row><row><cell>Arxiv, 2022</cell></row></table><note>a.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. https://meta.wikimedia.org/wiki/Language_proposal_policy</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">. https://md.taus.net/corona</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="35">. https://github.com/facebookresearch/stopes</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix H. Data Card for NLLB Multi-Domain Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Description a ? Dataset Summary</head><p>The NLLB Multi-Domain data is a collection of human translated data across four domains <ref type="bibr">(11810 sentences across news, formal speech, informal speech, and medical sources</ref> -News: 2810 English sentences from the WMT21 English-German development set, containing a sample of newspapers from 2020 <ref type="bibr" target="#b10">(Akhbardeh et al., 2021</ref>) -Unscripted Informal Speech: 3000 English utterances from the multi-session chat dataset of <ref type="bibr" target="#b177">Xu et al. (2022)</ref>, which contains on average 23 words per turn -Health: 3000 English sentences from a World Health Organisation report <ref type="bibr" target="#b53">(Donaldson and Rutter, 2017)</ref> and the English portion of the TAUS Corona Crisis Report. b )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Annotations</head><p>There are no extra annotations with the bitext.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Personal and Sensitive Information</head><p>Not applicable</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Considerations for Using the Data</head><p>? Social Impact of Dataset The dataset is specifically built to increase the translation quality and the language identification of the extremely low-resourced languages it contains. This helps improve the quality of different languages in machine translation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Discussion of Biases(#discussion-of-biases)</head><p>Biases on the dataset have not been studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix I. Data Card for Mined Bitext Metadata</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Description a ? Dataset Summary</head><p>We created mined bitext from publicly available web data for 148 English-centric and 1465 non-English-centric language pairs using the stopes mining library and the LASER3 encoders <ref type="bibr" target="#b83">Heffernan et al. (2022)</ref>. We open source the corresponding metadata and a script which enables researchers who have downloaded the specified files from CommonCrawl and ParaCrawl to recreate the full bitext data. Note that CommonCrawl answers takedown notices, so subsequent runs of the tool can end up with smaller amount of bitext.</p><p>? How to use the data You can access links to the data in the README at https://github.com/ facebookresearch/fairseq/tree/nllb Data Structure</p><p>? The metadata files are space separated, xz-compressed files. Each file corresponds to one bitext direction. For example, the file xho_Latn-yor_Latn.meta.xz contains all the metadata required to find the actual Xhosa and Yoruba aligned text data. Each line has 11 columns with the following format:</p><p>-If the metadata comes from Common Crawl: wet_file_url document_sha1 document_url line_number_in_document paragraph_digest sentence_digest lid_score laser_score direction language line_number_in_direction -If the metadata comes from other corpus: corpus_name.language not_used not_used line_number_in_document paragraph_digest sentence_digest lid_score laser_score direction language line_number_in_direction</p><p>? Paragraph and sentence digests are computed with xxh3_64_intdigest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Splits</head><p>? Given the noisy nature of the overall process, we recommend using the data only for training and use other datasets like Flores-200 for the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Creation</head><p>? Source Data Initial Data Collection and Normalization The monolingual data is from Common Crawl and ParaCrawl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Curation Rationale</head><p>We applied filtering based on language identification, emoji based filtering, and for some high-resource languages language model-based filtering. For more details on our data filtering please refer to Section 5.2. ? Who are the source language producers?</p><p>The source language was produced by writers of each website that have been crawled by Common Crawl and ParaCrawl ? Annotations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Annotation process</head><p>Parallel sentences in the monolingual data were identified using LASER3 encoders. <ref type="bibr" target="#b83">(Heffernan et al., 2022)</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards a cleaner document-oriented multilingual crawled corpus. CoRR, abs/2201.06642, 2022</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Abadji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro Javier Ortiz</forename><surname>Su?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Sagot</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2201.06642" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Parallel corpora for bi-directional statistical machine translation for seven ethiopian language pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Solomon Teferra Abate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><forename type="middle">Yifiru</forename><surname>Melese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Million</forename><surname>Tachbelie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solomon</forename><surname>Meshesha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wondwossen</forename><surname>Atinafu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaregal</forename><surname>Mulugeta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hafte</forename><surname>Assabie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ephrem</forename><surname>Binyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tewodros</forename><surname>Seyoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</title>
		<meeting>the First Workshop on Linguistic Resources for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="83" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Benchmarking neural machine translation for Southern African languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Martinus</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W19-3632" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Workshop on Widening NLP</title>
		<meeting>the 2019 Workshop on Widening NLP<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08" />
			<biblScope unit="page" from="98" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The AMARA corpus: Building parallel language resources for the educational domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Abdelali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2014/pdf/877_Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05" />
			<biblScope unit="page" from="1856" to="1862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the Use of Comparable Corpora to Improve SMT performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadaf</forename><surname>Abdul-Rauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/E09-1003" />
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ayodele Esther Awokoya, and Cristina Espa?a-Bonet. The effect of domain and diacritics in Yoruba-English neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Adelani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Ruiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesujoba</forename><surname>Alabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damilola</forename><surname>Adebonojo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adesina</forename><surname>Ayeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mofe</forename><surname>Adeyemi</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.mtsummit-research.6" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Biennial Machine Translation Summit</title>
		<meeting>the 18th Biennial Machine Translation Summit</meeting>
		<imprint>
			<publisher>Virtual</publisher>
			<date type="published" when="2021-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="61" to="75" />
		</imprint>
	</monogr>
	<note>Association for Machine Translation in the Americas</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A few thousand translations go a long way! leveraging pre-trained models for african news translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesujoba</forename><surname>David Ifeoluwa Adelani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Oluwadara Alabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Machel</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Ruiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Klakow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernie</forename><surname>Nabende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tajuddeen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freshia</forename><surname>Gwadabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sackey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">P</forename><surname>Bonaventure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">Chinenye</forename><surname>Dossou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Emezue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beukman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyo</forename><surname>Shamsuddeen Hassan Muhammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oreen</forename><surname>Dub Jarso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><forename type="middle">Niyongabo</forename><surname>Yousuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Rubungo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hacheme</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2205.02022</idno>
		<ptr target="https://arxiv.org/abs/2205.02022" />
		<editor>Eric Peter Wairagala, Muhammad Umair Nasir, Benjamin Ayoade Ajibade, Tunde Oluwaseyi Ajayi, Yvonne Wambui Gitau, Jade Abbott, Mohamed Ahmed, Millicent Ochieng, Anuoluwapo Aremu, Perez Ogayo, Jonathan Mukiibi, Fatoumata Ouoba Kabore, Godson Koffi Kalipe, Derguene Mbaye, Allahsera Auguste Tapo, Victoire Memdjokam Koagne, Edwin Munkoh-Buabeng, Valencia Wagner, Idris Abdulmumin, Ayodele Awokoya, Happy Buzaaba, Blessing Sibanda, Andiswa Bukula, and Sam Manthalu</editor>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SemEval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/S12-1051" />
	</analytic>
	<monogr>
		<title level="m">*SEM 2012: The First Joint Conference on Lexical and Computational Semantics</title>
		<meeting><address><addrLine>Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The low-resource double bind: An empirical study of pruning for low-resource machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orevaoghene</forename><surname>Ahia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
		<idno>abs/2110.03036</idno>
		<ptr target="https://arxiv.org/abs/2110.03036" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Machine translation for african languages: Community creation of datasets and models in uganda</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Akera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mukiibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lydia</forename><forename type="middle">Sanyu</forename><surname>Naggayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Babirye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Owomugisha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solomon</forename><surname>Nsumba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><surname>Nakatumba-Nabende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engineer</forename><surname>Bainomugisha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Mwebaze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quinn</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BK-z5qzEU-9" />
	</analytic>
	<monogr>
		<title level="m">3rd Workshop on African Natural Language Processing</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Santanu Pal, Allahsera Auguste Tapo, Marco Turchi, Valentin Vydrin, and Marcos Zampieri. Findings of the 2021 conference on machine translation (WMT21)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farhad</forename><surname>Akhbardeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkady</forename><surname>Arkhangorodsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magdalena</forename><surname>Biesialska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Espa?a-Bonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonie</forename><surname>Harter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Homan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwabena</forename><surname>Amponsah-Kaakyire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Morishita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Nagesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.wmt-1.1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Conference on Machine Translation</title>
		<meeting>the Sixth Conference on Machine Translation<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11" />
			<biblScope unit="page" from="1" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic capacity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2091" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Morphologically annotated corpora for seven Arabic dialects: Taizi, sanaani, najdi, jordanian, syrian, iraqi and Moroccan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Alshargi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahd</forename><surname>Dibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakhar</forename><surname>Alkhereyf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reem</forename><surname>Faraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basmah</forename><surname>Abdulkareem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sane</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouafaa</forename><surname>Kacha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Arabic Natural Language Processing Workshop</title>
		<meeting>the Fourth Arabic Natural Language Processing Workshop</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Realizing the potential of ai in africa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charity</forename><surname>Delmus Alupo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Omeiza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vernon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Towards Trustworthy Artificial Intelligence Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TICO-19: the translation initiative for COvid-19</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Cattelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Genzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franscisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosie</forename><surname>Lazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>?ktem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Paquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylwia</forename><surname>Tur</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.nlpcovid19-2.5</idno>
		<ptr target="https://aclanthology.org/2020.nlpcovid19-2.5" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on NLP for COVID-19</title>
		<meeting>the 1st Workshop on NLP for COVID-19</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
	<note>EMNLP 2020, Online</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Findings of the IWSLT 2021 evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Bremerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roldano</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maha</forename><surname>Elbayad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Salesky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>St?ker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wiesner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.iwslt-1.1</idno>
		<ptr target="https://aclanthology.org/2021.iwslt-1.1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021)</title>
		<meeting>the 18th International Conference on Spoken Language Translation (IWSLT 2021)<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08" />
			<biblScope unit="page" from="1" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Proposition d&apos;ajout de l&apos;?criture tifinaghe. Organisation internationale de normalisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Andries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Jeu universel des caract?res cod?s sur octets (JUC). ORGANISATION INTERNATIONALE DE NORMALISATION</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Language identification of hindi-english tweets using code-mixed BERT. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Mohd Zeeshan Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanvir</forename><surname>Sufyan Beg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohd Jazib</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghazali</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wasim</surname></persName>
		</author>
		<idno>abs/2107.01202</idno>
		<ptr target="https://arxiv.org/abs/2107.01202" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Massively multilingual neural machine translation in the wild: Findings and challenges. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><forename type="middle">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">F</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.05019" />
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Social levelling, or anti-standardization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">E</forename><surname>Mackenzie</surname></persName>
		</author>
		<idno type="DOI">10.1057/9781137284396_6</idno>
		<idno>978-1-137-28439-6. doi: 10.1057/ 9781137284396_6</idno>
		<ptr target="https://doi.org/10.1057/9781137284396_6" />
		<imprint>
			<date type="published" when="2013" />
			<publisher>Palgrave Macmillan UK</publisher>
			<biblScope unit="page" from="161" to="207" />
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai. Information fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Barredo Arrieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>D?az-Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><forename type="middle">Del</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bennetot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siham</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Barbado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvador</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Gil-L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Benjamins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="82" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Margin-based parallel corpus mining with multilingual sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3197" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond. TACL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="597" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient large scale language modeling with mixtures of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giri</forename><surname>Anantharaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halil</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandeep</forename><surname>Baines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Horo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/2112.10684</idno>
		<ptr target="https://arxiv.org/abs/2112.10684" />
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weighted Set-Theoretic Alignment of Comparable Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andoni</forename><surname>Azpeitia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva Mart?nez</forename><surname>Thierry Etchegoyhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garcia</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W17-2508" />
	</analytic>
	<monogr>
		<title level="m">BUCC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="41" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extracting Parallel Sentences from Comparable Corpora with STACC Variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andoni</forename><surname>Azpeitia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva Mart?nez</forename><surname>Thierry Etchegoyhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BUCC</title>
		<imprint>
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">English-akuapem twi parallel corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Azunre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Adu-Gyamfi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>Appiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Akwerh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salomey</forename><surname>Osei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Amoaba</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4432117</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4432117" />
	</analytic>
	<monogr>
		<title level="j">Salomey Afua Addo, Edwin Buabeng-Munkoh</title>
		<imprint>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">English-twi parallel corpus for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Azunre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salomey</forename><surname>Osei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salomey</forename><surname>Addo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Asamoah Adu-Gyamfi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Adabankah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Opoku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Asare-Nyarko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Nyarko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Amoaba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15625</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Azunre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salomey</forename><surname>Osei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salomey</forename><surname>Addo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Asamoah Adu-Gyamfi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Adabankah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Opoku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Asare-Nyarko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Nyarko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15475</idno>
		<title level="m">Cynthia Amoaba, et al. NLP for ghanaian languages</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Layer normalization. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<ptr target="http://arxiv.org/abs/1607.06450" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Building text and speech datasets for low resourced languages: A case of languages in east africa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Babirye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><surname>Nakatumba-Nabende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Katumba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Ogwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><forename type="middle">Tusubira</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mukiibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Medadi</forename><surname>Ssentanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilian</forename><forename type="middle">D</forename><surname>Wanzare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>David</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SO-U99z4U-q" />
	</analytic>
	<monogr>
		<title level="m">3rd Workshop on African Natural Language Processing</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung</forename><forename type="middle">Hyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ict-based copresence in transnational families and communities: Challenging the premise of face-to-face proximity in sustaining relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loretta</forename><surname>Baldassar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Nedelcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Merla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raelene</forename><surname>Wilding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Global Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="144" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A transformer-based neural machine translation model for arabic dialects that utilizes subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Laith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><forename type="middle">K E</forename><surname>Baniata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyoung</forename><surname>Ampomah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.3390/s21196509</idno>
		<ptr target="https://www.mdpi.com/1424-8220/21/19/6509" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ParaCrawl: Web-scale acquisition of parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Ba??n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinzhen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miquel</forename><surname>Espl?-Gomis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><forename type="middle">L</forename><surname>Forcada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faheem</forename><surname>Kirefu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Ortiz</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leopoldo</forename><forename type="middle">Pla</forename><surname>Sempere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gema</forename><surname>Ram?rez-S?nchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Elsa Sarr?as, Marek Strelec, Brian Thompson, William Waites, Dion Wiggins, and Jaume Zaragoza; Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="4555" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Nvidia a100 tensor core gpu: Performance and innovation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Choquette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wishwesh</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Giroux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Stam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronny</forename><surname>Krashinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="29" to="35" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The alignment problem: Machine learning and human values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Christian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>WW Norton &amp; Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Selection criteria for low resource language programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Tracey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4543" to="4549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">M?ori are trying to save their language from big tech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donavyn</forename><surname>Coffey</surname></persName>
		</author>
		<ptr target="https://www.wired.co.uk/article/maori-language-tech" />
		<imprint>
			<date type="published" when="2021-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xnli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05053</idno>
		<title level="m">Evaluating cross-lingual sentence representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.acl-main" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An analysis of gender bias studies in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Juss?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="495" to="496" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Morisienmt: A dataset for mauritian creole machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aneerav</forename><surname>Sukhoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.02421</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Indicbart: A pre-trained model for natural language generation of indic languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himani</forename><surname>Shrotriya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratyush</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<idno>abs/2109.02903</idno>
		<ptr target="https://arxiv.org/abs/2109" />
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Racial bias in hate speech and abusive language detection datasets. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debasmita</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.12516" />
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Storia linguistica dell&apos;Italia repubblicana: dal 1946 ai nostri giorni</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Tullio De</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">9788858113622</biblScope>
			<pubPlace>Laterza</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Parallel text dataset for Neural Machine Translation (French -&gt; Fongbe, French -&gt; Ewe)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Degila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Godson</forename><surname>Kalipe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamiil Tour?</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Momboladji</forename><surname>Balogoun</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4266935</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4266935" />
		<imprint>
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Language, meaning, and games: A model of communication, coordination, and evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Demichelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jorgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weibull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1292" to="1311" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00341</idno>
		<title level="m">Jukebox: A generative model for music</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Measuring the carbon intensity of ai in cloud instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Prewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Tachet Des</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Combes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Odmark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><forename type="middle">Sasha</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Decario</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buchanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.05229</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Healthier, fairer, safe: the global health journey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Donaldson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Rutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Health Organization</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Glam: Efficient scaling of language models with mixture-of-experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">Emma</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kellie</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toju</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cui</surname></persName>
		</author>
		<idno>abs/2112.06905</idno>
		<ptr target="https://arxiv.org/abs/2112.06905" />
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Mapping languages: The corpus of global language use. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Dunn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="999" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Short text language identification for under resourced languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardt</forename><surname>Duvenhage</surname></persName>
		</author>
		<idno>abs/1911.07555</idno>
		<ptr target="http://arxiv.org/abs/1911.07555" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abteen</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Mager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arturo</forename><surname>Oncevay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Chiruzzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annette</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan Vladimir Meza</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Gim?nez-Lugo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeth</forename><surname>Mager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolando</forename><surname>Coto-Solano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>AmericasNLI: Evaluating zero-shot natural</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Towards opening the black box of neural machine translation: Source and target interpretations of the transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Ferrando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><forename type="middle">I</forename><surname>G?llego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belen</forename><surname>Alastruey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costajuss?</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.11631" />
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Ensemble distillation for neural machine translation. CoRR, abs/1702.01802</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1702" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Alon Lavie, and Ond?ej Bojar. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitika</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Kiu</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.wmt-1.73" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Conference on Machine Translation</title>
		<meeting>the Sixth Conference on Machine Translation<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11" />
			<biblScope unit="page" from="733" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Value sensitive design: Shaping technology with moral imagination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Batya</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hendry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multi-level bootstrapping for extracting parallel sentences from a quasi-comparable corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Cheung</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/C04-1151/" />
	</analytic>
	<monogr>
		<title level="m">COLING 2004, 20th International Conference on Computational Linguistics, Proceedings of the Conference</title>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-08-27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Realtoxicityprompts: Evaluating neural toxic degeneration in language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchin</forename><surname>Samuel Gehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno>abs/2009.11462</idno>
		<ptr target="https://arxiv.org/abs/2009.11462" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Combating fake news in &quot;low-resource&quot; languages: Amharic fake news detection accompanied by resource crafting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fantahun</forename><surname>Gereme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tewodros</forename><surname>Ayall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dagmawi</forename><surname>Alemu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Modeling contested categorization in linguistic databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Good</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Calvin</forename><surname>Hendryx-Parker</surname></persName>
		</author>
		<ptr target="https://www.perspectiveapi.com" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMELD 2006 Workshop on Digital Language Documentation: Tools and standards: The state of the art</title>
		<meeting>the EMELD 2006 Workshop on Digital Language Documentation: Tools and standards: The state of the art</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2022" to="2027" />
		</imprint>
	</monogr>
	<note>Google Jigsaw. Perpective api</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Explaining sequence-level knowledge distillation as data-augmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03334</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Distill, adapt, distill: Training small, in-domain models for neural machine translation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duh</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2003.02877" />
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The NRC system for discriminating similar languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>L?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first workshop on applying NLP tools to similar languages, varieties and dialects</title>
		<meeting>the first workshop on applying NLP tools to similar languages, varieties and dialects</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="139" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Chasing carbon: The elusive environmental footprint of computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><forename type="middle">Guen</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Tse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu-Yeon</forename><surname>Hsien-Hsin Sean Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carole-Jean</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The FLORES evaluation datasets for low-resource machine translation: Nepali-English and Sinhala-English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1632</idno>
		<ptr target="https://aclanthology.org/D19-1632" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="6098" to="6111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Discriminating between similar nordic languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.vardial-1.8" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects</title>
		<meeting>the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects<address><addrLine>Kiyv, Ukraine</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-04" />
			<biblScope unit="page" from="67" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Introduction to arabic natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Introduction to Arabic Natural Language Processing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Morphological analysis and disambiguation for dialectal Arabic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramy</forename><surname>Eskander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadi</forename><surname>Tomeh</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/N13-1044" />
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<meeting><address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="426" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">English2gbe: A multilingual machine translation model for {Fon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Hacheme</surname></persName>
		</author>
		<idno>abs/2112.11482</idno>
		<ptr target="https://arxiv.org/abs/2112.11482" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Survey of Low-Resource Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jind?ich</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00446</idno>
		<ptr target="https://doi.org/10.1162/coli_a_00446" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2022-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">HornMT: Machine translation benchmark dataset for languages in the horn of africa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gebrekirstos</forename><forename type="middle">G</forename><surname>Asmelash Teka Hadgu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Gebremeskel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aregawi</surname></persName>
		</author>
		<ptr target="https://github.com/asmelashteka/HornMT" />
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Teaching and researching: Language and culture. Routledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><forename type="middle">Kelly</forename><surname>Hall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Hammarstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Forkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Haspelmath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Bank</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Glottolog database 4.6, 2022</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Achieving human parity on automatic chinese to english news translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Aue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Chowdhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arul</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1803.05567" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Planning for a standard language in modern norway</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einar</forename><surname>Haugen</surname></persName>
		</author>
		<idno>00035483</idno>
		<ptr target="http://www.jstor.org/stable/30022188" />
	</analytic>
	<monogr>
		<title level="j">Anthropological Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="8" to="21" />
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">KenLM: faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the EMNLP 2011 Sixth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Bitext mining using distilled sentence representations for low-resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?elebi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Out-of-the-box universal Romanization tool uroman</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-4003</idno>
		<ptr target="https://aclanthology.org/P18-4003" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018, System Demonstrations</title>
		<meeting>ACL 2018, System Demonstrations<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">The rural-urban digital divide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Blanks Hindman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism &amp; Mass Communication Quarterly</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="549" to="560" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<ptr target="https://arxiv.org/abs/1503.02531" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Khmer natural language processing tookit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoang</surname></persName>
		</author>
		<ptr target="https://github.com/VietHoang1512/khmer-nltk" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Iterative back-translation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy</forename><surname>Vu Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
		<meeting>the 2nd Workshop on Neural Machine Translation and Generation</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">BanFakeNews: A dataset for detecting fake news in Bangla</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Md Zobaer Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md Saiful</forename><surname>Ashraful Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipta</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2862" to="2871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">The internet as a human right</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename><forename type="middle">M</forename><surname>West</surname></persName>
		</author>
		<ptr target="https://www.brookings.edu/blog/techtank/2016/11/07/the-internet-as-a-human-right/" />
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4411" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changho</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Salas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jithin</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhat</forename><surname>Ram</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.03382</idno>
	</analytic>
	<monogr>
		<title level="m">Adaptive mixture-of-experts at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>translation</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Continual mixed-language pre-training for extremely low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Genta Indra Winata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.239</idno>
		<ptr target="https://aclanthology.org/2021.findings-acl.239" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08" />
			<biblScope unit="page" from="2706" to="2718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10437" to="10446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Text normalization for endangered languages: the case of Ligurian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Lusito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Maillard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.07861</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Deltalm: Encoder-decoder pre-training for language generation and translation by augmenting pretrained multilingual encoders. CoRR, abs/2106.13736</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Muzio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hany Hassan Awadalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2106.13736" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Hate speech detection: Challenges and solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao-Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katina</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ophir</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">221152</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Madhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sushane</forename><surname>Parthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyanka</forename><surname>Bedekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruchi</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratyush</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh M</forename><surname>Khapra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.03018</idno>
		<title level="m">Aksharantar: Towards building open transliteration tools for the next billion users</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Findings of the AmericasNLP 2021 shared task on open machine translation for indigenous languages of the Americas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Mager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arturo</forename><surname>Oncevay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abteen</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annette</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximena</forename><surname>Gutierrez-Vasques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Chiruzzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Gim?nez-Lugo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan Vladimir Meza</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolando</forename><surname>Coto-Solano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeth</forename><surname>Mager-Hois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.americasnlp-1.23</idno>
		<ptr target="https://aclanthology.org/2021.americasnlp-1.23" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas</title>
		<meeting>the First Workshop on Natural Language Processing for Indigenous Languages of the Americas<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="202" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Low-resource languages: A review of past work and future challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Magueresse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Carles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Heetderks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07264</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Mburisano covid-19 multilingual corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurette</forename><surname>Marais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilana</forename><surname>Wilken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Van Niekerk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Calteaux</surname></persName>
		</author>
		<ptr target="https://hdl.handle.net/20.500.12185/536" />
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Tagged back-translation revisited: Why does it really work?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Marie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Fujita</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.532</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.532" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="5990" to="5997" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">The Johns Hopkins University Bible corpus: 1600+ tongues for typological exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Wicks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Nicolai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yarowsky</surname></persName>
		</author>
		<idno>979-10-95546-34-4</idno>
		<ptr target="https://aclanthology.org/2020.lrec-1.352" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05" />
			<biblScope unit="page" from="2884" to="2892" />
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Umap: Uniform manifold approximation and projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Grossberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page">861</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Autshumato machine translation evaluation set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cindy</forename><forename type="middle">A</forename><surname>Mckellar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Centre for Text Technology (CTexT)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Techno-optimism or information imperialism: Paradoxes in online networking, social media and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mclennan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Technology for Development</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="380" to="399" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Language identification: a solved problem suitable for undergraduate instruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computing sciences in colleges</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="94" to="101" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Between words and characters: A brief history of open-vocabulary modeling and tokenization in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sabrina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Salesky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manan</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Gall?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10508</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">A large-scale study of machine translation in the turkic languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamshidbek</forename><surname>Mirzakhalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ataman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherzod</forename><surname>Kariev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otabek</forename><surname>Abduraufov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mammad</forename><surname>Hajili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sardana</forename><surname>Ivanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abror</forename><surname>Khaytbaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Laverghetta</surname><genName>Jr</genName></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04593</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Tackling online abuse: A survey of automated abuse detection methods. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushkar</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1908.06024" />
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Asian Federation of Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zaldivar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vasserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><surname>Inioluwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Raji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gebru</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/I17-2050" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* &apos;19</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency, FAT* &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="220" to="229" />
		</imprint>
	</monogr>
	<note>Model cards for model reporting</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Globalisation as a challenge to islamic cultural identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abu</forename><surname>Sadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nurullah</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Interdisciplinary Social Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="45" to="52" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">An exploration of vocabulary size and transfer effects in multilingual language models for african languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akintunde</forename><surname>Oladipo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Odunayo</forename><surname>Ogundepo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelechi</forename><surname>Ogueji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HOZmF9MV8Wc" />
	</analytic>
	<monogr>
		<title level="m">3rd Workshop on African Natural Language Processing</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Masakhane -machine translation for africa. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iroro</forename><surname>Orife</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blessing</forename><surname>Sibanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Whitenack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Siminyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Martinus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamiil Toure</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><forename type="middle">Z</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vukosi</forename><surname>Marivate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salomon</forename><surname>Kabongo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Musie</forename><surname>Meressa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Espoir</forename><surname>Murhabazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orevaoghene</forename><surname>Ahia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elan</forename><surname>Van Biljon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arshath</forename><surname>Ramkilowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adewale</forename><surname>Akinfaderin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>?ktem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wole</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghollah</forename><surname>Kioko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Degila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herman</forename><surname>Kamper</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2003.11529" />
		<editor>Chris Emezue, Kelechi Ogueji, and Abdallah Bashir</editor>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>Bonaventure Dossou</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro Javier Ortiz</forename><surname>Su?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<idno type="DOI">10.14618/IDS-PUB-9021</idno>
		<ptr target="https://hal.inria.fr/hal-02148693" />
	</analytic>
	<monogr>
		<title level="m">7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)</title>
		<editor>Piotr Ba?ski, Adrien Barbaresi, Hanno Biber, Evelyn Breiteneder, Simon Clematide, Marc Kupietz, Harald L?ngen, and Caroline Iliadi</editor>
		<meeting><address><addrLine>Cardiff, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
	<note>Leibniz-Institut f?r Deutsche Sprache</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4009</idno>
		<ptr target="https://aclanthology.org/N19-4009" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis-Miquel</forename><surname>Munguia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maud</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10350</idno>
		<title level="m">Carbon emissions and large neural network training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Data and its (dis) contents: A survey of dataset development and use in machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amandalynne</forename><surname>Paullada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><surname>Inioluwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Raji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patterns</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">100336</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">What to do about non-standard (or non-canonical) language in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1163</idno>
		<ptr target="https://aclanthology.org/D16-1163" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference on Natural Language Processing (KONVENS 2016)</title>
		<meeting>the 13th Conference on Natural Language Processing (KONVENS 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">chrf++: words helping character n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Popovi?</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W17-4770" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="612" to="618" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6319</idno>
		<ptr target="https://aclanthology.org/W18-6319" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Mining training data for language modeling across the world&apos;s languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manasa</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Breiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Van Esch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SLTU</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="61" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">BPE-dropout: Simple and effective subword regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Provilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Emelianenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.170</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.170" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="1882" to="1892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" />
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">United Nations general assembly resolutions: A six-language parallel corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Rafalovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the MT Summit XII</title>
		<meeting>the MT Summit XII<address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="292" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Effect of tokenisation strategies for low-resourced southern african languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenalea</forename><surname>Rajab</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SpMeq5M48W9" />
	</analytic>
	<monogr>
		<title level="m">3rd Workshop on African Natural Language Processing</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Deepspeed-moe: Advancing mixtureof-experts inference and training to power next-generation ai scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conglong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><forename type="middle">Yazdani</forename><surname>Aminabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ammar</forename><surname>Ahmad Awan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.05596</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Samanantar: The largest publicly available parallel corpora collection for 11 indic languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gowtham</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumanth</forename><surname>Doddapaneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravinth</forename><surname>Bheemaraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Jobanputra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujit</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshita</forename><surname>Diddee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahalakshmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyanshu</forename><surname>Kakwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aswin</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srihari</forename><surname>Nagaraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Deepak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Vivek Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratyush</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh Shantadevi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khapr</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00452</idno>
		<ptr target="https://aclanthology.org/2022.tacl-1.9" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="145" to="162" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Comet: A neural framework for mt evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Farinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2685" to="2702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using siamese bert-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Making monolingual sentence embeddings multilingual using knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4512" to="4525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Unsupervised neural machine translation with smt as posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirui</forename><surname>Shuo Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title level="m" type="main">Investigating failures of automatic translation in the case of unambiguous gender. CoRR, abs/2104.07838, 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adithya</forename><surname>Renduchintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2104.07838" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Mining the Web for Bilingual Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology" />
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="99" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">The Web as a Parallel Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/J03-3002" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="349" to="380" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title level="m" type="main">Infographic: English is the internet&apos;s universal language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Richter</surname></persName>
		</author>
		<ptr target="https://www.statista.com/chart/26884/languages-on-the-internet/" />
		<imprint>
			<date type="published" when="2022-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Standard and non-standard language attitudes in a creole continuum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Rickford</surname></persName>
		</author>
		<idno type="DOI">10.1515/9783110857320.145</idno>
		<ptr target="https://doi.org/10.1515/9783110857320.145" />
	</analytic>
	<monogr>
		<title level="m">Language of Inequality</title>
		<editor>Nessa Wolfson and Joan Manes</editor>
		<imprint>
			<publisher>De Gruyter Mouton</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="145" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Translationese as a language in &quot;multilingual&quot; nmt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parker</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Caswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7737" to="7746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Three directions for the design of human-centered machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samantha</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wesley</forename><forename type="middle">Hanwen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Liebling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lahav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>D?az</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloufar</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salehi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Google Research</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Measuring the reliability of hate speech annotations: The case of the european refugee crisis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Kurowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wojatzki</surname></persName>
		</author>
		<idno>abs/1701.08118</idno>
		<ptr target="https://aclanthology.org/2014.eamt-1.43" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual conference of the European Association for Machine Translation</title>
		<meeting>the 17th Annual conference of the European Association for Machine Translation<address><addrLine>Dubrovnik, Croatia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
	<note>European Association for Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Billions of parallel words for free: Building and using the EU bookshop corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raivis</forename><surname>Skadi??</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberts</forename><surname>Rozis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiga</forename><surname>Deksne</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2014/pdf/846_Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05" />
			<biblScope unit="page" from="1850" to="1855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Findings of the WMT 2021 shared task on quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Blain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Fomicheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrysoula</forename><surname>Zerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.wmt-1.71" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Conference on Machine Translation</title>
		<meeting>the Sixth Conference on Machine Translation<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11" />
			<biblScope unit="page" from="684" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Knowledge distillation for multilingual unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3525" to="3535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cass</forename><surname>Robert Sunstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Thaler</surname></persName>
		</author>
		<idno>abs/1512.00567</idno>
		<ptr target="http://arxiv.org/abs/1512.00567" />
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<date type="published" when="2003" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Libertarian paternalism</note>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Multilingual neural machine translation with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<title level="m" type="main">Multilingual translation with extensible multilingual pretraining and finetuning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chau</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2008.00401" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<monogr>
		<title level="m" type="main">The wili benchmark dataset for written language identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Thoma</surname></persName>
		</author>
		<idno>abs/1801.07779</idno>
		<ptr target="http://arxiv.org/abs/1801.07779" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in OPUS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Facebook AI&apos;s WMT21 news translation task submission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chau</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.wmt-1.19" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Conference on Machine Translation</title>
		<meeting>the Sixth Conference on Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11" />
			<biblScope unit="page" from="205" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">The dark side of digital politics: Understanding the algorithmic manufacturing of consent and the hindering of online dissidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiliano</forename><surname>Trer?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opening Governance</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Reliable Measures for Aligning Japanese-English News Articles and Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P03-1010" />
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Value-sensitive design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeroen</forename><surname>Van Der Hoven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemi</forename><surname>Manders-Huits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Ethics of Information Technologies</title>
		<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="329" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">The Eyes of the overworld</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Vance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Macmillan Reference USA</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Challenges and frontiers in abusive content detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertie</forename><surname>Vidgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebekah</forename><surname>Tromble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Margetts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third workshop on abusive language online</title>
		<meeting>the third workshop on abusive language online</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="80" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<title level="m" type="main">On the features of translationese. Digital Scholarship in the Humanities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Volansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Ordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuly</forename><surname>Wintner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="98" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">Deepnet: Scaling transformers to 1,000 layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2203" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Multi-task learning for multilingual neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.75</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-main.75" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="1022" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<title level="m" type="main">Extending multilingual bert to low-resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.13640" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Gradient vaccine: Investigating and improving multi-task optimization in massively multilingual models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<monogr>
		<title level="m" type="main">The success of open source</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Weber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Harvard University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<monogr>
		<title level="m" type="main">Challenges in detoxifying language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amelia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirsty</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Coppin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/2109.07445</idno>
		<ptr target="https://arxiv.org/abs/2109.07445" />
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">CCNet: Extracting high quality monolingual datasets from web crawl data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?douard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
		<meeting>The 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4003" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<monogr>
		<title level="m" type="main">Language identification with a reciprocal rank classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Widdows</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brew</surname></persName>
		</author>
		<idno>abs/2109.09862</idno>
		<ptr target="https://arxiv.org/abs/2109.09862" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Sustainable ai: Environmental implications, challenges and opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramya</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilge</forename><surname>Acun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Newsha</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiwan</forename><surname>Maeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gloria</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fiona</forename><surname>Aga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="795" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Applying the transformer to character-level transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mans</forename><surname>Hulden</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.163</idno>
		<ptr target="https://aclanthology.org/2021.eacl-main.163" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-04" />
			<biblScope unit="page" from="1901" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
		<ptr target="http://arxiv.org/abs/1609.08144" />
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<editor>Greg Corrado, Macduff Hughes, and Jeffrey Dean</editor>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Detoxifying language models risks marginalizing minority voices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eshaan</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.190</idno>
		<ptr target="https://aclanthology.org/2021.naacl-main.190" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="2390" to="2397" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<title level="m" type="main">Recipes for safety in open-domain chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<idno>abs/2010.07079</idno>
		<ptr target="https://arxiv.org/abs/2010.07079" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Beyond goldfish memory: Long-term opendomain conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.356</idno>
		<idno>00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 0.99 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 1.00 1.00 0.99 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 0.99 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 0.99 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 1.00 1.00 0.99 0.99 0.99 0.99 1.00 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 1.00 0.99 1.00 0.99 1.00 0.99 1.00 0.99 0.99 0.99 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 0.99 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 0.99 1.00 1.00 0.99 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 1.00 0.99 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 0.99 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 0.99 1.00 0.99 1.00 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 0.99 0.99 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 0.99 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 0.99 1.00 0.99 1.00 0.99 1.00 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 1.00 0.99 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 1.00 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.99 0.99 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.98 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.98 0.99 1.00 1.00 0.99 0.99 0.98 0.98 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.99 0.99 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 0.99 1.00 0.99 1.00 0.99 1.00 0.99 1.00 0.99 0.99 0.99 0.99 0.99 1.00 0.99 1.00 0.99 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 1.00 0.99 0.99 1.00 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 0.99 0.99 0.99 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 0.99 0.99 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 0.99 0.99 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.99 0.99 0.99 1.00 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.99 0.99 0.99 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.99 0.99 0.99 1.00 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 0.99 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 1.00 1.00 1.00 0.99 1.00 0.99 1.00 0.99 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 1.00 1.00 0.99 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 0</idno>
		<ptr target="https://aclanthology.org/2022.acl-long." />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5180" to="5197" />
		</imprint>
	</monogr>
	<note>Long Papers). 99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 1.00 1.00 0.99 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 1.00 0.99 0.99 1.00 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 0.99 0.99 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 0.99 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 0.98 0.99 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.98 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 1.00 0.98 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 0.99 0.99 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99. 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 1.00 1.00 1.00 0.99 0.99 0.99 0.99 1.00 0.99 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 1.00 0.99 0.99 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 0.99 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 1.00 0.99 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 1.00 0.99 0.99 1.00 0.99 0.99 1.00 1.00 0.98 0.99 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.98 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 0.99 0.99 1.00 1.00 1.00 0.98 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 0.99 0.99 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 0.99 0.99 0.99 1.00 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 1.00 0.99 0.99 1.00 1.00 0.98 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 1.00 0.98 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 0.99 0.99 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.. 99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 0.99 1.00 0.99 1.00 0.99 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 1.00 1.00 0.99 0.99 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.99 0.99 0.99 0.98 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 1.00 0.99 1.00 0.99 0.99 1.00 0.99 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 1.00 0.98 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 0.99 1.00 1.00 0.99 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 0.99 1.00 1.00 1.00 0.99 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00</note>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">They must also have a high level fluency (C1-C2) in English. For non-English translators, they are required to have a high level fluency of their source language. Translators are also required to have at least two to three years of translation experience in the relevant language pair if they have an academic degree in translation or linguistics and three to five years of translation experience if they do not have any relevant academic qualification. Translators also undergo a translation test every 18 months to assess their translation quality and have for reference for all future projects. ? Licensing Information We are releasing translations based on source sentences from the World Health Organization under the terms of CC-BY-SA. c We are releasing translations based on source sentences from TAUS, Multi-Session Chat, and WMT under the terms of CC-BY-NC. d ? Citation Information NLLB Team et al, No Language Left Behind: Scaling Human-Centered Machine Translation</title>
	</analytic>
	<monogr>
		<title level="m">Additional Information ? Dataset Curators All translators who participated in the NLLB Multi-Domain data data creation underwent a vetting process by our translation vendor partners. Translators are required to be native speakers and educated in the target language</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>dataset_ card.html. Note that this card overlaps significantly with the previous NLLB-Seed card</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
