<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detection in Crowded Scenes: One Proposal, Multiple Predictions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuangeng</forename><surname>Chu</surname></persName>
							<email>xgchu@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anlin</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>sunjian@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Detection in Crowded Scenes: One Proposal, Multiple Predictions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a simple yet effective proposal-based object detector, aiming at detecting highly-overlapped instances in crowded scenes. The key of our approach is to let each proposal predict a set of correlated instances rather than a single one in previous proposalbased frameworks. Equipped with new techniques such as EMD Loss and Set NMS, our detector can effectively handle the difficulty of detecting highly overlapped objects. On a FPN-Res50 baseline, our detector can obtain 4.9% AP gains on challenging CrowdHuman dataset and 1.0% MR ?2 improvements on CityPersons dataset, without bells and whistles. Moreover, on less crowed datasets like COCO, our approach can still achieve moderate improvement, suggesting the proposed method is robust to crowdedness. Code and pre-trained models will be released at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Proposal-based framework has been widely used in modern object detection systems <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33]</ref>, both for one-stage <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b9">10]</ref> and two/multi-stage <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2]</ref> methods. The paradigm in general has a two-step pipeline: first, generating overcomplete object proposals in handcraft (e.g. predefined anchors <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b20">21]</ref>) or learnable (e.g. RPNs <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13]</ref>) manner; then, predicting a single instance corresponding to each proposal box with a confidence score and a refined location. To remove duplicate predictions, methods such as Non-maximum Suppression (NMS) are usually required for post-processing.</p><p>Although proposal-based approaches have achieved state-of-the-art performances <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> in popular datasets such as COCO <ref type="bibr" target="#b21">[22]</ref> and PASCAL VOC <ref type="bibr" target="#b7">[8]</ref>, it is still very challenging for crowded detection in practice. <ref type="figure" target="#fig_0">Fig. 1</ref> (a) shows a common failure case: the detector fails to predict instances heavily overlapped with others (indicated in dashed box).</p><p>This kind of typical failure in crowded scenes is mainly ascribed to two reasons. First, highly overlapped instances (as well as their associated proposals) are likely to have very similar features. As a result, it is difficult for a detector to generate distinguishing prediction for each proposal respectively (illustration is show in <ref type="figure">Fig. 2</ref> for a concrete example). Second, since instances may heavily overlap each other, the predictions are very likely to be mistakenly suppressed by NMS, as depicted in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>.</p><p>Previous works have tried to address this issue from different perspectives, such as sophisticated NMS <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b16">17]</ref>, new loss functions <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b39">40]</ref>, re-scoring <ref type="bibr" target="#b18">[19]</ref>, part-based detectors <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b2">3]</ref>. However, as we will analyze later (Sec. 2), current works are either too complex or less effective for handing highly-overlapped cases, or degrading the performance of less-overlapped cases.</p><p>In this paper, a new scheme is introduced to handle this difficulty: for each proposal box, instead of predicting a single instance, as usual, we suggest predicting a set of instances that might be highly overlapped, as described in <ref type="figure">Fig. 2</ref>. With this scheme, the predictions of nearby proposals are expected to infer the same set of instances, rather than distinguishing individuals, which is much easy to be learned. We also introduce several techniques in the new scheme. Firstly, a EMD loss is proposed to supervise the learning of instance set prediction. Secondly, a new postprocessing method named Set NMS is introduced to suppress the duplicates from different proposals, which aims at overcoming the drawbacks of na?ve NMS in crowd scenes. Lastly, an optional refinement module (RM) is designed to handle the potential false positives.</p><p>Our method is simple and almost cost-free. It is applicable to all the proposal-based detectors such as <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13]</ref>. The major modification is adding a prediction branch, which only brings negligible extra cost. But the improvement is significant: on CrowdHuman [32] dataset, our method boosts the AP score by 4.5% (without refinement module) or 4.9% (with refinement module); in addition, the recall on crowded instances improves by 8.9%. More importantly, fewer false positive appears, suggested by the slightly improved MR ?2 index even without refinement module. Besides, on less crowded datasets, our method can still obtain moderate gains. For example, on CityPersons we achieve 0.9% and 1.0% improvements in AP and MR ?2 over the baseline; and on COCO, <ref type="bibr" target="#b21">[22]</ref> it obtains 1.0% higher AP score. All experiments conducted on different datasets illustrate that our method can handle all scenes gracefully, regardless of the crowdedness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>As mentioned in the introduction, the paradigm of proposal-based object detectors mainly involves two steps: the first step is proposal box generation, which could be achieved by Selective Search <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref>, predefined/learnable anchors <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b44">45]</ref> or Region Proposal Networks (RPNs) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2]</ref>, etc. The second step is instance prediction, i.e. predicting the refined detection results corresponding to each proposal box. We primarily focus on the second step in this paper.</p><p>For instance prediction, current state-of-the-art object  <ref type="figure">Figure 2</ref>. A typical case in crowded detection. A knife and a fork share almost the same bounding boxes. Three proposal boxes (red, green and blue, best viewed in color) are heavily overlapped. (a) Single predication paradigm (see Sec. 2). Each proposal box is expected to predict a single instance <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b12">13]</ref> (may be empty) , which is intrinsically difficult as the proposals share very similar feature. Moreover, after NMS, it is very likely that only one prediction survives. (b) In our approach, each proposal predicts a set of instances. Our Set NMS can easily remove the duplicate prediction sets together (not illustrated in the figure).</p><p>detection frameworks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref> usually attach a detection function to each proposal box, which is used to determine whether the proposal is associated to some ground truth instance; if true, further to predict the corresponding class label and the refined bounding box for the object. This mechanism implies that each proposal box corresponds to single ground truth (usually the one with the largest overlap to the proposal box). Therefore, the proposal boxes have to be over-completed to ensure every instance has a chance to be detected, which introduces many duplicates to the predictions. As a result, duplicate removal methods such as Non-Maximum Suppression (NMS) are necessary for those frameworks to filter out the duplicate results. Although the above paradigm seems to obtain outstanding results on some benchmarks such as COCO <ref type="bibr" target="#b21">[22]</ref> and PASCAL VOC <ref type="bibr" target="#b7">[8]</ref>. It suffers from missing detection in crowded scenarios due to post-processing methods, e.g. NMS. <ref type="figure" target="#fig_0">Fig. 1 (a)</ref> shows an example: people in the dashed boxes are suppressed by the nearby boxes mistakenly. Thus, several approaches or workarounds have been proposed to address this limitation, which can be categorized as follows:</p><p>Advanced NMS. The effectiveness of na?ve NMS is based on the assumption that multiple instances rarely occur at the same location, which is no longer satisfied in the crowded scenes. Several improved NMS approaches have been proposed. For example, Soft-NMS <ref type="bibr" target="#b0">[1]</ref> and Softer-NMS <ref type="bibr" target="#b15">[16]</ref> suggest decaying the confidence score of the neighboring predictions for suppression rather than directly discard them. <ref type="bibr" target="#b29">[30]</ref> employs Quadratic Binary Optimization to predict instances, taking advantage of the prior distribution of ground truth's sizes. However, such heuristic variants of NMS are not always valid under different circumstances. Thus, more complex mechanisms may be introduced, for example, <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref> uses a neural network for more sophisticated and data-dependent duplicate removal. Although these methods raise the upper bound of na?'ve NMS, the pipeline becomes much more complex and costly in computation. Other works such as <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17]</ref> propose to predict different NMS thresholds for different bounding boxes. As the major drawback, they need an extra structure for IoU/density estimation, which introduces more hyperparameters. Besides, it is still difficult to distinguish heavily overlapped boxes as in <ref type="figure">Fig 2 (a)</ref>.</p><p>Loss functions for crowded detection. A few previous works propose new loss functions to address the problem of crowded detection. For example, <ref type="bibr" target="#b42">[43]</ref> proposes Aggregation Loss to enforce proposals to be close and locate compactly to the corresponding ground truth. <ref type="bibr" target="#b39">[40]</ref> proposes Repulsion Loss, which introduces extra penalty to proposals intertwine with multiple ground truths. The quality of detections in crowded scenes is improved with the help of these loss functions. However, since traditional NMS is still needed in the frameworks, it is still difficult to recall the overlapped instances illustrated in <ref type="figure">Fig 2 (a)</ref>.</p><p>Re-scoring. In many detection frameworks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> a proposal box is bound to a ground truth as long as the overlap is larger than a given threshold, which usually leads to a many-to-one relation between proposals and groundtruth instances thus NMS is required to remove the duplicate proposals. Instead, if we redesign the loss function to encourage one-to-one relation, the NMS procedure may be eliminated to avoid miss detection. We name the method rescoring. Some of the previous works follow the idea. For example, in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, each ground-truth instance is associated strictly to one proposal box during training. However, in the architectures of <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, due to lack of connections between proposals, the prediction may be ambiguous, because it is not sure for one proposal to determine whether the related instance has been predicted by another proposal. Actually, in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> NMS is still involved. RelationNet <ref type="bibr" target="#b18">[19]</ref>, instead, explicitly models the relations between proposals, which is supposed to overcome the limitations of <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. Using re-scoring, RelationNet obtains outstanding performance on COCO <ref type="bibr" target="#b21">[22]</ref> dataset even without NMS. However, in a more crowded dataset CrowdHuman <ref type="bibr" target="#b31">[32]</ref>, we find RelationNet with re-scoring performs relatively poor (see Sec. 4 for details). It may be because on CrowdHuman dataset, proposals have to be much denser than those on COCO. As a result, the re-scoring network needs to generate different predictions from very close proposals (so their features and relations are also very similar, as shown in <ref type="figure">Fig. 2 (a)</ref>), which is infeasible for neural networks.</p><p>There are other approaches on crowded detection, for example, part-based detectors <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b2">3]</ref>, which is mainly used in detecting special instances such as pedestrian. The discussions are omitted in this paper.</p><p>In conclusion, based on the above analyses, we argue that object detection in the crowded scene may be fundamentally difficult, or at least nontrivial and complex for the mentioned existing proposal-based frameworks. The key issue lies in the basic paradigm of predicting only one instance for each proposal box. It inspires us to explore new instance prediction schemes, i.e. multiple instance prediction for each proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach: Multiple Instance Prediction</head><p>Our approach is motivated by the observation: consider there are multiple objects heavily overlapped with each other, like the case in <ref type="figure">Fig. 2</ref>; if one proposal corresponds to any of the objects, it is very likely to overlap all the other objects. So, for such a proposal box, rather than predict a single object, why not predict them all? Formally, for each proposal box b i , the new scheme suggests predicting the correlated set of ground-truth instances G(b i ) instead of a single object:</p><formula xml:id="formula_0">G(b i ) = {g j ? G|IoU(b i , g j ) ? ?} ,<label>(1)</label></formula><p>where G is the set of all the ground truth boxes and ? is a given threshold of intersection-over-union (IoU) ratio. <ref type="figure">Fig. 2</ref> (b) visualizes the concept. Compared with the previous single-instance-prediction framework, we find our new scheme may greatly ease the learning in crowded scenes. As shown in <ref type="figure">Fig. 2 (b)</ref>, all of the three proposal boxes are assigned to the same set of ground-truth instance -it is a feasible behavior since the three proposals actually share almost the same feature. While for the previous singleinstance-prediction paradigm ( <ref type="figure">Fig. 2 (a)</ref>), each proposal has to produce distinguishing predictions, which might be intrinsically difficult.</p><p>We introduce the details of our approach as follows:</p><p>Instance set prediction. For each proposal box b i , most of the modern proposal-based detection frameworks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13</ref>] employ a detection function to predict a pair (c i , l i ) to represent the associated instance, where c i is the class label with confidence and l i is the relative coordinates. In our approach, to predict a set of instances, we introduce a simple extension -just by introducing K detection functions to generate a set of predictions P(b i ):</p><formula xml:id="formula_1">P(b i ) = (c (1) i , l (1) i ), (c (2) i , l (2) i ), . . . , (c (K) i , l (K) i ) , (2)</formula><p>where K is a given constant standing for the maximum cardinality of G(b i ) in the dataset (see Eq. 1). P(b i ) can be simply implemented by introducing extra prediction branches in most of the existing detection frameworks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24]</ref>, which is shown in <ref type="figure" target="#fig_2">Fig. 3</ref> (a). Note that even though K is fixed for all proposals, the network could still predict some c (k) i to background class, representing that the k-th detection function does not predict instance for the proposal b i .</p><p>EMD loss. We aim to design a loss L(b i ) to minimize the gap between predictions P(b i ) and ground-truth instances G(b i ) corresponding to the proposal b i , which can be cataloged into the problem of set distance measurement. Similar problems have been discussed in some early object detection papers, such as <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">35]</ref>. Inspired by them, we design the following EMD loss to minimize the Earth Mover's Distance between the two sets:</p><formula xml:id="formula_2">L(b i ) = min ??? K k=1 L cls (c (k) i , g ? k ) + L reg (l (k) i , g ? k ) (3)</formula><p>where ? represents a certain permutation of (1, 2, . . . , K) whose k-th item is ? k ; g ? k ? G(b i ) is the ? k -th groundtruth box; L cls (?) and L reg (?) are classification loss and box regression loss respectively, following the common definitions as <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13]</ref>. Note that in Eq. 3, we assume |G(b i )| = K; if not, we add some "dummy" boxes (whose class label is regarded as background and without regression loss) to G(b i ) until it is satisfied. Intuitively, the formulation in Eq. 3 implies to explore all possible one-toone matches between predictions and ground truths, thus finding the "best match" with the smallest loss. It is also worth noting that if K = 1, Eq. 3 becomes equivalent to the loss in traditional single-instance-prediction frameworks, implying that our EMD loss is a natural generalization to the commonly-used detection loss <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Set NMS. In our approach, although each proposal is able to predict multiple associated instances, if na?ve NMS is still involved for post-processing it is impossible to detect objects effectively in crowded scenes. Fortunately, because of the EMD loss, instances predicted by one proposal are expected to be unique by definition. In other words, duplicates exist only between the predictions from different proposals, as illustrated in <ref type="figure">Fig. 2 (b)</ref>. With this prior, we introduce a simple patch to na?ve NMS pipeline, named Set NMS -each time before one box suppressing another one in the NMS algorithm, we insert an additional test to check whether the two box come from the same proposal; if yes, we skip the suppression. Experiments in Sec. 4 also suggest that only when multiple-instance-prediction and Set NMS are used together can our approach achieve significant improvement in crowded detection.</p><p>Refinement module. In our approach, each proposal is expected to generate a set of instances rather than a single one, which may suffer from increase in false positives since more predictions are generated.</p><p>Although the failure cases are rarely observed in our experiments on real images, we introduce an optional refinement module in case of the risk, as shown in <ref type="figure" target="#fig_2">Fig. 3 (b)</ref>. The module simply takes the predictions as input, combining them with proposal feature, then performs a second round of predicting. We expect the refinement module to correct the possible false predictions.</p><p>Discussion: relation to previous methods.</p><p>Predicting multiple instance is not new. Double-person detector <ref type="bibr" target="#b36">[37]</ref> models person pairs in the DPM <ref type="bibr" target="#b8">[9]</ref> framework. In the deep learning era, some early detection systems <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b6">7]</ref> also imply the high-level idea of multipleinstance-prediction, while the methods are not proposalbased. For example, MultiBox <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36]</ref> directly predicts all the instances in an image patch; YOLO v1/v2 <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> generates multiple predictions for each cell (i.e. predicting all instances centered at a certain location). Special loss functions are also proposed in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> for set prediction, whose design purposes are similar to our EMD loss.</p><p>The most related previous work to us is <ref type="bibr" target="#b34">[35]</ref> which introduces LSTM to decode instance boxes in each grid of an image. Similar to our EMD loss, they use Hungarian Loss for multiple instance supervision. For post-processing, a box stitching method is employed to merge the predictions produced by adjacent grids. They mainly evaluated the method on head detection task, which shows some capability to predict crowded objects. However, since the method does not make use of proposals, it may have difficulty in detecting objects of various sizes/shapes, such as pedestrians or general objects. Moreover, the LSTM predictor is complex, which may be nontrivial to integrated in current state-ofthe-art detection frameworks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13]</ref> efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>Theoretically, our approach can be applied to most of the state-of-the-art proposal-based detectors, no matter onestage <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref> or two-stage <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13]</ref> frameworks. In this paper, we choose FPN <ref type="bibr" target="#b19">[20]</ref> with RoIAlign [13] as a baseline detector to evaluate our method. In FPN, the Region Proposal Network (RPN) branch takes the responsibility for proposal generation, while the RCNN (or named RoI) branch is used to predict the instance corresponding to the RoI proposal. So, our method is attached to the latter branch. From Sec. 3, easy to see that there is only one extra hyper-parameter in our approach -K, the maximum cardinality of G(?) (refer to Eq. 2). In the rest of the paper, we let K = 2, which we find is satisfied for almost all the images and proposals in many detection datasets like CrowdHuman <ref type="bibr" target="#b31">[32]</ref>, COCO <ref type="bibr" target="#b21">[22]</ref> and CityPersons <ref type="bibr" target="#b41">[42]</ref>. <ref type="figure" target="#fig_2">Fig. 3 (a)</ref> illustrates the usage of our method in FPN (only RCNN branch is shown). Based on the original architecture, only slight modifications need to be made: just attach an additional instance prediction head to the tail. EMD loss is applied to the two predictions instead of the original loss. The refinement module is optional; if applied, we use the refined results as the final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>In this section, we evaluate our approach from different perspectives. Intuitively, a detection algorithm specially optimized for crowded scenes tends to recall more instances, however, often have the risk of increasing false positive predictions. Our benchmarks focus on both of the opposite aspects.</p><p>Datasets. An ideal object detector for crowded scenes should be robust to instance distributions, i.e. not only effective for crowded detections, but also stable to detect single/less-crowded objects. We adopt three datasets -CrowdHuman <ref type="bibr" target="#b31">[32]</ref>, CityPersons <ref type="bibr" target="#b41">[42]</ref> and COCO [22]for comprehensive evaluations on heavily, moderately and slightly overlapped situations respectively. <ref type="table" target="#tab_0">Table 1</ref>  "instance density" of each dataset. Since our proposed approach mainly aims to improve crowded detections. So, we perform most of the comparisons and ablations on Crowd-Human. Note that the experiment on uncrowded dataset like COCO is to verify whether our method does harm to isolated object detection, not for significant performance improvements.</p><p>Evaluation metrics We mainly take the following three criteria for different purposes:</p><p>? Averaged Precision (AP), which is the most popular metric for detection. AP reflects both the precision and recall ratios of the detection results. In our experiment, we empirically find AP is more sensitive to the recall scores, especially on crowded dataset like CrowdHuman. Larger AP indicates better performance.</p><p>? MR ?2 <ref type="bibr" target="#b5">[6]</ref>, which is short for log-average Miss Rate on False Positive Per Image (FPPI) in [10 ?2 , 100], is commonly used in pedestrian detection. MR ?2 is very sensitive to false positives (FPs), especially FPs with high confidences will significantly harm the MR ?2 ratio. Smaller MR ?2 indicates better performance.</p><p>? Jaccard Index (JI) <ref type="bibr" target="#b23">[24]</ref> is mainly used to evaluate the counting ability of a detector. Different from AP and MR ?2 which are defined on the prediction sequence with decreasing confidences, JI evaluates how much the prediction set overlaps the ground truths. Usually, the prediction set can be generated by introducing a confidence score threshold. In this paper, for each evaluation entry, we report the best JI score by exploring all possible confidence thresholds. We use the official SDK of CrowdHuman <ref type="bibr" target="#b31">[32]</ref> for JI calculation. Larger JI indicates better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detailed Settings.</head><p>Unless otherwise specified, we use standard ResNet-50 <ref type="bibr" target="#b14">[15]</ref> pre-trained on ImageNet <ref type="bibr" target="#b30">[31]</ref> as the backbone network for all the experiments. The baseline detection framework is FPN <ref type="bibr" target="#b19">[20]</ref>, while using RoIAlign <ref type="bibr" target="#b12">[13]</ref> instead of original RoIPooling. As for anchor settings, we use the same anchor scales as <ref type="bibr" target="#b19">[20]</ref>, while the aspect ratios are set to H : W = {1 : 1, 2 : 1, 3 : 1} for CrowdHuman and CityPersons, and {2 : 1, 1 : 1, 1 : 2} for COCO. For training, we use the same protocol as in <ref type="bibr" target="#b19">[20]</ref>. The batch size is 16, split to 8 GPUs. Each training runs for 30 epochs. During training, the sampling ratio of positive to negative proposals for RoI branch is 1 : 1 for CrowdHuman and 1 : 3 for CityPersons and COCO. Multi-scale training and test are not applied; instead, the short edge of each image is resized to 800 pixels for both training and test. All box overlap IoU thresholds (e.g. ? in Eq. 1, NMS thresholds, and those in calculating evaluation metrics) are set to 0.5 by default. For our method, we use K = 2 (see Eq. 2). The refinement module in <ref type="figure" target="#fig_2">Fig. 3</ref> is enabled by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment on CrowdHuman</head><p>CrowdHuman <ref type="bibr" target="#b31">[32]</ref> contains 15,000, 4,370 and 5,000 images for training, validation and test respectively. For fair comparison, we retrain most of the involved models with our own implementation under the same settings. Results are mainly evaluated on the validation set, using full-body benchmark in <ref type="bibr" target="#b31">[32]</ref>.</p><p>Main results and ablation study. <ref type="table" target="#tab_1">Table 2</ref> shows the ablation experiments of the proposed methods in Sec. 3, including multiple instance prediction with EMD loss, set NMS and refinement module. The baseline is FPN <ref type="bibr" target="#b19">[20]</ref> using NMS (IoU threshold is 0.5) for post-processing. It is clear that our methods consistently improve the performances in all criteria. Especially, even without refinement module our method still obtains 4.5% improvements in AP and 2.2% in JI, suggesting that more instances may correctly detected; more importantly, we find the MR ?2 ratio also improves, indicating that our model does not introduce more false predictions. The refinement module affects little on AP and JI, while further boosting MR ?2 by ?0.8%, suggesting that the module mainly reduces false positives as we expected. Comparisons with various NMS strategies. In <ref type="figure" target="#fig_0">Fig. 1</ref>, since some instances are mistakenly suppressed by NMS, one possible hypothesis is that the predictions may be improved by using different NMS strategies. to 45.4%), indicating that more false positives are introduced. Soft-NMS <ref type="bibr" target="#b0">[1]</ref> can boost AP score, but no improvements are obtained in MR ?2 and JI. In contrast, our method achieves the best scores in all the three metrics even without refinement module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIP Set NMS RM AP/% MR</head><p>Comparisons with previous works. To our knowledge, very few previous works on crowded detection report their results on CrowdHuman. To compare, we benchmark two methods -GossipNet <ref type="bibr" target="#b17">[18]</ref> and RelationNet <ref type="bibr" target="#b18">[19]</ref> -which are representative works categorized into advanced NMS and re-scoring approaches respectively (see Sec. 2 for the analyses). For GossipNet, we use the open-source implementation to benchmark 1 . And for RelationNet, we re-implement the re-scoring version 2 . All methods use FPN <ref type="bibr" target="#b19">[20]</ref> as the base detector with the same training settings. <ref type="table" target="#tab_3">Table 4</ref> lists the comparison results. Surprisingly, both RelationNet and GossipNet suffer from significant drop in AP and MR ?2 . Further analyses indicate that the two methods have better recall ratio than baseline NMS for crowded objects (see <ref type="table">Table 5</ref>), however, tend to introduce too many false positive predictions. Though it is still too early to claim <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref> do not work on CrowdHuman (we have not fully explored the hyper-parameters), at least the two methods are nontrivial for tuning. In contrast, our method is not only effective, but also very simple, as it has almost no additional hyper-parameters. <ref type="table" target="#tab_3">Table 4</ref> also compares a recent work AdaptiveNMS <ref type="bibr" target="#b22">[23]</ref>, which is an enhanced NMS strategy for crowded detection. In <ref type="bibr" target="#b22">[23]</ref>, CrowdHuman results based on FPN are reported. Note that since the baseline are not aligned, we cannot make the direct comparison with our results. From the numbers, we find that our method can achieve significant improvement from a stronger baseline (especially in AP), in addi-tion, the pipeline is much simpler.  <ref type="bibr" target="#b19">[20]</ref>. Higher values of AP and JI indicate better performance, which is in contrast to the MR ?2 . We use only one more stage in both of our Cascade R-CNN implementation instead of two for better performance in CrowdHuman.</p><p>Analysis on recalls. To further understand the effectiveness of our method on crowded objects, we compare the recalls of different approaches for both crowded and uncrowded instances respectively. Results are shown in Table. 5. Note that recall relates to the confidence score threshold. For fair comparison, we use the thresholds corresponding to the best JI index for each entry respectively. From the table we find that for FPN baseline/Soft-NMS, recall of crowded objects is much lower than that of uncrowded objects, implying the difficulty of crowded detection. In contrast, our method greatly improves the recall ratio of the crowded instances (from 54.4% to 63.3%, by 8.9%), in addition, uncrowded recall is also slightly improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on CityPersons</head><p>CityPersons <ref type="bibr" target="#b41">[42]</ref> is one of the widely used benchmarks for pedestrian detection. The dataset contains 5, 000 images (2, 975 for training, 500 for validation, and 1, 525 for testing, respectively). Each image is with a size of 1024 ? 2048. Following the common practice in previous works, all of the object detectors are trained on the training (reasonable) subset and tested on the validation (reasonable) subset with the enlarged resolution by 1.3? compared to the original one, which is slightly different from the settings we used for CrowdHuman <ref type="bibr" target="#b31">[32]</ref> and COCO <ref type="bibr" target="#b21">[22]</ref>. To obtain a better baseline, we follow the strategy proposed  <ref type="table">Table 5</ref>. Detection boxes recalled on CrowdHuman validation set. Only boxes with confidence score higher than a certain threshold are taken into account. The confidence thresholds are subject to the best JI scores respectively and noted in the "Conf" column. Numbers in the last three columns indicate the number of recalled boxes. "Crowd" means the corresponding ground-truth box overlaps with some other ground truth with IoU&gt;0.5, otherwise marked "Sparse". Note that recalls of Soft-NMS <ref type="bibr" target="#b0">[1]</ref> are the same with the NMS baseline, which is because the confidence threshold is relativity high (0.7), thus NMS is roughly equivalent to Soft-NMS.</p><p>in <ref type="bibr" target="#b2">[3]</ref>, namely evolving ground-truths into proposals by jittering.Other hyper-parameters remains the same as that in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Conf.  <ref type="table">Table 6</ref>. Detection recalls on CityPersons validation set. Please refer to the caption in <ref type="table">Table 5</ref> for the descriptions. Note that recalls of Soft-NMS <ref type="bibr" target="#b0">[1]</ref> are the same as the NMS baseline, which is ascribed to the confidence threshold is relativity high (0.6), thus NMS is roughly equivalent to Soft-NMS.</p><p>Qualitative results. <ref type="table" target="#tab_6">Table 7</ref> compares our method with FPN baselines with na?ve NMS and Soft-NMS respectively. Our approach improves AP and MR ?2 by 0.9% and 1.0% respectively over the NMS baseline, indicating the effectiveness our method. <ref type="table" target="#tab_6">Table 7</ref> also lists some other stateof-the-art results on CityPersons. Though it may be unfair for direct comparisons due to different hyper-parameter settings, however, at least it implies our method achieves significant gains over a relatively strong baseline. <ref type="table">Table 6</ref> further analyzes the recalls of different methods. Similar to those in CrowdHuman (refer to <ref type="table">Table 6</ref>), our method mainly significantly boosts the recall on crowded objects -from 64 increased to 96 out of a total of 108 instances in the validation set. The comparison further indicates our approach is very effective to deal with crowded scenes again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone MR ?2 AP FPN + NMS Res-50 11.7% 95.2% FPN + Soft-NMS <ref type="bibr" target="#b0">[1]</ref> 11.8% 95.3%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Res-50 10.7% 96.1%</p><p>AF-RCNN <ref type="bibr" target="#b41">[42]</ref> 12.8% -OR-CNN <ref type="bibr" target="#b42">[43]</ref> VGG-16 11.0% -Adaptive-NMS <ref type="bibr" target="#b22">[23]</ref> 10.8% -FRCNN (our impl.) <ref type="bibr" target="#b28">[29]</ref> Res-50 11.6% 95.0% Repulsion Loss <ref type="bibr" target="#b39">[40]</ref> 11.6% - </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on COCO</head><p>According to <ref type="table" target="#tab_0">Table 1</ref>, the crowdedness of COCO [22] is very low, which is out of our design purpose. So, we do not expect a significant performance gain on COCO. Instead, the purpose of introducing COCO is to verify: 1) whether our method generalizes well to multi-class detection problems; 2) whether the proposed approach is robust to different crowdedness, especially to isolated instances.</p><p>Following the common practice of <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, we use a  <ref type="table" target="#tab_7">Table 8</ref>), which may be because larger objects are more likely to overlap. The experiment suggests our method is not only very effective on crowded scenes, but also able to deal with multiple classes and isolated instances without performance drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a very simple yet effective proposal-based object detector, specially designed for crowded instance detection. The method makes use of the concept of multiple instance prediction, introducing new techniques such as EMD loss, Set NMS and refinement module. Our approach is not only effective, but also flexible to cooperate with most state-of-the-art proposal-based detection frameworks; in addition, also generalizes well to less crowded scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CrowdHuman Testing Benchmark</head><p>The CrowdHuman <ref type="bibr" target="#b31">[32]</ref> testing subset has 5, 000 images and the annotations of testing subset have not yet been released. To push the upper-bound of object detection research, the Detection In the Wild Challenge was held in CVPR 2019. The CrowdHuman testing subset is served as a benchmark in this challenge and this allows us to compare our approach to state-of-the-art methods on CrowdHuman.</p><p>To improve the performance of our approach, we replace the ResNet-50 <ref type="bibr" target="#b14">[15]</ref> with a larger model: SEResNeXt101 <ref type="bibr">[?, ?]</ref>, the short edge of test images are resized to 1200 pixels and all other settings are the same as described in our paper. We then submit our result to the test server and find that our method outperforms all the results in this challenge. The results are shown in  <ref type="table">Table 9</ref>. Part of the leaderboard and our results. The baseline model is our reimplemented FPN <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation on Number of Heads</head><p>For completeness, we further explore the only hyperparameter of our method: K in this section. In our paper, we let K = 2 because we find it is satisfied for almost all the images and proposals in CrowdHuman. If we make the K larger, the network will be able to detect instances in more crowded scenes. To explore the performance under the different values of the K, an experiment is conducted on the CrowdHuman dataset and all the settings remain the same as described in our paper except the value of K is changed. We show the results of different K values in the <ref type="table">Table.</ref>10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Results of Our Method</head><p>In this section we will show more results of our method on a video from YouTube and the CrowdHuman validation dataset. The visualization thresh is set to 0.7 to remove the redundant boxes in the results.</p><p>The video is in the attached file, and the results on the CrowdHuman validation dataset is shown in the <ref type="figure">Figure.</ref>   <ref type="table" target="#tab_0">Table 11</ref>. Experiment on RetinaNet <ref type="bibr" target="#b20">[21]</ref>. All hyperparameters between the RetinaNet baseline and our method remain the same.</p><p>The results show that one-stage detectors can also benefit from our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Our Method in One-Stage Detector</head><p>To verify the effectiveness of our method in one-stage detectors, we conducted experiments on RetinaNet <ref type="bibr" target="#b20">[21]</ref>. We adopt standard ResNet-50 pre-trained on ImageNet as the backbone network. The anchor ratios are set to H : W = {1 : 1, 2 : 1, 3 : 1} because of the shape of human instances. We use a batch size of 16 pictures over 8 GPUs, and train the network with a learning rate of 0.005 for 50 epochs. The other hyperparameters are kept the same as <ref type="bibr" target="#b20">[21]</ref>. For our method, we predict two predictions based on each anchor. And the Cross-Entropy Loss in EMD Loss is replaced by Focal Loss <ref type="bibr" target="#b20">[21]</ref>. The results are shown in Tabel11, which indicates that our method can also have gain in one-stage detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GossipNet</head><p>RelationNet Ours <ref type="figure" target="#fig_4">Figure 5</ref>. Visual comparison of the baseline, GossipNet, RelationNet, and our approach. The blue boxes are the detection results, the white boxes are the missed instances, and the orange boxes are redundant boxes. The green boxes in our method are multiple predictions form one proposal.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Human detection in crowds. (a) Results predicted by FPN [20] baseline. The dashed boxes indicate the missed detections (suppressed by NMS mistakenly). (b) Results of our method applied to FPN. All instances are correctly predicted. Boxes of the same color stem from the identical proposal (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Overall architecture. (a) boxA and boxB are the two instances predicted by one proposal, using our EMD Loss. Refinement module is an optional step. (b) The refinement module concatenates feature and box information to optimize the results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visual comparison of the baseline and our approach. The first row are the results produces by FPN with NMS. The last row are the results of our approach. The scores threshold for visualization is 0.3. Boxes with the same color stem from the identical proposal. The dashed boxes are the missed detection ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5</head><label>5</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Instance density of each dataset. The threshold for overlap statistics is IoU &gt; 0.5. *Averaged by the number of classes.</figDesc><table><row><cell>lists the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation experiments evaluated on CrowdHuman validation set. The baseline model (the first line) is our reimplemented FPN<ref type="bibr" target="#b19">[20]</ref> with ResNet-50<ref type="bibr" target="#b14">[15]</ref> backbone. MIP -multiple instance prediction with EMD loss. RM -refinement module.</figDesc><table><row><cell></cell><cell cols="2">?2 /% JI/%</cell></row><row><cell>85.8</cell><cell>42.9</cell><cell>79.8</cell></row><row><cell>87.4</cell><cell>42.8</cell><cell>80.8</cell></row><row><cell>90.3</cell><cell>42.2</cell><cell>82.0</cell></row><row><cell>90.7</cell><cell>41.4</cell><cell>82.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 Table 3 .</head><label>33</label><figDesc>Comparisons of different NMS strategies on CrowdHuman validation set. The baseline model is FPN<ref type="bibr" target="#b19">[20]</ref>.</figDesc><table><row><cell>JI/%</cell></row></table><note>explores some variants. For na?ve NMS, compared with the default setting (0.5), slightly enlarging the IoU threshold (from 0.5 to 0.6) may help to recall more instances, so AP increases; however, the MR ?2 index becomes much worse (from 42% Method IoU* AP/% MR ?2 /%* IoU thresh- old for post-processing. RM -refinement module.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>also evaluate our method on the Cascade R-CNN<ref type="bibr" target="#b1">[2]</ref> framework. We add the EMD loss and Set NMS into the last stage of Cascade R-CNN. The results show our method can still boost the performance of Cascade R-CNN significantly on crowded datasets like CrowdHuman.</figDesc><table><row><cell>Method</cell><cell cols="3">AP/% MR ?2 /% JI/%</cell></row><row><cell>FPN baseline</cell><cell>85.8</cell><cell>42.9</cell><cell>79.8</cell></row><row><cell>FPN + Soft-NMS [1]</cell><cell>88.2</cell><cell>42.9</cell><cell>79.8</cell></row><row><cell>RelationNet [19] (our impl.)</cell><cell>81.6</cell><cell>48.2</cell><cell>74.6</cell></row><row><cell>GossipNet [18] (our impl.)</cell><cell>80.4</cell><cell>49.4</cell><cell>81.6</cell></row><row><cell>Ours</cell><cell>90.7</cell><cell>41.4</cell><cell>82.3</cell></row><row><cell>FPN baseline (impl. by [23])</cell><cell>83.1</cell><cell>52.4</cell><cell>-</cell></row><row><cell>AdaptiveNMS (impl. by [23])</cell><cell>84.7</cell><cell>49.7</cell><cell>-</cell></row><row><cell cols="2">CascadeR-CNN [2] (our impl.) 86.2</cell><cell>40.2</cell><cell>80.4</cell></row><row><cell>CascadeR-CNN + Ours</cell><cell>90.6</cell><cell>38.7</cell><cell>83.9</cell></row></table><note>Table 4. Comparisons of various crowded detection methods on CrowdHuman validation set. All methods are based on FPN detec- tor</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Comparisons of different methods on CityPersons validation set. All models are evaluated with enlarged resolution of 1.3? compared to the original size. Models in the upper part are trained by ours with the same FPN base detector. Models in the lower part are trained with other protocols.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Method AP AP 50 AP 75 AP S AP M AP L Comparisons on COCO [22] minival set. All models are based on FPN detector. Results are evaluated in all the 80 classes. subset of 5000 images in the original validation set (named minival) for validation, while using the remaining images in the original training and validation set for training.Table 8shows the comparisons with FPN and FPN+Soft-NMS baselines. Moderate improvements are obtained, e.g. 1.0% better than na?ve NMS and 0.5% better than Soft-NMS in AP. Interestingly, large objects achieve the most significant improvement (see AP L in</figDesc><table><row><cell>FPN</cell><cell>37.5 59.6 40.4 23.0 41.2 48.6</cell></row><row><cell>Soft-NMS</cell><cell>38.0 59.4 41.5 23.3 41.8 49.0</cell></row><row><cell>Ours</cell><cell>38.5 60.5 41.5 23.0 41.8 50.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Table.9, and the full leaderboard is accessible on the official website of CrowdHuman Track Leaderboard 3 .</figDesc><table><row><cell cols="3">Rank Team Name Institution</cell><cell>mJI/%</cell></row><row><cell>1</cell><cell>zack0704</cell><cell>Tencent AI Lab</cell><cell>77.46</cell></row><row><cell>2</cell><cell>boke</cell><cell cols="2">Sun Yat-Sen University 75.25</cell></row><row><cell>3</cell><cell cols="2">ZNuanyang Zhejiang University</cell><cell>74.46</cell></row><row><cell></cell><cell>Method</cell><cell>Backbone</cell><cell>mJI/%</cell></row><row><cell></cell><cell>Baseline</cell><cell>ResNet-50</cell><cell>72.20</cell></row><row><cell></cell><cell>Ours</cell><cell>ResNet-50</cell><cell>76.60</cell></row><row><cell></cell><cell>Ours</cell><cell>SEResNeXt101</cell><cell>77.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>https://www.objects365.org/crowd human track.html AP/% MR ?2 /% JI/% Ablation experiments evaluated on the CrowdHuman validation set. It worth noting that if K = 1, the architecture is the same as the single-instance-prediction baseline.</figDesc><table><row><cell>K = 1</cell><cell>85.8</cell><cell>42.9</cell><cell>79.8</cell></row><row><cell>K = 2</cell><cell>90.7</cell><cell>41.4</cell><cell>82.3</cell></row><row><cell>K = 3</cell><cell>90.7</cell><cell>41.6</cell><cell>82.1</cell></row><row><cell>Method</cell><cell></cell><cell cols="3">AP/% MR ?2 /% JI/%</cell></row><row><cell cols="2">RetinaNet Baseline</cell><cell>82.0</cell><cell>56.3</cell><cell>73.2</cell></row><row><cell>Ours</cell><cell></cell><cell>82.7</cell><cell>54.7</cell><cell>74.0</cell></row></table><note>.3</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/hosang/gossipnet<ref type="bibr" target="#b1">2</ref> We use the re-scoring version rather than NMS, as NMS is clearly not suitable for crowded detection. We have checked COCO<ref type="bibr" target="#b21">[22]</ref> scores to ensure correct re-implementation.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Soft-NMS -Improving Object Detection With One Line of Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cascade r-cnn: High quality object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09756</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pedhunter: Occlusion robust pedestrian detector in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06826</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bernt Schiele, and Pietro Perona. Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="743" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable object detection using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2147" to="2154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">DSSD : Deconvolutional single shot detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno>abs/1701.06659</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">european conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bounding box regression with uncertainty for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2888" to="2897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A convnet for non-maximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="192" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning non-maximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11575</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive nms: Refining pedestrian detection in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Sequential context encoding for duplicate removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno>abs/1810.08770</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, Faster, Stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolov3: An incremental improvement. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optimized pedestrian detection for multiple and occluded people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitapa</forename><surname>Rujikietgumjorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection snip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SNIPER: Efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2325" to="2333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1441</idno>
		<title level="m">Dragomir Anguelov, and Sergey Ioffe. Scalable, high-quality object detection</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detection and tracking of occluded people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="69" />
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning strong parts for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1904" to="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2965" to="2974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Repulsion loss: Detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Metaanchor: Learning to detect objects with customized anchors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="320" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Citypersons: A diverse dataset for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Occlusion-aware r-cnn: detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="637" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Occluded pedestrian detection through guided attention in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cascade region proposal and global context for deep object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shicai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bi-box regression for pedestrian detection and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunluan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
