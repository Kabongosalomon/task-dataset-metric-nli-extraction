<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SelfReformer: Self-Refined Network with Transformer for Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisi</forename><surname>Lin</surname></persName>
							<email>wslin@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SelfReformer: Self-Refined Network with Transformer for Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The global and local contexts significantly contribute to the integrity of predictions in Salient Object Detection (SOD). Unfortunately, existing methods still struggle to generate complete predictions with fine details. There are two major problems in conventional approaches: first, for global context, CNN-based encoders cannot effectively catch longrange dependencies, resulting in incomplete predictions. Second, downsampling the ground truth to fit the size of predictions will introduce inaccuracy as the ground truth details are lost during interpolation or pooling. To address the abovementioned problems, we employed a Transformer as our encoder backbone for better long-range dependency modeling. Meanwhile, we developed a branch and framed a patchwise SOD task to learn the global context instead of assuming they are the high-level features in the encoder. Besides, for better details, we adopt Pixel Shuffle from Super-Resolution (SR) to reshape the predictions in each decoder stage back to the size of ground truth instead of the reverse. Furthermore, we developed a Context Refinement Module (CRM) to fuse global context with decoder features and automatically locate and refine the local details. The proposed network can guide and correct itself based on the global and local context generated <ref type="figure">(Fig.1)</ref>, thus is named, Self-Refined Transformer (SelfReformer). Extensive experiments and evaluation results on five benchmark datasets demonstrate the outstanding performance of the network, and we achieved the state-of-the-art. Code will be released at https://github.com/BarCodeReader/SelfReformer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Introduction SOD aims to locate and segment the object that catches human attention in a visual scene. Due to its wide applications, such as AR/VR <ref type="bibr" target="#b23">(Qin et al. 2020</ref>) and image captioning <ref type="bibr" target="#b37">(Xu et al. 2015;</ref><ref type="bibr" target="#b11">Fang et al. 2015)</ref>, it has gained growing interest in recent years. Most of the state-of-the-art models are CNNbased and often have an architecture of encoder-decoder where images are firstly encoded into multi-level features, followed by a decoder for feature fusion and saliency prediction. To further improve the accuracy, most of the work tries to develop better fusion modules <ref type="bibr" target="#b44">Pang et al. 2020)</ref>, extra refinement networks <ref type="bibr" target="#b24">(Qin et al. 2019</ref><ref type="bibr" target="#b23">(Qin et al. , 2020</ref>, utilizing different modalities like depth or contour <ref type="bibr" target="#b43">Zhao et al. 2019)</ref>, and adopting * Corresponding author. attention modules <ref type="bibr" target="#b32">(Wang et al. 2019;</ref><ref type="bibr" target="#b42">Zhang et al. 2018</ref>). These methods achieved remarkable results in the SOD task. However, CNN-based networks are limited in learning longrange relationships, resulting in a lack of global structural consistency in predictions. In recent years, Transformer <ref type="bibr" target="#b28">(Vaswani et al. 2017</ref>) was proposed to model long-range dependencies in language processing and was further extended to vision tasks. The vision transformers (ViT) <ref type="bibr" target="#b40">(Yuan et al. 2021;</ref><ref type="bibr" target="#b5">Chu et al. 2021;</ref><ref type="bibr" target="#b31">Wang et al. 2021b</ref>) split the image into patches then apply multi-head self-attention and multi-layer perceptrons to capture long-range dependencies. When applied to SOD, the transformer-based networks <ref type="bibr" target="#b25">Ren et al. 2021</ref>) are effective in modeling global context, thus generating predictions with better structural integrity.</p><p>However, there are still two big challenges for better SOD. First, SOD is a densely supervised task that requires the ground truth in different resolutions for each decoder stage. Using interpolation or pooling, fine features in the ground truth are lost, and the decoder is trained against inaccurate ground truths, resulting in poor details in predictions. Noteworthily, for input size of 224 ? 224, transformers like T2T-ViT <ref type="bibr" target="#b40">(Yuan et al. 2021)</ref> and PVT <ref type="bibr" target="#b31">(Wang et al. 2021b</ref>) usually have much smaller size of feature maps (56 ? 56 max), and how to restore the fine features from this small size for accurate SOD still remain unsolved. Second, though existing studies utilizing global and local contexts like feedback network  and multi-level fusion Figure 2: Illustration of conventional approach (grey box) and the Pixel Shuffle (PS). Details in the GT is preserved after PS but lost using interpolation. Best view in color. <ref type="bibr" target="#b44">(Pang et al. 2020)</ref>, the concept of global and local context is still intuitive as we do not have a method to obtain and qualify them explicitly. Thus, finding a better representation of global and local contexts and obtaining them in a controllable manner is still an open question.</p><p>We address the abovementioned problems from three aspects. First, to preserve the structural properties of ground truth, we adopt Pixel Shuffle (PS) <ref type="bibr" target="#b26">(Shi et al. 2016a</ref>) from Super-Resolution (SR) as the up/downsampling method. Unlike pooling or interpolation, Pixel Shuffle can reshape a high resolution (HR) image into groups of stacked low resolution (LR) images without changing the pixel values ( <ref type="figure">Fig.2)</ref>. Thus each decoder stage will have the same ground truth instead of multiple inconsistent LR images. Second, to obtain the global context more precisely, we reframe the SOD task into a patch-wise saliency detection problem and supervise a branch to learn the information explicitly ( <ref type="figure" target="#fig_0">Fig.1d</ref>). We split input images into n ? n nonoverlapping patches whereby the developed branch identifies which patch contains saliency. Compared with existing approaches where global contexts are assumed to be the high-level encoder features, in our work, the obtained global context is learned via a supervised task. Lastly, we developed a Context Refinement Module (CRM) to fuse global context features and refine local unconfident regions. The CRM will firstly fuse global contexts with decoder features for better detection completeness and generate a prediction. Then based on the unsure regions in the prediction, a local context feature map ( <ref type="figure" target="#fig_0">Fig.1e</ref>) is generated to guide the network for fine structure segmentation. Thus the CRM is a two-stage module where the final predictions with a better quality were obtained based on the refinement map generated from its first predictions.</p><p>To sum up, our contributions are as follows: ? We proposed an end-to-end Transformer-based network equipped with a global and local context branch for better structural integrity and local details in predictions. ? We demonstrated that Pixel Shuffle yields better training results than interpolation and pooling methods in preserving fine structures. This is the first work that applied Pixel Shuffle on the SOD task to our best knowledge. ? We introduced a measurable method to obtain globalcontext by framing a supervised patch-wise SOD. For local-context, we developed a CRM to automatically locate and refine unsure regions for better details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Global and Local Context Fusion</head><p>In encoder-decoder structure, features in deeper layers have a global view but local details are diluted because of too many convolution operations and resolution reduction, while shallower layers contain more local contexts. Many studies in SOD demonstrated that global and local context fusion can boost model performance. In SCNet <ref type="bibr" target="#b13">(Hou et al. 2017</ref>), Hou et al. showed that global context is capable of locating the salient object while local context is for preserving details. Thus they introduced short-connection to fuse global and local information in deeper and shallower layers. Similarly, PoolNet ) introduced a pyramid pooling module to further capture the global semantic information from the encoder, followed by a feature aggregation module to recover the diluted information in the encoder. In PFS-Net <ref type="formula">(</ref> two-stage model by firstly generating a coarse global context map to locate the salient object, followed by an attention-based sampler to zoom in the target region, and lastly refining the details by fusing local context features from encoders. It has been proved that context fusion will improve the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision Transformers</head><p>Transformers were firstly introduced in natural language processing <ref type="bibr" target="#b28">(Vaswani et al. 2017;</ref><ref type="bibr" target="#b7">Devlin et al. 2019;</ref><ref type="bibr" target="#b16">Lan et al. 2020</ref>) and were extended to computer vision tasks such as image classification <ref type="bibr" target="#b8">(Dosovitskiy et al. 2021</ref>) and semantic segmentation <ref type="bibr" target="#b45">(Zheng et al. 2021</ref>) due to their capability of modeling long-range dependencies. Networks like DETR <ref type="bibr" target="#b2">(Carion et al. 2020</ref>) and its variants <ref type="bibr" target="#b6">(Dai et al. 2021;</ref><ref type="bibr" target="#b15">Kim et al. 2021</ref>) used a combination of CNN and Transformer for various computer vision tasks <ref type="bibr" target="#b30">(Wang et al. 2021a;</ref><ref type="bibr" target="#b18">Li et al. 2021</ref>). Following the Visual Transformer's (ViT) success in image classification, some studies extend the Transformer for dense prediction tasks, e.g., semantic segmentation or depth estimation. SETR <ref type="bibr" target="#b45">(Zheng et al. 2021</ref>) and PVT <ref type="bibr" target="#b31">(Wang et al. 2021b</ref>) employ ViT as the encoder and use several convolutional layers to upsample encoder features for dense prediction. In SOD, VST (Liu et al. 2021) adopted T2T-ViT <ref type="bibr" target="#b40">(Yuan et al. 2021)</ref> as the backbone and achieved remarkable results. The input images were unfolded into partially overlapped patches for self-attention, and a reverse T2T (rT2T) mechanism was developed to reconstruct the predictions gradually. The effectiveness of self-attention in modeling long-range dependencies makes Transformer promising in SOD tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel Shuffle</head><p>Pixel Shuffle <ref type="bibr" target="#b27">(Shi et al. 2016b</ref>) was originally applied in the task of Single Image Super-Resolution (SISR) to upscale a (a) low-resolution (LR) image r times into a high-resolution (HR) image, and its reverse operation is Pixel-unshuffle ( <ref type="figure">Fig.2)</ref>. Different from interpolation methods, by reshaping the input image from [r 2 C, H, W ] to [C, rH, rW ], one can obtain an HR image without changing any pixel values. In SISR, the primary purpose of Pixel Shuffle is to keep the feature map at a small size to achieve a higher inference speed. Another advantage of Pixel Shuffle compared with interpolation or pooling is that we can unshuffle an HR image to LR without losing any structural details since pixels are relocated and values are unchanged. This is useful for encoder-decoder networks as the decoder is usually densely supervised against resized ground truth. When we increase the downsampling scale using interpolation or pooling methods, more and more details are lost ( <ref type="figure" target="#fig_1">Fig.3b-d)</ref>; consequently, the decoder is trained against inaccurate ground truths. The shuffle and unshuffle operations ensure that details in HR ground truth remain in its LR form, regardless of scaling factors. In this work, for simplicity, we use the term Pixel Shuffle to represent both shuffle and unshuffle operations in between layers with different scales.</p><formula xml:id="formula_0">(b) (c) (d) (e) (f) (g) (h)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Methods</head><p>Overall Architecture</p><p>We adopt PVT as the encoder backbone for better modeling of long-range dependencies. In order to preserve fine structures, Pixel Shuffle is applied across all stages when feature maps need to be scaled. To obtain global context information, we framed a new task by predicting whether a patch of the ground truth contains saliency and developed the global context branch. Sharing the same encoder, the Context Refinement Module (CRM) is developed to fuse global contexts with decoder features and refine local details in the prediction. The network architecture is shown in <ref type="figure">Fig.4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pyramid Vision Transformer (PVT) Backbone</head><p>As shown in <ref type="figure">Fig.4</ref>, input image in the shape of 224 ? 224 will be cut into patches for self-attention, and PVT will output four groups of features in the shape of 56 ? 56, 28 ? 28, 14 ? 14, and 7 ? 7. For more efficient computation of the multi-head self-attention, PVT introduced a sequence reduction method to reduce the scale of K and V by firstly reshaping the input sequence X i ? R (HW ?C) int? X i ? R ( HW r ?C?r) then apply an MLP network to reduce the channel of C ? r back to C, the process is formulated as:</p><formula xml:id="formula_1">X i = LN (M LP (Reshape(X, r)))<label>(1)</label></formula><p>where LN (?) stands for layer normalization <ref type="bibr" target="#b1">(Ba, Kiros, and Hinton 2016)</ref>. The self-attention is performed based on the reducedK andV :</p><formula xml:id="formula_2">Attention(Q,K,V ) = Sof tmax( QK T ? d head )V (2)</formula><p>As a result, the total computation is reduced r times and hence more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel Shuffle as the Up/down Sampling Method</head><p>SOD is a densely supervised task where the ground truth image needs to be downsampled into multiple LR images to fit the size of each decoder stage. We noticed that given the input size of 224 ? 224, PVT's largest feature map size is 56 ? 56, and conventional downsampling methods like interpolation or pooling are no longer viable to generate accurate ground truth images for the decoder. As shown in <ref type="figure" target="#fig_1">Fig.3, in (b)</ref>, fine structures are damaged and inconsistent in the bilinear sampled ground truth. In (c) and (d), the generated GTs via max and average pooling become inaccurate.</p><p>As we increase the downsampling factor, the methods mentioned above will discard or change more and more pixel values, resulting in different inconsistent GTs for each decoder stage. In contrast, Pixel Shuffle rearranges the GT from I H?W ?1 into multi-channel LR images I H r ? W r ?r 2 , since no pixel is discarded nor changed thus the structural properties are preserved. Though each pixel shuffled channel contains incomplete GT in (e) -(h) due to the reshaping process, the overall image is still the same once we shuffle them back to a single channel. Thus Pixel Shuffle is a more suitable method for downsampling the ground truth owing to its ability to unshuffle an HR image into LR images without changing the value, as illustrated in <ref type="figure">Fig.2</ref> previously. Different from all downsampling methods, by using Pixel Shuffle, we will train each decoder stage against the full-scale ground truth instead of its downgraded LR images. Predictions from each decoder stage will now become P H r ? W r ?r 2 instead of P H r ? W r ?1 , and this training scheme will enable each decoder stage to capture as much information as possible to restore the fine structures of the salient object. To formulate the unshuffle process, given an HR image or feature map I ? R H?W ?C and a scaling factor r, it can be described as:</p><p>PS(I x,y,c , r) = I x/r , y/r ,C?r?mod(y,r)+C?mod(x,r)+c</p><p>(3) where x, y and c represent pixel coordinates and channel index in high-resolution (HR) space. <ref type="figure">Figure 4</ref>: Network architecture for SelfReformer. Pre-trained PVT-v2 is employed as the encoder backbone. Encoder features are fed into the global context branch for patch-wise classification to obtain a low-resolution global context map. The map is further fused in CRM to locate the salient object. A CRM is developed for detail refinement where it utilizes its first prediction as a clue and generates features to improve its second prediction. In between each stage, Pixel Shuffle is applied to avoid the loss of details caused by interpolation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global-Context Branch</head><p>The global context is the clue indicating where are the salient objects. Though evidence indicates high-level encoder features contain global context and contribute to the completeness of predictions, we still lack a method to evaluate how much and how good are the global context we obtained from the encoder. Hence, we aim to design a supervised task to explicitly learn the information from the input image and the ground truth pair. Since in Transformer, input images are split into patches, therefore, we frame this supervised task as which patch contains salient object. The ground truth of this task can thus be easily obtained from the original ground truth images. For each patch, the branch is only required to predict a single value indicating the likelihood of the presence of the salient object, and the obtained global-context map will be passed to the decoder as guidance to locate the salient object. Since it is a patch-wise prediction instead of full-scale pixel-wise, the designed task is easier than the salient object detection scoped for the decoder. The developed branch will learn a representation of the global context in a controllable manner, and its features will be used as a map to guide the decoder network.</p><p>To build the branch, as shown in <ref type="figure">Fig.5</ref>, we firstly apply Pixel Shuffle to reshape all encoder features to 14 ? 14 then concatenate and use a few Conv-BN-ReLU layers for feature fusion. Then a transformer block T F from the original Transformer and an MLP layer are employed for patch-wise saliency prediction P g . For simplicity, let f f use represent <ref type="figure">Figure 5</ref>: Global Context Branch. A patch-wise SOD task is framed to catch the global context explicitly. the fused features being passed to the Transformer, and the global context branch can be described as:</p><formula xml:id="formula_3">P g = Sigmoid(M LP (T F(Reshape(f f use ))))<label>(4)</label></formula><p>where Reshape is the tensor operation from [B, C, H, W ] to [B, H ? W, C]. The self-attention in T F is the same as the original transformer:</p><formula xml:id="formula_4">Attention(Q, K, V ) = Sof tmax( QK T ? d head )V<label>(5)</label></formula><p>To obtain the ground truth G g for this branch, we firstly apply Pixel Shuffle to reshape the original ground truth G from 224 ? 224 ? 1 to 14 ? 14 ? 256, then apply max(?) function along the channel dimension c:</p><formula xml:id="formula_5">G g = max c?C ((PS(G)) i,j,c )<label>(6)</label></formula><p>where i, j and c represent pixel coordinates and channel indices. The branch is supervised using Binary Cross-Entropy(BCE) Loss:</p><formula xml:id="formula_6">L g = BCE(P g , G g )<label>(7)</label></formula><p>The obtained map is then passed to Context Refinement Module (CRM) for fusion, and Pixel Shuffle is applied accordingly to match different scales in each decoder stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Refinement Module (CRM)</head><p>We propose CRM to guide the network for better semantic integrity and refine its predictions for richer details. Key steps and results are shown in <ref type="figure" target="#fig_2">Fig.6</ref> where the global context map is fused with decoder features; then a local refinement map is generated for fine structure segmentation. Thus CRM is a two-stage module where we handle global and local information separately, as shown in <ref type="figure">Fig.4</ref>. For global information, to match the feature map dimension, Pixel Shuffle PS is applied on the global context features f g with different scaling factors r depending on the decoder stage. Decoder features f d are fused with f g via a few Conv-BN-ReLU layers denoted as F 1 , and the first stage prediction P 1 is obtained and supervised against the ground truth. Mathematically, this process can be described as:</p><formula xml:id="formula_7">P 1 = Sigmoid(F 1 (f d , PS(f g , r)))<label>(8)</label></formula><p>Above obtained P 1 contains unconfident regions in the presence of grey areas in the image. These areas are considered as hard pixels to the network. Noteworthily, due to the property of the Sigmoid function, values of hard pixels are close to 0.5 while values are close to 0 or 1 for confident predictions. By multiplying P 1 with 1 ? P 1 , the unconfident area are highlighted, and features can be extracted as the local-context map M l to guide the second stage to focus and refine the unsure regions in P 1 . Denote the designed multiplication as H, we adopt a single Conv-BN-ReLU layer F 2 to obtain the map:</p><formula xml:id="formula_8">H(P 1 ) = P 1 * (1 ? P 1 )<label>(9)</label></formula><p>M l = f d * F 2 (H(P 1 )) + f d (10) where * represents element-wise multiplication. We further adopt a transformer block T F and another Conv-BN-ReLU layer F 3 to generate the final prediction P 2 :</p><formula xml:id="formula_9">P 2 = Sigmoid(F 3 (T F(M l )))<label>(11)</label></formula><p>The obtained P 2 has better quality in fine structures than P 1 , which will be discussed in ablation studies. The proposed CRM achieves self-refine as it adopts global context to guide the decoder for better completeness, and automatically refines the details in the prediction. We apply the weighted BCE loss (L w ) for each decoder stage as used in F3Net :</p><formula xml:id="formula_10">L l = 4 i=1 ? i (L i w (P 1 , G) + L i w (P 2 , G))<label>(12)</label></formula><p>where subscript i represents each decoder stage as listed in <ref type="figure">Fig.4</ref>, and the values of ? 1?4 are [0.5, 0.7, 0.9, 1.1] respectively. The total loss of the network L is simply the sum of L g and L l as described above.   <ref type="formula" target="#formula_1">2015)</ref> with default hyperparameters was adopted to train the network. We trained the network for 200 epochs with batch size of 16. Learning rates for the encoder, global context branch, and the decoder were set to 10 ?5 , 10 ?5 , and 10 ?4 , respectively, and were halved every 40 epochs. During testing, images are resized to 224?224, and the predictions from last decoder stage (56?56?16) were firstly upscaled to 224?224?1 using Pixel Shuffle, and bilinear interpolation was applied to further resize it back to its original size. MAE. MAE is the mean absolute element-wise differences between ground truths? and predictions x: M AE = 1 n n i=1 |x i ?? i |. E-measure. By using local pixel values and the imagewise mean, E ? <ref type="bibr" target="#b10">(Fan et al. 2018</ref>) calculates the similarity between the ground truth and predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Datasets and Metrics</head><p>S-measure. S ? <ref type="bibr" target="#b9">(Fan et al. 2017</ref>) aims to measure the region and object level of structural similarities between the ground truth and the prediction, denoted as S o and S r . It is defined as S ? = ?S o + (1 ? ?)S r with ? = 0.5. <ref type="table">Table 1</ref>: Quantitative comparisons between our SelfReformer and other 11 methods on five benchmark datasets. Text in bold indicates the best performance, and superscript * stands for Transformer based network. Postfix BI of our work stands for network using bilinear interpolation instead of Pixel Shuffle, and full represents our proposed network.  Figure 7: Precision-Recall Curves (first row) and F-measure Curves (second row) comparison on five saliency benchmark datasets. As shown above, our network achieved the best results among all networks across five datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUTS-TE</head><formula xml:id="formula_11">HKU-IS PASCAL-S ECSSD DUT-OMRON F ? ? M? E ? ? S ? ? F ? ? M? E ? ? S ? ? F ? ? M? E ? ? S ? ? F ? ? M? E ? ? S ? ? F ? ? M? E ? ? S ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons with state-of-the-art</head><p>We compare our method against 11 state-of-the-art networks in the field, namely, F 3 Net (Wei, Wang, and Huang 2020), GateNet Quantitative Evaluation. As shown in <ref type="table">Table.</ref>1, our network achieved the best results in all metrics calculated across the five benchmark datasets. It demonstrates outstanding performances of the proposed SelfReformer. Besides, <ref type="figure">Fig.7</ref> shows the precision-recall curve of the abovelisted networks, and our network consistently outperformed all other methods.</p><p>Qualitative Evaluation. Visual comparisons are listed in <ref type="figure" target="#fig_5">Fig.8</ref>. Compared with other methods, our predictions are more accurate in structural completeness and contain richer details (rows 1, 2, and 6). Prediction completeness demonstrates the effectiveness of the global context branch, while rich details indicate the success of Pixel Shuffle and CRM. Moreover, our network excels in dealing with challenging scenarios like a small object among complex backgrounds (row 3), the unique object among its peers (row 4), and multiple salient objects (row 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>We investigate the effectiveness of proposed modules and methods, i.e., global-context branch, CRM and Pixel Shuffle. For more ablation studies, please refer to supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Global-Context Branch</head><p>We study the impact of the global-context branch by removing it and training the rest of the network, i.e., the first stage of CRM in <ref type="figure">Fig.4</ref> will no longer fuse f g with decoder features. The evaluation results on DUTS-TE and PASCAL-S are listed in <ref type="table" target="#tab_2">Table 2</ref>, and we can observe significant improvement with the presence of a global context branch.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUTS-TE</head><formula xml:id="formula_12">PASCAL-S F ? ? M? E ? ? S ? ? F ? ? M? E ? ? S ? ? w/o</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of CRM</head><p>We compare the two predictions obtained in the CRM visually and quantitatively. <ref type="table" target="#tab_4">Table 3</ref> reflects the improvement in accuracy between the predictions. In <ref type="figure" target="#fig_6">Fig.9</ref>, we can observe that the second prediction is refined by the local context features generated from the first prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Pixel Shuffle</head><p>We replace Pixel Shuffle (PS) with bilinear interpolation (BI) in our network to compare the difference. Better details are restored in the predictions when using PS during the training, as shwon in <ref type="figure" target="#fig_0">Fig.10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we have proposed a novel Transformer-based network named SelfReformer which can guide itself with global and local contexts. In order to obtain a better global context, we framed a supervised patch-wise saliency detection task to obtain the global fature explicitly. Meanwhile, since interpolation or pooling methods damage fine features in the ground truth, we adopted Pixel Shuffle as the up/downsampling method for details preservation. Besides, we developed CRM to guide the decoder with global context information and generate a local context map for better details in predictions. The proposed network demonstrated excellent performance in locating salient objects accurately with rich fine features. Evaluation results indicate the SelfReformer achieved the state-of-the-art across five benchmark datasets in all four related evaluation metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visualization of global and local context obtained by our network. (a) Input, (b) Ground truth, (c) Prediction, (d) Patch-wise global context map, (e) Local context map, (f) Illustration of how global and local context benefits the prediction. Best view in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Visual comparisons of vairous downsampling methods from 224 ? 224 to 56 ? 56. (a) GT, (b) bilinear interpolation, (c) max pooling(kernel = 4, stride = 4), (d) average pooling(kernel = 4, stride = 4), (e) stacks of pixel shuffled images, (f)-(h) examples of different channels of pixel shuffled image. Best view in zoon-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Illustration of fusion global and local context in CRM. From left to right: Input, Ground Truth, Globalcontext map, Local-context map, Overlay of global and local context. The global-context map can locate the salient object while the local-context map is to refine details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>DUT-OMRON<ref type="bibr" target="#b39">(Yang et al. 2013</ref>)(5168 images), ECSSD (Yan et al. 2013)(1000 images), PASCAL-S (Li et al. 2014)(850 images), HKUIS (Li and Yu 2015)(4447 images), and DUTS-TE (Wang et al. 2017)(5019 Images) are our evaluation datasets, and the evaluation metrics are as follows:F ? -measure. The F ? -measure<ref type="bibr" target="#b0">(Achanta et al. 2009</ref>) is calculated based on the precision and recall value of saliency maps: F ? = (1+? 2 )?P recision?Recall ? 2 ?P recision+Recall where ? 2 is set to 0.3<ref type="bibr" target="#b0">(Achanta et al. 2009</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, GCPA), MINet (Pang et al. 2020), U 2 Net (Qin et al. 2020), LDF (Wei et al. 2020), MSFNet (Zhang et al. 2021), PFSNet (Ma, Xia, and Li 2021), DCN (Wu, Su, and Huang 2021), PAKRN (Xu et al. 2021), and VST (Liu et al. 2021). Results were calculated using the code provided by F 3 Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Visual comparisons between the proposed method and 10 state-of-the-art networks. * stands for Transformer based networks. More comparisons are listed in the supplementary material. Best view in zoom-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative comparisons for CRM. (a) top to bottom: Input, Global-context map, GT, (b)-(e) maps generated from decoder stage 4 to stage 1: first row: first stage predictions, second row: local context maps, last row: second stage predictions. MAE is marked at the top left corner. Image GT PS PS zoomed BI BI zoomed Figure 10: Qualitative comparisons of predictions when using BI and PS during training. Pixel Shuffle is effective in restoring more details in the salient object prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparisons for the effectiveness of global-context branch.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>global context .912 .028 .914 .904 .892 .053 .869 .867 w/ global context .916 .026 .920 .911 .894 .050 .872 .874</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparisons for the effectiveness of local-context branch on predictions at decoder stage 2. M? E ? ? S ? ? F ? ? M? E ? ? S ? ? First Stage .926 .035 .945 .914 .817 .051 .866 .847 Second Stage .927 .033 .947 .915 .818 .049 .872 .848</figDesc><table><row><cell>Decoder</cell><cell>HKU-IS</cell><cell>DUT-OMRON</cell></row><row><cell>Stage 2</cell><cell>F ? ?</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>S?sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Layer Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-End Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th</title>
		<editor>Vedaldi, A.</editor>
		<editor>Bischof, H.</editor>
		<editor>Brox, T.</editor>
		<editor>and Frahm, J.</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Global Context-Aware Progressive Aggregation Network for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10599" to="10606" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Twins: Revisiting the Design of Spatial Attention in Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Ranzato, M.</editor>
		<editor>Beygelzimer, A.</editor>
		<editor>Dauphin, Y.</editor>
		<editor>Liang, P.</editor>
		<editor>and Vaughan, J. W.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9355" to="9366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">UP-DETR: Unsupervised Pre-Training for Object Detection With Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2021-06-19" />
			<biblScope unit="page" from="1601" to="1610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>Burstein, J.</editor>
		<editor>Doran, C.</editor>
		<editor>and Solorio, T.</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<imprint>
			<date type="published" when="2021-05-03" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structure-Measure: A New Way to Evaluate Foreground Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enhanced-alignment Measure for Binary Foreground Map Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<ptr target="ijcai.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
		<editor>Lang, J., ed.</editor>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-13" />
			<biblScope unit="page" from="698" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="1473" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>abs/1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeply Supervised Salient Object Detection with Short Connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="5300" to="5309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Empirical Evaluation of Activation Functions in Deep Convolution Neural Network for Facial Expression Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Kasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bakhtyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Devi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 43rd International Conference on Telecommunications and Signal Processing</title>
		<imprint>
			<publisher>TSP</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="204" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">HOTR: End-to-End Human-Object Interaction Detection With Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<editor>Bengio, Y.</editor>
		<editor>and LeCun, Y.</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
			<biblScope unit="page" from="74" to="83" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Diverse Part Discovery: Occluded Person Re-Identification With Part-Aware Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2021-06-19" />
			<biblScope unit="page" from="2898" to="2907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Secrets of Salient Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014-06-23" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Simple Pooling-Based Design for Real-Time Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="3917" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual Saliency Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-10-10" />
			<biblScope unit="page" from="4702" to="4712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-Scale Interactive Network for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021</title>
		<meeting><address><addrLine>Pang, Y; Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-13" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9410" to="9419" />
		</imprint>
	</monogr>
	<note>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">U 2 -Net: Going deeper with nested U-structure for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">V</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">R</forename><surname>Za?ane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>J?gersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">107404</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BASNet: Boundary-Aware Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">V</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>J?gersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="7479" to="7489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unifying Global-Local Representations in Salient Object Detection with Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<idno>abs/2108.02759</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to Detect Salient Objects with Image-Level Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="3796" to="3805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2021-06-19" />
			<biblScope unit="page" from="1571" to="1580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-10-10" />
			<biblScope unit="page" from="548" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Salient Object Detection With Pyramid Attention and Salient Edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="1448" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">F 3 Net: Fusion, Feedback and Focus for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12321" to="12328" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Label Decoupling Framework for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-06-13" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13022" to="13031" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Decomposition and Completion Network for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6226" to="6239" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Locate Globally, Segment Locally: A Progressive Architecture With Knowledge Review Network for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021-02-02" />
			<biblScope unit="page" from="3004" to="3012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Bach, F. R.</editor>
		<editor>and Blei, D. M.</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-11" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical Saliency Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013-06-23" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Saliency Detection via Graph-Based Manifold Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013-06-23" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E H</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-10-10" />
			<biblScope unit="page" from="538" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Auto-MSFNet: Search Multi-scale Fusion Network for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Prabhakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM &apos;21: ACM Multimedia Conference, Virtual Event</title>
		<editor>Shen, H. T</editor>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-10-20" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Progressive Attention Guided Recurrent Network for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA; Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
	<note>2018 IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">EGNet: Edge Guidance Network for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="8778" to="8787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Suppress and Balance: A Simple Gated Network for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<editor>Vedaldi, A.</editor>
		<editor>Bischof, H.</editor>
		<editor>Brox, T.</editor>
		<editor>and Frahm, J.</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rethinking Semantic Segmentation From a Sequenceto-Sequence Perspective With Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<date type="published" when="2021-06-19" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
		<respStmt>
			<orgName>Computer Vision Foundation / IEEE</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
