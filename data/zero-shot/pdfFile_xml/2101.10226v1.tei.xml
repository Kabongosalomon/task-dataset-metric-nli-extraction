<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lightweight Convolutional Neural Network with Gaussian-based Grasping Representation for Robotic Grasping Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Zhijun</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Jianjie</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alois</forename><surname>Knoll</surname></persName>
						</author>
						<title level="a" type="main">Lightweight Convolutional Neural Network with Gaussian-based Grasping Representation for Robotic Grasping Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Efficient Grasping Detection</term>
					<term>Gaussian-based Grasping Representation</term>
					<term>Receptive Field Module</term>
					<term>Multi- Dimension Attention Fusion</term>
					<term>Fully Convolutional Neural Net- work</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The method of deep learning has achieved excellent results in improving the performance of robotic grasping detection. However, the deep learning methods used in general object detection are not suitable for robotic grasping detection. Current modern object detectors are difficult to strike a balance between high accuracy and fast inference speed. In this paper, we present an efficient and robust fully convolutional neural network model to perform robotic grasping pose estimation from n-channel input image of the real grasping scene. The proposed network is a lightweight generative architecture for grasping detection in one stage. Specifically, a grasping representation based on Guassian kernel is introduced to encode training samples, which embodies the principle of maximum central point grasping confidence. Meanwhile, to extract multiscale information and enhance the feature discriminability, a receptive field block (RFB) is assembled to the bottleneck of our grasping detection architecture. Besides, pixel attention and channel attention are combined to automatically learn to focus on fusing context information of varying shapes and sizes by suppressing the noise feature and highlighting the grasping object feature. Extensive experiments on two public grasping datasets, Cornell and Jacquard demonstrate the state-of-the-art performance of our method in balancing accuracy and inference speed. The network is an order of magnitude smaller than other excellent algorithms, while achieving better performance with accuracy of 98.9% and 95.6% on the Cornell and Jacquard datasets, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Intelligent robots are widely used in industrial manufacturing fields, such as human-robot cooperation, robot assembly, and robot welding. The robots need an effective automated manipulation system to complete the task of picking and placing. Although grasping is a very simple action for humans, it is still a challenging task for robots, which involves subsystems such as perception, planning and extection. Grasping detection is a basic skill for robots to perform grasping and manipulation tasks in the unstructured enviroments of the real world. In order to improve the performance of robotic grasping, it is necessary to develop a robust algorithm to predict the location and orientation of the grasping objects.</p><p>Early grasping detection works are mainly based on traditional methods, such as serach algorithm. However, these algorithms cannot work effectively in complex real scenarios <ref type="bibr" target="#b0">[1]</ref>. In recent years, deep learning-based methods have achieved excellent results in robotic grasping detection. Based on twodimension space can be projected into the three-dimensional space to guide the robot to grasp, a five-dimensional grasp configuration is proposed to represent grasp rectangle <ref type="bibr" target="#b1">[2]</ref>. Due to the simplification of the grasping object dimension, the deep convolutional neural network can be used to learn extracting features mroe suitable for specific tasks than hand-engineered features by taking 2-D images as input. Many works, such as <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, train the neural network to predict the grasping rectangle of objects, and select the one with the highest grasp probability score from multiple grasp candidate rectangles as the best grasp result. Some one or two-stage deep learning methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref> that have achieved great success in object detection have been modified to perform grasping detection task. For example, <ref type="bibr" target="#b10">[10]</ref> refers to some key ideas of Faster RCNN <ref type="bibr" target="#b9">[9]</ref> in the field of object detection to carry out robotic grasping from the input RGB-D images. In addition, other works, such as <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[11]</ref>, implemented highprecision grasp detection on Cornell grasping dataset based on the one stage object detection method <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Although these object detection-based methods achieve better accuracy in robotic grasping detection, their design based on horizontal rectangular box is not suitable for angular grasp detection task, and most of them have complex network structure, so it is difficult to achieve a good balance in detection accuracy and speed. In <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, the authors improve the performance of grasping detection by demploying an oriented anchor box mechanism to match the grasp rectangles. However, although these methods have achieved some improvement in accuracy or speed, the size of network parameters of their algorithms is still too large to be suitable for real-time applications. To solve these problems mentioned above, a new grasping representation is proposed by <ref type="bibr" target="#b14">[14]</ref>. Different from previous works, which used the method of sampling grasping candidate rectangle, <ref type="bibr" target="#b14">[14]</ref> applies generated convolutional neural network to directly regress grasp points, which simplifies the definition of grasping representation and achieves high real-time performance based on the lightweight architecture. Inspired by <ref type="bibr" target="#b14">[14]</ref>, the authors of <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref> utilize some ideas of algorithms in vision segmentation tasks to predict robotic grasping pose from extracted pixel-wise features. Recently, the residual structure is introduced into the generated neural network model <ref type="bibr" target="#b17">[17]</ref>, which achieved state-of-the-art grasping detection accuracy on Cornell and Jacquard grasping datasets. However, they all have a shortcoming that although they take the location with the largest grasping score as the center point coordinate, they fail to highlight the importance of the largest grasping probability at the center point.</p><p>In this work, we utilize 2-D Guassian kernel to encode training samples to emphasize that the center point position with the highest grasping confidence score. On the basis of Guassian-based grasping representation, we develop a lightweight generative architecture for robotic grasping pose estimation. Referring to the receptive field structure in human visual system, we combine the residual block and a receptive field block module in the bottleneck layer to enhance the feature discriminability and robustness. In addition, in order to reduce the information loss in the sampling process, we fuse low-level features with depth features in the decoder process, and use a multi-dimensional attention network composed of pixel attention network and channel attention network to suppress redundant features and highlight meaningful features in the fusion process. Extensive experiments demonstrate that our algorithm achieves state-of-the-art performance in accuracy and inference speed on the public grasping datasets Cornell and Jacquard with a small network parameter size. Concretely, the main contributions of this paper are as follows:</p><p>? We propose a Gaussian-based grasping representation, which relects the maximum grasping score at the center point location and can signigicantly improve the grasping detection accuracy. <ref type="bibr">?</ref> We develope a lightweight generative architecture which achieves high detection accuracy and real-time running speed with small network parameters. ? A receptive field block module is embedded in the bottleneck of the network to enhance its feature discriminability and robustness, and a multi-dimensional attention fusion network is developed to suppress redundant features and enhance target features in the fusion process. ? Evaluation on the public Cornell and Jacquard grasping datasets demonstrate that the proposed generative based grasping detection algorithm achieves state-of-the-art performance of both speed and detection accuracy. The rest of this paper is organized as follows: previous works related to the grasp detection are reviewed in section 2. Robotic grasping system is introduced in section 3,. Detailed description of the proposed grasping detection method is illustrated in section 4. Dataset analysis is presented in section 5. Experiments based on the public grasping datasets, Cornell and Jacquard are discussed in section 6. Finaly, we conclude our work in section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>For 2D planar robotic grasping where the grasp is constrained in one direction, the methods can be divided into oriented rectangle-based grasp representation methods and contact point-based grasp representation methods. The comparision of the two grasp representations are presented in <ref type="figure" target="#fig_0">Fig. 1</ref>. We will review the relevant works below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Methods of oriented rectangle-based grasp representation</head><p>The goal of grasping detection is to find the appropriate grasp pose for the robot through the visual information of the grasping object, so as to provide reliable perception information for subsequent planning and control process, and achieve successful grasp. Grasp is a widely studied topic in the field of robotics, and the approaches used can be summmarized as anlytic methods and empirical methods. The analytical methods use mathematical and physical models in geometry, motion and dynamics to carry out the calculation for grasping <ref type="bibr" target="#b18">[18]</ref>. Its theoretical foundation is solid, but the deficiency lies in that the model between the robot manipulator and the grasping object in the real 3-dimensional world is very complex, and it is difficult to realize the model with high precision. In contrast, empirical methods do not strictly rely on real-world modeling methods, and some works utilize data information from known objects to build models to predict the grasping pose of new objects <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b21">[20]</ref>, <ref type="bibr" target="#b22">[21]</ref>. A new grasp representation is proposed in <ref type="bibr" target="#b24">[22]</ref>, where a simplified fivedimensional oriented rectangle grasp representation is used to replace the seven-dimensional grasp pose consisting of 3D location, 3D orientation and the opening and closing distance of the plate gripper. Based on the oriented rectangles grasp configuration, the deep learning approaches can be successfully applied to the grasping detection task, which mainly include classification-based methods, regression-based methods and detection-based methods <ref type="bibr" target="#b25">[23]</ref>.</p><p>Classification-based Methods: A first deep learning-based robotic grasing detection method is presented in <ref type="bibr" target="#b1">[2]</ref>, the authors achieve excellent results by using a two-step cascaded structure with two deep networks. In <ref type="bibr" target="#b26">[24]</ref>, grasping proposals are estimated by sampling grasping locations and adjacent image patches. The grasp orientation is predicted by dividing angle into 18 disccrete angles. Since grasping dataset is scant, a large simulation database called Dex-Net 2.0 is built in <ref type="bibr" target="#b27">[25]</ref>. On the basis of Dex-Net 2.0, a Grasp-Quality Covolutional Neural Network (GQ-CNN) is developed to classify the potential grasps. Although the network is trained on synthetic data, the proposed method still works well in the real world. Moreover, a classification-based robotic grasping detection method with spatial transformer network (STN) is proposed in <ref type="bibr" target="#b28">[26]</ref>. The results of evalating on Cornell grasping dataset indicate that their multi-stage STN algorithm peforms well. The grasping detection method based on classification is a more direct and reasonable method, many aspects of which are worth further study.</p><p>Regression-based Methods: Regression-based methods is to directly predict grasp parameters of location and orientation by training a model. A first regression-based single shot grasping detection approach is proposed in <ref type="bibr" target="#b2">[3]</ref>, in which the authors use AlexNet to extract feature and achieve real-time performance by removing the process of searching potential grasps. Combing RGB and depth data, a multi-modal fusion method is introduced in <ref type="bibr" target="#b29">[27]</ref>. With fusing RGB and depth features, the proposed method directly regress the grasp parameters and improve the grasping detection accuracy on the Cornell grasping dataset. Similar to <ref type="bibr" target="#b29">[27]</ref>, the authors of <ref type="bibr" target="#b30">[28]</ref> use ResNet as backbone to integrate RGB and depth information and further improves the performance of grasping detection. In addition, a graping detection method based on ROI (Region of Interest) is proposed in <ref type="bibr" target="#b22">[21]</ref>. In this work, the authors regress grasp pose on ROI features and achieve better performance in object overlapping challenge scene. The regression-based method is effective, but its disadvantage is that it is more incilined to learn the mean value of the ground truth grasps.</p><p>Detection-based Methods: Many detection-based methods refer to some key ideas from object detection, such as anchor box. Based on the prior knowledge of these anchor boxes, the regression problem of grasping parameters is simplified. In <ref type="bibr" target="#b31">[29]</ref>, vision and tactile sensing are fused to build a hybrid architecture for robotic grasping. The authors use anchor box to do axis aligned and grasp orientation is predicted by considering grasp angle estimation as classification problem. The grasp angle estimation methods used in <ref type="bibr" target="#b31">[29]</ref> is extened by <ref type="bibr" target="#b10">[10]</ref>. By transforming the angel estimation into classification problem, the method of <ref type="bibr" target="#b10">[10]</ref> achieves high grasping detection accuracy on Cornell dataset based on FasterRCNN <ref type="bibr" target="#b9">[9]</ref>. Different from the horizontal anchor box used in object detection, the authors of <ref type="bibr" target="#b12">[12]</ref> specially design an oriented anchor box mechanism for grasping task and improve the performance of model by combing end-to-end fully convolutional neural network. Morever, <ref type="bibr" target="#b32">[30]</ref> further extend the method of <ref type="bibr" target="#b12">[12]</ref> and proposes a deep neural network architecture that performs better on the Jacquard dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Methods of contact point-based grasp representation</head><p>The grasping representation based on oriented rectangle is widely used in robotic grasping detection task. In terms of the real plate grasping task, the gripper does not need so much information to perform the grasping action. A new simplified contact point-based grasping representation is introduced in <ref type="bibr" target="#b14">[14]</ref>, which consists of grasp quality, center point, oriented angle and grasp width. Based on this grasping representation, GGCNN and GGCNN2 are developed to predict the grasping pose, and their methods achieve excellent performance in both detection accuracy and inference speed. Refer to <ref type="bibr" target="#b14">[14]</ref>, the grasping detection performance is improved by a fully convolutional neural network with pixel-wise way in <ref type="bibr" target="#b15">[15]</ref>. Both <ref type="bibr" target="#b14">[14]</ref> and <ref type="bibr" target="#b15">[15]</ref> take depth data as input, a generative residual convolutional neural network is proposed in <ref type="bibr" target="#b17">[17]</ref> to generate grasps, which take n-channel images as input.</p><p>Recently, the authors of <ref type="bibr" target="#b16">[16]</ref> take some ideas from image segmentation to perform three-finger robotic grasping detection. Similar to <ref type="bibr" target="#b16">[16]</ref>, a orientation attentive grasp synthesis (ORANGE) framwork is developed in <ref type="bibr" target="#b33">[31]</ref>, which achieves better results on Jacquard dataset based on the GGCNN and Unet model. In this paper, we propose a Guassian-based grasping representation to highlight the importance of center point. We further develop a lightweight generative architecture for robotic grasping detection, which performs well in inference speed and accuracy on two public datasets, Cornell and Jacquard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ROBOTIC GRASPING SYSTEM</head><p>In this section, we give an overview of the robotic grasping system settings and illustrate the principles of Gaussian-based grasping representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. System Setting</head><p>A robotic grasping system usually consists of a robot arm, perception sensors, grasping objects and workspace. In order to complete the grasping task successfully, not only the grasp pose of objects needs to be obtained, but the subsystem of planning and control is involved. In grasping detection part, we consider limiting the manipulator to the normal direction of the workspace so that it becomes a goal for perception in 2D space. Through this setting, most of the grasping objects can be considered as flat objects by placing them reasonably on the workbench. Instead of building 3D point cloud data, the whole grasping system can reduce the cost of storage and calculation and improve its operation capacity. The grasp pose of flat objects can be treated as a rectangle. Since the size of each plate gripper is fixed, we use a simplified grasping representation mentioned in section II-B to perform grasp pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Gaussian-based grasp representation</head><p>For given RGB images or depth information of different objects, the grasping detection system should learn how to obtain the optimal grasp configuration for subsequent tasks. Many works, such as <ref type="bibr" target="#b31">[29]</ref>, <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b12">[12]</ref>, are based on fivedimensional grasping representation to generate grasp pose.</p><formula xml:id="formula_0">g = {x, y, ?, w, h}<label>(1)</label></formula><p>where, (x, y) is the coordinates of the center point, ? represents the orientation of the grasping rectangle, and the weight and height of the grasping rectangle are denoted by (w, h). Rectangular box is frequently used in object detection, but it is not suitable for grasping detection task. As the size of gripper is usually a known variable, a simplified representation is introduced in <ref type="bibr" target="#b14">[14]</ref> for high-precision, real-time robotic grasping. The new grasping representation for 3-D pose is defined as:</p><formula xml:id="formula_1">g = {p, ?, w, q}<label>(2)</label></formula><p>where, the center point location in Cartesian coordinates is p = (x, y, z). ? and w are the rotation angle of the gripper around the z axis and the opening and closing distance of the gripper, respectively. Sicne the five-dimensional grasping representation lacks the scale factor to evaluate the grasping quality, q is added to the new representation as a scale to measure the probability of grasp success. In addition, the definition of the new grasping representation in 2-D space can be described as,?</p><formula xml:id="formula_2">= {p,?,?,q}<label>(3)</label></formula><p>where,p = (u, v) represents the center point in the image coordinates.? denotes the orientation in the camera frame.? andq still represent the opening and closing distance of the gripper and the grasp quality, respectively. When we know the calibration result of the grasping system, the grasp pose? can be converted to the world coordinates g by matrix operation,</p><formula xml:id="formula_3">g = T RC (T CI (?))<label>(4)</label></formula><p>where, T RC and T CI represent the transform matrices of the camera frame to the world frame and 2-D image space to the camera frame respectively. Moreover, the grasp map in the image space is denoted as:</p><formula xml:id="formula_4">G = {?, W, Q} ? R 3?W ?H<label>(5)</label></formula><p>where, each pixel in the grasp maps, ?, W, Q, is filled with the corresponding?,?,q values. In this way, it can be ensured that the center point coordinates in the subsequent inference process can be found by searching for the pixel value of the maximum grasp quality,? * = maxQ?. In <ref type="bibr" target="#b14">[14]</ref>, the authors filled a rectangular area around the center point with 1 indicating the highest grasping quality, and the other pixels were 0. The model is trained by this method to learn the maximum grasp quality of the center point. Because all pixels in the rectangular area have the best grasping quality, it leads to a defect that the importance of the center point is not highlighted, resulting the ambiguity to the model. In this work, we use 2-D Gaussian kernel to regularize the grasping representation to indicate where the object center might exist, as is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The novel Gaussian-based grasping representation is represented as g k , the corresponding Gaussian-based grasp map is defined as:</p><formula xml:id="formula_5">G K = {?, W, Q K } ? R 3?W ?H where, Q K = K(x, y) = exp(? (x ? x 0 ) 2 2? 2 x ? (y ? y 0 ) 2 2? 2 y )</formula><p>where,</p><formula xml:id="formula_6">? x = T x , ? y = T y<label>(6)</label></formula><p>In Eq. 6, the generated grasp quality map is decided by the center point location (x 0 , y 0 ), the parameter ? x and ? y , and the corresponding scale factor T x and T y . By this method, the peak of Gaussian distribution is the center coordinate of the grasp rectangle. In this work, we will discuss the impact of parameter settings in more detail in section VI-F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHOD</head><p>In this section, we introduce a lightweight generative architecture for robotic grasping detection. <ref type="figure" target="#fig_2">Fig. 3</ref> presents the structure of our grasping detection model. The input data is transformed by downsampling block into feature maps with smaller size, more channels and richer semantic information. In the bottleneck, resnet block and multi-scale receptive fields block module are combined to extract more discriminability and robustness features. Meanwhile, a multi-dimensional attention fusion network consisted of pixel attention subnetwork and channel attention sub-network is used to fuse shallow and deep semantic features before upsampling, while suppressing redundant features and enhancing the meaningful features during the fusion process. Finally, based on the extracted features, four task-specific sub-networks are added to predict grasp quality, angle (the form of sin(2?) and cos(2?)), and width (the opening and closing distance of the gripper) respectively. We will illustrate the details of each component of the proposed grasping network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Basic Network Architecture</head><p>The proposed generative grasping architecture is composed of the downsampling block, the bottleneck layer, the multidimensional attention fusion network and the upsampling block, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. A downsampling block consists of covolution layer with kernel size of 3x3 and maximum pooling layer with kernel size of 2x2, which can be represented as Eq. 7.</p><formula xml:id="formula_7">x d = f maxpool (f n conv (f n?1 conv (...f 0 conv (I)...)))<label>(7)</label></formula><p>In this work, we use 2 down-sampling blocks and 2 convolutional layers in the down-sampling process. Specifically, the first down-sampling block is composed of 4 convolutional layers (n = 3) and 1 maximum pooling layer, and the second down-sampling layer is composed of 2 convolutional layers (n = 1) and 1 maximum pooling layer. After the down-sampled data pass through 2 convolutional layers, they are fed into a bottleneck layer consisting of 3 residual blocks (k = 2) and 1 receptive fields block module (RFBM) to further extract features. Since RFBM is composed of vary scale convolutional filters, we can acquire more rich image details. More details about RFBM will be discussed in section IV-B. The output of the bottleneck can be formulated as Eq. 8.</p><formula xml:id="formula_8">x b = f RF BM (f k res (f k?1 res (...f 0 res (f 1 conv (f 0 conv (x d ))</formula><p>)...))) (8) The output x b of the bottleneck is fed into multidimensional attention fusion network (MDAFN) and upsampling block. The multi-dimensional attention fusion network composed of pixel attention and channel attention subnetwork can suppress the noise feature and enhance the effective feature during the fusion of shallow feature and deep feature. The MDAFN will be illustrated in more detail in section IV-C. In upsampling block, the pixshuffle layer <ref type="bibr" target="#b34">[32]</ref> is used to increase feature resolution with the scale factor set to 2. In this work, the number of multi-dimensional attention fusion networks and upsampling blocks are both 2, and the output can be expressed as Eq. 9. Final network layer is composed of 4 task-specific convolutional filters with kernel size 3x3. The final output results can be given as Eq. 10.</p><formula xml:id="formula_9">x u = f 1 pixshuf f le (f 1 M DAF N (f 0 pixshuf f le (f 0 M DAF N (x b ))))<label>(9)</label></formula><formula xml:id="formula_10">g q = max q (f 0 conv (x u )), g cos(2?) = max q (f 1 conv (x u )), g sin(2?) = max q (f 2 conv (x u )), g w = max q (f 3 conv (x u )),<label>(10)</label></formula><p>where, the position of the center point is the pixel coordinates of the largest grasp quality g q , the opening and closing distance of the gripper is g w , and the grasp angle can be computed by g angle = arctan( g sin(2?) g cos(2?) )/2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-scale Receptive Fields Block Module</head><p>In neuroscience, researchers have found that there is a eccentricity function in the human visual cortex that adjusts the size of the receptive field of vision <ref type="bibr" target="#b35">[33]</ref>. This mechanism can help to emphasize the importance of the area near the center. In this work, we introduce a multi-scale receptive field block (RFB) <ref type="bibr" target="#b36">[34]</ref> to assemble the bottleneck layer of our grasping detection architecture for improving the ability of extracting multi-scale information and enhancing the feature dicriminability. The receptive field block module is composed of multi-branch covolution layers with different kernels corresponding to the receptive fields of different sizes. Moreover, the dilated convolution layer is used to control the eccentricity, and the features extracted by the branches of the different receptive fields are recombined to form the final representation, as shown in <ref type="figure" target="#fig_3">Fig 4.</ref> In each branch, the convolutional layer with a specific kernel size is followed by a dilated convolutional layer with a corresponding dilation rate, which uses a combination of different kernel sizes (1x1, 3x3, 7x1, 1x7). The features extracted from the four branches are concatenated and then added to the input data to obtain the final multi-scale feature output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-dimensional Attention Fusion Network</head><p>When humans look at an image, we don't pay attention to everything in the image, but instead focus on what's interesting to us. The attention mechanism in the visual system focuses limited attention on the important information, thus saving resources and obtaining the most effective information quickly. In the field of computer vision, some attention mechanisms with few parameters, fast speed and excellent effect have been developed <ref type="bibr" target="#b37">[35]</ref>, <ref type="bibr" target="#b38">[36]</ref>, <ref type="bibr" target="#b39">[37]</ref>, <ref type="bibr" target="#b40">[38]</ref>. In order to perceive the grasping objects effectively from the complex background, a multidimensional attention network composed of pixel attention subnetwork and channel attention subnetwork is designed to suppress the noise feature and highlight the object feature, as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. Specificaly, the shallow features and the deep features are concatenated together, and the fused features are fed into a multi-dimensional attention network to automatically learn the importance of the fused features at the pixel level and the channel level. In pixel attention subnetwork, the feature map F passes through a 3x3 covolution layer to generate an attention map by covolution operation. The attention map is further computed with sigmoid to abtain the corresponding pixel-wise weight score. Moreover, SENet <ref type="bibr" target="#b38">[36]</ref> is used as the channel attention subnetwork, which obtains 1x1xC features through global average pooling, and then uses two fully connection layers and the corresponding activation function Relu to build the correlation between channels, and finally outputs the weight score of the feature channel through sigmoid operation. Both the pixel-wise and channelwise weight maps are multiplied with the feature map F to obtain a novel output with reduced noise and enhanced object information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss Function</head><p>For a dataset including grasping objects O = {O 1 ...O n }, input images I = {I 1 ...I n }, and corresponding grasp labels L = {L 1 ...L n }, We propose a lightweight fully convoluton neural network to approximate the complex function F : I ???, where F represents a neural network model with weighted parameters, I is input image data, and? denotes grasp prediction. We train our model to learn the mapping function F by optimizing the minimum error between grasp prediction? and the corresponding label L. In this work, we consider the grasp pose estimation as regression problem, therefore the Smooth L1 loss is used as our regression loss function. The loss function L r of our grasping detection model is defined as :</p><formula xml:id="formula_11">L r (?, L) = N i m?{q,cos2?,sin2?,w} Smooth L1 (? m i ? L m i ) (11)</formula><p>where Smooth L1 is formulated as:</p><formula xml:id="formula_12">Smooth L1 (x) = (?x) 2 /2, if |x|&lt;1; |x| ? 0.5/? 2 , otherwise.</formula><p>where N is the number of grasp candidates. q, w represent the grasp quality and the opening and closing distance of the gripper, respectively, and (cos(2?), sin(2?)) is the form of orientation angle. In Smooth L1 fuction, ? is the hyperparameter that controls the smooth area, and it is set to 1 in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DATASET ANALYSIS</head><p>Since the deep learning has become popular, large public datasets, such as ImageNet, COCO, KITTI, etc, have been driving the progress of algorithms. However, in the field of robotic grasping detection, the number of available grasping datasets is insufficient. Dexnet, Cornell, and Jacquard are famous common grasping datasets that serve as a platform to compare the performance of the state-of-the-art grasping detection algorithms. In Tab. I, it presents a summary of the different grasping datasets.</p><p>Dexnet Grasping Dataset: The Dexterity Network (Dex-Net) is a research project established by UC Berkeley Automation Lab that provides code, dataset, and algorithms for grasping task. At present, the project has released four versions of the dataset, namely Dex-Net 1.0, Dex-Net 2.0, Dex-Net 3.0, and Dex-Net 4.0. Dex-Net 1.0 is a synthetic dataset with over 10000 unique 3D object models and 2.5 million corresponding grasp labels. Based on Dex-Net 1.0, thousands of 3D objects with arbitrary poses are used to generate more than 6.7 million ponit clouds and grasps, which constitute the Dex-Net 2.0 dataset. Dex-Net 3.0 is built to study the grasp using suction-based end effectors. Recently, a extension of previous versions, Dex-Net 4.0, has been developed, which can perform training for parallel-jaw and suction gripper. Since Dex-Net dataset includes only synthetic point cloud data and no RGB information of the grasp objects, the experiment of this work is mainly carried out on Cornell and Jacquard grasping dataset.</p><p>Cornell Grasping Dataset: The Cornell dataset, which is widely used as a benchmark evaluation platform, was collected in the real world with the RGB-D camera. Some example imgaes are shown in <ref type="figure" target="#fig_5">Fig 6.</ref> The dataset is composed of 885 images with a resolution of 640?480 pixels of 240 different objects with positive grasps (5110) and negative grasps (2909). RGB images and corresponding point cloud data of each object with various poses are provided. However, the scale of Cornell dataset is small for training our convolutional neural network model. In this work, we use online data augement methods including random cropping, zooms and rotation to extend the dataset to avoid overfitting during training. Jacquard Grasping Dataset: Jacquard is a large grasping dataset created through simulation based on CAD models. Because no manual collection and annotation is required, the Jacquard dataset is larger than the Cornell dataset, containing 50k images of 11k objects and over 1 million grasp labels. In <ref type="figure" target="#fig_6">Fig. 7</ref>, it presents some images from the Jacquard datset. Furthermore, the dataset also provides a standard simulation environment to perform simulated grasp trials (SGTs) under a consistent condition for different algorithms. In this work, we use SGTs as a benchmark to fairly compare the performance of various algorithms in the robot arm grasp. Since the Jacquard dataset is large enough, we do not use any data auguement methods to it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENT</head><p>To verify the generalization capability of the proposed lightweight generative model, we conducted experiments on two public grasping datasets, Cornell and Jacquard. Extensive experiments results indicate that our algorithm has high inference speed while achieving high grasp detection accuracy, and the size of network parameters is an order of magnitude smaller than most previous excellent algorithms. In addition, we also explore the impact of different network designs on algorithm performance and discuss the shortcomings of our method. <ref type="figure">Fig. 8</ref>: The detection results of grasping network on Cornell dataset. The first three rows are the maps for grasp quality, angle and width representing the opening and closing distance of the gripper. And, the last row is the best grasp outputs for several objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Metrics</head><p>Simillar to many previous works, the metric used in this paper to evaluate our model on the Cornell and Jacquard datasets is rectangle metric. Specifically, a pridicted grasp is regarded a correct grasp when it meets the following two conditions:  <ref type="figure">Fig. 9</ref>: The detection results of grasping network on Jacquard dataset. The first three rows are the maps for grasp quality, angle and width representing the opening and closing distance of the gripper. And, the last row is the best grasp outputs for several objects.</p><p>? Angle difference: the difference of orientation angle between the predicted grasp and corresponding grasp label is less than 30 ? . ? Jaccard index: the Jaccard index of the predicted grasp and corresponding grasp label is greater than 25%, which can be formulated as Eq. 12.</p><formula xml:id="formula_13">J(gp, g l ) = |gp ? g l | gp ? g l<label>(12)</label></formula><p>where g p and g l denote the predicted grasp rectangle and the area of the corresponding grasp label, respectively. g p ? g t represents the intersection of predicted grasp and the corresponding grasp label. And the union of predicted grasp and the corresponding grasp label is represented as g p ? g t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data preprocessing</head><p>The experiments for this work are performed on the Cornell and Jacquard grasping dataset. Due to the small data size of Cornell, we conducted online data augmentation to train our network. Meanwhile, Jacquard dataset has sufficient data, so we train the network directly on it without adopting any data augmentation method. The images of Cornell and Jacquard are resized to 300x300 to feed into the network. In addition, the data labels are encoded for training. A 2D Gaussian kernel is used to encode each ground-truth positive grasp so that the corresponding region satisfies the Gaussian distribution, where the peak of the Gaussian distribution is the coordinate of the center point. We also use sin(2?) and cos(2?) to encode the grap angle, where ? ? [? ? 2 , ? 2 ]. The resulting corresponding valuses range from -1 to 1. By using this method, ambiguity can be avoided in the Angle learning process, which is beneficial to the convergence of the network. Similarly, the grasp width representing the opening and closing distance of the gripper is scaled to a range of 0 to 1 during the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Methodology</head><p>In training period, we train our generative model end to end on a Nvidia GTX2080Ti GPU with 22GB memory. The grasping network is achieved based on Pytorch 1.2.0 with cudnn-7.5 and cuda-10.0 pacakges. The popular Adam optimizer is used to optimize the network for back propagation during training process. Futhermore, The initial learning rate is defiend as 0.001 and the batch size of 8 is used in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments on Cornell Grasping Dataset</head><p>Following the previous works <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, the Cornell dataset is divided into two different ways to validate the generalization ability of the model:</p><p>? Image-wise level: the images of dataset are randomly divided. The images of each grasp object in the training set and test set are different. Image-wise level method is used to test the generalization ability of the network to new grasp pose.</p><p>? Object-wise level: the object instances of dataset are randomly divided. All the images of the same object are split into the same set (training set or test set). Objectwise level method is used to validate the generalization ability of the network for new object, which is not seen in the training process. The comparison of the grasp detection accuracy of our model and other methods on the Cornell dataset is presented in <ref type="table">Table.</ref> II. Experiment results indicate that the proposed grasp detection algorithm achieves high accuracy of 98.9% and 97.8% in image-wise and object-wise split with an inference time of 6ms. Compared with other state-of-the-art algorithms, our model maintains a better balance betweeen accuracy and real-time performance. By changing the mode of input data, we can find that our generated grasping detection achitecture can get excellent performance with the input of depth data. And, the results in object-wise split demonstrate that the combination of depth data and RGB data with rich color and texture information enables the model to have more robust generalization ability to unseen objects. In <ref type="figure">Fig. 8</ref>, we plot the grasping detection results of some objects for display. Only the grasp candidate with the highest grasp quality is selected as the final output, and the top-1 grasp is visualised in the last row. The first three rows are the maps for grasp quality, angle and width representing the opening and closing distance of the gripper. It can be seen from the figure that our algorithm can provide reliable grasp candidate for objects with different shapes and poses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiments on Jacquard Grasping Dataset</head><p>Similar to the Cornell dataset, we trian our network on Jacquard dataset to perform grasp pose estimation. The results are summarized in <ref type="table">Table.</ref> III. Taking depth data as input, the proposed method obtains state-of-the-art performance with a detection accuracy of 95.6%, which exceeds the existing methods and reaches the best result on Jacquard dataset. The experimental results in <ref type="table" target="#tab_0">Table. II and Table.</ref> III demonstrate that our algorithm not only achieves excellent performance on the Cornell dataset but also outperforms other methods on the Jacquard dataset. Some detection examples are displayed in <ref type="figure">Fig. 9</ref>. As with the Cornell dataset, grasp quality, Angle, width representing the opening and closing distance of the gripper, and the best detection results on the jacquard dataset are presented in the figure.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Ablation Study</head><p>In order to further explore the impact of different components on grasping pose learning, we trained our models of different network Settings in image-wise split of Cornell dataset with RGBD data as input. The experimental results are summarized in <ref type="table">Table.</ref> IV. It can be obtained from the detection accuracy evaluation results in the <ref type="table">Table.</ref> IV that Gaussian-based grasp representation (GGR), receptive field block module (RFBM) and multi-dimensional attention fusion network (MDAFN) can all bring performance improvement to the network, and all components combined together can get the best grasping detection performance. Moreover, we also discuss the impact of different scale factor Settings (T) on the model, as shown in the <ref type="figure" target="#fig_0">Fig. 10</ref>. In this work, the scale factors T x and T y mentioned in section III-B are set to T x = T y = T with values ranging from {4, 8, 16, 32, 64}. When the T = 16 , the model in object-wise split of Cornell dataset reaches the best detection accuracy of 97.8. In the process of experiment, we found the different density of annotation for a particular dataset should be set the size of the corresponding scale factor value, which can slow the instability of the nerwork learning caused by labels overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Comparison of network parameter sizes</head><p>In <ref type="table">Table.</ref> V, some comparisons of network sizes used for grasping predictions are listed. Many works, such as <ref type="bibr" target="#b26">[24]</ref>, <ref type="bibr" target="#b47">[44]</ref>, <ref type="bibr" target="#b48">[45]</ref>, <ref type="bibr" target="#b10">[10]</ref>, contain thousands or millions of network parameters. In order to improve the real-time performance of the grasping algorithm, we developed a lightweight generative <ref type="figure" target="#fig_0">Fig. 11</ref>: The detection results of multiple grasping objects. The first column is the grasp outputs of corresponding RGB images for several objects. The last three columns are the maps for grasp quality, angle and width representing the opening and closing distance of the gripper.  <ref type="bibr" target="#b26">[24]</ref> 60 million -Levine <ref type="bibr" target="#b47">[44]</ref> 1 million 0.2-0.5s Johns <ref type="bibr" target="#b48">[45]</ref> 60 million -Chu <ref type="bibr" target="#b10">[10]</ref> 216 million 120ms Morrison <ref type="bibr" target="#b14">[14]</ref> 66 k 3ms Ours 4.67 million 5ms grasping detection architecture, which achieves high detection acurracy and fast running speed, and its network size of 4.67M is an order of magnitude smaller than other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Objects in clutter</head><p>To validate the generalization ability of the proposed model in clutter scene, we use the model trained on the Cornell dataset to test in a more realistic multi-object enviroment. The detection results are presented in <ref type="figure" target="#fig_0">Fig. 11</ref>. Although the model is trained on a single object dataset, it is still able to effectively predict the grasp pose of multiple objects. In complex scenarios, the proposed model has better generalization ability to perform grasp pose estimation for multiple objects simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Failure cases analysis</head><p>During the experiment, it was found that although the proposed algorithm achieved high detection accuracy, it still failed to detect some cases, as shown in <ref type="figure" target="#fig_0">Fig. 12</ref>. For some objects in the Jacquard dataset with complex shapes, our model does not work well. Furthermore, in the clutter scenes, smaller objects among multiple objects are often missed by the model, and the detection quality of the model for large boxe is not good as well. However, these shortcomings can be addressed by increasing the diversity of the training dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we proposed a Gaussian-based grasp representation to highlight the maximum grasp quality at the center position. Based on Gaussian-based grasp representation, a lightweight generative architecture with a receptive field block module and multi-dimensional attention fusion network was developed for grasp pose estimation. Experiments on two common public datasets, Cornell and Jacquad, show that our model has a very fast inference speed while achieving a high detection accuracy, and it reaches a detection accuracy of 98.9 and 95.6 respectively on Cornell and Jacquard datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>A comparision between the methods of oriented rectangle-based grasp representation and the methods of contact point-based grasp representation. The top branch is the workflow of the model using the oriented rectangle as grasp representation, and the bottom branch is the workflow of the model using the contact point grasp representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Gaussian-based grasp representation: The 2-D Gaussian kernel is applied to the grasp quality map to highlight the max grasp quality of its central point position. (a) the schematic diagram of grasp quality weight distribution after 2-D Gaussian function deployment, and (b) the schematic diagram of grasp representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The structure of our lightweight generative grasping detection algorithm. I and Conv denote the input data and covolution filter, respectively. The proposed method consisits of the downsampling block, the bottleneck layer, the multi-dimensional attention fusion network and the upsampling block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Receptive field block module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Multi-dimensional attention fusion network. The top branch is the pixel-level attention subnetwork, and the bottom branch is the channel-level attention subnetwork.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Qualitative images from Cornell grasping dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Qualitative images from Jacquard grasping dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 :</head><label>10</label><figDesc>The grasp detection accuracy when using different scale factors of Gaussian kernel</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 :</head><label>12</label><figDesc>Failed detection cases with single and multiple objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Description of the public Grasping Datasets</figDesc><table><row><cell>Dataset</cell><cell cols="2">Modality Objects</cell><cell>Images</cell><cell>Grasps</cell></row><row><cell>Dexnet</cell><cell>Depth</cell><cell>1500</cell><cell>6.7M</cell><cell>6.7M</cell></row><row><cell>Cornell</cell><cell>RGB-D</cell><cell>240</cell><cell>885</cell><cell>8019</cell></row><row><cell>Jacquard</cell><cell>RGB-D</cell><cell>11K</cell><cell>54K</cell><cell>1.1M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Detection Accuracy (%) of Different Methods on Cornell Dataset</figDesc><table><row><cell>Author</cell><cell>Method</cell><cell>Input Size</cell><cell cols="2">Accuracy(%) Image-Wise Object-Wise</cell><cell>Time (ms)</cell></row><row><cell>Jiang [22]</cell><cell>Fast Search</cell><cell>227 ? 227</cell><cell>60.5</cell><cell>58.3</cell><cell>5000</cell></row><row><cell>Lenz [2]</cell><cell>SAE</cell><cell>227 ? 227</cell><cell>73.9</cell><cell>75.6</cell><cell>1350</cell></row><row><cell>Karaoguz [39]</cell><cell>GRPN</cell><cell>-</cell><cell>88.7</cell><cell>-</cell><cell>200</cell></row><row><cell>Chu [10]</cell><cell>FasterRcnn</cell><cell>227 ? 227</cell><cell>96.0</cell><cell>96.1</cell><cell>120</cell></row><row><cell>Zhang [27]</cell><cell>Multimodal Fusion</cell><cell>224 ? 224</cell><cell>88.9</cell><cell>88.2</cell><cell>117</cell></row><row><cell>Zhou [12]</cell><cell>FCGN</cell><cell>320 ? 320</cell><cell>97.7</cell><cell>96.6</cell><cell>117</cell></row><row><cell>Wang [40]</cell><cell>Two-stage, Cloosed Loop</cell><cell>-</cell><cell>85.3</cell><cell>-</cell><cell>140</cell></row><row><cell>Redmon [3]</cell><cell>AlexNet, MultiGrasp</cell><cell>224 ? 224</cell><cell>88.0</cell><cell>87.1</cell><cell>76</cell></row><row><cell>Kumra [28]</cell><cell>ResNet-50</cell><cell>224 ? 224</cell><cell>89.2</cell><cell>88.9</cell><cell>103</cell></row><row><cell>Kumra [17]</cell><cell>GR-ConvNet</cell><cell>300? 300</cell><cell>97.7</cell><cell>96.8</cell><cell>-</cell></row><row><cell>Asif [41]</cell><cell>GraspNet</cell><cell>224 ? 224</cell><cell>90.6</cell><cell>90.2</cell><cell>24</cell></row><row><cell>Guo [29]</cell><cell>ZF-Net, MultiGrasp</cell><cell>-</cell><cell>93.2</cell><cell>89.1</cell><cell>-</cell></row><row><cell>Park [11]</cell><cell>FCNN</cell><cell>360? 360</cell><cell>96.6</cell><cell>95.4</cell><cell>20</cell></row><row><cell>Morrison [14]</cell><cell>GGCNN</cell><cell>300? 300</cell><cell>73.0</cell><cell>69.0</cell><cell>3</cell></row><row><cell>Zhang [21]</cell><cell>ROI-GD</cell><cell>-</cell><cell>93.6</cell><cell>93.5</cell><cell>40</cell></row><row><cell>Song [14]</cell><cell>Matching Strategy</cell><cell>320? 320</cell><cell>96.2</cell><cell>95.6</cell><cell>-</cell></row><row><cell>Wang [42]</cell><cell>GPWRG</cell><cell>400? 400</cell><cell>94.4</cell><cell>91.0</cell><cell>8</cell></row><row><cell></cell><cell>Efficient Grasping-D</cell><cell></cell><cell>98.9</cell><cell>95.5</cell><cell>6</cell></row><row><cell>Our</cell><cell>Efficient Grasping-RGB</cell><cell>300? 300</cell><cell>96.6</cell><cell>91.0</cell><cell>6</cell></row><row><cell></cell><cell>Efficient Grasping-RGB-D</cell><cell></cell><cell>98.9</cell><cell>97.8</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Detection Accuracy (%) of Different Methods on Jacquard Dataset</figDesc><table><row><cell>Author</cell><cell>Method</cell><cell>Accuracy(%)</cell></row><row><cell cols="2">Depierre [43] Jacquard</cell><cell>74.2</cell></row><row><cell cols="2">Morrison [14] GG-CNN2</cell><cell>84</cell></row><row><cell>Zhou [12]</cell><cell>FCGN-RGD</cell><cell>92.8</cell></row><row><cell>Zhang [21]</cell><cell>ROIGD-RGD</cell><cell>93.6</cell></row><row><cell>Song [13]</cell><cell>Resnet-101-RGD</cell><cell>93.2</cell></row><row><cell>Kumra [17]</cell><cell>GR-ConvNet-RGB-D</cell><cell>94.6</cell></row><row><cell></cell><cell>Efficient Grasping-D</cell><cell>95.6</cell></row><row><cell>Ours</cell><cell>Efficient Grasping-RGB</cell><cell>91.6</cell></row><row><cell></cell><cell>Efficient Grasping-RGB-D</cell><cell>93.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>The impact of different network Settings on detection performance</figDesc><table><row><cell>+ GGR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ RFBM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ MDAFN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Acurracy (%)</cell><cell>97.8</cell><cell>94.4</cell><cell>96.6</cell><cell>98.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Network size comparison of different methods</figDesc><table><row><cell>Author</cell><cell>Parameters (Approx.)</cell><cell>Time</cell></row><row><cell>Lenz [2]</cell><cell>-</cell><cell>13.5s</cell></row><row><cell>Pinto and Gupta</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Grasp moduli spaces and spherical harmonics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Pokorny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bekiroglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="389" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning for detecting robotic grasps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<idno type="DOI">10.1177/0278364914549607</idno>
		<ptr target="https://doi.org/10.1177/0278364914549607" />
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="705" to="724" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time grasp detection using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2015 IEEE International Conference on</title>
		<meeting><address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Densely supervised grasp detector (DSGD)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harrer</surname></persName>
		</author>
		<idno>abs/1810.03962</idno>
		<ptr target="http://arxiv.org/abs/1810.03962" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-object grasping detection with hierarchical feature fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="43" to="884" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robotic grasp detection using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<idno>abs/1611.08036</idno>
		<ptr target="http://arxiv.org/abs/1611.08036" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1612.08242</idno>
		<ptr target="http://arxiv.org/abs/1612.08242" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
		<idno>arxiv:1512.02325Comment: ECCV 2016</idno>
		<ptr target="http://arxiv.org/abs/1512.02325" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep grasp: Detection and localization of grasps with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Vela</surname></persName>
		</author>
		<idno>abs/1802.00520</idno>
		<ptr target="http://arxiv.org/abs/1802.00520" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Real-time, highly accurate robotic grasp detection using fully convolutional neural networks with high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Se Young Chun Dongwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chun</surname></persName>
		</author>
		<idno>abs/1809.05828</idno>
		<ptr target="http://arxiv.org/abs/1809.05828" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fully convolutional grasp detection network with oriented anchor box</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<idno>abs/1803.02209</idno>
		<ptr target="http://arxiv.org/abs/1803.02209" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A novel robotic grasp detection method based on region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0736584519308105" />
	</analytic>
	<monogr>
		<title level="j">Robotics and Computer-Integrated Manufacturing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101963</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning robust, real-time, reactive robotic grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leitner</surname></persName>
		</author>
		<idno type="DOI">10.1177/0278364919859066</idno>
		<ptr target="https://doi.org/10.1177/0278364919859066" />
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="183" to="201" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient fully convolution neural network for generating pixel wise robotic grasps with high resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1902.08950</idno>
		<ptr target="http://arxiv.org/abs/1902.08950" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sgdn: Segmentation-based grasp detection network for unsymmetrical three-finger gripper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Antipodal robotic grasping using generative residual convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sahin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robotic grasping and contact: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bicchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065)</title>
		<meeting>2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia (Cat. No.00CH37065)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="348" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Detecting layered structures of partially occluded objects for bin picking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Inagaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5786" to="5791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gq-stn: Optimizing one-shot grasp detection based on robustness classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gari?py</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Ruel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chaib-Draa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gigu?re</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3996" to="4003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Roibased robotic grasp detection for object overlapping scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4768" to="4775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient grasping from rgbd images: Learning using a new rectangle representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moseson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Vision-based robotic grasping from object localization, pose estimation, grasp detection to motion planning: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lian</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.06658" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3406" to="3413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ojea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<idno>abs/1703.09312</idno>
		<ptr target="http://arxiv.org/abs/1703.09312" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Classification based grasp detection using spatial transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chun</surname></persName>
		</author>
		<idno>abs/1803.01356</idno>
		<ptr target="http://arxiv.org/abs/1803.01356" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust robot grasp detection in multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengshan</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.1051/matecconf/201713900060</idno>
		<ptr target="https://doi.org/10.1051/matecconf/201713900060" />
	</analytic>
	<monogr>
		<title level="j">MATEC Web Conf</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robotic grasp detection using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A hybrid deep architecture for robotic grasp detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1609" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Optimizing correlated graspability score and grasp regression for better grasp prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Depierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dellandr?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Orientation attentive robot grasp synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gkanatsios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chalvatzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1609.05158</idno>
		<ptr target="http://arxiv.org/abs/1609.05158" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Field block net for accurreceptiveate and fast object deteccomputational neuroimaging and population recep-tive eldstion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trends in Cognitive Sciences</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1903.06586" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Object detection approach for robot grasp detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karaoguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jensfelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4953" to="4959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robot grasp detection using multimodal deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1177/1687814016668077</idno>
		<ptr target="https://doi.org/10.1177/1687814016668077" />
	</analytic>
	<monogr>
		<title level="j">Advances in Mechanical Engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1687814016668077</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graspnet: An efficient convolutional neural network for real-time grasp detection for low-powered devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4875" to="4882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<idno type="DOI">10.24963/ijcai.2018/677</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2018/677" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient fully convolution neural network for generating pixel wise robotic grasps with high resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1902.08950</idno>
		<ptr target="http://arxiv.org/abs/1902.08950" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Jacquard: A large scale dataset for robotic grasp detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Depierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dellandr?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1803.11469</idno>
		<ptr target="http://arxiv.org/abs/1803.11469" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quillen</surname></persName>
		</author>
		<idno type="DOI">10.1177/0278364917710318</idno>
		<ptr target="https://doi.org/10.1177/0278364917710318" />
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="421" to="436" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep learning a grasp function for grasping under gripper pose uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4461" to="4468" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
