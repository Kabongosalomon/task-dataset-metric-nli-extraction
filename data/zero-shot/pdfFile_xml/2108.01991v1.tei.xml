<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lung Sound Classification Using Co-tuning and Stochastic Normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-08">Aug 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">F</forename><surname>Pernkopf</surname></persName>
						</author>
						<title level="a" type="main">Lung Sound Classification Using Co-tuning and Stochastic Normalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-08">Aug 2021</date>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Adventitious lung sound classification</term>
					<term>respi- ratory disease classification</term>
					<term>crackles</term>
					<term>wheezes</term>
					<term>co-tuning for transfer learning</term>
					<term>stochastic normalization</term>
					<term>ICBHI dataset</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we use pre-trained ResNet models as backbone architectures for classification of adventitious lung sounds and respiratory diseases. The knowledge of the pretrained model is transferred by using vanilla fine-tuning, cotuning, stochastic normalization and the combination of the cotuning and stochastic normalization techniques. Furthermore, data augmentation in both time domain and time-frequency domain is used to account for the class imbalance of the ICBHI and our multi-channel lung sound dataset. Additionally, we apply spectrum correction to consider the variations of the recording device properties on the ICBHI dataset. Empirically, our proposed systems mostly outperform all state-of-the-art lung sound classification systems for the adventitious lung sounds and respiratory diseases of both datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>R ESPIRATORY diseases have become one of the main causes of death in society. According to the World Health Organization (WHO), the "big five" respiratory diseases, which include asthma, chronic obstructive pulmonary disease (COPD), acute lower respiratory tract infections, lung cancer and tuberculosis, cause the mortality of more than 3 million people each year worldwide. Currently, CoViD-19, a special form of viral pneumonia related to the coronavirus identified firstly in Wuhan (China) in 2019 <ref type="bibr" target="#b0">[1]</ref>, has caused globally more than 158 million infections and 3,296,000 deaths [2]. On <ref type="bibr">March 11, 2020</ref>, the WHO officially announced that CoViD-19 has reached global pandemic status. Furthermore, according to <ref type="bibr" target="#b1">[3]</ref>, the "big five" lung diseases, except lung cancer, have increased during CoViD-19 epidemics. These respiratory diseases are characterised by highly similar symptoms, i.e. the adventitious breathing, which could be a confounding factor during diagnosis <ref type="bibr" target="#b0">[1]</ref>. Due to their severe consequences, particularly in the case of CoViD-19, an early and accurate diagnosis of these types of diseases has become crucial.</p><p>Lung sounds convey relevant information related to pulmonary disorders with adventitious breathing sounds such as crackles, wheezes, or both of crackles and wheezes <ref type="bibr" target="#b2">[4]</ref>, <ref type="bibr" target="#b3">[5]</ref>. In the last decades, to facilitate a more objective assessment of the lung sound for diagnosis of pulmonary diseases/conditions, computational methods i.e. computational lung sound analysis Thanks to the Vietnamese -Austrian Government scholarship and the Austrian Science Fund (FWF) under the project number I2706-N31.</p><p>T. Nguyen, is with Signal Processing and Speech Communication Lab. Graz University of Technology, Austria (e-mail: t.k.nguyen@tugraz.at).</p><p>F. Pernkopf is with Signal Processing and Speech Communication Lab. Graz University of Technology, Austria (e-mail: pernkopf@tugraz.at).</p><p>(CLSA) <ref type="bibr" target="#b4">[6]</ref>, <ref type="bibr" target="#b5">[7]</ref> have been developed. The CLSA systems automatically detect and classify adventitious lung sounds by using digital recording devices, signal processing techniques and machine learning algorithms. They are also carefully evaluated in real-life scenarios and can be used as portable easy-to-use devices without the necessity of expert interaction; especially, beneficial when facing infectious diseases as CoViD- <ref type="bibr">19.</ref> In CLSA systems, there are two popular classification tasks, namely (i) adventitious lung sound and (ii) respiratory disease classification. In adventitious lung sound classification, recognition of normal and abnormal sounds (i.e. either crackles or wheezes or both of them) is important; while for respiratory disease classification, several categories have been considered e.g. binary classification (health and pathological), ternary chronic classification (healthy, chronic and non-chronic diseases) or six class classification of distinct pathologies. The systems have been evaluated on non-public datasets such as R.A.L.E. <ref type="bibr" target="#b6">[8]</ref> or multi channel lung sound data <ref type="bibr" target="#b7">[9]</ref> (ours) and public datasets i.e. the ICBHI 2017 dataset <ref type="bibr" target="#b3">[5]</ref> or the Abdullah University Hospital 2020 dataset <ref type="bibr" target="#b8">[10]</ref>. Due to limitations in the amount and quality of available data, the performance and generalization of the lung sound classification system may suffer over-estimated results. To deal with these challenges, different feature extraction methods <ref type="bibr" target="#b9">[11]</ref>, <ref type="bibr" target="#b10">[12]</ref>, <ref type="bibr" target="#b11">[13]</ref>, <ref type="bibr" target="#b12">[14]</ref>, conventional machine learning <ref type="bibr" target="#b10">[12]</ref>, <ref type="bibr" target="#b13">[15]</ref>, <ref type="bibr" target="#b14">[16]</ref>, <ref type="bibr" target="#b15">[17]</ref>, <ref type="bibr" target="#b16">[18]</ref>, deep learning <ref type="bibr" target="#b17">[19]</ref>, <ref type="bibr" target="#b18">[20]</ref>, <ref type="bibr" target="#b19">[21]</ref>, <ref type="bibr" target="#b20">[22]</ref>, data augmentation and transfer learning from ImageNet <ref type="bibr" target="#b21">[23]</ref>, <ref type="bibr" target="#b22">[24]</ref>, or audio scene datasets <ref type="bibr" target="#b23">[25]</ref> have been explored.</p><p>In this work, we improve the generalization ability and model performance for adventitious lung sound classification and respiratory disease classification systems using the ICBHI 2017 dataset and our multi-channel lung sound dataset. We exploit transfer learning approaches such as co-tuning <ref type="bibr" target="#b24">[26]</ref> for different architectures of residual neural networks (ResNets). We use pre-trained ResNet models of the ImageNet classification task as backbone architectures, which requires a 3-channel input i.e. color RGB images. Therefore the spectrograms are converted into 3 channels for the model input. Particularly, logmel spectrograms are replicated into three channels for the adventitious lung sound task or converted into RGB color spectrogram for respiratory disease classification. The pretrained models are exploited systematically in the following compositions.</p><p>? Firstly, we fine-tune the pre-trained model on a target domain and update all top (i.e. feature representation) layers and bottom (i.e. task-specific) layers. We call this vanilla fine-tuning.</p><p>? Secondly, we apply co-tuning for transfer learning <ref type="bibr" target="#b24">[26]</ref>, in which representation layers and task-specific layers of both source domain and target domain are collaboratively fine-tuned. Co-tuning further updates task-specific layers of the pre-trained model using a learned category relationship between source and target domains. ? Thirdly, we replace Batch Normalization (BN) layers, which suffer from poor performance in case of a data distribution shift between training and test data. We introduce stochastic normalization (StochNorm) <ref type="bibr" target="#b25">[27]</ref> in each residual block of the pre-trained backbone architecture. StochNorm is a parallel structure normalizing the activation of each channel by either mini-batch statistics or moving statistics to avoid influence of sample statistics during training. Thus, it is considered as a regularization method. Furthermore, fine-tuning inherits further prior knowledge of moving statistics of the pre-trained networks compared to vanilla fine-tuning. Both properties help to avoid over-fitting on small datasets such as the ICBHI and our lung sound dataset. ? Finally, we combine co-tuning and stochastic normalization techniques to take advantages of both techniques. In addition, we apply data augmentation in both time domain and time-frequency domain to account for the class imbalance in the datasets. Furthermore, we use spectrum correction for lung sound classification to compensate the recording device variations in the ICBHI dataset. The main contributions of the paper are:</p><p>? We propose robust classification systems for adventitious lung sounds and respiratory diseases for the ICBHI and our multi-channel lung sound dataset. ? We exploit transferred knowledge of pre-trained models by vanilla fine-tuning, co-tuning, stochastic normalization techniques and a combination of both co-tuning and stochastic normalization. ? We introduce spectrum correction to improve the generalization ability by accounting for the recording device differences. ? In addition to commonly used data augmentation techniques, we double the size of the training dataset by flipping samples in target domain. This enhances the performance of adventitious lung sound classification. ? We review state-of-the-art adventitious lung sound and respiratory disease classification systems for the ICBHI and our multi-channel lung sound dataset. The outline of the paper is as follows: In Section II, we introduce the lung sound databases. In Section III, we present our lung sound classification systems. In Section IV, we present the experimental setup including the evaluation metrics and the experimental results. We review related works in Section V. Finally, we conclude the paper in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DATABASES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ICBHI 2017 Dataset</head><p>The ICBHI 2017 database <ref type="bibr" target="#b3">[5]</ref> consists of 920 annotated audio samples from 126 subjects corresponding to patient pathological conditions i.e. healthy and seven distinct disease categories (Pneumonia, Bronchiectasis, COPD, upper respiratory tract infection (URTI), lower respiratory tract infection (LRTI), Bronchiolitis, Asthma). The audios were recorded using different stethoscopes i.e. AKGC417L, Meditron, Litt3200 and LittC2SE. The recording duration ranges from 10s to 90s and the sampling rate ranges from 4000Hz to 44100Hz. Each recording is composed of a certain number of breathing cycles with corresponding annotations of the beginning and the end, and the presence/absence of crackles and/or wheezes. The annotations of the database supports to split audio recordings into respiratory cycles. The cycle duration ranges from 0.2s to 16s and the average cycle duration is 2.7s. The database includes 6898 different respiratory cycles with 3642 normal cycles, 1864 crackles, 886 wheezes, and 506 cycles containing of both crackles and wheezes.</p><p>We propose a classification system for the following tasks.</p><p>? ALSC: Adventitious lung sound classification (ALSC) is separated into two sub-tasks for respiratory cycles. The first one is a 4-class task classifying respiratory cycles into four classes (Normal, Crackles, Wheezes and both Crackles and Wheezes). The second sub-tasks is a 2class task of normal and abnormal lung sounds including Crackles, Wheezes and both Crackles and Wheezes. We evaluate our system on the official ICBHI data split. The dataset was divided by the ICBHI challenge into 60% for training and 40% for testing. Both sets are composed with different patients. ? RDC: Respiratory disease classification (RDC) also consists of two sub-tasks for audio recordings. The first one is a 3-class task classifying audio recordings into three groups of Healthy, Chronic Diseases (i.e. COPD, Bronchiectasis and Asthma) and Non-Chronic Diseases (i.e. URTI, LRTI, Pneumonia and Bronchiolitis). The second sub-tasks is a 2-class task (healthy/unhealthy), where the unhealthy class comprises of the seven diseases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-channel Lung Sound Database</head><p>The multi-channel lung sound database <ref type="bibr" target="#b7">[9]</ref>, <ref type="bibr" target="#b17">[19]</ref>, <ref type="bibr" target="#b26">[28]</ref> has been recorded in a clinical trial. It contains lung sounds of 16 healthy subjects and 7 patients diagnosed with idiopathic pulmonary fibrosis (IPF). We used our 16-channel lung sound recording device (see <ref type="figure" target="#fig_0">Fig. 1</ref>) to record lung sounds over the posterior chest at two different airflow rates, with 3 -8 respiratory cycles within 30s. The lung sounds were recorded with a sampling frequency of 16kHz. The sensor signals are filtered with a Bessel high-pass filter with a cut-off frequency of 80Hz and a slope of 24dB/oct. We extracted full respiratory cycles using the airflow signal from all recordings. We manually annotated respiratory cycles in cooperation with respiratory experts from Medical University of Graz, Austria. The number of breathing cycles with/without IPF are shown in <ref type="table" target="#tab_0">Table I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TRANSFER LEARNING FOR LUNG SOUND CLASSIFICATION</head><p>The proposed systems include two key stages i.e. feature processing and classification as shown in <ref type="figure">Fig. 2</ref>. Firstly,  the respiratory cycles/ recordings are pre-processed in time domain and transformed into log-mel spectrograms of fixed size. Secondly, the features are fed to the CNN model where co-tuning or stochastic normalization are explored for the different classification tasks. During inference, the label of an input respiratory cycle/ recording is determined via majority voting <ref type="bibr" target="#b27">[29]</ref> of the predicted labels of the individual segments with a length of 8 seconds belonging to the same recording.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Audio Pre-processing and Feature Extraction</head><p>We use the audio pre-processing and feature extraction techniques presented in <ref type="bibr" target="#b20">[22]</ref> for both datasets. Audio recordings are resampled to 16kHz for the ALSC tasks of the ICBHI challenge and our dataset, while the RDC tasks use 4kHz of sampling rate. Similar to our previous works on ALSC of ICBHI and our multi-channel dataset <ref type="bibr" target="#b20">[22]</ref>, <ref type="bibr" target="#b28">[30]</ref>, the respiratory cycles are split without overlap into segments. Furthermore, we apply sample padding in time-reversed order to achieve fixed-length segments without abrupt signal changes. For the RDC task of the ICBHI dataset, recordings are split into segments of the same length using 50% overlap. Different segment lengths are investigated in Section IV. Then we also applied sample padding to the segments being shorter than the fixed length. This means that for both respiratory cycles and recordings, the same splitting and sample padding procedure is applied to obtain the fixed-length segments for the ALSC and RDC tasks, respectively.</p><p>We use a window size of 512 samples for the fast Fourier transform (FFT) using 50% overlap between the windows. The number of mel frequency bins is chosen as 50 and 45 for the ICBHI dataset and our multi-channel dataset, respectively. The logarithmic scale is applied to the magnitude of the mel spectrograms. The log-mel spectrograms are normalized with zero mean and unit variance. Then these spectrograms are duplicated into three channels to match the input size of the pre-trained ResNet model for the ALSC task. However, for the RDC task of the ICBHI dataset, we convert the spectrogram into a RGB color image and enlarge the image to twice the size using linear interpolation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spectrum Correction</head><p>We observe a different frequency response across devices which results in a performance degradation for underrepresented devices. Hence, we calibrate the features of the audio segments by applying spectrum correction instead of training or fine-tuning the model for a specific device <ref type="bibr" target="#b29">[31]</ref>, <ref type="bibr" target="#b30">[32]</ref>. The spectrum correction or calibration proposed in <ref type="bibr" target="#b31">[33]</ref>, which was first applied for acoustic scene classification, scales the frequency response of the recording devices. In particular, the calibration coefficients are calculated for each device based on data from reference devices. <ref type="table" target="#tab_0">Table II</ref> shows the recorded data portions of each recording device of the ICBHI dataset. The magnitude spectrum s k i of each segment i recorded by the device k is an averaged spectrum along the time axis of all FFT windows. The mean device spectrums k = 1</p><formula xml:id="formula_0">N k N k i=1 s k i , where device k records N k segments corresponding to N k spectra. The reference spectrum s ref is furthermore averaged over all mean device spectra of the D reference devices s ref = 1</formula><p>|D| k?Ds k , where D contains the indices of the reference devices. We investigate different cases of reference devices based on their prominence i.e only one device either AKGC417L or Meditron or both AKGC417L and Meditron, or all recording devices. The scaling coefficients of each device (c k ) is the element-wise fraction (i.e for each frequency bin) of the reference spectrum and its corresponding device spectrum c k = s ref s k . The magnitude of the STFTs of each device is scaled by using the corresponding coefficient vector c k for the frequency bins. We empirically observed that the normalization in spectrogram domain is more successful than in log-mel domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Augmentation</head><p>The ICBHI 2017 dataset is extremely imbalanced with around 53% of respiratory cycles belonging to the normal class and 86% of audio recordings belonging to COPD. Furthermore, with our multi-channel lung sound dataset, around 71% of respiratory cycles are annotated as normal class. Therefore, we use data augmentation in both time domain and timefrequency domain in order to balance the training dataset and prevent over-fitting.</p><p>1) Time Domain: For ALSC of the ICBHI dataset, we use time stretching to increase/reduce the sampling rate of an audio signal without affecting its pitch <ref type="bibr" target="#b32">[34]</ref>. It is used to double the number of segments of the wheeze, and both wheeze and crackle classes. We use a random sampling rate uniformly distributed with ?10% of the original sampling rate. For RDC of ICBHI, time stretching is used for all classes to double the number of samples. Furthermore, on the doubled training set further data augmentation methods <ref type="bibr" target="#b0">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Co-tuning for Transfer Learning Stochastic Normalization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4-class</head><p>Feature Processing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3-class 2-class 4-class 3-class 2-class</head><formula xml:id="formula_1">H ? t G s F ? ResNet50 Backbone Fig. 2.</formula><p>Proposed transferred knowledge systems using co-tuning for transfer learning or stochastic normalization.</p><p>adjusting, noise addition, pitch adjusting and speed adjusting are randomly applied based on a predefined probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Time-Frequency Domain:</head><p>? Vocal tract length perturbation (VTLP) selects a random wrap factor ? for each recording and maps the frequency f of the signal bandwidth to a new frequency f ? <ref type="bibr" target="#b33">[35]</ref>. We select ? from a uniform distribution ? ? U(0.9, 1.1) and set the maximum signal bandwidth to F hi = [3200, 3800]. VTLP is applied directly to the mel filter bank rather than distorting each spectrogram. VTLP is applied to enlarge the dataset for all classes in both tasks for both the original training set and the time stretched data. ? Additionally, we double the log-mel features by adding the flipped log-mel features (in frequency axis) for the ALSC and crackle detection task of our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Exploiting Transferred Knowledge 1) Transfer Learning:</head><p>Deep neural networks (DNNs) trained from scratch require large amounts of data. As data collecting is a time consuming task for lung sound data, transferring pre-trained parameters from DNNs, which are trained on other datasets e.g. ImageNet is advantageous. Less data of the target task is required, faster training is enabled, and usually better performance after fine-tuning the model on the target task is achieved <ref type="bibr" target="#b34">[36]</ref>. Therefore, fine-tuning brings great benefit to the research community.</p><p>Given a DNN M 0 pre-trained on a source dataset</p><formula xml:id="formula_2">D s = {(x i s , y i s )} ms i=1 , transfer learning aims to fine-tune M 0 on a target dataset D t = {(x i t , y i t )} mt i=1 .</formula><p>In this work, D s is selected from ImageNet and D t is the ICBHI 2017 dataset or our multi-channel lung sound dataset. Only D t and the pre-trained model M 0 are available during fine-tuning. Because D s and D t are different domains, which may have different input spaces X s and X t , corresponding to different output spaces Y s and Y t , respectively. Therefore, M 0 can not be directly applied to the target data. It is common practice, to split M 0 into two parts: a general representation function F?0 (parametrized by? 0 ) and a task-specific function G ? 0 s (parametrized by ? 0 s ), which denotes the last layers of the pre-trained model. Usually, the representation function is retained and the task-specific function is replaced by a randomly initialized function H ?t (parameterized by ? t ) whose output space matches Y t . Hence, we optimize</p><formula xml:id="formula_3">(? * , ? * t ) = argmin ?,?t 1 |D t | mt i=1 l(H ?t (F?(x i t )), y i t ),<label>(1)</label></formula><p>where l(?) is a loss function such as cross-entropy for classification. We will call this vanilla fine-tuning. Pre-trained parameters? 0 provide a good starting point for the optimization. It means that the vanilla fine-tuning for a target dataset can be beneficial by transferring the knowledge of the part F?0 of the source dataset.</p><p>In this work, we explore different depths of ResNet architectures i.e. ResNet18, ResNet34, ResNet50 and ResNet101 as neural network backbones.</p><p>2) Co-tuning: Co-tuning for transfer learning enables full knowledge transfer of the pre-trained models using a twostep framework <ref type="bibr" target="#b24">[26]</ref>. The first step is learning the relationship between source categories and target categories from the pre-trained model with calibrated predictions. Secondly, target labels (one-hot labels) and source labels (probabilistic labels) translated by the category relationship, collaboratively supervise the fine-tuning process. Co-tuning empirically proves its ability in enhancement of the performance compared to vanilla fine-tuning of the ImageNet pre-trained models <ref type="bibr" target="#b24">[26]</ref>. In this work, we apply co-tuning to fully exploit the ImageNet pretrained models for significantly distinct datasets such as the ICBHI and our multi-channel lung sound dataset. The cotuning block in <ref type="figure">Fig. 2</ref> shows the source output layer G ?s , the target output layer H ?t , the ResNet50 backbone F? and category relationship, which is the relationship between output spaces i.e. the conditional distribution p(y s |y t ).</p><p>In the training, the category relationship p(y s |y t ) is needed to translate target labels y t into probabilistic source categories y s , which is use to fine-tune the task-specific function G ? 0 s . The gradient of G ? 0 s can be back-propagated into F?. Both outputs y t and y s collaboratively supervise the transfer learning process described as</p><formula xml:id="formula_4">(? * , ? * t , ? * s ) = argmin ?,?t,?s 1 |D t | mt i=1 [l(H ?t (F?(x i t )), y i t ) +?l(G ?s (F?(x i t )), p(y s |y t = y i t ))],<label>(2)</label></formula><p>where ? trades off the target and source supervisions. Variables ? and ? s are initialized from pre-trained weights? 0 and ? 0 s . In this way, the pre-trained parameters? 0 , ? 0 s are fully exploited in the collaborative training. During inference, the task specific layers G ? 0 s are removed to avoid the additional cost. The category relationship p(y s |y t ) is computed based on the output of task-specific function G ? 0 s (i.e. a probability distribution over source categories Y s ) and target labels Y t by two ways:</p><p>? Direct approach: The category relationship is determined as average of the predictions of the pre-trained source model over all samples of each target category i.e.</p><formula xml:id="formula_5">p(y s |y t = y) ? 1 |D t | (x,yt)?Dt|yt=y M 0 (x),<label>(3)</label></formula><p>where the pre-trained model M 0 is considered as a probabilistic model approximating the conditional distribution M 0 (x) ? p(y s |x). ? Reverse approach: When categories in the pre-trained dataset are diverse enough to compose a target category, we can use a reverse approach. We learn the mapping y s ? y t from (M 0 (x t ), y t ) pairs, where y t is target labels and M 0 (x) ? p(y s |x) is a probability distribution over source categories Y s . Then p(y s |y t ) can be calculated from p(y t |y s ) by Bayes's rule. In addition, according to <ref type="bibr" target="#b24">[26]</ref>, it is necessary to calibrate the neural network, i.e. calibrating the probability output of the pre-trained model, to enhance performance.</p><p>3) Stochastic Normalization (StochNorm): In <ref type="bibr" target="#b25">[27]</ref>, stochastic normalization is proposed to avoid over-fitting during finetuning on small dataset. It replaces the Batch Normalization (BN) layers. It implement a two-branch architecture including one branch normalized by mini-batch statistics and another branch normalized by moving statistics (specified in detail below). A stochastic selection mechanism like Dropout is used between the two branches to avoid over-depending on some sample statistics. This is interpreted as an architecture regularization.</p><p>Furthermore, StochNorm uses the moving statistics of the pre-trained model for the initial statistics to better exploit prior knowledge in pre-trained networks.</p><p>Given a mini-batch of feature maps of each channel z = {z i } m i=1 and a moving statistic update rate ? ? (0, 1). The normalization process in the two branches is calculated a?</p><formula xml:id="formula_6">z i,0 = z i ?? ?? 2 + ? ,? i,1 = z i ? ? ? ? 2 + ? ,<label>(4)</label></formula><p>during training, where the mean ? and variance ? 2 of the current mini-batch data of size m</p><formula xml:id="formula_7">? ? 1 m m i=1 z i , ? 2 ? 1 m m i=1 (z i ? ?) 2<label>(5)</label></formula><p>are used as usual, while the other branch uses moving statistics ? and? 2 of the training dat?</p><formula xml:id="formula_8">? ?? + ?(? ??),? 2 ?? 2 + ?(? 2 ?? 2 ).<label>(6)</label></formula><p>The moving statistics are initialized by using corresponding parameters from the pre-trained model. During forward propagation, either? i,0 or? i,1 is randomly selected with probability p in each channel of the normalization layers and each step of training, i.e.?</p><formula xml:id="formula_9">i = (1 ? s)? i,0 + s? i,1 ,<label>(7)</label></formula><p>where s is the branch-selection variable generated from a Bernoulli distribution s ? Bernoulli(p). The learnable scale and shift parameters ?, ? can be applied after the stochastic selection as usual</p><formula xml:id="formula_10">y i ? ?x i + ?.<label>(8)</label></formula><p>Stochastic normalization in <ref type="figure">Fig. 2</ref> uses a ResNet backbone where BN layers are replaced by StochNorm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Combination of Co-tuning and Stochastic Normalization:</head><p>We empirically evaluate the combination of co-tuning and StochNorm for lung sound classification. To do that, the category relationship is initially calculated based on the pretrained ResNet models, followed by replacing BN layers with the StochNorm modules in the backbone of the original cotuning architecture (i.e replacing the green ResBlocks from Co-tuning by the blue ResBlocks of Stochastic Normalization in <ref type="figure">Fig. 2)</ref>. After that, the fine-tuning of the co-tuning is processed on the new architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we first provide details of the experimental setup. Furthermore, we empirically evaluate the following cases:</p><p>? Transfer learning of different pre-trained ImageNet ResNet models on the ICBHI dataset. ? Ablation study for respiratory segment length, spectrum corrections and flipping data augmentation.</p><p>? Transfer learning of different ResNet models pre-trained on ImageNet and ICBHI for our multi-channel lung sound dataset. Our systems for the ALSC and RDC tasks on the ICBHI dataset are also compared against state-of-the-art works for the official ICBHI data split and five-fold cross-validation. Additionally, we compare our best system for crackle detection to our previous work on the multi-channel lung sound dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Metrics</head><p>We use the evaluation metrics supported by the ICBHI Challenge <ref type="bibr" target="#b3">[5]</ref> for ALSC of 4 classes. The evaluation is based on respiratory cycles using sensitivity (SE), specificity (SP), average score (AS), known as the average of the sensitivity and the specificity, and the harmonic score (HS), known as the harmonic mean of the sensitivity and the specificity. For 2 classes, we determine SE and SP as in <ref type="bibr" target="#b18">[20]</ref> and <ref type="bibr" target="#b12">[14]</ref> and AS and HS as in <ref type="bibr" target="#b3">[5]</ref>.</p><p>Similarly, for RDC of 3 classes and 2 classes, a recordingwise evaluation is performed using SE and SP as in <ref type="bibr" target="#b18">[20]</ref> and <ref type="bibr" target="#b12">[14]</ref> and AS and HS as in <ref type="bibr" target="#b3">[5]</ref>.</p><p>Furthermore, for our multi-channel lung sound dataset, we calculate Precision (P + ), Sensitivity or Recall (Se), and the F 1 -score (F 1 ) as specified in <ref type="bibr" target="#b17">[19]</ref>. Precision provides information about how many of the respiratory cycles recognized as crackles are actually true. Sensitivity provides information about how many respiratory cycles containing crackles are actually recognized as crackles. The F 1 -score is the harmonic mean of precision and sensitivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup</head><p>We evaluate our ALSC system for 4 and 2 classes on the official ICBHI 2017 dataset split, which consists of 60% recordings for the training set and 40% for the test set. Each patients is either in the training or test set. The reported performance is the average accuracy of five independent runs. For the RDC task of 3 and 2 classes, our proposed system is evaluated on both the official dataset split and five-fold crossvalidation. Again, data of each patients is not shared between folds. For five-fold cross-validation, one fold is used as test set, the remaining folds are used for training. As co-tuning requires a validation set to compute the category relationship, we randomly select 20% of the samples from the training set. For cross-validation, the average of the best performance on the test sets is represented.</p><p>Due to the limited amount of data samples in our multichannel lung sound dataset, we use 7-fold cross-validation with the recordings of each IPF subject appearing once in the test set. Each subject is assigned to either training, validation or test set. The best model is selected based on the best accuracy on the validation set. The reported performance of the system is an average accuracy of seven folds using the same data splittings.</p><p>Experiments are implemented based on Pytorch <ref type="bibr" target="#b35">[37]</ref>. For vanilla fine-tuning, the learning rate and number of epochs is set to 0.001 and 150 for all tests, respectively. The fine-tuning of co-tuning and stochastic normalization techniques 2 updates the weights after each mini-batch. The learning rate of the feature representation layers and the last layer are set to 0.001 and 0.01, respectively. The fine-tuning process optimizes the cross entropy loss using SGD with a momentum of 0.9. The batch size is 32 for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Transfer learning techniques for different ResNets:</head><p>We evaluate the vanilla fine-tuning (VanillaFineTuning), cotuning (CoTuning), stochastic normalization (StochNorm) and the combination of co-tuning and stochastic normalization (CoTuning-StochNorm) for different ResNet architectures trained on the ImageNet dataset for the ALSC task of 4 classes (see <ref type="figure">Fig. 3</ref>) and the RDC task of 3 classes (see <ref type="figure" target="#fig_2">Fig. 5</ref>) on the official ICBHI dataset split. These systems use a segment length of 8s, spectrum correction using reference data s ref of all devices and all data augmentation methods introduced in Section C. <ref type="figure">Fig. 3</ref> shows that ResNet50 is the best performing architecture to build the backbone for these transfer learning techniques of the 4-class ALSC task. ResNet101 is also performing well except for vanilla fine-tuning. Co-tuning achieved the best performance of ?58% compared to the other techniques. Although CoTuning and StochNorm improve significantly the performance of VanillaFineTuning, the combination of cotuning and StochNorm is not able to outperform the original techniques for this task. <ref type="figure">Fig. 4</ref> visualizes of the average pooling outputs of the ResNet50 architecture for different transfer learning techniques projected to 2D by t-distributed stochastic neighbourhood embedding (t-SNE) <ref type="bibr" target="#b36">[38]</ref>. Distributions of the training set using vanilla fine-tuning, co-tuning, stochastic normalization and combination of co-tuning and stochastic normalization are shown at a), b), c) and d), respectively. Comparing to vanilla fine-tuning (a) and stochastic normalization technique (c), the distribution of 4 classes using co-tuning (b) and the combination of co-tuning and stochastic normalization (d) bring a large margin. It shows that the collaborative fine-tuning using the category relationship of source and target domain is useful for the adventitious lung sound classification task.</p><p>In <ref type="figure" target="#fig_2">Fig. 5</ref>, we see that the different transfer learning techniques using the ResNet101 model achieve the best performance for the 3-class RDC task. The ResNet50 model works better than others for the vanilla fine-tuning. CoTuning-StochNorm and StochNorm achieved a better performance compared to CoTuning and VanillaFineTuning. It proves the efficiency of the stochastic normalization in the fine-tuning process for the RDC task.</p><p>2) Respiratory segment length: The length of respiratory cycles in the ICBHI dataset varies in a wide range. Hence, we applied cycle splitting into segments and perform sample padding in order to obtain fixed-length segments. We observe different segment lengths for the ResNet 50 model fine-tuned by co-tuning and applied data augmentation in both time domain and time-frequency domain with spectrum correction. Results are shown in <ref type="table">Table 3</ref>. The best score is obtained with 8s fixed-length segments (AS) for 4 classes. We also use 8s as the fixed length for other tasks of the ICBHI and our lung sound dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Spectrum correction:</head><p>We experiment on the ICBHI dataset without spectrum calibration and with spectrum calibration using different reference spectra s ref , which are determined by one or more devices. No-Calib denotes that no spectrum correction is applied. Calib-Dev1 and Calib-Dev2 denote calibration using data of device AKGC417L and Meditron, respectively. Calib-Dev1Dev2 denotes calibration using data of both devices of AKGC417L and Meditron and Calib-AllDev denotes spectrum adaptation using reference data of four devices. From Table IV, we can see that co-tuning of the ResNet50 model using reference data of all devices achieves the best performance, it is 1.62% (absolute) better than without using spectrum calibration. Thus, we apply spectrum calibration for both adventitious lung sound classification and respiratory diseases classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Flipping data augmentation:</head><p>We apply data augmentation in time domain and VTLP in order to balance the dataset.  In this section we focus on the influence of feature flipping data augmentation (see Section III). <ref type="figure">Fig. 6</ref> shows that when the system does not use spectrum correction, doubling the size of the augmented training set by the flipping technique always performs well for vanilla fine-tuning, co-tuning and stochastic normalization. It improves significantly the performance of vanilla fine-tuning and stochastic normalization of about 3% and 2%, respectively. For co-tuning, the flipping data augmentation achieves an improvement of 1% accuracy. Furthermore, we can see from <ref type="figure">Fig. 6</ref> that using the combination of spectrum calibration and flipping data augmentation always enhances the robustness of the adventitious lung sound classification systems. 5) Effect of pre-trained model on the multi-channel lung sound dataset: According to the above evaluation of transfer learning techniques for different residual neural networks for the 4-class ALSC task, co-tuning achieves the best performance. Thus, we evaluate the effect of pre-trained models using co-tuning (CoTuning) for the 2-class ALSC task on our multi-channel lung sound dataset. It is shown in <ref type="figure" target="#fig_3">Fig. 7</ref>. Co-tuning using the ImageNet pre-trained model always outperforms that of the ICBHI pre-trained model. The smaller ResNet architectures tend to work better for co-tuning. We also can see from <ref type="figure" target="#fig_3">Fig. 7</ref> that the ResNet34 backbone system achieves the best performance, followed by ResNet18, ResNet50 and ResNet101. In addition, <ref type="figure" target="#fig_3">Fig. 7</ref> shows that transferred knowledge from full pre-trained models of ICBHI and the ImageNet dataset by co-tuning to our small lung sound dataset can achieve better accuracy than vanilla fine-tuning (VanillaFineTuning) using the ImageNet pre-trained model.</p><p>Overall, the best segment length for lung sound classification tasks is 8s. Spectrum correction is useful to improve the performance of our ALSC and RDC system on the ICBHI dataset. This helps to correct the different frequency responses of the recording devices. The ALSC system using the flipping data augmentation enhances performance on both ICBHI and our multi-channel lung sound dataset. The new transfer learning methods always outperform vanilla transfer learning. Co-tuning works better for the ALSC task while StochNorm and its combination with the co-tuning achieve higher performance for the RDC task. Furthermore, ResNet34 and ResNet50 are more suitable for the ALSC tasks, while a large ResNet101 model tends to be more robust for the RDC task in most of transfer learning settings. <ref type="table" target="#tab_0">Table VI and Table VII</ref> show the comparison of our best systems of different transfer learning techniques and state-of-the-art systems (see Section V for more details on the systems) for the ALSC and RDC tasks, respectively. Our best systems are presented in bold and the highest scores are presented in bold and italic. It is notable that the performances on the official 60/40 ICBHI separation without common patients in both sets are significantly lower than that of randomly 80/20 splitting i.e. 5-fold cross validation and overlap of the same patients in both sets. Despite of the same fixed length for segments, the RDC systems always achieve considerably higher performance compared to the ALSC system for different sub-tasks. The RDC tasks have the full audio recordings which consists of many available respiratory cycles, while the ALSC tasks are processed and evaluated on respiratory cycles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance Comparison 1) Comparison to state-of-the-art systems using the ICBHI dataset:</head><p>We evaluate our proposed system on the official ICBHI split for the 4 and 2 class ALSC tasks. Our best systems of  different fine-tuning techniques outperform the other ALSC systems. Our system using co-tuning of the ResNet50 pretrained model achieve the highest ICBHI average score at 58.29% and 64.74% for the 4-class and 2-class ALSC task, respectively. Our RDC systems are evaluated on the official ICBHI split and the 5-fold cross-validation method. On the official dataset split of the 3-class RDC task, our systems achieves the best performance with the ResNet101 pre-trained architecture combined with stochastic normalization. It obtains 92.72% of the ICBHI average score. While for the 5-fold cross-validation evaluation, the stochastic normalization of the ResNet101 model obtains the best average score 95.73%. It has an around 5% better average score compared to the stateof-the-art systems. On the 2-class RDC task, our systems using stochastic normalization achieves the average scores of 93.77% and 98.20% for the official splitting and 5-fold cross-validation, respectively. Our best 2-class RDC system outperforms all compared systems on the former evaluation, but it scores 1.02% lower compared to the system in <ref type="bibr" target="#b37">[39]</ref>.</p><p>2) Comparison for our multi-channel lung sound dataset: Table V compares our best systems using different transfer learning techniques with our previous system using fine-tuning for a multi-input CNN model <ref type="bibr" target="#b28">[30]</ref> on the multi-channel lung sound dataset. We can see that our best transfer learning systems outperform the previous system. The co-tuning system using the ResNet34 model pre-trained on ImageNet achieves the best performance, closely followed by the StochNorm system using the pre-train ResNet50 model. The best F 1 -score is 2.82% better than for the multi-input fine-tuned system <ref type="bibr" target="#b28">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RELATED WORKS</head><p>We review recent works on ALSC and RDC using the ICBHI 2017 dataset and binary ALSC works (i.e. crackle detection) using the multi-channel lung sound database. In general, it is difficult to compare the score of some proposed methods for ICBHI as a substantial work does not use the official data splitting or use a different evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Lung Sound Classification on ICBHI dataset</head><p>There are two main directions: (i) conventional classifiers for low-level features of time or frequency domain, (ii) deep neural networks and robust machine learning techniques for spectral features.</p><p>1) Conventional approach: Jakovljevic et al. <ref type="bibr" target="#b10">[12]</ref> used hidden Markov models and Gaussian mixture models for MFCCs, which were applied after spectrum subtraction for noise suppression. Their system achieved an average score of 39.56% on the official ICBHI train-set split and 49.5% on 10fold cross-validation for the ALSC task with 4 classes. Serbes et al. <ref type="bibr" target="#b38">[40]</ref> proposed a 4-class ALSC system using support vector machines (SVMs) for STFT and wavelet features. It achieved 49.86% of average score on the official ICBHI data split. Furthermore, Chambres et al. <ref type="bibr" target="#b9">[11]</ref> proposed a system using a boosted tree method for low-level features of spectral information i.e. bark-bands, energy-bands, mel-bands, MFCCs and other features i.e. rythm features and tonal features for both of the ALSC and binary RDC task. Their performance on the official ICBHI split were 49.63% and 85% for the 4class ALSC and 2-class RDC tasks, respectively. A binary RDC system using the RUSBoost algorithm, which combines random under sampling and boosting techniques (i.e decision tree as a base classifier) was also introduced <ref type="bibr" target="#b45">[47]</ref>. The input of the classifier are features selected from MFCCs, discrete wavelet transform (DWT) and time domain features. The proposed system was evaluated on their own ICBHI dataset split and achieved 87.1% of average score. In addition, Mukherjee et al. <ref type="bibr" target="#b37">[39]</ref> developed a method to detect patients with respiratory infections. They extracted features based on Linear Predictive Coefficient (LPC) for a multilayer perceptron classifier (MPC). The method was evaluated on the ICBHI dataset using 5-fold cross-validation and achieved 99.22% of accuracy for the 2-class ALSC task.</p><p>2) Deep learning approach: Deep learning systems use CNNs, Recurrent neural networks (RNNs) and hybrid architectures. They are combined with machine learning techniques such as data augmentation, ensemble methods and transfer learning to enhance robustness. For the RNN-based systems, Kochetov et al. <ref type="bibr" target="#b29">[31]</ref> proposed a system using a noise making RNN (NMRNN) and MFCC features to classify cycles of lung sounds into four categories. The performance was evaluated based on 5-fold cross-validation. It is the first work which considers the effect of the recording devices on the performance. They achieved a score of 64.8% and 68.5% with training data from all devices and the most often occurring recording device (i.e. AKGC417L), respectively. Furthermore, a CNN network for MFCCs was used, which achieved 61% and 83% of average score for the 4-class ALSC and 3-class RDC task with random train-test split of 80% and 20% <ref type="bibr" target="#b43">[45]</ref>. In <ref type="bibr" target="#b18">[20]</ref>, Perna et. al. also introduced different architectures of RNNs such as long short time memory (LSTM), gated recurrent units (GRU), bidirectional-LSTM (BiLSTM) and bidirectional-GRU (BiGRU) for MFCC features to perform 4class and 2-class ALSC and the ternary and 2-class RDC task. The results on random train-test ICBHI split of 80% and 20% with over-lapping of the same patients in both sets are 74% and 81% of average score for the 4-class and 2-class ALSC task, respectively. The average performance of the RDC tasks of 3 and 2 classes is 84% and 91%, respectively.</p><p>Furthermore, CNNs or hybrid architectures have been used. In <ref type="bibr" target="#b20">[22]</ref>, we proposed a lung sound classification using a snapshot ensemble of CNNs for log-mel spectrograms. We applied temporal stretching and vocal tract length perturbation (VTLP) for data augmentation to deal with the class-imbalance of the ICBHI dataset. Our system achieved 78.4% and 83.7% of average score on the random train-test split of 80% and 20% with common patients in both sets for the ALSC task of 4 classes and 2 classes, respectively. Acharya et al. <ref type="bibr" target="#b22">[24]</ref> introduced a deep CNN-RNN model for mel spectrograms to classify adventitious lung sounds into four classes. The performance for 5-fold cross validation evaluation was 66.31%. When this system was combined with a patient specific model tuning strategy, its performance increased up to 71.81% of average score. Similarly, Pham et al. <ref type="bibr" target="#b44">[46]</ref>, <ref type="bibr" target="#b12">[14]</ref> introduced lung sound classification systems for anomaly sounds and respiratory diseases. In the first work <ref type="bibr" target="#b44">[46]</ref>, they proposed various deep learning architectures mainly based on CNNs and RNNs using gammatone filtered spectrograms. They use a 80%-20% dataset split, where data from one subject may exist in both training and test set. An average ensemble of these systems achieved 80% and 86% of the average score for the 4-class and 2-class ALSC, respectively. The proposed CNN -mixture of expert (MoE) model was suitable for the RDC task of 3 classes and 2 classes with a performance of 90% and 91%, respectively. In <ref type="bibr" target="#b12">[14]</ref> they proposed a CNN-MoE neural network for different feature types i.e. MFCCs, log-mel, gammatone filter and constant Q transform spectrogram. The gammatone filter spectrogram was suggested for the ALSC tasks, while log-mel spectrogram worked better for the RDC task. The average score of the 4-class ALSC task was 47% for the ICBHI official dataset split. For 5-fold cross-validation with data of the same patient in both sets, their performance was 78.6% and 84% for the ALSC task of 4 classes and 2 classes, respectively. On the 3-class RDC task they achieved 85% of average score on the ICBHI official dataset split and 91% on 5-fold cross-validation.</p><p>Recently, CNN-based systems from diverse architectures i.e. VGGNets, ResNets have been more and more introduced. Minami et al. <ref type="bibr" target="#b39">[41]</ref> proposed a 4-class ALSC system using a VGG16 neural network for the combination of STFT spectrogram and scalogram. The performance was 54% of average score on the official ICBHI dataset. Ma et al. proposed two ALSC systems for four classes <ref type="bibr" target="#b40">[42]</ref>, <ref type="bibr" target="#b41">[43]</ref>. The first one used an improved Bi-ResNet deep learning architecture based on STFT The proposed systems achieved 50.16% and 52.26% on the official data split, respectively. The latter work <ref type="bibr" target="#b41">[43]</ref> was also evaluated using 5-fold cross-validation and achieved an average score of 64.21%.</p><p>Yang et al. <ref type="bibr" target="#b42">[44]</ref> proposed a 4-class ALSC system combining the ResNet18 architecture with Squeeze-and-Excitation and spatial attention blocks using STFT spectrogram features. They obtained 49.55% of average score on the official ICBHI datset split. Demir et al. proposed a 4-class ALSC system using pre-trained models for STFT spectrograms converted into color images. In the first approach <ref type="bibr" target="#b21">[23]</ref>, the pre-trained model was used as feature extractor and combined to an SVM classifier. In the second approach <ref type="bibr" target="#b21">[23]</ref>, the pre-trained model was fine-tuned on the ICBHI dataset. They achieved 65.5% and 63.09% of accuracy for 10-fold cross-validation, respectively. In <ref type="bibr" target="#b46">[48]</ref>, they introduced a parallel pooling CNN model for deep feature extraction. It is combined to a linear discriminant analysis (LDA) classifier and random subspace ensembles (RSE). The performance of the proposed system was 71.5% for 10-fold cross-valuation. However, the evaluation metrics are different.</p><p>Additionally, Gairola et. al. <ref type="bibr" target="#b30">[32]</ref> proposed a RespireNet model based on ResNet34 and fully connected layers with a set of techniques i.e. device specific fine-tuning, concatenationbased augmentation, blank region clipping and smart padding to improve the accuracy. The average score for the 4-class ALSC task was 56.2% and 68.5% for the official ICBHI dataset split and 5-fold cross-validation, respectively. They also evaluated the proposed system for the ALSC task of two classes and obtained 77.0% accuracy on 5-fold crossvalidation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Lung Sound Classification on our multi-channel dataset</head><p>In <ref type="bibr" target="#b17">[19]</ref>, Messner et al. introduced an event detection approach with bidirectional gated recurrent neural networks (Bi-GRNNs) using MFCCs to identify crackles in respiratory cycles. The proposed system was evaluated on the first version of the multi-channel lung sound dataset including 10 lunghealthy subjects and 5 patients with IPF. The performance was 72% of F-score on 5-fold cross-validation evaluation.</p><p>In <ref type="bibr" target="#b26">[28]</ref>, a classification framework using lung sound signals of all recording channels was introduced to identify healthy and pathological breathing cycles. Lung sounds of one breath cycle of all recording channels were first transformed into STFT spectrograms. Then, the spectrogram were stacked into one compact feature vector. These features were fed into a CNN-RNN model for classification. Its score was 92% for 7-fold cross-validation evaluation.</p><p>We proposed a multi-input CNN model based on transfer learning for the ALSC task of crackles and normal sounds, namely crackle detection <ref type="bibr" target="#b28">[30]</ref> on the multi-channel lung sound classification dataset. The multi-input CNN model shares the same network architecture of the pre-trained CNN model trained on the ICBHI dataset for respiratory cycles and their corresponding respiratory phases. Our system achieved an Fscore of 84.71% on 7-fold cross-validation evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We propose robust fine-tuning frameworks to classify adventitious lung sounds and recognize respiratory diseases from lung auscultation recordings using the ICBHI and our multi-channel lung sound datasets. Transferred knowledge of pre-trained models from different ResNet architectures are exploited by vanilla fine-tuning, co-tuning, stochastic normalization and the combination of co-tuning and stochastic normalization techniques. Furthermore, spectrum correction and flipping data augmentation are introduced to improve the robustness of our system. Empirically, our proposed systems outperform almost all state-of-the-art systems for adventitious lung sound and respiratory disease classification. In addition, we also evaluate our adventitious lung sound classification approach using co-tuning on our multi-channel lung sound dataset to detect crackles using different pre-trained models of the ImageNet and ICBHI dataset. The best co-tuning system for 2-class lung sound classification achieves a better performance (2.82%) compared to our previous work using a multiinput convolutional neural network. We also review state-ofthe-art classification systems for adventitious lung sounds and respiratory diseases using the ICBHI dataset and our multichannel lung sound dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Multi-channel lung sound recording device.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Comparison of vanilla transfer learning, co-tuning, stochastic normalization and both co-tuning and stochastic normalization of different ResNet backbones for the adventitious lung sound classification task of 4 classes. Average pooling output representations reduced into 2D by tdistributed stochastic neighborhood embedding (t-SNE) of the ResNet50 backbone architecture. Distributions of training set of a) vanilla fine-tuning, b) co-tuning, c) stochastic normalization d) combination of co-tuning and stochastic normalization. The color indicates the classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison of vanilla transfer learning, co-tuning, stochastic normalization and both co-tuning and stochastic normalization of different ResNet backbones for the respiratory disease classification task of 3 classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Comparison of co-tuning and vanilla fine-tuning of different ResNet architectures pre-trained on ICBHI and ImageNet for crakle detection. Our multi-channel lung sound dataset and flipping data augmentation are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell cols="5">NUMBER OF SUBJECTS AND CYCLES IN THE DATASET</cell></row><row><cell># Subjects</cell><cell></cell><cell cols="2"># Respiratory Cycles</cell><cell></cell></row><row><cell>Healthy</cell><cell>IPF</cell><cell>Normal</cell><cell>Crackles</cell><cell>Total</cell></row><row><cell>16</cell><cell>7</cell><cell>4405</cell><cell>1791</cell><cell>6196</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II PERCENTAGE</head><label>II</label><figDesc>OF SAMPLES RECORDED BY EACH DEVICE OF THE ICBHI</figDesc><table><row><cell></cell><cell></cell><cell>DATASET</cell><cell></cell><cell></cell></row><row><cell>Device</cell><cell>AKGC417L</cell><cell>Meditron</cell><cell>Litt3200</cell><cell>LittC2SE</cell></row><row><cell>% Sample</cell><cell>63%</cell><cell>21%</cell><cell>9%</cell><cell>7%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III RESPIRATORY</head><label>III</label><figDesc></figDesc><table><row><cell></cell><cell cols="6">SEGMENT LENGTH: AVERAGE SCORE (AS) FOR VARIOUS</cell></row><row><cell cols="7">INPUT LENGTH SIZES USING CO-TUNING OF RESNET50 AS BACKBONE</cell></row><row><cell cols="7">NETWORK AND DATA AUGMENTATION WITHOUT SPECTRUM</cell></row><row><cell></cell><cell></cell><cell cols="3">CALIBRATION.</cell><cell></cell><cell></cell></row><row><cell>Length.</cell><cell>4 sec</cell><cell>5 sec</cell><cell>6 sec</cell><cell>7 sec</cell><cell>8 sec</cell><cell>9 sec</cell></row><row><cell>AS</cell><cell>54.22</cell><cell>56.25</cell><cell>56.13</cell><cell>56.55</cell><cell>56.67</cell><cell>56.58</cell></row><row><cell></cell><cell>? 2.27</cell><cell>? 1.71</cell><cell>? 1.17</cell><cell>? 0.91</cell><cell>? 1.16</cell><cell>? 1.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF SPECTRUM CORRECTION METHODS USING DIFFERENT REFERENCE DATA. CO-TUNING OF THE RESNET50 WITH DATA AUGMENTATION IS USED.</figDesc><table><row><cell></cell><cell>No-Calib</cell><cell>Calib-Dev1</cell><cell>Calib-Dev2</cell><cell>Calib-Dev1Dev2</cell><cell>Calib-AllDev</cell></row><row><cell>AS</cell><cell>56.67</cell><cell>57.85</cell><cell>57.85</cell><cell>57.34</cell><cell>58.29</cell></row><row><cell></cell><cell>? 1.16</cell><cell>? 0.40</cell><cell>? 1.00</cell><cell>? 0.26</cell><cell>? 0.46</cell></row><row><cell cols="6">Fig. 6. Comparison of vanilla transfer learning, co-tuning and stochastic</cell></row><row><cell cols="6">normalization using ResNet50 for the cases of spectrum correction and</cell></row><row><cell cols="3">flipping data augmentation.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V COMPARISON</head><label>V</label><figDesc></figDesc><table><row><cell cols="4">BETWEEN THE PROPOSED SYSTEMS AND THE SYSTEM</cell></row><row><cell cols="4">IN [30] USING OUR MULTI-CHANNEL LUNG SOUND DATASET FOR</cell></row><row><cell cols="2">CRACKLE DETECTION TASK</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Se(%)</cell><cell cols="2">P+(%) F1-Score(%)</cell></row><row><cell>MI-CNN, ICBHI FineTuning [30]</cell><cell>85.32</cell><cell>84.11</cell><cell>84.77</cell></row><row><cell>ResNet34, ICBHI CoTuning (Ours)</cell><cell>81.42</cell><cell>94.70</cell><cell>86.81</cell></row><row><cell>ResNet34, ImageNet CoTuning (Ours)</cell><cell>82.17</cell><cell>95.88</cell><cell>87.59</cell></row><row><cell>ResNet50, ImageNet StochNorm (Ours)</cell><cell>81.82</cell><cell>95.22</cell><cell>87.42</cell></row><row><cell>ResNet50, ImageNet CoTuning-StochNorm (Ours)</cell><cell>80.84</cell><cell>94.90</cell><cell>86.46</cell></row><row><cell>ResNet50, ImageNet VanillaFineTuning (Ours)</cell><cell>77.42</cell><cell>96.59</cell><cell>85.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>BETWEEN THE PROPOSED SYSTEMS AND STATE-OF-THE-ART SYSTEMS FOR ALSC TASKS OF 4-CLASS AND 2-CLASS.</figDesc><table><row><cell>Task</cell><cell>Method</cell><cell>Train/Test</cell><cell>SP(%)</cell><cell>SE(%)</cell><cell>AS(%)</cell><cell>HS(%)</cell></row><row><cell>ALSC, 4-class</cell><cell>MFCCs, HMM [12]</cell><cell>official 60/40</cell><cell>-</cell><cell>-</cell><cell>39.56</cell><cell>-</cell></row><row><cell>ALSC, 4-class</cell><cell>STFT+Wavelet Spectrogram, SVMs [40]</cell><cell>official 60/40</cell><cell>-</cell><cell>-</cell><cell>49.86</cell><cell>-</cell></row><row><cell>ALSC, 4-class</cell><cell>Low Level Feature, Decition Tree [11]</cell><cell>official 60/40</cell><cell>77.80</cell><cell>48.90</cell><cell>49.98</cell><cell>49.86</cell></row><row><cell>ALSC, 4-class</cell><cell>STFT+Wavelet Spectrogram, CNN [41]</cell><cell>official 60/40</cell><cell>81</cell><cell>28</cell><cell>54</cell><cell>42</cell></row><row><cell>ALSC, 4-class</cell><cell>STFT+Wavelet Spectrogram, Bi-ResNet [42]</cell><cell>official 60/40</cell><cell>69.20</cell><cell>31.12</cell><cell>52.79</cell><cell>-</cell></row><row><cell>ALSC, 4-class</cell><cell>Gamatone Spectrogram, CNN-MoE, DA [14]</cell><cell>official 60/40</cell><cell>68</cell><cell>26</cell><cell>47</cell><cell>-</cell></row><row><cell>ALSC, 4-class</cell><cell>STFT Spectrogram, ResNet-NonLocal, DA [43]</cell><cell>official 60/40</cell><cell>63.20</cell><cell>41.32</cell><cell>52.26</cell><cell>-</cell></row><row><cell>ALSC, 4-class</cell><cell>STFT Spectrogram, ResNet-SE-SA [44]</cell><cell>official 60/40</cell><cell>81.25</cell><cell>17.84</cell><cell>49.55</cell><cell>-</cell></row><row><cell>ALSC, 4-class</cell><cell>Logmel Spectrogram, ResNet-FC, DA, BRC, Device Fine-tuning [32]</cell><cell>official 60/40</cell><cell>72.30</cell><cell>40.10</cell><cell>56.20</cell><cell>-</cell></row><row><cell>ALSC, 4-class</cell><cell>Logmel Spectrogram, ResNet50, DA, SC, Vanilla Fine-tuning (Ours)</cell><cell>official 60/40</cell><cell>76.33</cell><cell>37.37</cell><cell>56.85</cell><cell>50.11</cell></row><row><cell>ALSC, 4-class</cell><cell>Logmel Spectrogram, ResNet50, DA, SC, StochNorm (Ours)</cell><cell>official 60/40</cell><cell>78.86</cell><cell>36.40</cell><cell>57.63</cell><cell>49.61</cell></row><row><cell>ALSC, 4-class</cell><cell>Logmel Spectrogram, ResNet50, DA, SC, CoTuning (Ours)</cell><cell>official 60/40</cell><cell>79.34</cell><cell>37.24</cell><cell>58.29</cell><cell>50.58</cell></row><row><cell>ALSC, 4-class</cell><cell>Logmel Spectrogram, ResNet101, DA, SC, CoTuning-StochNorm (Ours)</cell><cell>official 60/40</cell><cell>78.55</cell><cell>35.97</cell><cell>57.26</cell><cell>49.27</cell></row><row><cell>ALSC, 4-class</cell><cell>MFCCs, NMRNN, Device Training [31]</cell><cell>5 folds</cell><cell>75</cell><cell>62</cell><cell>68.5</cell><cell>-</cell></row><row><cell>ALSC, 4-class</cell><cell>Mel Spectrogram, CNN-RNN. [24]</cell><cell>5 folds</cell><cell>84.14</cell><cell>48.63</cell><cell>66.38</cell><cell>-</cell></row><row><cell>ALSC, 4-class</cell><cell>STFT Spectrogram, ResNet-NonLocal, DA [43]</cell><cell>5 folds</cell><cell>64.73</cell><cell>63.69</cell><cell>64.21</cell><cell>-</cell></row><row><cell>ALSC, 4-class</cell><cell>Logmel Spectrogram, ResNet-FC, DA, BRC, Device Fine-tuning [32]</cell><cell>5 folds</cell><cell>83.30</cell><cell>53.70</cell><cell>68.50</cell><cell>-</cell></row><row><cell>ALSC, 4-class</cell><cell>MFCCs, CNN [45]</cell><cell>overlap 80/20</cell><cell>77</cell><cell>45</cell><cell>61</cell><cell>-</cell></row><row><cell>ALSC, 4-class</cell><cell>MFCCs, LSTM [20]</cell><cell>overlap 80/20</cell><cell>85</cell><cell>62</cell><cell>74</cell><cell>-</cell></row><row><cell>ALSC, 4-class</cell><cell>Logmel Spectrogram, CNN Snapshot Ensembles, DA [22]</cell><cell>overlap 80/20</cell><cell>87.30</cell><cell>69.40</cell><cell>78.40</cell><cell>-</cell></row><row><cell>ALSC, 4-class</cell><cell>Gamatone Spectrogram, Ensemble, DA [46]</cell><cell>overlap 80/20</cell><cell>86</cell><cell>73</cell><cell>80</cell><cell>-</cell></row><row><cell>ALSC, 4-class</cell><cell>Gamatone Spectrogram, CNN-MoE, DA [14]</cell><cell>overlap 80/20</cell><cell>-</cell><cell>-</cell><cell>78.6</cell><cell>-</cell></row><row><cell>ALSC, 2-class</cell><cell>Logmel Spectrogram, ResNet50, DA, SC, Vanilla Fine-tuning (Ours)</cell><cell>official 60/40</cell><cell>76.33</cell><cell>52.12</cell><cell>64.22</cell><cell>61.87</cell></row><row><cell>ALSC, 2-class</cell><cell>Logmel Spectrogram, ResNet50, DA, SC, StochNorm (Ours)</cell><cell>official 60/40</cell><cell>78.86</cell><cell>49.79</cell><cell>64.32</cell><cell>60.69</cell></row><row><cell>ALSC, 2-class</cell><cell>Logmel Spectrogram, ResNet50, DA, SC, CoTuning (Ours)</cell><cell>official 60/40</cell><cell>79.34</cell><cell>50.14</cell><cell>64.74</cell><cell>61.30</cell></row><row><cell>ALSC, 2-class</cell><cell>Logmel Spectrogram, ResNet101, DA, SC, CoTuning-StochNorm (Ours)</cell><cell>official 60/40</cell><cell>78.56</cell><cell>48.67</cell><cell>63.61</cell><cell>59.98</cell></row><row><cell>ALSC, 2-class</cell><cell>CNN [32]</cell><cell>5 folds</cell><cell>80.90</cell><cell>73.10</cell><cell>77.0</cell><cell>-</cell></row><row><cell>ALSC, 2-class</cell><cell>Logmel, ResNet-FC, DA, BRC, Device Fine-tuning [32]</cell><cell>5 folds</cell><cell>80.90</cell><cell>73.10</cell><cell>77.0</cell><cell>-</cell></row><row><cell>ALSC, 2-class</cell><cell>MFCCs, LSTM [20]</cell><cell>overlap 80/20</cell><cell>-</cell><cell>-</cell><cell>81</cell><cell>-</cell></row><row><cell>ALSC, 2-class</cell><cell>Logmel Spectrogram, CNN Snapshot Ensembles, DA [22]</cell><cell>overlap 80/20</cell><cell>87.30</cell><cell>80.10</cell><cell>83.70</cell><cell>-</cell></row><row><cell>ALSC, 4-class</cell><cell>Gamatone Spectrogram, Ensemble, DA [46]</cell><cell>overlap 80/20</cell><cell>86</cell><cell>85</cell><cell>86</cell><cell>-</cell></row><row><cell>ALSC, 2-class</cell><cell>Gamatone Spectrogram, CNN-MoE, DA [14]</cell><cell>overlap 80/20</cell><cell>-</cell><cell>-</cell><cell>84.0</cell><cell>-</cell></row><row><cell cols="2">and wavelet features. Another system used non-local block</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ResNet with mixup data augmentation for STFT spectrograms.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII COMPARISON</head><label>VII</label><figDesc>BETWEEN THE PROPOSED SYSTEMS AND STATE-OF-THE-ART SYSTEMS FOR RDC TASK OF 3-CLASS AND 2-CLASS.</figDesc><table><row><cell>Task</cell><cell>Method</cell><cell>Train/Test</cell><cell>SP(%)</cell><cell>SE(%)</cell><cell>AS(%)</cell><cell>HS(%)</cell></row><row><cell>RDC, 3-class</cell><cell>Gamatone Spectrogram, CNN-MoE, DA [14]</cell><cell>official 60/40</cell><cell>-</cell><cell>-</cell><cell>84.0</cell><cell>-</cell></row><row><cell>RDC, 3-class</cell><cell>Logmel Spectrogram, ResNet34, DA, SC, Vanilla Fine-tuning (Ours)</cell><cell>official 60/40</cell><cell>65.88</cell><cell>87.47</cell><cell>76.68</cell><cell>74.48</cell></row><row><cell>RDC, 3-class</cell><cell>Logmel Spectrogram, ResNet101, DA, SC, StochNorm (Ours)</cell><cell>official 60/40</cell><cell>90.59</cell><cell>92.53</cell><cell>91.56</cell><cell>91.35</cell></row><row><cell>RDC, 3-class</cell><cell>Logmel Spectrogram, ResNet101, DA, SC, CoTuning (Ours)</cell><cell>official 60/40</cell><cell>81.18</cell><cell>90.22</cell><cell>85.70</cell><cell>85.23</cell></row><row><cell>RDC, 3-class</cell><cell cols="2">Logmel Spectrogram, ResNet101, DA, SC, CoTuning-StochNorm (Ours) official 60/40</cell><cell>91.77</cell><cell>93.68</cell><cell>92.72</cell><cell>92.57</cell></row><row><cell>RDC, 3-class</cell><cell>MFCCs, CNN [45]</cell><cell>overlap 80/20</cell><cell>76</cell><cell>89</cell><cell>83</cell><cell>-</cell></row><row><cell>RDC, 3-class</cell><cell>MFCCs, LSTM [20]</cell><cell>overlap 80/20</cell><cell>82</cell><cell>98</cell><cell>90</cell><cell>-</cell></row><row><cell>RDC, 3-class</cell><cell>Gamatone Spectrogram, CNN-MoE, DA [46]</cell><cell>overlap 80/20</cell><cell>83</cell><cell>96</cell><cell>90</cell><cell>-</cell></row><row><cell>RDC, 3-class</cell><cell>Gamatone Spectrogram, CNN-MoE, DA [14]</cell><cell>5 folds</cell><cell>86</cell><cell>95</cell><cell>91</cell><cell>-</cell></row><row><cell>RDC, 3-class</cell><cell>Logmel Spectrogram, ResNet50, DA, SC, Vanilla Fine-tuning (Ours)</cell><cell>5 folds</cell><cell>83.02</cell><cell>90.40</cell><cell>86.71</cell><cell>84.85</cell></row><row><cell>RDC, 3-class</cell><cell>Logmel Spectrogram, ResNet101, DA, SC, StochNorm (Ours)</cell><cell>5 folds</cell><cell>100</cell><cell>91.47</cell><cell>95.73</cell><cell>95.48</cell></row><row><cell>RDC, 3-class</cell><cell>Logmel Spectrogram, ResNet101, DA, SC, CoTuning (Ours)</cell><cell>5 folds</cell><cell>96.67</cell><cell>90.18</cell><cell>93.42</cell><cell>9.06</cell></row><row><cell>RDC, 3-class</cell><cell cols="2">Logmel Spectrogram, ResNet101, DA, SC, CoTuning-StochNorm (Ours) 5 folds</cell><cell>97.78</cell><cell>91.44</cell><cell>94.61</cell><cell>94.46</cell></row><row><cell>RDC, 2-class</cell><cell>Low Level Feature, Decition Tree [11]</cell><cell>official 60/40</cell><cell>-</cell><cell>-</cell><cell>85</cell><cell>-</cell></row><row><cell>RDC, 2-class</cell><cell>Gamatone Spectrogram, CNN-MoE, DA [14]</cell><cell>official 60/40</cell><cell>-</cell><cell>-</cell><cell>84.1</cell><cell>-</cell></row><row><cell>RDC, 2-class</cell><cell>Logmel Spectrogram, ResNet34, DA, SC, Vanilla Fine-tuning (Ours)</cell><cell>official 60/40</cell><cell>65.88</cell><cell>96.43</cell><cell>81.16</cell><cell>77.58</cell></row><row><cell>RDC, 2-class</cell><cell>Logmel Spectrogram, ResNet101, DA, SC, StochNorm (Ours)</cell><cell>official 60/40</cell><cell>90.59</cell><cell>93.90</cell><cell>92.25</cell><cell>90.02</cell></row><row><cell>RDC, 2-class</cell><cell>Logmel Spectrogram, ResNet101, DA, SC, CoTuning (Ours)</cell><cell>official 60/40</cell><cell>81.18</cell><cell>94.29</cell><cell>87.73</cell><cell>87.12</cell></row><row><cell>RDC, 2-class</cell><cell cols="2">Logmel Spectrogram, ResNet101, DA, SC, CoTuning-StochNorm (Ours) official 60/40</cell><cell>91.77</cell><cell>95.77</cell><cell>93.77</cell><cell>93.60</cell></row><row><cell>RDC, 2-class</cell><cell>MFCCs, LSTM [20]</cell><cell>overlap 80/20</cell><cell>82</cell><cell>99</cell><cell>91</cell><cell>-</cell></row><row><cell>RDC, 2-class</cell><cell>Gamatone Spectrogram, CNN-MoE, DA [46]</cell><cell>overlap 80/20</cell><cell>83</cell><cell>99</cell><cell>91</cell><cell>-</cell></row><row><cell>RDC, 2-class</cell><cell>Gamatone Spectrogram, CNN-MoE, DA [14]</cell><cell>5 folds</cell><cell>86</cell><cell>98</cell><cell>92</cell><cell>-</cell></row><row><cell>RDC, 2-class</cell><cell>LPC, MLP classifier [39]</cell><cell>5 folds</cell><cell>-</cell><cell>-</cell><cell>99.22</cell><cell>-</cell></row><row><cell>RDC, 2-class</cell><cell>Logmel Spectrogram, ResNet50, DA, SC, Vanilla Fine-tuning (Ours)</cell><cell>5 folds</cell><cell>83.02</cell><cell>96.03</cell><cell>89.52</cell><cell>87.23</cell></row><row><cell>RDC, 2-class</cell><cell>Logmel Spectrogram, ResNet101, DA, SC, StochNorm (Ours)</cell><cell>5 folds</cell><cell>100</cell><cell>96.41</cell><cell>98.20</cell><cell>98.15</cell></row><row><cell>RDC, 2-class</cell><cell>Logmel Spectrogram, ResNet101, DA, SC, CoTuning (Ours)</cell><cell>5 folds</cell><cell>96.67</cell><cell>94.46</cell><cell>95.56</cell><cell>95.28</cell></row><row><cell>RDC, 2-class</cell><cell cols="2">Logmel Spectrogram, ResNet101, DA, SC, CoTuning-StochNorm (Ours) 5 folds</cell><cell>97.78</cell><cell>95.15</cell><cell>96.46</cell><cell>96.42</cell></row><row><cell>RDC, 2-class</cell><cell>MFCCs, DWT, time domain features, RUSBoost-DT [47]</cell><cell>50/50</cell><cell>93</cell><cell>86</cell><cell>87.10</cell><cell>-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/makcedward/nlpaug</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Code are available at https://github.com/thuml/Cotuning and https://github.com/thuml/StochNorm.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This research was supported by the Vietnamese -Austrian Government scholarship and by the Austrian Science Fund (FWF) under the project number I2706-N31. We acknowledge NVIDIA for providing GPU computing resources. The authors would like to thank our colleague, Alexander Fuchs for feedback and fruitful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in wuhan, china: a descriptive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The lancet</title>
		<imprint>
			<biblScope unit="volume">395</biblScope>
			<biblScope unit="issue">10223</biblScope>
			<biblScope unit="page" from="507" to="513" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The &quot;big five&quot; lung diseases in covid-19 pandemic-a google trends analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mt Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Morais-Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pulmonology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">71</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Auscultation of the respiratory system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Madabhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dogra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of thoracic medicine</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">158</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A respiratory sound database for the development of automated classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Filos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perantoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kaimakamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsiavas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>J?come</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Precision Medicine Powered by pHealth and Connected Health</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="33" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Computerized lung sound analysis as diagnostic aid for the detection of abnormal lung sounds: a systematic review and meta-analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gurung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Scrafford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tielsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Checkley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Respiratory medicine</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1396" to="1403" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic adventitious respiratory sound analysis: A systematic review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">X A</forename><surname>Pramono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodriguez-Villegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">177926</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rale: A computer-assisted instructional package</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dataset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Respiratory Care</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">1006</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A robust multichannel lung sound recording device</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Messner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swatek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BIODEVICES</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A dataset of lung sounds recorded from the chest wall using an electronic stethoscope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fraiwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fraiwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Khassawneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ibnian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data in Brief</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">106913</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic detection of patient with respiratory diseases using lung sound analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chambres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Desainte-Catherine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Content-Based Multimedia Indexing (CBMI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hidden markov model based respiratory sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jakovljevi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lon?ar-Turukalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biomedical and Health Informatics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Triple-classification of respiratory sounds using optimized s-transform and deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="32845" to="32852" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cnn-moe based framework for classification of respiratory anomalies and lung disease detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Palaniappan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mertins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mcloughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classification of lung sounds in patients with asthma, emphysema, fibrosing alveolitis and healthy lungs by using self-organizing maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Malmberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kallio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haltsonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Katila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ara</forename><surname>Sovij?rvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Physiology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="129" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pattern recognition methods applied to respiratory sounds classification into normal and wheeze classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bahoura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in biology and medicine</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="824" to="843" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wheezing recognition algorithm using recordings of respiratory sounds at the mouth in a pediatric population</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delclaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in biology and medicine</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="40" to="50" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lung sound classification based on hilbert-huang transform features and multilayer perceptron network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="765" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Crackle and breathing phase detection in lung sounds with deep bidirectional gated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Messner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fediuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swatek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scheidl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Smolle-Juttner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Olschewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 Proceedings of EMBC</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="356" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep auscultation: Predicting respiratory anomalies and diseases via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Perna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagarelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="50" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Classification of lung sounds using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aykanat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>K?l??</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kurt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saryal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">65</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lung sound classification using snapshot ensemble of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 42nd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="760" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional neural networks based efficient approach for classification of lung diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sengur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bajaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Health information science and systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep neural network for respiratory sound classification in wearable devices enabled by patient specific model tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on biomedical circuits and systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="535" to="544" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lung sound recognition algorithm based on vggish-bigru</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="139438" to="139449" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Co-tuning for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stochastic normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-channel lung sound classification with convolutional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Messner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fediuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swatek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scheidl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Smolle-J?ttner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Olschewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="page">103831</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Majority voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Allan M Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Welfare Economics and Social Choice Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="196" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Crackle detection in lung sounds using transfer learning and multi-input convolitional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14921</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Noise masking recurrent neural network for respiratory sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kochetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Putin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balashov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Filchenkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shalyto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="208" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Respirenet: A deep neural network for accurately detecting abnormal lung sounds in limited data setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gairola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.00196</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Acoustic scene classification for mismatched recording devices using heated-up softmax and spectrum correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kosmider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="126" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A review of time-scale modification of music signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Driedger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meinard</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vocal tract length perturbation (vtlp) improves speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Na</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML Workshop on Deep Learning for Audio, Speech and Language</title>
		<meeting>ICML Workshop on Deep Learning for Audio, Speech and Language</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">117</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4918" to="4927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">86</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic lung health screening using respiratory sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sreerama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Obaidullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Santosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Systems</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An automated lung sound preprocessing and classification system based onspectral analysis methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Serbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ulukaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Kahya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biomedical and Health Informatics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="45" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatic classification of large-scale respiratory sound dataset based on convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Minami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mabu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hirano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kido</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 19th International Conference on Control, Automation and Systems (ICCAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="804" to="807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lungbrn: A smart digital stethoscope for detecting respiratory disease using biresnet deep learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Biomedical Circuits and Systems Conference (BioCAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Lungrn+ nl: An improved adventitious lung sound classification using non-local block resnet neural network with mixup data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2020</title>
		<meeting>Interspeech, 2020</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2902" to="2906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adventitious respiratory classification using attentive residual neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parada-Cabaleiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bj?rn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuller12</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Interspeech</title>
		<meeting>the Interspeech</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2912" to="2916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Convolutional neural networks learning from respiratory data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Perna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2109" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust deep learning framework for predicting respiratory anomalies and diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mcloughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Palaniappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 42nd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="164" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A novel method for automatic identification of respiratory disease from acoustic recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">H</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Imtiaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodriguez-Villegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2589" to="2592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Classification of lung sounds with cnn model using parallel pooling structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Ismael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sengur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="105376" to="105383" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
