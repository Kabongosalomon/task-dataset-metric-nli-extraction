<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BillSum: A Corpus for Automatic Summarization of US Legislation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastassia</forename><surname>Kornilova</surname></persName>
							<email>anastassia@fiscalnote.com</email>
							<affiliation key="aff0">
								<orgName type="institution">FiscalNote Research Washington</orgName>
								<address>
									<region>DC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Eidelman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">FiscalNote Research Washington</orgName>
								<address>
									<region>DC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BillSum: A Corpus for Automatic Summarization of US Legislation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic summarization methods have been studied on a variety of domains, including news and scientific articles. Yet, legislation has not previously been considered for this task, despite US Congress and state governments releasing tens of thousands of bills every year.</p><p>In this paper, we introduce BillSum, the first dataset for summarization of US Congressional and California state bills (https://github.com/ FiscalNote/BillSum). We explain the properties of the dataset that make it more challenging to process than other domains. Then, we benchmark extractive methods that consider neural sentence representations and traditional contextual features. Finally, we demonstrate that models built on Congressional bills can be used to summarize California bills, thus, showing that methods developed on this dataset can transfer to states without human-written summaries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The growing number of publicly available documents produced in the legal domain has led political scientists, legal scholars, politicians, lawyers, and citizens alike to increasingly adopt computational tools to discover and digest relevant information. In the US Congress, over 10,000 bills are introduced each year, with state legislatures introducing tens of thousands of additional bills. Individuals need to quickly process them, but these documents are often long and technical, making it difficult to identify the key details. While each US bill comes with a human-written summary from the Congressional Research Service (CRS), 1 similar summaries are not available in most state and local legislatures. 1 http://www.loc.gov/crsinfo/ Automatic summarization methods aim to condense an input document into a shorter text while retaining the salient information of the original. To encourage research into automatic legislative summarization, we introduce the BillSum dataset, which contains a primary corpus of 22,218 US Congressional bills and reference summaries split into a train and a test set. Since the motivation for this task is to apply models to new legislatures, the corpus contains an additional test set of 1,237 California bills and reference summaries. We establish several benchmarks and show that there is ample room for new methods that are better suited to summarize technical legislative language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Research into automatic summarization has been conducted in a variety of domains, such as news articles <ref type="bibr" target="#b12">(Hermann et al., 2015)</ref>, emails <ref type="bibr">(Nenkova and Bagga, 2004)</ref>, scientific papers <ref type="bibr">(Teufel and Moens, 2002;</ref><ref type="bibr" target="#b4">Collins et al., 2017)</ref>, and court proceedings <ref type="bibr" target="#b11">(Grover et al., 2004;</ref><ref type="bibr">Saravanan et al., 2008;</ref><ref type="bibr">Kim et al., 2013)</ref>. The later area is most similar to BillSum in terms of subject matter. However, the studies in that area either apply traditional domain-agnostic techniques or take advantage of the unique structures that are consistently present in legal proceedings (e.g precedent, law, background). <ref type="bibr">2</ref> While automatic summarization methods have not been applied to legislative text, previous works have used the text to automatically predict bill passage and legislators' voting behavior <ref type="bibr" target="#b7">(Gerrish and Blei, 2011;</ref><ref type="bibr">Yano et al., 2012;</ref><ref type="bibr" target="#b6">Eidelman et al., 2018;</ref><ref type="bibr" target="#b6">Kornilova et al., 2018)</ref>. However, these studies treated the document as a "bag-of-words" and did not consider the importance of individual sentences. Recently, documents from state governments have been subject to syntactic parsing for knowledge graph construction <ref type="bibr">(Kalouli et al., 2018)</ref> and textual similarity analysis <ref type="bibr">(Linder et al., 2018)</ref>. Yet, to the best of our knowledge, BillSum is the first corpus designed, specifically for summarization of legislation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>The BillSum dataset consists of three parts: US training bills, US test bills and California test bills. The US bills were collected from the Govinfo service provided by the United States Government Publishing Office (GPO). 3 Our corpus consists of bills from the 103rd-115th (1993-2018) sessions of Congress. The data was split into 18,949 train bills and 3,269 test bills. For California, bills from the 2015-2016 session were scraped directly from the legislature's website; 4 the summaries were written by their Legislative Counsel.</p><p>The BillSum corpus focuses on mid-length legislation from 5,000 to 20,000 character in length. We chose to measure the text length in characters, instead of words or sentences, because the texts have complex structure that makes it difficult to consistently measure words. The range was chosen because on one side, short bills introduce minor changes and do not require summaries. While the CRS produces summaries for them, they often contain most of the text of the bill. On the other side, very long legislation is often composed of several large sections. The summarization problem thus becomes more akin in its formulation to multi-document summarization, a more challenging task that we leave to future work. The resulting corpus includes about 20% of all US bills from this time period, where a majority of removed bills are either shorter than 5000 characters or identified as a near duplicate of a bill in the dataset. <ref type="bibr">5</ref> For the summaries, we chose a 2000 character limit as 90% of summaries are of this length or shorter; the limit here is, also, set in characters to be consistent with our document length cut-offs.</p><p>The distribution of both text and summary lengths is shown in <ref type="figure">Figure 1</ref>. Interestingly, there is little correlation between the bill and human summary length, with most summaries ranging from 1000 to 2000 characters.</p><p>For a closer comparison to other datasets, <ref type="table">Table  1</ref> provides statistics on the number of words in the texts, after we simplify the structure of the texts. Stylistically, the BillSum dataset differs from other summarization corpora. <ref type="figure" target="#fig_1">Figure 2</ref> presents an example Congressional bill. The nested, bulleted structure is common to most bills, where each bullet can represent a sentence or a phrase. Yet, content-wise, this is a straightforward example that states key details about the proposed grant in the outer bullets. In more challenging cases, the bill may state edits to an existing law, without whose context the change is hard to interpret, such as:</p><p>Section 4 of the Endangered Species Act of 1973 (16 U.S.C. 1533) is amended in subsection (a) in paragraph (1), by inserting "with the consent of the Governor of each State in which the endangered species or threatened species is present"</p><p>The average bill will contain both types of language, encouraging the study of both domainspecific and general summarization methods on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Benchmark Methods</head><p>To establish benchmarks on summarization performance, we evaluate several extractive summarization approaches by first scoring individual sentences, then using a selection strategy to pick mean min 25th 50th 75th max  the best subset <ref type="bibr" target="#b2">(Carbonell and Goldstein, 1998)</ref>. While we briefly considered abstractive summarization models <ref type="bibr" target="#b3">(Chopra et al., 2016)</ref>, we found that the existing models trained on news and Wikipedia data produced ungrammatical results, and that the size of dataset is insufficient for the necessary retraining. Recent works have successfully fine-tuned models for other NLP tasks to specific domains (Lee et al., 2019), but we leave to future work the exploration of similar abstractive strategies.</p><p>The scoring task is framed as a supervised learning problem. First, we create a binary label for each sentence indicating whether it belongs in the summary <ref type="bibr" target="#b8">(Gillick et al., 2008)</ref>. <ref type="bibr">6</ref> We compute a Rouge-2 Precision score of a sentence relative to the reference summary and simplify it to a binary value based on whether it is above or below 0.1 <ref type="bibr">(Lin, 2004;</ref><ref type="bibr">Zopf et al., 2018)</ref>. As an example, the sentences in the positive class are highlighted in green in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Second, we build several models to predict the label. For the models, we consider two aspects of a sentence: its importance in the context of the document (4.1) and its general summary-like properties (4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Document Context Model (DOC)</head><p>A good summary sentence contains the main ideas mentioned in the document. Thus, researchers have designed a multitude of features to capture this property. We evaluate how several common ones transfer to our task:</p><p>The position of the sentence can determine how informative the sentence is <ref type="bibr">(Seki, 2002)</ref>. We encode this feature as a fraction of 'sentence position / total sentence count', to restrict this feature to the 0?1 range regardless of the particular document's length. In addition, we include a binary feature for whether the sentence is near a section header.</p><p>An informative sentences will contain words that are important to a given document relative to others. Following a large percentage of previous works, we capture this property using TF-IDF <ref type="bibr">(Seki, 2002;</ref><ref type="bibr">Ramos et al., 2003)</ref>. First, we calculate a document-level TF-IDF weight for each word, then take the average and the maximum of these weights for a sentence as features. To relate language between sentences, "sentence-level" TF-IDF features are created using each sentence as a document for the background corpus; the average and max of the sentence's word weights are used as features.</p><p>We train a random forest ensemble model over these features with 50 estimators <ref type="bibr" target="#b0">(Breiman, 2001)</ref>. 7 This method was chosen because it best captured the interactions between the small number of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Summary Language Model (SUM)</head><p>We hypothesize that certain language is more common in summaries than in bill texts. Specifically, that summaries primarily contain general effects of the bill (e.g awarding a grant) while language detailing the administrative changes will only appear in the text (e.g inserting or modifying relatively minor language to an existing statute). Thus, a good summary should contain only the major actions.</p><p>Hong and Nenkova (2014) quantify this aspect using hand-engineered features based on the the likelihood of words appearing in summaries as opposed to the text. Later, <ref type="bibr" target="#b1">Cao et al. (2015)</ref> built a Convolutional Neural Network (CNN) to predict if a sentence belongs in the summary and showed that this straightforward network outperforms engineered features. We follow their approach, using the BERT model as our classifier <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref>. BERT can be adapted for and has achieved state-of-the-art performance on a number of NLP tasks, including binary sentiment classification. <ref type="bibr">8</ref> To adapt the model to our domain, we pretrain the Bert-Large Uncased model on the "nextsentence prediction" task using the US training dataset for 20,000 steps with a batch size of 32. <ref type="bibr">9</ref> The pretraining stategy has been successfully applied to tune BERT for tasks in the biomedical domain <ref type="bibr">(Lee et al., 2019)</ref>. Using the pretrained model, the classification setup for BERT is trained on sentences and binary labels for 3 epochs over the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ensemble and Sentence Selection</head><p>To combine the signals from the DOC and SUM models, we create an ensemble averaging the two probability outputs. <ref type="bibr">10</ref> To create the final summary, we apply the Maximal Marginal Relevance (MMR) algorithm <ref type="bibr" target="#b9">(Goldstein et al., 2000)</ref>. MMR iteratively constructs a summary by including the highest scoring sentence with the following formula:</p><formula xml:id="formula_0">s next = max s?D?Scur 0.7 * f (s) ? 0.3 * sim(s, S cur )</formula><p>where D is the set of all the sentences in the document, S cur are the sentences in the summary so far, f (s) is the sentence score from the model, sim is the cosine similarity of the sentence to S cur , and 0.7 and 0.3 are constants chosen experimentally to balance the two properties. This method allows us to pick relevant sentences while minimizing redundancies. We repeat this process until we reach the length limit of 2000 characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>To estimate the upper bound on our approach, an oracle summarizer is created by using the true Rouge-2 Precision scores with the MMR selection strategy. In addition, we evaluate the following unsupervised baselines: SumBasic (Nenkova and Vanderwende, 2005), Latent Semantic Analysis (LSA) <ref type="bibr" target="#b10">(Gong and Liu, 2001)</ref> and <ref type="bibr">TextRank (Mihalcea and Tarau, 2004)</ref>. The final results are shown in <ref type="table" target="#tab_1">Table 2</ref>. The Rouge F-Score is used because it considers both the completeness and conciseness of the summary method. <ref type="bibr">11,</ref><ref type="bibr">12</ref> We evaluated the DOC, SUM, and ensemble classifiers separately. All three of our models outperform the other baselines, demonstrating that there is a "summary-like" signal in the language across bills. The SUM model outperforms the DOC model showing that a strong language model can capture general summary-like features; this result is in line with <ref type="bibr" target="#b1">Cao et al. (2015)</ref> and <ref type="bibr" target="#b4">Collins et al. (2017)</ref> sentence level neural network performance. However, in those studies incorporating several contextual features improved the performance, while DOC+SUM performs similarly to DOC. In future work we plan to incorporate contextual features into the neural network directly;</p><p>10 Additional experiments using Linear Regression with the actual Rouge-2 Precision score as the target, but found that they produced similar results. 11 Precision and recall scores are listed in the supplemental material for additional context. <ref type="bibr">12</ref> Rouge scores calculated using https://github. com/pcyin/PyRouge <ref type="bibr" target="#b4">Collins et al. (2017)</ref> showed that this strategy is effective for scientific article summarization. In addition, we plan to explore additional sentence selection strategies instead of always adding sentences up to the 2000 character limit.</p><p>Next, we applied our US model to CA bills. Overall, the performance is lower than on US bills <ref type="table" target="#tab_1">(Table 2b</ref>), but all three supervised methods perform better than the unsupervised baselines, suggesting that models built using the language of US Bills can transfer to other states. Interestingly, the SUM model performs similarly to the DOC in the CA dataset, suggesting that the BERT model may have overfit to the US language. An additional reason for the similar performance is the difference in the structure of the summaries: In California the provided summaries state not only the proposed changes, but the relevant pieces of the existing law, as well (see Appendix C.3 for a more in-depth discussion). We hypothesize that a model trained on multi-state data would transfer better, thus we plan to expand the dataset to include all twenty-three states with human-written summaries. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Summary Language Analysis</head><p>The success of the SUM model suggests that certain language is more summary-like. Following a study by Hong and Nenkova (2014) on news summarization, we apply KL-divergence based metrics to quantify which words were more summary-like. The metrics are calculated by:</p><p>1. Calculate the probability of unigrams appearing in the bill text and in the summaries (P t (w) and P s (w) respectively).</p><p>2. Calculate KL scores as : KL w (S|T ) = P s (w) * ln Ps(w) Pt(w) and the opposite.</p><p>A large value of KL(S|T ) indicates that the word is summary-like and KL(T |S) indicates a text-like word. <ref type="table" target="#tab_2">Table 3</ref> shows the most summarylike and text-like words in bills and resolutions. For both document types, the summary-like words tend to be verbs or department names; the text-like words mostly refer to types of edits or background content (e.g "reporting the rise of.."). This follows our intuition about summaries being more action driven. While a complex model, like BERT, may capture these signals internally; understanding the significant language explicitly is important both for interpret ability and for guiding future models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we introduced BillSum, the first corpus for legislative summarization. This is a challenging summarization dataset due to the technical nature and complex structure of the bills. We have established several baselines and demonstrated that there is a large gap in performance relative to the oracle, showing that the problem has ample room for further development. We have also shown that summarization methods trained on US Bills transfer to California bills -thus, the summarization methods developed on this dataset could be used for legislatures without human written summaries. 3. Computing cosine similarity between the texts and the summaries for each pair of bills and averaging the two similarities.</p><p>4. Iteratively adding bills to the dataset, skipping examples that were more than 96% similar to any bills already added.</p><p>After this procedure is run, the data still includes some bills with identical titles. This can happen for two reasons: either the title is generic and refers to two unrelated bills, or one is a reintroduction of the other with enough modified content to not be considered a duplicate. We put all the bills with identical titles in the train partition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional ROUGE Scores</head><p>As discussed in the Results section, F-Scores encourage a balance between comprehensiveness and conciseness. However, as it is useful to analyze the precision and recall scores separately, both are presented in <ref type="table" target="#tab_4">Table 4</ref> for US Bills and in <ref type="table" target="#tab_5">Table 5</ref> for CA Bills. All tested methods favor recall, since they consistently generate a 2000 character summary, instead of stopping early when a concise summary may be sufficient. For both datasets, the difference in Recall between the Oracle and DOC+SUM summarizer is a lot smaller than for Recall; which suggests that a lot of useful summary content can be found with an extractive method. In future work, we will focus on extracting more granular snippets to improve precision.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Bill Examples</head><p>We highlight several example bills to showcase the different types of bills found in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Complex Structure Example</head><p>In the Data section, we discussed some of the challenges with processing bills: complex formatting and technical language. <ref type="figure" target="#fig_2">Figure 3</ref> is an excerpt from a particularly difficult example:</p><p>? The text interleaves several layers of bullets. Lines 3, 15, 27 represent the same level (points <ref type="formula">(3)</ref> and (4) omitted for space); lines 16, 17, 19 and 21 go together, as well. These multiple levels need to be handled carefully, or the summarizer will extract snippets that can not be interpreted without context.</p><p>? Lines 22-26 both introduce new language for the law and use the bulleted structure.</p><p>? Line 27 states that the existing "subsection (f)" is being removed and replaced. While lines 28 onward state the new text, the meaning of the change relative to the current text is not clear.</p><p>The human-written summary for this bill was: (Sec. 4)"Women's business center" shall mean a project conducted by any of the following eligible entities:</p><p>? a private nonprofit organization;</p><p>? a state, regional, or local economic development organization;</p><p>? a state-chartered development, credit, or finance corporation;</p><p>? a junior or community college; or</p><p>? any combination of these entities.</p><p>The SBA may award up to $250,000 of financial assistance to eligible entities per project year to conduct projects designed to provide training and counseling meeting the needs of women, especially socially and economically disadvantaged women.</p><p>Most of the relevant details are capture in the text between lines 8-14 and 20-24. For examples similar to this one, the summary language is extracted almost directly from the text, but, parsing them correctly from the original structure is a nontrivial task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Paraphrase Example</head><p>For a subset of the bills, the CRS will paraphrase the technical language. In these cases, extractive summarization methods are particularly limited. Consider the example in <ref type="figure" target="#fig_3">Figure 4</ref> and its summary:</p><p>This bill amends the Endangered Species Act of 1973 to revise the process by which the Department of the Interior or the Department of Commerce, as appropriate, reviews petitions to list a species on the endangered or threatened species list. Specifically, the bill establishes a process for the appropriate department to declare a petition backlog and discharge the petitions when there is a backlog. While the bill elaborates of the "'process", the summary states that one was created. This type of summary would be hard to construct by a purely extractive method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 California Example</head><p>The California bills follow the same general patterns as US bills, but the format of some summaries is different. In <ref type="figure" target="#fig_4">Figure 5</ref>: the summary, first, explains the existing law, then explains the change. The additional context is useful, and in the future we may build a system that references the existing law to create better summaries. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: Bill Lengths</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Example US Bill</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>US H.R.1680 (115th)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>US H.R.6355 (115th)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>California Bill Summary</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>ROUGE F-scores (%) of different methods.</figDesc><table><row><cell></cell><cell cols="3">Rouge-1 Rouge-2 Rouge-L</cell></row><row><cell>Oracle</cell><cell>45.11</cell><cell>28.74</cell><cell>37.38</cell></row><row><cell>SumBasic</cell><cell>30.74</cell><cell>14.16</cell><cell>23.92</cell></row><row><cell>LSA</cell><cell>32.64</cell><cell>15.69</cell><cell>26.26</cell></row><row><cell>TextRank</cell><cell>34.35</cell><cell>17.77</cell><cell>27.80</cell></row><row><cell>DOC</cell><cell>38.51</cell><cell>21.38</cell><cell>31.49</cell></row><row><cell>SUM</cell><cell>40.69</cell><cell>23.88</cell><cell>33.65</cell></row><row><cell>DOC + SUM</cell><cell>40.80</cell><cell>23.83</cell><cell>33.73</cell></row><row><cell></cell><cell cols="2">(a) Congressional Bills</cell><cell></cell></row><row><cell></cell><cell cols="3">Rouge-1 Rouge-2 Rouge-L</cell></row><row><cell>Oracle</cell><cell>48.61</cell><cell>32.83</cell><cell>41.94</cell></row><row><cell>SumBasic</cell><cell>35.47</cell><cell>16.18</cell><cell>29.98</cell></row><row><cell>LSA</cell><cell>35.06</cell><cell>16.34</cell><cell>29.93</cell></row><row><cell>TextRank</cell><cell>35.81</cell><cell>18.10</cell><cell>29.97</cell></row><row><cell>DOC</cell><cell>38.35</cell><cell>19.76</cell><cell>32.80</cell></row><row><cell>SUM</cell><cell>38.90</cell><cell>20.79</cell><cell>33.20</cell></row><row><cell>DOC + SUM</cell><cell>39.65</cell><cell>21.14</cell><cell>34.05</cell></row><row><cell></cell><cell>(b) CA Bills</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Examples of summary and text like words</figDesc><table><row><cell cols="2">Summary-like prohibit, DOD, VA, allow,</cell></row><row><cell></cell><cell>penalty, prohibit, EPA, elim-</cell></row><row><cell></cell><cell>inate, implement, require</cell></row><row><cell>Text-like</cell><cell>estimate, average, report,</cell></row><row><cell></cell><cell>rise, section, finish, percent,</cell></row><row><cell></cell><cell>debate</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>ROUGE Scores of Congressional Bills</figDesc><table><row><cell></cell><cell cols="3">Rouge-1 Rouge-2 Rouge-L</cell></row><row><cell>Oracle</cell><cell>40.94</cell><cell>25.82</cell><cell>38.29</cell></row><row><cell>SumBasic</cell><cell>24.63</cell><cell>11.71</cell><cell>22.36</cell></row><row><cell>LSA</cell><cell>27.34</cell><cell>13.30</cell><cell>24.96</cell></row><row><cell>TextRank</cell><cell>29.86</cell><cell>15.37</cell><cell>26.99</cell></row><row><cell>DOC</cell><cell>32.61</cell><cell>17.93</cell><cell>30.10</cell></row><row><cell>SUM</cell><cell>34.59</cell><cell>20.15</cell><cell>32.18</cell></row><row><cell>DOC + SUM</cell><cell>34.77</cell><cell>20.11</cell><cell>32.21</cell></row><row><cell></cell><cell cols="2">(a) Precision Scores</cell><cell></cell></row><row><cell></cell><cell cols="3">Rouge-1 Rouge-2 Rouge-L</cell></row><row><cell>Oracle</cell><cell>58.19</cell><cell>39.17</cell><cell>54.52</cell></row><row><cell>SumBasic</cell><cell>47.37</cell><cell>21.90</cell><cell>42.88</cell></row><row><cell>LSA</cell><cell>46.53</cell><cell>23.10</cell><cell>42.32</cell></row><row><cell>TextRank</cell><cell>46.49</cell><cell>25.36</cell><cell>41.88</cell></row><row><cell>DOC</cell><cell>54.16</cell><cell>32.31</cell><cell>49.92</cell></row><row><cell>SUM</cell><cell>56.68</cell><cell>35.56</cell><cell>52.62</cell></row><row><cell>DOC + SUM</cell><cell>56.69</cell><cell>35.56</cell><cell>52.62</cell></row><row><cell></cell><cell cols="2">(b) Recall Scores</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="4">: ROUGE Scores of California Bills</cell></row><row><cell></cell><cell cols="3">Rouge-1 Rouge-2 Rouge-L</cell></row><row><cell>Oracle</cell><cell>45.00</cell><cell>30.85</cell><cell>42.79</cell></row><row><cell>SumBasic</cell><cell>33.72</cell><cell>16.30</cell><cell>30.41</cell></row><row><cell>LSA</cell><cell>34.84</cell><cell>17.24</cell><cell>31.69</cell></row><row><cell>TextRank</cell><cell>36.66</cell><cell>19.28</cell><cell>32.61</cell></row><row><cell>DOC</cell><cell>38.31</cell><cell>20.49</cell><cell>34.67</cell></row><row><cell>SUM</cell><cell>41.67</cell><cell>22.10</cell><cell>37.45</cell></row><row><cell>DOC + SUM</cell><cell>39.86</cell><cell>22.34</cell><cell>36.17</cell></row><row><cell></cell><cell cols="2">(a) Precision Scores</cell><cell></cell></row><row><cell></cell><cell cols="3">Rouge-1 Rouge-2 Rouge-L</cell></row><row><cell>Oracle</cell><cell>62.96</cell><cell>43.83</cell><cell>59.58</cell></row><row><cell>SumBasic</cell><cell>40.47</cell><cell>17.89</cell><cell>36.36</cell></row><row><cell>LSA</cell><cell>38.23</cell><cell>17.28</cell><cell>34.61</cell></row><row><cell>TextRank</cell><cell>37.79</cell><cell>18.97</cell><cell>33.50</cell></row><row><cell>DOC</cell><cell>41.50</cell><cell>21.33</cell><cell>37.42</cell></row><row><cell>SUM</cell><cell>39.04</cell><cell>21.73</cell><cell>35.25</cell></row><row><cell>DOC + SUM</cell><cell>42.51</cell><cell>22.82</cell><cell>38.41</cell></row><row><cell></cell><cell cols="2">(b) Recall Scores</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Kanapala et al. (2017) provide a comprehensive overview of the works in legal summarization.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/unitedstates/ congress 4 http://leginfo.legislature.ca.gov 5 Often the same bill is introduced multiple times, either across chambers or across sessions. To avoid including such duplicates, we removed any bill that had a 96% cosine similarity to an existing bill in the dataset. In addition, we ensured that the remaining bills with duplicate titles were all in the train partition. For additional details about this procedure, see Appendix A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">As noted in Section 3, it is difficult to define sentence boundaries for this task due to the bulleted structure of the documents. We simplify the text with the following heuristic: if a bullet is shorter than 10 words, we treat it as a part of the previous sentence; otherwise, we treat it as a full sentence. This cut-off was chosen by manually analyzing a sample of sentences. A more sophisticated strategy would be to check if each bullet is a sentence fragment with a syntactic parser and then reconstruct full sentences; however, the former approach is sufficient for most documents.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Implemented with scikit-learn.org 8 All code described are used directly from https:// github.com/google-research/bert 9 This is the pretraining procedure recommended by the authors of BERT on their github website.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1010933404324</idno>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning summary prior representation for extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Houfeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="829" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The use of mmr, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="DOI">10.1145/290941.291025</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;98</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;98<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A supervised approach to extractive summarisation of scientific papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03946</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">How predictable is your state? leveraging lexical and contextual information for predicting legislative floor action at the state level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastassia</forename><surname>Vlad Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kornilova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Argyle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05284</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting legislative roll calls from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (icml-11)</title>
		<meeting>the 28th international conference on machine learning (icml-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="489" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The icsi summarization system at tac</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Benoit Favre</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hakkani-T?r</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-document summarization by sentence extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhu</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Kantrowitz</surname></persName>
		</author>
		<idno type="DOI">10.3115/1117575.1117580</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 NAACL-ANLPWorkshop on Automatic Summarization</title>
		<meeting>the 2000 NAACL-ANLPWorkshop on Automatic Summarization<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
	<note>NAACL-ANLP-AutoSum &apos;00</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generic text summarization using relevance measure and latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="19" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The HOLJ corpus. supporting summarisation of legal texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Hughson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2004 5th International Workshop on Linguistically Interpreted Corpora</title>
		<meeting><address><addrLine>Geneva, Switzerland. COLING</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
