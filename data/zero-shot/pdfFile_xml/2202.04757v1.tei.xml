<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A novel encoder-decoder network with guided transmission map for single image dehazing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le-Anh</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Myongji University</orgName>
								<address>
									<postCode>17058</postCode>
									<settlement>Yongin</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokyong</forename><surname>Moon</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">MindinTech, Inc</orgName>
								<address>
									<postCode>05854</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Chul</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Myongji University</orgName>
								<address>
									<postCode>17058</postCode>
									<settlement>Yongin</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A novel encoder-decoder network with guided transmission map for single image dehazing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image dehazing</term>
					<term>dark channel prior</term>
					<term>spatial pyramid pooling</term>
					<term>U-Net</term>
					<term>object detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A novel Encoder-Decoder Network with Guided Transmission Map (EDN-GTM) for single image dehazing scheme is proposed in this paper. The proposed EDN-GTM takes conventional RGB hazy image in conjunction with its transmission map estimated by adopting dark channel prior as the inputs of the network. The proposed EDN-GTM utilizes U-Net for image segmentation as the core network and utilizes various modifications including spatial pyramid pooling module and Swish activation to achieve state-of-the-art dehazing performance. Experiments on benchmark datasets show that the proposed EDN-GTM outperforms most of traditional and deep learning-based image dehazing schemes in terms of PSNR and SSIM metrics. The proposed EDN-GTM furthermore proves its applicability to object detection problems. Specifically, when applied to an image preprocessing tool for driving object detection, the proposed EDN-GTM can efficiently remove haze and significantly improve detection accuracy by 4.73% in terms of mAP measure. The code is available at: https://github.com/tranleanh/edn-gtm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The deterioration of digital image quality results in severe degradation of visibility and it affects the performance of various vision-based tasks. For instance, a self-driving system or even a human driver can be deprived of vision when facing inclement weather conditions which can result in unfortunate accidents and fatalities. Hence, visibility enhancement is an extremely critical task in real-world applications. Generally, haze can be considered as one of the most important phenomena causing image visibility degradation. As a challenging problem in computer vision, however, accurate estimation of the transmission map in a hazy image has been a major obstacle in haze removal or dehazing task <ref type="bibr" target="#b0">[1]</ref>. Numerous single image dehazing approaches have been proposed in attempt to enhance the visibility of hazy images and some of them have achieved significant progress. Typically, haze removal algorithms can be categorized into two paradigms: traditional methods and deep learning-based methods. In terms of traditional approaches, Meng et al. <ref type="bibr" target="#b1">[2]</ref> have proposed an efficient dehazing method by enforcing the boundary constraint and contextual regularization for sharper restored images. Zhu et al. <ref type="bibr" target="#b2">[3]</ref> have developed a color attenuation prior (CAP) that creates a linear model for modeling the scene depth of hazy image and learns the parameters of the model with a supervised learning manner. Noticeably, He et al. <ref type="bibr" target="#b3">[4]</ref> have proposed the dark channel prior (DCP) which is developed based on the statistics of haze-free outdoor images. The DCP is combined with the haze imaging model to directly estimate the haze thickness and the haze-free image is recovered subsequently. On the other hand, convolutional neural networks (CNNs) have brought an explosive rise in performance across a great number of image-based learning tasks. More recently, haze removal approaches such as AOD-Net <ref type="bibr" target="#b0">[1]</ref>, DehazeNet <ref type="bibr" target="#b4">[5]</ref>, MSCNN <ref type="bibr" target="#b5">[6]</ref>, and PPD-Net <ref type="bibr" target="#b6">[7]</ref> have also universally adopted CNNs as the principal component in their schemes. These CNN-based approaches have recently produced dramatic improvement in performance on benchmark datasets and have been gradually replacing traditional handcrafted graphical models. However, there still exists a room for improvement in CNN-based approaches. More detailed discussions of CNN-based haze removal approaches are presented in Section 2.2. In order to take advantage of both traditional and deep learning-based methods for achieving more improved haze removal performance, a novel encoder-decoder network is proposed in this paper. The proposed scheme, called Encoder-Decoder Network with Guided Transmission Map (EDN-GTM), utilizes the transmission map extracted by using DCP as additional input to the network in order to achieve an improved performance.</p><p>The remainder of this paper is organized as follows: Section 2 provides the preliminaries of research on single image dehazing. Next, the EDN-GTM scheme is proposed in Section 3. Experiments on benchmark data sets along with the analysis are presented in Section 4. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Atmospheric Scattering Model</head><p>The atmospheric scattering model used for the description of a hazy image is expressed as <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b3">[4]</ref>:</p><formula xml:id="formula_0">( ) = ( ) ( ) + ?1 ? ( )?<label>(1)</label></formula><p>where I(x), J(x), A, and t(x) denote the observed intensity, the scene radiance, the global atmospheric light, and the transmission map, respectively. When the atmospheric light is homogenous, t(x) can be expressed as <ref type="bibr" target="#b0">[1]</ref>[4]:</p><formula xml:id="formula_1">( ) = ? ( )<label>(2)</label></formula><p>where ? represents the scattering coefficient of the atmosphere and d(x) is the scene depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dehazing using Convolutional Neural Networks</head><p>Cai et al. <ref type="bibr" target="#b4">[5]</ref> have introduced DehazeNet which predicts the medium transmission map from hazy image and hazefree image is restored subsequently based on the atmospheric scattering model. However, DehazeNet performs dehazing on image-patch level. As a result, DehazeNet does not fully utilize high-level information from a large region of input <ref type="bibr" target="#b7">[8]</ref>. Ren et al. <ref type="bibr" target="#b5">[6]</ref> have proposed a multi-scale CNN (MSCNN) which consists of a coarse-scale network for predicting a holistic transmission map and a fine-scale network for refining the result locally. However, MSCNN shows somewhat less effective performance in dark scenes <ref type="bibr" target="#b5">[6]</ref>. Different from aforementioned approaches that consider only the prediction of transmission map, Li et al. <ref type="bibr" target="#b0">[1]</ref> have proposed an all-in-one dehazing network (AOD-Net) which directly learns the mapping between hazy image and haze-free image. Despite the advantage in computational cost, AOD-Net does not explore the information of transmission map independently. Therefore, the restored images by AOD-Net still contain some haze residues <ref type="bibr" target="#b7">[8]</ref>. Qin et al. <ref type="bibr" target="#b8">[9]</ref> have proposed a feature fusion attention network (FFA-Net) which applies attention mechanisms to directly restore haze-free image. FFA-Net adopts L1 reconstruction loss to train the network. However, using L1 loss or L2 loss on raw pixels as sole optimization may produce blurry restored image <ref type="bibr" target="#b9">[10]</ref>. In addition to conventional CNNs, several studies have adopted generative adversarial networks (GANs) for image dehazing and have shown promising results. Qu et al. <ref type="bibr" target="#b10">[11]</ref> proposed Enhanced Pix2pix Dehazing Network (EPDN) which is comprised of two main components: a GAN model to generate pseudo realistic image on a coarse scale followed by an enhancer to refine the pseudo realistic image and produce fine-scale dehazed image. Dong et al. <ref type="bibr" target="#b11">[12]</ref> also proposed a GAN model with fusion-discriminator (FD-GAN) which takes frequency information as additional priors and was able to generate more natural-looking dehazed images with less color distortion. the statistics of haze-free outdoor images while I-HAZE dataset contains indoor-scene images; second, DCP can be invalid when scene object is identical to the air light over a large local region while the input images include the wall scenes that appear similar with the air light in outdoor scenes. However, when we revisit the inverse transmission maps which are shown in <ref type="figure" target="#fig_0">Fig. 1 (d)</ref>, we have found that they are still able to precisely represent the haze thickness in the scenes even though the image data involves indoor scenes while DCP is probably confused in the areas of the wall scenes. On the other hand, CNNs can have a potential to deal with the similarity between the haze regions and the wall scenes because CNNs have the capability of extracting and analyzing advanced features of objects in image. In addition, as described in Eq. (2), the transmission map has a very close relationship with the depth information which benefits many vision applications such as image-based learning models like CNNs. Therefore, it is convinced that the transmission map estimated by using DCP can be utilized as guidance for a CNN model to achieve an improved dehazing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Transmission Map as Additional Input Channel for CNNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Design</head><p>Inspired by the success of EPDN <ref type="bibr" target="#b10">[11]</ref> and FD-GAN <ref type="bibr" target="#b11">[12]</ref>, GAN framework is applied to the proposed dehazing scheme. The proposed scheme and network architectures are illustrated in <ref type="figure">Fig. 2</ref>. In the generative network design, U-Net <ref type="bibr" target="#b12">[13]</ref> has been chosen as the core network because it is one of the most powerful encoder-decoder networks applied to image restoration and segmentation <ref type="bibr" target="#b19">[20]</ref>. U-Net is comprised of two paths; a contraction path (encoder) for extracting and analyzing advanced features and an expansion path (decoder) for feature synthesis. For a further upgrading the U-Net for dehazing task, three main modifications are applied: 1) a spatial pyramid pooling (SPP) module is plugged into the bottleneck to increase the receptive field and separate out the significant context features <ref type="bibr" target="#b13">[14]</ref>; 2) ReLU activation is replaced with Swish activation because Swish function has been shown to consistently outperform ReLU function on deep networks <ref type="bibr" target="#b14">[15]</ref>; 3) one convolution layer with the size of 3x3 is added before each of the down-scaling step and the up-scaling step to increase the receptive field and capture more high-level features from larger regions in the input image. In terms of the discriminator design, the encoder of U-Net is adopted in order to encourage the discriminator to have the same capability of extracting and analyzing advanced features with the generator so that the two networks compete each other to boost their performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Function</head><p>For the sake of both pixel quality and human perception, a combination of adversarial loss, MSE loss and perceptual loss <ref type="bibr" target="#b9">[10]</ref> is adopted in the proposed scheme. The adversarial loss is implemented as in <ref type="bibr" target="#b9">[10]</ref> because it has achieved notable success in image restoration problem. The adversarial loss Ladv is defined as:</p><formula xml:id="formula_2">= 1 ? ? ? ( )? (3) =1</formula><p>where B, D(x), G(x), and z denote the number of samples in a mini-batch, output of the discriminator, the generated image, and the hazy input image, respectively. The MSE loss is written as:</p><formula xml:id="formula_3">= 1 ?( ( ) ? ) 2 (4) =1</formula><p>where N is the number of pixels in the generated image and I is the ground-truth image. The perceptual loss <ref type="bibr" target="#b9">[10]</ref> to measure the perceptual similarity in feature space is defined as:</p><formula xml:id="formula_4">= 1 ?( ? ( )? ? ( )) 2 =1<label>(5)</label></formula><p>where M is the number of elements in the feature map at layer Conv3-3 of the VGG16 model <ref type="bibr" target="#b9">[10]</ref>, and represents the ? activated value of .</p><p>By combining all the related loss functions, the integral loss function L I for optimizing the generator can be formulated as:</p><formula xml:id="formula_5">= 1 + 2 + 3<label>(6)</label></formula><p>and the critic loss function for training the discriminator is defined as [10]:</p><formula xml:id="formula_6">= 4 1 ?( ( ) ? ? ( )? =1 )<label>(7)</label></formula><p>For our present scheme, the weight values for the integral loss function and the critic loss function are set as 1 = 100, 2 = 100, <ref type="bibr" target="#b2">3</ref> = 100, and 4 = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Data Preparation</head><p>Four benchmark datasets, I-HAZE, O-HAZE, Dense-HAZE, and NH-HAZE [7] <ref type="bibr" target="#b18">[19]</ref>, are used for experiments on dehazing tasks, while WAYMO dataset <ref type="bibr" target="#b15">[16]</ref> which contains approximately 100K driving images for experiments on object detection tasks <ref type="bibr" target="#b20">[21]</ref>. However, we consider utilizing only 1,100 images to conduct dehazing experiments. Data augmentation is carried out to improve the learning capability of the network: first, for every training image, a random crop method is adopted by cropping 5 image patches which have the same width/height ratio with the original image. Subsequently, a horizontal flipping method is also adopted to double the number of training samples and concurrently create new geometry variations of the training image textures. To synthesize hazy data for WAYMO dataset, a pretrained Monodepth2 model <ref type="bibr" target="#b16">[17]</ref>, which was trained on driving scenario data to estimate the depth maps of image data in WAYMO dataset, is utilized. After obtaining the depth maps, Eq. (2) and Eq. (1) are then applied sequentially to generate transmission maps and synthesized hazy images. In order to avoid generating only a certain amount of haze in all images, the value of ? is set to be a randomly chosen number between 1.0 to 3.0. This selection method of ? can generate different degrees of haze in the synthesized image data and make the training data more diverse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Settings</head><p>The experiments were performed on GeForce GTX TITAN X Graphics Cards. The network input size is 512x512. The number of training epochs is 400. In the first 200 epochs, the learning rate is fixed to 10 -4 and in the final 200 epochs, we linearly decay the learning rate to zero. The dehazing performance is measured by using peak signal-tonoise ratio (PSNR) and structural similarity index measure (SSIM) metrics. The PSNR, however, is chosen as our primary performance measure because we consider the pixel quality of the restored image that can benefit other computer vision tasks such as object detection. The network is trained with the batch size of 1 sample which shows empirically better results on validation in image restoration task <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>The performance of the proposed EDN-GTM scheme on I-HAZE and O-HAZE datasets is first compared with those of other approaches in <ref type="table">Table 1</ref> and <ref type="figure" target="#fig_2">Fig. 3</ref>. As shown in <ref type="table">Table 1</ref>, on I-HAZE dataset, the proposed EDN-GTM  <ref type="figure" target="#fig_2">Fig. 3</ref> (a) and <ref type="figure" target="#fig_2">Fig. 3 (b)</ref>, respectively. Experiments on Dense-HAZE and NH-HAZE datasets are then performed. Note that Dense-HAZE and NH-HAZE datasets are more challenging than I-HAZE and O-HAZE datasets. The performance of the proposed EDN-GTM scheme is also compared with those of other approaches and summarized in <ref type="table" target="#tab_0">Table 2</ref>, where the best and the secondbest results are shown in red and blue colors, respectively. On Dense-HAZE dataset, the proposed EDN-GTM scheme gives the second-best performance in terms of PNSR (15.43 dB) while showing the best performance in terms of SSIM (0.5200). On NH-HAZE dataset, the proposed EDN-GTM scheme achieves the best dehazing performance in both PSNR (20.24 dB) and SSIM (0.7178). Some of typical dehazing results from various methods on Dense-HAZE and NH-HAZE datasets are presented in <ref type="figure" target="#fig_2">Fig. 3 (c)</ref> and <ref type="figure" target="#fig_2">Fig. 3 (d)</ref>, respectively.</p><p>As summarized in <ref type="table">Table 1</ref> and <ref type="figure" target="#fig_2">Fig. 3</ref>, the proposed EDN-GTM scheme achieves favorable results on all the datasets in our experiments when compared with other recent dehazing methods. It demonstrates that the proposed EDN-GTM scheme has a well-designed architecture that can perform efficiently on haze removal tasks. In addition, we notice that transmission map plays a very important role in guiding the proposed EDN-GTM scheme to obtain excellent results in image dehazing problem.</p><p>For further evaluation of the applicability of the proposed EDN-GTM scheme to driving object detection problems, the proposed EDN-GTM scheme is utilized as a preprocessing tool for hazy images. Visual dehazing results on synthesized WAYMO hazy dataset are indicated in <ref type="figure">Fig. 4</ref>. After dehazing the input images by adopting the proposed EDN-GTM scheme, a pre-trained YOLOv4 object detection model <ref type="bibr" target="#b13">[14]</ref> which was trained on the original WAYMO dataset is utilized to evaluate object detection performance on hazy and dehazed images. The detection results after performing experiments on 1,100 images are summarized in <ref type="table" target="#tab_1">Table 3</ref>. As shown in <ref type="table" target="#tab_1">Table 3</ref>, the proposed EDN-GTM scheme helps to improve the mean average precision (mAP) by 4.73%. <ref type="figure">Fig. 5</ref> shows visual detection results on hazy and dehazed images, where the red and green boxes indicate the ground truth objects and the detections, respectively. As can be seen from <ref type="figure">Fig. 5</ref>, the dehazed images obtained through the proposed EDN-GTM scheme can provide much improved detection results than what the original hazy images can provide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, a novel encoder-decoder generative network with guided transmission map (EDN-GTM) for single image dehazing is proposed. The proposed EDN-GTM scheme utilizes the transmission map extracted by adopting dark channel prior as an additional channel in the input to improve dehazing performance. The network architecture is inspired by U-Net and several variations to the network including spatial pyramid pooling module and Swish activation are implemented to achieve the best dehazing performance. To enhance the learning efficiency, various data augmentation methods such as random crop and flip are adopted when preparing training data. The experiments on benchmark datasets show that the proposed EDN-GTM scheme outperforms most of conventional and deep learning-based algorithms in PSNR and SSIM metrics. Moreover, experiments on object detection problem show that the proposed EDN-GTM scheme can be successfully applied as a preprocessing tool for dehazing images so that the object detection accuracy can be improved.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Results of DCP on I-HAZE Dataset: (a) Input image, (b) Dehazed image, (c) Ground truth, (d) Inverse transmission map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>shows typical visual dehazing results of DCP on I-HAZE dataset<ref type="bibr" target="#b6">[7]</ref> which contains indoor-scene image data. The results indicate that when the image data involves indoor scenes, the outputs of DCP are not visually compelling and considerably suffer from color distortion. It can be explained by two reasons: first, DCP is based onFig. 2. Network architectures: (a) Design of the proposed scheme, (b) Architecture of EDN-GTM, and (c) Architecture of Discriminator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Visual dehazing results on high-resolution imagery datasets: (a) I-HAZE, (b) O-HAZE, (c) Dense-HAZE, and (d) NH-HAZE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Quantitative dehazing results on Dense-HAZE and NH-HAZE datasets. Visual dehazing results on synthesized hazy data (in each pair, left: hazy image, right: dehazed image). scheme achieves the best dehazing performance in terms of PSNR (22.90 dB) while showing the second-best performance in terms of SSIM (0.8270). On O-HAZE dataset, the proposed EDN-GTM scheme gives the second-best performance in terms of PNSR (23.46 dB) while showing the best performance in terms of SSIM (0.8198). More quantitative dehazing results on IHAZE and O-HAZE datasets are summarized inTable 1, where the best and the second-best results are shown in red and blue colors, respectively. Typical visual dehazing results of EDN-GTM and other methods on I-HAZE and O-HAZE datasets are shown in</figDesc><table><row><cell>Fig. 4.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Table 1. Quantitative dehazing results on I-HAZE and O-HAZE datasets.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>I-HAZE</cell><cell></cell><cell>O-HAZE</cell><cell></cell></row><row><cell>Method</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>DCP (TPAMI'10) [4]</cell><cell>14.43</cell><cell>0.7516</cell><cell>16.78</cell><cell>0.6532</cell></row><row><cell>CAP (TIP'15) [3]</cell><cell>12.24</cell><cell>0.6065</cell><cell>16.08</cell><cell>0.5965</cell></row><row><cell>MSCNN (ECCV'16) [6]</cell><cell>15.22</cell><cell>0.7545</cell><cell>17.56</cell><cell>0.6495</cell></row><row><cell>AOD-Net (ICCV'17) [1]</cell><cell>13.98</cell><cell>0.7323</cell><cell>15.03</cell><cell>0.5385</cell></row><row><cell>PPD-Net (CVPRW'18) [7]</cell><cell>22.53</cell><cell>0.8705</cell><cell>24.24</cell><cell>0.7205</cell></row><row><cell>EDN-GTM (our)</cell><cell>22.90</cell><cell>0.8270</cell><cell>23.46</cell><cell>0.8198</cell></row><row><cell></cell><cell cols="2">Dense-HAZE</cell><cell cols="2">NH-HAZE</cell></row><row><cell>Method</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>DCP (TPAMI'10) [4]</cell><cell>10.06</cell><cell>0.3856</cell><cell>10.57</cell><cell>0.5196</cell></row><row><cell>DehazeNet (TIP'16) [5]</cell><cell>13.84</cell><cell>0.4252</cell><cell>16.62</cell><cell>0.5238</cell></row><row><cell>AOD-Net (ICCV'17) [1]</cell><cell>13.14</cell><cell>0.4144</cell><cell>15.40</cell><cell>0.5693</cell></row><row><cell>MSBDN (CVPR'20) [18]</cell><cell>15.37</cell><cell>0.4858</cell><cell>19.23</cell><cell>0.7056</cell></row><row><cell>AECR-Net (CVPR'21) [19]</cell><cell>15.80</cell><cell>0.4660</cell><cell>19.88</cell><cell>0.7173</cell></row><row><cell>EDN-GTM (our)</cell><cell>15.43</cell><cell>0.5200</cell><cell>20.24</cell><cell>0.7178</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Object detection accuracy on hazy and dehazed images Detection performance on hazy and dehazed images. (in each pair, left: hazy, right: dehazed, red: ground truth, green: detection)</figDesc><table><row><cell>Fig. 5.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Hazy images</cell><cell>Dehazed images</cell></row><row><cell>mAP</cell><cell>41.91%</cell><cell>46.64%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">AOD-Net: All-in-One Dehazing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>eeding of the IEEE International Conference on Computer Vision (ICCV)<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="4770" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient Image Dehazing with Boundary Constraint and Contextual Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>eeding of the IEEE International Conference on Computer Vision (ICCV)<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Fast Single Image Haze Removal Algorithm Using Color Attenuation Prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3522" to="3533" />
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Single Image Haze Removal Using Dark Channel Prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DehazeNet: An End-to-End System for Single Image Haze Removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5187" to="5198" />
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Single Image Dehazing via Multi-Scale Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 14th European Conference on Computer Vision (ECCV)</title>
		<meeting>eeding of the 14th European Conference on Computer Vision (ECCV)<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-scale Single Image Dehazing Using Perceptual Pyramid Deep Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V-M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>eeding of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Single Image Dehazing via Multi-scale Convolutional Neural Networks with Holistic Edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="240" to="259" />
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FFA-Net: Feature Fusion Attention Network for Single Image Dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-02-07" />
			<biblScope unit="page" from="11908" to="11915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8183" to="8192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enhanced Pix2pix Dehazing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FD-GAN: Generative Adversarial Networks with Fusion-discriminator for Single Image Dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-02-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">YOLOv4: Optimal Speed and Accuracy of Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H-M</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Searching for activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q-V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalability in Perception for Autonomous Driving: Waymo Open Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2446" to="2454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O-M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3828" to="3838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-Scale Boosted Dehazing Network with Dense Feature Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2157" to="2167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contrastive Learning for Compact Single Image Dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Virtual Conference</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Virtual Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10551" to="10560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust U-Net-based Road Lane Markings Detection for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L-A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-H</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on System Science and Engineering (ICSSE)</title>
		<meeting>the International Conference on System Science and Engineering (ICSSE)<address><addrLine>Quang Binh, Vietnam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="62" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhancement of Robustness in Object Detection Module for Advanced Driver Assistance Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L-A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T-D</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D-C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-H</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on System Science and Engineering (ICSSE)</title>
		<meeting>the International Conference on System Science and Engineering (ICSSE)<address><addrLine>Ho Chi Minh City, Vietnam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="158" to="163" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
