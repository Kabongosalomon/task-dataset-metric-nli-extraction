<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transfer Learning with Jukebox for Music Source Separation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09">Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadhah</forename><surname>Zai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">El</forename><surname>Amri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Tautz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Ritter</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Melnik</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transfer Learning with Jukebox for Music Source Separation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09">Sep 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we demonstrate how a publicly available, pretrained Jukebox model can be adapted for the problem of audio source separation from a single mixed audio channel. Our neural network architecture, which is using transfer learning, is quick to train and the results demonstrate performance comparable to other state-of-the-art approaches that require a lot more compute resources, training data, and time. We provide an open-source code implementation of our architecture (https://github.com/wzaielamri/unmix)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Related Work</head><p>Source separation is an important issue in many fields such as audio processing, image processing <ref type="bibr" target="#b9">[9]</ref>, EEG <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b8">8]</ref>, etc. Music source separation from mixed audio is a challenging problem, especially if the source itself should be learned from a dataset of examples. Additionally, such models are expensive to train from scratch. We tested our model on the MUSDB18-HQ <ref type="bibr">[14]</ref> dataset which supplies full songs with ground truth stems of 'bass', 'drums', 'vocals' and 'other', which includes instruments such as guitars, synths, etc. The task is to separate a mixed audio channel into the separately recorded instruments, called stems here. Most baseline models in the Music Demixing Challenge 2021 <ref type="bibr" target="#b11">[11]</ref> used masking of input transformed to the frequency domain by short-time Fourier transformation such as UMX <ref type="bibr" target="#b18">[18]</ref>. This older technique transforms the waveform into a three dimensional input that can be viewed as a picture. The model then computes a 'mask' which substracts parts of the audio in frequency domain before transforming back to listenable audio. Demucs [1] on the other hand showed a successful approach that works in waveform domain, where an autoencoder, based on a bidirectional long short-term memory network, is used. This success of such a solution (using waveforms) encouraged us to adapt Jukebox <ref type="bibr" target="#b2">[3]</ref>, a powerful, generative model using multiple, deep Vector Quantized-Variational Autoencoders (VQ-VAE) [12] to automatically generate real sounding music, and using its publicly available pre-trained weights for the task.</p><p>Transfer learning helped deep learning models reach new heights in many domains, such as natural language processing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">13]</ref> and computer vision <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref>. Although relatively unexplored for the audio domain, <ref type="bibr" target="#b6">[7]</ref> proved feature representation learned on speech data could be used to classify sound events. Their results verify that cross-acoustic transfer learning performs significantly better than a baseline trained from scratch. TRILL <ref type="bibr" target="#b16">[16]</ref> showed great results of pretraining deep learning models with an unsupervised task on a big dataset of speech samples. Its learned representations exceeded SOTA performance on several downstream tasks with datasets of limited size.</p><p>We take a similar approach that is heavily based on Jukebox <ref type="bibr" target="#b2">[3]</ref>. It uses multiple VQ-VAEs to compress raw audio into discrete codes. They are trained self-supervised, on a large dataset of about 1.2 million songs, needing the compute power of 256 V100 to train in an acceptable time. Our experiments show that Jukebox's learned representations can be used for the task of source separation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture</head><p>Our architecture utilizes the default Jukebox's <ref type="bibr" target="#b2">[3]</ref> variant of the publicly available pre-trained VQ-VAE model. Jukebox uses three separated VQ-VAEs. We use only the smallest one with the strongest compression, due to the low resource usage. It employs dilated 1-D convolutions in multiple residual blocks to find a less complex sequence representation of music. An audio sequence x t gets mapped by an encoder E 1 to a latent space e t = E 1 (x t ) of 64 dimensions so that it can be mapped to the closest prototype vector in a collection C of vectors called codebook. These 2048 prototype vectors, denoted c st , are learned in training and help to form a high-quality representation.</p><p>The rate of compression for a sequence is called the hop length, for which a value of 8 is used. It depends on the stride values of the convolutional layers. We set the stride value to 2 as well as the downsampling to 3. All other values remain as defined in <ref type="bibr" target="#b2">[3]</ref>. After mapping to the codebook, a decoder D aims to reconstruct the original sequence. In summary, equation (1)</p><formula xml:id="formula_0">y t = D(argmin c ( E 1 (x t ) ? c) ) for c ? C<label>(1)</label></formula><p>describes a full forward pass through the VQ-VAE, where y t is the prediction for an input sequence x t and . is the euclidean norm. For further technical details on the used VQ-VAE architecture, refer to the paper of Dhariwal et al <ref type="bibr" target="#b2">[3]</ref>. The model is fine-tuned on data for one stem, learning good representations for a single instrument. In addition, we train a second encoder E 2 , identical to the one already mentioned, to project an input sequence of the mixture to the space already known by the codebook and decoder. For deployment, the encoder of the VQ-VAE is switched with the second one, effectively mapping from the full mixture to one stem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data</head><p>Our models are trained on the MUSDB18-HQ [14] dataset, also used in the music demixing challenge <ref type="bibr" target="#b11">[11]</ref>. It consists of 150 full-length songs, sampled at 44KHz, providing the full audio mixture and four stems, 'vocals', 'bass', 'drums', and 'other' for each sample, which can be regarded as ground truth in the context of source separation. We train on the full train set composed of 100 songs, testing is done on the remaining 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>For each stem i=1..4, we train a model in two phases (see <ref type="figure" target="#fig_0">Fig. 1</ref>). In the first phase, the model is trained on data that present the chosen stem in isolation (i.e. not embedded in a mixture). This produces a VQ-VAE with a "single stem encoder" (SE i ) that can map a single stem into a good latent representation, followed by a "stem decoder" (SD i ) tuned to reconstruct the input after the "discretization bottleneck" as faithfully as possible. Training of each VQ-VAE is based on the same three losses as chosen in the original Jukebox paper <ref type="bibr" target="#b2">[3]</ref> : L = L recons + L codebook + ?L commit . However, our final goal is to process each stem when it is part of a mixture of all four stems. Such embedding will introduce distortion of each stem, requiring to replace each single stem encoder SE i from phase 1 by a corresponding "mixture stem encoder" (ME i ) that is trained in phase 2, to map its changed (mixture embedded) stem i input onto the representation (stem-i codebook prototypes) created in phase by the SE i -SD i encoder-decoder pair. So, for each stem i (now omitting index i in the following) for each training sample (x mt : the sequence of the mixed audio, x st : the sequence of stem audio), we feed x st , to the already trained encoder SE, producing e st . Separately, the full mixture x mt is passed through the new encoder ME, yielding e mt . Now, we can backpropagate through ME using MSE loss ||e st ? e mt || 2 (keeping SE fixed throughout phase 2). It would also be possible to train E2 endto-end, similar to training phase 1. This could be done in two ways, freezing the weights of the rest of the VQ-VAE or fine-tuning further. With frozen weights, the best reachable performance would be identical to our current approach, but training time would be increased. Continuing to fine-tune would most probably lead to more source bleeding because the embeddings no longer represent only the stem they were trained on in training phase 1, so we decided against training end-to-end.</p><p>Finally, we obtain our mixture-adapted final VQ-VAE by concatenating the trained mixture stem encoder ME with the stem decoder SD. Note that this procedure will be carried out for each of the four stems, yielding four correspondingly optimized "stem mixture encoder-decoders" that together provide our decomposition of the mixture input into its stem constituents. On a more technical note, in both training phases and deployment, the data is processed chunk-wise, with a size of about 9 seconds.</p><p>For a clear overview of the content of this section, refer to figure 1. All conducted experiments that will be defined in the next section were computed on two Tesla GPUs with 16Gb each. The length of each input sequence is equal to 393216 data points as used by Jukebox. The batch size is equal to 4. SDR stem = 10 log 10 n s(n) 2 + ? n s(n) ??(n) 2 + ?</p><formula xml:id="formula_1">(2)</formula><p>To benchmark the conducted experiments, signal-to-distortion ratio (SDR) metric is used, which is a common metric in other SOTA papers <ref type="bibr" target="#b0">[1]</ref>[18] <ref type="bibr" target="#b5">[6]</ref>[15] <ref type="bibr" target="#b17">[17]</ref>. It is computed by equation <ref type="formula">(2)</ref>, as stated in <ref type="bibr" target="#b11">[11]</ref>, where s(n) is the values of the ground truth and?(n) depicts the values of the prediction. A small value ? = 10 ?7 is added to avoid division by zero. 'Total' SDR is the mean SDR for all stems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SDR Values Method</head><p>Drum Bass Other Vocal Total Our Approach (i) 4.925 4.073 2.695 5.060 4.188 Our Approach trained from scratch (ii) -0.002 -0.087 -0.026 0.00 -0.028 Our Approach without finetuning(iii) 1.06 -1.072 0.79 0.33 0.279  <ref type="table">Table 2</ref>: Comparison of SDR values per stem and in total. Our approach outperforms both the ScaledMixturePredictor, the basic baseline in the Music Demixing Challenge <ref type="bibr" target="#b11">[11]</ref> and Wave-U-Net <ref type="bibr" target="#b17">[17]</ref>, a classic approach of source seperation in the waveform domain while Demucs <ref type="bibr" target="#b0">[1]</ref> achieves current SOTA performance on the Dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>The main key point of this paper consists of demonstrating that it is possible to get decent audio quality by using transfer learning. For this, we did three different experiments (i), (ii), and (iii) on the four audio stems. In experiment (i) we trained each audio stem's stem encoder SE from scratch without using any pretraining values. For the second experiment (ii) we trained the SE's with initial weights chosen as the pre-trained weights of Jukebox. Experiment (iii) is conducted to show the impact of fine-tuning the SE. We skip fine-tuning in training phase 1 and use the weights of Jukebox as is and train a ME like in the other phases.</p><p>For all these VQ-VAE, we pick the checkpoint 80K and train the corresponding mixture encoders ME in phase 2. For these, and in both experiments (i) (ii) and (iii), we initialized their weights randomly. For the first experiment, we found out that all results are bad, and no good audio quality is reached for the extracted stems. The SDR values are equal to or near 0 for all four stems. For the second experiment, the model converges after 32 hours of training in total on two Tesla GPU units with 16GB of VRAM each. (iii) shows that fine-tuning in phase 1 is important. <ref type="figure" target="#fig_1">Figure 2</ref> demonstrates decent SDR values for networks trained with pretrained weights in comparison to others trained with randomly initialized weights from scratch. It can also be deduced that in order to get fairly good SDR val-ues, it is enough to train until early checkpoint values, such as 20K. Then, the checkpoint 20K is reached after 16 hours for each of the two models on two Tesla GPUs. <ref type="table">Table 2</ref> gives a comparison of different approaches for audio signal separation. Here, our approach achieves comparable results when benchmarked against other state-of-the-art networks.</p><p>In terms of deployment, we need 0.73 seconds of CPU processing time for an audio chunk of 8.91 seconds per stem, which translates into an RTF (Real-Time Factor) of 0.082. That seems quick, but the current implementation of our model takes an audio chunk of 8.91 seconds. As with most approaches to audio source separation, direct online processing is not possible. However, a single code vector of the VQ-VAE represents a sliding window of an input vector of 49152 timesteps, which corresponds to about 1s of audio with a sampling rate of 44.1k. So, we think that an adapted architecture could be more flexible. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We demonstrate how transfer learning can be used for a problem of audio signal processing, in particular for the separation of an audio signal from a single mixed audio channel into four different stems: 'drums', 'bass', 'vocals', and 'other'. We show that it is possible to be successful with a small data set and relatively short training time on just two GPUs by fine-tuning pre-trained weights of Jukebox <ref type="bibr" target="#b2">[3]</ref>. Similar results could not be achieved in a comparable timeframe when training from scratch or without fine-tuning, showing the potential for reduced training times and improving results through the use of transfer learning in the audio domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Visualization of the proposed transfer learning model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>SDR results of the 4 audio signal stems for the second experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of SDR values per stem and in total for three different versions of our approach.</figDesc><table><row><cell></cell><cell>SDR Values</cell></row><row><cell>Method</cell><cell>Drum Bass Other Vocal Total</cell></row><row><cell>DEMUCS</cell><cell>6.509 6.470 4.018 6.496 5.873</cell></row><row><cell>Our Approach</cell><cell>4.925 4.073 2.695 5.060 4.188</cell></row><row><cell>Wave-U-Net</cell><cell>4.22 3.21 2.25 3.25 3.23</cell></row><row><cell cols="2">ScaledMixturePredictor 0.578 0.745 1.136 1.090 0.887</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Demucs: Deep extractor for music sources with extra unlabeled data remixed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<idno>abs/1909.01174</idno>
		<ptr target="http://arxiv.org/abs/1909.01174" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<title level="m">Jukebox: A generative model for music</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep transfer learning for image-based structural damage recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Mosalam</surname></persName>
		</author>
		<idno type="DOI">https:/onlinelibrary.wiley.com/doi/abs/10.1111/mice.12363</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1111/mice.12363" />
	</analytic>
	<monogr>
		<title level="j">Computer-Aided Civil and Infrastructure Engineering</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="748" to="768" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A new image classification method using cnn transfer learning and web data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2017.11.028</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0957417417307844" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="43" to="56" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spleeter: a fast and efficient music source separation tool with pre-trained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hennequin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khlif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Voituret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moussallam</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.02154</idno>
		<ptr target="https://doi.org/10.21105/joss.02154" />
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">50</biblScope>
			<biblScope unit="page">2154</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cross-acoustic transfer learning for sound event classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2504" to="2508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICASSP.2016.7472128</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2016.7472128" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eeg correlates of sensorimotor processing: independent components involved in sensory and motor processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Melnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Hairston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ferris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>K?nig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Critic guided segmentation of rewarding objects in first-person views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Melnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Limberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Artificial Intelligence (K?nstliche Intelligenz)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Systems, subjects, sessions: to what extent do these factors influence eeg data?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Melnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Legkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Izdebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>K?rcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Hairston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ferris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>K?nig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in human neuroscience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">150</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Music demixing challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fabbro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>St?ter</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2108.13559</idno>
		<ptr target="https://arxiv.org/abs/2108.13559" />
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1711.00937</idno>
		<ptr target="http://arxiv.org/abs/1711.00937" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Mimilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<idno>abs/1910.10683</idno>
		<ptr target="http://arxiv.org/abs/1910.1068314" />
		<imprint>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
	<note>MUSDB18-HQ an uncompressed version of musdb18</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<idno type="DOI">10.5281/zenodo.3338373</idno>
		<ptr target="https://doi.org/10.5281/zenodo.3338373" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">All for one and one for all: Improving music separation by bridging networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sawata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Towards learning a universal non-semantic representation of speech. Interspeech 2020</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D C</forename><surname>Quitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Emanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Haviv</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2020-1242</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2020-1242" />
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Wave-u-net: A multi-scale neural network for end-to-end audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dixon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Open-unmix -a reference implementation for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.01667</idno>
		<ptr target="https://doi.org/10.21105/joss.01667" />
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">41</biblScope>
			<biblScope unit="page">1667</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
