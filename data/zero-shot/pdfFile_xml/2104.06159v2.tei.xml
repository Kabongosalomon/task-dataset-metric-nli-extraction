<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Muesli: Combining Improvements in Policy Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Schmitt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Weber</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
						</author>
						<title level="a" type="main">Muesli: Combining Improvements in Policy Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel policy update that combines regularized policy optimization with model learning as an auxiliary loss. The update (henceforth Muesli) matches MuZero's state-of-the-art performance on Atari. Notably, Muesli does so without using deep search: it acts directly with a policy network and has computation speed comparable to model-free baselines. The Atari results are complemented by extensive ablations, and by additional results on continuous control and 9x9 Go.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reinforcement learning (RL) is a general formulation for the problem of sequential decision making under uncertainty, where a learning system (the agent) must learn to maximize the cumulative rewards provided by the world it is embedded in (the environment), from experience of interacting with such environment <ref type="bibr" target="#b72">(Sutton &amp; Barto, 2018</ref>). An agent is said to be value-based if its behavior, i.e. its policy, is inferred (e.g by inspection) from learned value estimates <ref type="bibr" target="#b70">(Sutton, 1988;</ref><ref type="bibr" target="#b84">Watkins, 1989;</ref><ref type="bibr" target="#b56">Rummery &amp; Niranjan, 1994;</ref><ref type="bibr" target="#b76">Tesauro, 1995)</ref>. In contrast, a policy-based agent directly updates a (parametric) policy <ref type="bibr" target="#b86">(Williams, 1992;</ref><ref type="bibr" target="#b73">Sutton et al., 2000)</ref> based on past experience. We may also classify as model free the agents that update values and policies directly from experience <ref type="bibr" target="#b70">(Sutton, 1988)</ref>, and as model-based those that use (learned) models <ref type="bibr" target="#b46">(Oh et al., 2015;</ref> to plan either global <ref type="bibr" target="#b71">(Sutton, 1990)</ref> or local <ref type="bibr" target="#b54">(Richalet et al., 1978;</ref><ref type="bibr" target="#b31">Kaelbling &amp; Lozano-P?rez, 2010;</ref><ref type="bibr" target="#b63">Silver &amp; Veness, 2010)</ref> values and policies. Such distinctions are useful for communication, but, to master the singular goal of optimizing rewards in an environment, agents often combine ideas from more than one of these areas <ref type="bibr" target="#b65">Silver et al., 2016;</ref><ref type="bibr" target="#b59">Schrittwieser et al., 2020)</ref>.</p><p>In this paper, we focus on a critical part of RL, namely policy * Equal contribution <ref type="bibr">1</ref>   (a) Muesli and other policy updates; all these use the same IMPALA network and a moderate amount of replay data (75%). Shades denote standard errors across 5 seeds. (b) Muesli with the larger MuZero network and the high replay fraction used by MuZero (95%), compared to the latest version of MuZero . These large scale runs use 2 seeds. Muesli still acts directly with the policy network and uses one-step look-aheads in updates.</p><p>optimization. We leave a precise formulation of the problem for later, but different policy optimization algorithms can be seen as answers to the following crucial question:</p><p>given data about an agent's interactions with the world, and predictions in the form of value functions or models, how should we update the agent's policy?</p><p>We start from an analysis of the desiderata for general policy optimization. These include support for partial observability and function approximation, the ability to learn stochastic policies, robustness to diverse environments or training regimes (e.g. off-policy data), and being able to represent knowledge as value functions and models. See Section 3 for further details on our desiderata for policy optimization.</p><p>Then, we propose a policy update combining regularized policy optimization with model-based ideas so as to make progress on the dimensions highlighted in the desiderata. More specifically, we use a model inspired by MuZero  to estimate action values via one-step look-ahead. These action values are then plugged into a modified Maximum a Posteriori Policy Optimiza-arXiv:2104.06159v2 <ref type="bibr">[cs.</ref>LG] 31 Mar 2022 tion (MPO) <ref type="bibr" target="#b0">(Abdolmaleki et al., 2018)</ref> mechanism, based on clipped normalized advantages, that is robust to scaling issues without requiring constrained optimization. The overall update, named Muesli, then combines the clipped MPO targets and policy-gradients into a direct method <ref type="bibr" target="#b82">(Vieillard et al., 2020)</ref> for regularized policy optimization.</p><p>The majority of our experiments were performed on 57 classic Atari games from the Arcade Learning Environment <ref type="bibr" target="#b5">(Bellemare et al., 2013;</ref><ref type="bibr" target="#b41">Machado et al., 2018)</ref>, a popular benchmark for deep RL. We found that, on Atari, Muesli can match the state of the art performance of MuZero, without requiring deep search, but instead acting directly with the policy network and using one-step look-aheads in the updates. To help understand the different design choices made in Muesli, our experiments on Atari include multiple ablations of our proposed update. Additionally, to evaluate how well our method generalises to different domains, we performed experiments on a suite of continuous control environments (based on MuJoCo and sourced from the OpenAI Gym <ref type="bibr" target="#b7">(Brockman et al., 2016)</ref>). We also conducted experiments in 9x9 Go in self-play, to evaluate our policy update in a domain traditionally dominated by search methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>The environment. We are interested in episodic environments with variable episode lengths (e.g. Atari games), formalized as Markov Decision Processes (MDPs) with initial state distribution ? and discount ? ? [0, 1); ends of episodes correspond to absorbing states with no rewards.</p><p>The objective. The agent starts at a state S 0 ? ? from the initial state distribution. At each time step t, the agent takes an action A t ? ?(A t |S t ) from a policy ?, obtains the reward R t+1 and transitions to the next state S t+1 . The expected sum of discounted rewards after a state-action pair is called the action-value or Q-value q ? (s, a):</p><p>q ? (s, a) = E t=0 ? t R t+1 |?, S 0 = s, A 0 = a .</p><p>(1)</p><p>The value of a state s is v ? (s) = E A??(?|s) [q ? (s, A)] and the objective is to find a policy ? that maximizes the expected value of the states from the initial state distribution:</p><formula xml:id="formula_0">J(?) = E S?? [v ? (S)] .<label>(2)</label></formula><p>Policy improvement. Policy improvement is one of the fundamental building blocks of reinforcement learning algorithms. Given a policy ? prior and its Q-values q ?prior (s, a), a policy improvement step constructs a new policy ? such that v ? (s) ? v ?prior (s) ?s. For instance, a basic policy improvement step is to construct the greedy policy:</p><p>arg max ? E A??(?|s) q ?prior (s, A) .</p><p>Regularized policy optimization. A regularized policy optimization algorithm solves the following problem:</p><formula xml:id="formula_2">arg max ? E A??(?|s) q ?prior (s, A) ? ?(?) ,<label>(4)</label></formula><p>whereq ?prior (s, a) are approximate Q-values of a ? prior policy and ?(?) ? R is a regularizer. For example, we may use as the regularizer the negative entropy of the policy ?(?) = ??H[?], weighted by an entropy cost ? <ref type="bibr" target="#b87">(Williams &amp; Peng, 1991)</ref>. Alternatively, we may also use ?(?) = ? KL(? prior , ?), where ? prior is the previous policy, as used in TRPO <ref type="bibr" target="#b61">(Schulman et al., 2015)</ref>.</p><p>Following the terminology introduced by Vieillard et al.</p><p>(2020), we can then solve Eq. 4 by either direct or indirect methods. If ?(a|s) is differentiable with respect to the policy parameters, a direct method applies gradient ascent to</p><formula xml:id="formula_3">J(s, ?) = E A??(?|s) q ?prior (s, A) ? ?(?).<label>(5)</label></formula><p>Using the log derivative trick to sample the gradient of the expectation results in the canonical (regularized) policy gradient update <ref type="bibr" target="#b73">(Sutton et al., 2000)</ref>.</p><p>In indirect methods, the solution of the optimization problem (4) is found exactly, or numerically, for one state and then distilled into a parametric policy. For example, Maximum a Posteriori Policy Optimization (MPO) <ref type="bibr" target="#b0">(Abdolmaleki et al., 2018)</ref> uses as regularizer ?(?) = ? KL(?, ? prior ), for which the exact solution to the regularized problem is ? MPO (a|s) = ? prior (a|s) exp q ?prior (s, a) ?</p><formula xml:id="formula_4">1 z(s) ,<label>(6)</label></formula><p>where z(s) = E A??prior(?|s) exp q? prior (s,A) ? is a normalization factor that ensures that the resulting probabilities form a valid probability distribution (i.e. they sum up to 1).</p><p>MuZero. MuZero  uses a weakly grounded <ref type="bibr">(Grimm et al., 2020)</ref> transition model m trained end to end exclusively to support accurate reward, value and policy predictions: m(s t , a t , a t+1 , . . . , a t+k ) ? (R t+k+1 , v ? (S t+k+1 ), ?(?|S t+k+1 )). Since such model can be unrolled to generate sequences of rewards and value estimates for different sequences of actions (or plans), it can be used to perform Monte-Carlo Tree Search, or MCTS <ref type="bibr" target="#b11">(Coulom, 2006)</ref>. MuZero then uses MCTS to construct a policy as the categorical distribution over the normalized visit counts for the actions in the root of the search tree; this policy is then used both to select actions, and as a policy target for the policy network. Despite MuZero being introduced with different motivations, <ref type="bibr" target="#b16">Grill et al. (2020)</ref> showed that the MuZero policy update can also be interpreted as approximately solving a regularized policy optimization problem with the regularizer ?(?) = ? KL(? prior , ?) also used by the TRPO algorithm <ref type="bibr" target="#b61">(Schulman et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Desiderata and motivating principles</head><p>First, to motivate our investigation, we discuss a few desiderata for a general policy optimization algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Observability and function approximation</head><p>Being able to learn stochastic policies, and being able to leverage Monte-Carlo or multi-step bootstrapped return estimates is important for a policy update to be truly general. This is motivated by the challenges of learning in partially observable environments <ref type="bibr">(?str?m, 1965)</ref> or, more generally, in settings where function approximation is used <ref type="bibr" target="#b72">(Sutton &amp; Barto, 2018)</ref>. Note that these two are closely related: if a chosen function approximation ignores a state feature, then the state feature is, for all practical purposes, not observable.</p><p>In POMDPs the optimal memory-less stochastic policy can be better than any memory-less deterministic policy, as shown by <ref type="bibr" target="#b67">Singh et al. (1994)</ref>. As an illustration, consider the MDP in <ref type="figure" target="#fig_1">Figure 2</ref>; in this problem we have 4 states and, on each step, 2 actions (up or down). If the state representation of all states is the same ?(s) = ?, the optimal policy is stochastic. We can easily find such policy with pen and paper: ? * (up|?(s)) = 5 8 ; see Appendix B for details. It is also known that, in these settings, it is often preferable to leverage Monte-Carlo returns, or at least multi-step bootstrapped estimators, instead of using one-step targets . Consider again the MDP in <ref type="figure" target="#fig_1">Figure 2</ref>: boostrapping from v ? (?(s)) produces biased estimates of the expected return, because v ? (?(s)) aggregates the values of multiple states; again, see Appendix B for the derivation.</p><p>Among the methods in Section 2, both policy gradients and MPO allow convergence to stochastic policies, but only policy gradients naturally incorporate multi-step return estimators. In MPO, stochastic return estimates could make the agent overly optimistic (E[exp(G)] ? exp(E[G])).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Policy representation</head><p>Policies may be constructed from action values or they may combine action values and other quantities (e.g., a direct parametrization of the policy or historical data). We argue that the action values alone are not enough.</p><p>First, we show that action values are not always enough to represent the best stochastic policy. Consider again the MDP in <ref type="figure" target="#fig_1">Figure 2</ref> with identical state representation ?(s) in all states. As discussed, the optimal stochastic policy is ? * (up|?(s)) = 5 8 . This non-uniform policy cannot be inferred from Q-values, as these are the same for all actions and are thus wholly uninformative about the best probabilities: q ? * (?(s), up) = q ? * (?(s), down) = 1 4 . Similarly, a model on its own is also insufficient without a policy, as it would produce the same uninformative action values. State 4 is terminal. At each step, the agent can choose amongst two actions: up or down. The rewards range from -1 to 1, as displayed. The discount is 1. If the state representation ?(s) is the same in all states, the best stochastic policy is ? * (up|?(s)) = 5 8 .</p><p>One approach to address this limitation is to parameterize the policy explicitly (e.g. via a policy network). This has the additional advantage that it allows us to directly sample both discrete  and continuous <ref type="bibr" target="#b79">(van Hasselt &amp; Wiering, 2007;</ref><ref type="bibr">Degris et al., 2012;</ref><ref type="bibr" target="#b64">Silver et al., 2014)</ref> actions. In contrast, maximizing Q-values over continuous action spaces is challenging. Access to a parametric policy network that can be queried directly is also beneficial for agents that act by planning with a learned model (e.g. via MCTS), as it allows to guide search in large or continuous action space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Robust learning</head><p>We seek algorithms that are robust to 1) off-policy or historical data; 2) inaccuracies in values and models; 3) diversity of environments. In the following paragraphs we discuss what each of these entails.</p><p>Reusing data from previous iterations of policy ? <ref type="bibr" target="#b39">(Lin, 1992;</ref><ref type="bibr" target="#b55">Riedmiller, 2005;</ref><ref type="bibr" target="#b42">Mnih et al., 2015)</ref> can make RL more data efficient. However, if computing the gradient of the objective E A??(?|s) q ?prior (s, A) on data from an older policy ? prior , an unregularized application of the gradient can degrade the value of ?. The amount of degradation depends on the total variation distance between ? and ? prior , and we can use a regularizer to control it, as in Conservative Policy Iteration <ref type="bibr" target="#b33">(Kakade &amp; Langford, 2002)</ref>, Trust Region Policy Optimization <ref type="bibr" target="#b61">(Schulman et al., 2015)</ref>, and Appendix C.</p><p>Whether we learn on or off-policy, agents' predictions incorporate errors. Regularization can also help here. For instance, if Q-values have errors, the MPO regularizer ?(?) = ? KL(?, ? prior ) maintains a strong performance bound <ref type="bibr" target="#b82">(Vieillard et al., 2020)</ref>. The errors from multiple iterations average out, instead of appearing in a discounted sum of the absolute errors. While not all assumptions behind this result apply in an approximate setting, Section 5 shows that MPO-like regularizers are helpful empirically. Finally, robustness to diverse environments is critical to ensure a policy optimization algorithm operates effectively in novel settings. This can take various forms, but we focus on robustness to diverse reward scales and minimizing problem dependent hyperparameters. The latter are an especially subtle form of inductive bias that may limit the applicability of a method to established benchmarks .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Rich representation of knowledge</head><p>Even if the policy is parametrized explicitly, we argue it is important for the agent to represent knowledge in multiple ways <ref type="bibr">(Degris &amp; Modayil, 2012)</ref> to update such policy in a reliable and robust way. Two classes of predictions have proven particularly useful: value functions and models.</p><p>Value functions <ref type="bibr" target="#b70">(Sutton, 1988;</ref><ref type="bibr" target="#b74">Sutton et al., 2011)</ref> can capture knowledge about a cumulant over long horizons, but can be learned with a cost independent of the span of the predictions <ref type="bibr" target="#b78">(van Hasselt &amp; Sutton, 2015)</ref>. They have been used extensively in policy optimization, e.g., to implement forms of variance reduction <ref type="bibr" target="#b86">(Williams, 1992)</ref>, and to allow updating policies online through bootstrapping, without waiting for episodes to fully resolve <ref type="bibr" target="#b73">(Sutton et al., 2000)</ref>.</p><p>Models can also be useful in various ways: 1) learning a model can act as an auxiliary task <ref type="bibr" target="#b57">(Schmidhuber, 1990;</ref><ref type="bibr" target="#b74">Sutton et al., 2011;</ref><ref type="bibr" target="#b29">Jaderberg et al., 2017;</ref><ref type="bibr" target="#b14">Guez et al., 2020)</ref>, and help with representation learning; 2) a learned model may be used to update policies and values via planning <ref type="bibr" target="#b85">(Werbos, 1987;</ref><ref type="bibr" target="#b71">Sutton, 1990;</ref><ref type="bibr" target="#b17">Ha &amp; Schmidhuber, 2018)</ref>; 3) finally, the model may be used to plan for action selection <ref type="bibr" target="#b54">(Richalet et al., 1978;</ref><ref type="bibr" target="#b63">Silver &amp; Veness, 2010)</ref>. These benefits of learned models are entangled in MuZero. Sometimes, it may be useful to decouple them, for instance to retain the benefits of models for representation learning and policy optimization, without depending on the computationally intensive process of planning for action selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Robust yet simple policy optimization</head><p>The full list of desiderata is presented in <ref type="table">Table 1</ref>. These are far from solved problems, but they can be helpful to reason about policy updates. In this section, we describe a policy optimization algorithm designed to address these desiderata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Our proposed clipped MPO (CMPO) regularizer</head><p>We use the Maximum a Posteriori Policy Optimization (MPO) algorithm <ref type="bibr" target="#b0">(Abdolmaleki et al., 2018)</ref> as starting point, since it can learn stochastic policies (1a), supports discrete and continuous action spaces (2c), can learn stably from off-policy data (3a), and has strong performance bounds even when using approximate Q-values (3b). We then improve the degree of control provided by MPO on the total variation distance between ? and ? prior (3a), avoiding sensitive domain-specific hyperparameters (3d).</p><p>MPO uses a regularizer ?(?) = ? KL(?, ? prior ), where ? prior is the previous policy. Since we are interested in learning from stale data, we allow ? prior to correspond to arbitrary previous policies, and we introduce a regularizer ?(?) = ? KL(? CMPO , ?), based on the new target</p><formula xml:id="formula_5">? CMPO (a|s) = ? prior (a|s) exp clip(? dv(s, a), ?c, c) z CMPO (s) ,<label>(7)</label></formula><p>where? dv(s, a) is a non-stochastic approximation of the advantageq ?prior (s, a) ?v ?prior (s) and the factor z CMPO (s) ensures the policy is a valid probability distribution. The ? CMPO term we use in the regularizer has an interesting relation to natural policy gradients <ref type="bibr" target="#b34">(Kakade, 2001)</ref>: ? CMPO is obtained if the natural gradient is computed with respect to the logits of ? prior and then the expected gradient is clipped (for proof note the natural policy gradient with respect to the logits is equal to the advantages <ref type="bibr" target="#b1">(Agarwal et al., 2019)</ref>).</p><p>The clipping threshold c controls the maximum total variation distance between ? CMPO and ? prior . Specifically, the total variation distance between ? and ? is defined as D TV (? (?|s), ?(?|s)) = 1 2 a |? (a|s) ? ?(a|s)|. <ref type="formula">(8)</ref> As discussed in Section 3.3, constrained total variation supports robust off-policy learning. The clipped advantages allows us to derive not only a bound for the total variation distance but an exact formula:</p><p>Theorem 4.1 (Maximum CMPO total variation distance) For any clipping threshold c &gt; 0, we have: max ?prior,? dv,s D TV (? CMPO (?|s), ? prior (?|s)) = tanh( c 2 ). Mean return relative to multiplier=1 alien beam_rider breakout gravitar hero ms_pacman phoenix robotank seaquest time_pilot <ref type="figure">Figure 3</ref>. (a) The maximum total variation distance between ?CMPO and ?prior is exclusively a function of the clipping threshold c. (b) A comparison (on 10 Atari games) of the Muesli sensitivity to the regularizer multiplier ?. Each dot is the mean of 5 runs with different random seeds and the black line is the mean across all 10 games. With Muesli's normalized advantages, the good range of values for ? is fairly large, not strongly problem dependent, and ? = 1 performs well on many environments.</p><p>We refer readers to Appendix D for proof of Theorem 4.1; we also verified the theorem predictions numerically.</p><p>Note that the maximum total variation distance between ? CMPO and ? prior does not depend on the number of actions or other environment properties (3d). It only depends on the clipping threshold as visualized in <ref type="figure">Figure 3a</ref>. This allows to control the maximum total variation distance under a CMPO update, for instance by setting the maximum total variation distance to , without requiring the constrained optimization procedure used in the original MPO paper. Instead of the constrained optimization, we just set c = 2 arctanh( ). We used c = 1 in our experiments, across all domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">A novel policy update</head><p>Given the proposed regularizer ?(?) = ? KL(? CMPO , ?), we can update the policy by direct optimization of the regularized objective, that is by gradient descent on</p><formula xml:id="formula_6">L PG+CMPO (?, s) = ?E A??(?|s) ? dv(s, A) + ? KL(? CMPO (?|s), ?(?|s)),<label>(9)</label></formula><p>where the advantage terms in each component of the loss can be normalized using the approach described in Section 4.5 to improve the robustness to reward scales.</p><p>The first term corresponds to a standard policy gradient update, thus allowing stochastic estimates of? dv(s, A) that use Monte-Carlo or multi-step estimators (1b). The second term adds regularization via distillation of the CMPO target, to preserve the desiderata addressed in Section 4.1.</p><p>Critically, the hyper-parameter ? is easy to set (3d), because even if ? is high, ? KL(? CMPO (?|s), ?(?|s)) still proposes improvements to the policy ? prior . This property is missing in popular regularizers that maximize entropy or minimize a distance from ? prior . We refer to the sensitivity analysis depicted in <ref type="figure">Figure 3b</ref> for a sample of the wide range of values of ? that we found to perform well on Atari. We used ? = 1 in all other experiments reported in the paper.</p><p>Both terms can be sampled, allowing to trade off the computation cost and the variance of the update; this is especially useful in large or continuous action spaces (2b), (2c).</p><p>We can sample the gradient of the first term by computing the loss on data generated on a prior policy ? prior , and then use importance sampling to correct for the distribution shift wrt ?. This results in the estimator</p><formula xml:id="formula_7">? ?(a|s) ? b (a|s) (G v (s, a) ?v ?prior (s)),<label>(10)</label></formula><p>for the first term of the policy loss. In this expression, ? b (a|s) is the behavior policy; the advantage (G v (s, a) ? v ?prior (s)) uses a stochastic multi-step bootstrapped estimator G v (s, a) and a learned baselinev ?prior (s).</p><p>We can also sample the regularizer, by computing a stochastic estimate of the KL on a subset of N actions a (k) , sampled from ? prior (s). In which case, the second term of Eq. 9 becomes (ignoring an additive constant):</p><formula xml:id="formula_8">? ? N N k=1 exp(clip(? dv(s, a (k) ), ?c, c)) z CMPO (s) log ?(a (k) |s) ,<label>(11)</label></formula><p>where? dv(s, a) =q ?prior (s, a) ?v ?prior (s) is computed from the learned valuesq ?prior andv ?prior (s). To support sampling just few actions from the current state s, we can estimate z CMPO (s) for the i-th sample out of N as:</p><formula xml:id="formula_9">z (i) CMPO (s) = z init + N k =i exp(clip(? dv(s, a (k) ), ?c, c)) N ,<label>(12)</label></formula><p>where z init is an initial estimate. We use z init = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Learning a model</head><p>As discussed in Section 3.4, learning models has several potential benefits. Thus, we propose to train a model alongside policy and value estimates (4b). As in MuZero (Schrittwieser et al., 2020) our model is not trained to reconstruct observations, but is rather only required to provide accurate estimates of rewards, values and policies. It can be seen as an instance of value equivalent models <ref type="bibr">(Grimm et al., 2020)</ref>.</p><p>For training, the model is unrolled k &gt; 1 steps, taking as inputs an initial state s t and an action sequence a &lt;t+k = a t , a t+1 , ..., a t+k?1 . On each step the model then predicts rewardsr k , valuesv k and policies? k . Rewards and values are trained to match the observed rewards and values of the states actually visited when executing those actions.</p><p>Policy predictions? k after unrolling the model k steps are trained to match the ? CMPO (?|s t+k ) policy targets computed in the actual observed states s t+k . The policy component of the model loss can then be written as:</p><formula xml:id="formula_10">L m (?, s t ) = K k=1 KL(? CMPO (?|s t+k ),? k (?|s t , a &lt;t+k )) K .<label>(13)</label></formula><p>This differs from MuZero in that here the policy predictions? k (?|s t , a &lt;t+k ) are updated towards the targets ? CMPO (?|s t+k ), instead of being updated to match the targets ? MCTS (?|s t+k ) constructed from the MCTS visitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Using the model</head><p>The first use of a model is as an auxiliary task. We implement this by conditioning the model not on a raw environment state s t but, instead, on the activations h(s t ) from a hidden layer of the policy network. Gradients from the model loss L m are then propagated all the way into the shared encoder, to help learning good state representations.</p><p>The second use of the model is within the policy update from Eq. 9. Specifically, the model is used to estimate the action valuesq ?prior (s, a), via one-step look-ahead:</p><formula xml:id="formula_11">q ?prior (s, a) =r 1 (s, a) + ?v 1 (s, a),<label>(14)</label></formula><p>and the model-based action values are then used in two ways. First, they are used to estimate the multi-step return G v (s, A) in Eq. 10, by combining action values and observed rewards using the Retrace estimator . Second, the action values are used in the (nonstochastic) advantage estimate? dv(s, a) =q ?prior (s, a) ? v ?prior (s) required by the regularisation term in Eq. 11.</p><p>Using the model to compute the ? CMPO target instead of using it to construct the search-based policy ? MCTS has advantages: a fast analytical formula, stochastic estimation of KL(? CMPO , ?) in large action spaces (2b), and direct support for continuous actions (2c). In contrast, MuZero's targets ? MCTS are only an approximate solution to regularized policy optimization <ref type="bibr" target="#b16">(Grill et al., 2020)</ref>, and the approximation can be crude when using few simulations.</p><p>Note that we could have also used deep search to estimate action-values, and used these in the proposed update. Deep search would however be computationally expensive, and may require more accurate models to be effective (3b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Normalization</head><p>CMPO avoids overly large changes but does not prevent updates from becoming vanishingly small due to small  Without clipping, we found that performance degraded quickly as the scale increased. In contrast, with CMPO, performance was almost unaffected by scales ranging from 10 ?3 to 10 3 . advantages. To increase robustness to reward scales (3c), we divide advantages? dv(s, a) by the standard deviation of the advantage estimator. A similar normalization was used in PPO <ref type="bibr" target="#b62">(Schulman et al., 2017)</ref></p><formula xml:id="formula_12">, but we estimate E St,At (G v (S t , A t ) ?v ?prior (S t )</formula><p>) 2 using moving averages, to support small batches. Normalized advantages do not become small, even when the policy is close to optimal; for convergence, we rely on learning rate decay.</p><p>All policy components can be normalized using this approach, but the model also predict rewards and values, and the corresponding losses could be sensitive to reward scales. To avoid having to tune, per game, the weighting of these unnormalized components (4c), (4d), we compute losses in a non-linearly transformed space <ref type="bibr" target="#b50">(Pohlen et al., 2018;</ref>, using the categorical reparametrization introduced by MuZero .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">An empirical study</head><p>In this section, we investigate empirically the policy updates described in the Section 4. The full agent implementing our recommendations is named Muesli, as homage to MuZero. The Muesli policy loss is L PG+CMPO (?, s)+L m (?, s). All agents in this section are trained using the Sebulba podracer architecture <ref type="bibr" target="#b25">(Hessel et al., 2021)</ref>.</p><p>First, we use the 57 Atari games in the Arcade Learning Environment <ref type="bibr" target="#b5">(Bellemare et al., 2013)</ref> to investigate the key design choices in Muesli, by comparing it to suitable baselines and ablations. We use sticky actions to make the environments stochastic <ref type="bibr" target="#b41">(Machado et al., 2018)</ref>. To ensure comparability, all agents use the same policy network, based on the IMPALA agent <ref type="bibr">(Espeholt et al., 2018)</ref>. When applicable, the model described in Section 4.3 is parametrized by an LSTM <ref type="bibr" target="#b26">(Hochreiter &amp; Schmidhuber, 1997)</ref>, with a diagram in <ref type="figure" target="#fig_0">Figure 10</ref> in the appendix. Agents are trained using uniform experience replay, and estimate multi-step returns using Retrace .</p><p>In <ref type="figure" target="#fig_0">Figure 1a</ref> we compare the median human-normalized score on Atari achieved by Muesli to that of several baselines: policy gradients (in red), PPO (in green), MPO (in grey) and a policy gradient variant with TRPO-like KL(? b , ?) regularization (in orange). The updates for each baseline are reported in Appendix F, and the agents differed only in the policy components of the losses. In all updates we used the same normalization, and trained a MuZero-like model grounded in values and rewards. In MPO and Muesli, the policy loss included the policy model loss from Eq. 13. For each update, we separately tuned hyperparameters on 10 of the 57 Atari games. We found the performance on the full benchmark to be substantially higher for Muesli (in blue). In the next experiments we investigate how different design choices contributed to Muesli's performance.</p><p>In <ref type="figure" target="#fig_4">Figure 4</ref> we use the Atari games beam rider and gravitar to investigate advantage clipping. Here, we compare the updates that use clipped (in blue) and unclipped (in red) advantages, when first rescaling the advantages by factors ranging from 10 ?3 to 10 3 to simulate diverse return scales. Without clipping, performance was sensitive to scale, and degraded quickly when scaling advantages by a factor of 100 or more. With clipping, learning was almost unaffected by rescaling, without requiring more complicated solutions such as the constrained optimization introduced in related work to deal with this issue <ref type="bibr" target="#b0">(Abdolmaleki et al., 2018)</ref>.</p><p>In <ref type="figure" target="#fig_5">Figure 5</ref>   of direct and indirect optimization. A direct MPO update uses the ? KL(?, ? prior ) regularizer as a penalty; c.f. Mirror Descent Policy Optimization <ref type="bibr" target="#b77">(Tomar et al., 2020)</ref>. Indirect MPO first finds ? MPO from Eq. 6 and then trains the policy ? by the distillation loss KL(? MPO , ?). Note the different direction of the KLs. <ref type="bibr" target="#b82">Vieillard et al. (2020)</ref> observed that the best choice between direct and indirect MPO is problem dependent, and we found the same: compare the ordering of direct MPO (in green) and indirect CMPO (in yellow) on the two Atari games alien and robotank. In contrast, we found that the Muesli policy update (in blue) was typically able to combine the benefits of the two approaches, by performing as well or better than the best among the two updates on each of the two games. See <ref type="figure" target="#fig_0">Figure 13</ref> in the appendix for aggregate results across more games.</p><p>In <ref type="figure" target="#fig_6">Figure 6a</ref> we evaluate the importance of using multi-step bootstrapped returns and model-based action values in the policy-gradient-like component of Muesli's update. Replacing the multi-step return with an approximateq ?prior (s, a) (in red in <ref type="figure" target="#fig_6">Figure 6a</ref>) degraded the performance of Muesli (in blue) by a large amount, showing the importance of leveraging multi-step estimators. We also evaluated the role of model-based action value estimates q ? in the Retrace estimator, by comparing full Muesli to an ablation (in green) where we instead used model-free valuesv in a V-trace estimator <ref type="bibr">(Espeholt et al., 2018)</ref>. The ablation performed worse.</p><p>In <ref type="figure" target="#fig_6">Figure 6b</ref> we compare the performance of Muesli when using different numbers of actions to estimate the KL term in Eq. 9. We found that the resulting agent performed well, in absolute terms (? 300% median human normalized performance) when estimating the KL by sampling as little as a In <ref type="figure" target="#fig_7">Figure 7a</ref> we show the impact of different parts of the model loss on representation learning. The performance degraded when only training the model for one step (in green). This suggests that training a model to support deeper unrolls (5 steps in Muesli, in blue) is a useful auxiliary task even if using only one-step look-aheads in the policy update. In <ref type="figure" target="#fig_7">Figure 7a</ref> we also show that performance degraded even further if the model was not trained to output policy predictions at each steps in the future, as per Eq. 13, but instead was only trained to predict rewards and values (in red). This is consistent with the value equivalence principle (Grimm et al., 2020): a rich signal from training models to support multiple predictions is critical for this kind of models.</p><p>In <ref type="figure" target="#fig_7">Figure 7b</ref> we compare Muesli to an MCTS baseline. As in MuZero, the baseline uses MCTS both for acting and learning. This is not a canonical MuZero, though, as it uses the (smaller) IMPALA network. MCTS (in purple) performed worse than Muesli (in blue) in this regime. We ran another MCTS variant with limited search depth (in green); this was better than full MCTS, suggesting that with insufficiently large networks, the model may not be sufficiently accurate to support deep search. In contrast, Muesli performed well even with these smaller models (3b).</p><p>Since we know from the literature that MCTS can be very effective in combination with larger models, in <ref type="figure" target="#fig_0">Figure 1b</ref>   one-step look-aheads in the policy update. We note that the resulting median score matches MuZero and is substantially higher than all other published agents, see <ref type="table" target="#tab_4">Table 2</ref> to compare the final performance of Muesli to other baselines.</p><p>Next, we evaluated Muesli on learning 9x9 Go from selfplay. This requires to handle non-stationarity and a combinatorial space. It is also a domain where deep search (e.g. MCTS) has historically been critical to reach non-trivial performance. In <ref type="figure" target="#fig_8">Figure 8a</ref> we show that Muesli (in blue) still outperformed the strongest baselines from <ref type="figure" target="#fig_0">Figure 1a</ref>, as well as CMPO on its own (in yellow). All policies were evaluated against Pachi <ref type="bibr" target="#b4">(Baudi? &amp; Gailly, 2011)</ref>. Muesli reached a ?75% win rate against Pachi: to the best of our knowledge, this is the first system to do so from self-play alone without deep search. In the Appendix we report even stronger win rates against GnuGo <ref type="bibr" target="#b8">(Bump et al., 2005)</ref>.</p><p>In <ref type="figure" target="#fig_8">Figure 8b</ref>, we compare Muesli to MCTS on Go; here, Muesli's performance (in blue) fell short of that of the MCTS baseline (in purple), suggesting there is still value in using deep search for acting in some domains. This is demonstrated also by another Muesli variant that uses deep search at evaluation only. Such Muesli/MCTS[Eval] hybrid (in light blue) recovered part of the gap with the MCTS baseline, without slowing down training. For reference, with the pink vertical line we depicts published MuZero, with its even greater data efficiency thanks to more simulations, a different network, more replay, and early resignation.</p><p>Finally, we tested the same agents on MuJoCo environments in OpenAI Gym <ref type="bibr" target="#b7">(Brockman et al., 2016)</ref>, to test if Muesli can be effective on continuous domains and on smaller data budgets (2M frames). Muesli performed competitively. We refer readers to <ref type="figure" target="#fig_0">Figure 12</ref>, in the appendix, for the results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Stochastic estimation details</head><p>In the policy-gradient term in Eq. 10, we clip the importance weight ?(A|s) ? b (A|s) to be from [0, 1]. The importance weight clipping introduces a bias. To correct for it, we use ?-LOO action-dependent baselines <ref type="bibr" target="#b12">(Gruslys et al., 2018)</ref>.</p><p>Although the ?-LOO action-dependent baselines were not significant in the Muesli results, the ?-LOO was helpful for the policy gradients with the TRPO penalty ( <ref type="figure" target="#fig_0">Figure 16</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The illustrative MDP example</head><p>Here we will analyze the values and the optimal policy for the MDP from <ref type="figure" target="#fig_9">Figure 9</ref>, when using the identical state representation ?(s) = ? in all states. With the state representation ?(s), the policy is restricted to be the same in all states. Let's denote the probability of the up action by p.</p><p>Given the policy p = ?(up|?(s)), the following are the values of the different states:</p><formula xml:id="formula_13">v ? (3) = p + (1 ? p)(?1) = 2p ? 1 (15) v ? (2) = p ? (?1) + (1 ? p) = ?2p + 1 (16) v ? (1) = p ? (1 + v ? (2)) + (1 ? p)v ? (3) (17) = ?4p 2 + 5p ? 1.<label>(18)</label></formula><p>Finding the optimal policy. Our objective is to maximize the value of the initial state. That means maximizing v ? (1). We can find the maximum by looking at the derivatives. The derivative of v ? (1) with respect to the policy parameter is:</p><formula xml:id="formula_14">dv ? (1) dp = ?8p + 5.<label>(19)</label></formula><p>The second derivative is negative, so the maximum of v ? (1) is at the point where the first derivative is zero. We conclude that the maximum of v ? (1) is at p * = 5 8 . Finding the action values of the optimal policy. We will now find the q * ? (?(s), up) and q * ? (?(s), down). The q ? (?(s), a) is defined as the expected return after the ?(s), when doing the action a :</p><formula xml:id="formula_15">q ? (?(s), a) = s P ? (s |?(s))q ? (s , a),<label>(20)</label></formula><p>where P ? (s |?(s)) is the probability of being in the state s when observing ?(s).</p><p>In our example, the Q-values are:</p><formula xml:id="formula_16">q ? (?(s), up) = 1 2 (1 + v ? (2)) + 1 2 p ? (?1) + 1 2 (1 ? p) (21) = ?2p + 3 2 (22) q ? (?(s), down) = 1 2 v ? (3) + 1 2 p + 1 2 (1 ? p)(?1) (23) = 2p ? 1<label>(24)</label></formula><p>We can now substitute the p * = 5 8 in for p to find the q * ? (?(s), up) and q * ? (?(s), down):</p><formula xml:id="formula_17">q * ? (?(s), up) = 1 4 (25) q * ? (?(s), down) = 1 4 .<label>(26)</label></formula><p>We see that these Q-values are the same and uninformative about the probabilities of the optimal (memory-less) stochastic policy. This generalizes to all environments: the optimal policy gives zero probability to all actions with lower Q-values. If the optimal policy ? * (?|?(s)) at a given state representation gives non-zero probabilities to some actions, these actions must have the same Q-values q * ? (?(s), a). Bootstrapping from v ? (?(s)) would be worse. We will find the v ? (?(s)). And we will show that bootstrapping from it would be misleading. In our example, the v ? (?(s)) is:</p><formula xml:id="formula_18">v ? (?(s)) = 1 2 v ? (1) + 1 2 pv ? (2) + 1 2 (1 ? p)v ? (3)<label>(27)</label></formula><p>= ?4p 2 + 9 2 p ? 1.</p><p>We can notice that v ? (?(s)) is different from v ? (2) or v ? (3). Estimating q ? (?(s), up) by bootstrapping from v ? (?(2)) instead of v ? (2) would be misleading. Here, it is better to estimate the Q-values based on Monte-Carlo returns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The motivation behind Conservative Policy Iteration and TRPO</head><p>In this section we will show that unregularized maximization of E A??(?|s) q ?prior (s, A) on data from an older policy ? prior can produce a policy worse than ? prior . The size of the possible degradation will be related to the total variation distance between ? and ? prior . The explanation is based on the proofs from the excellent book by <ref type="bibr" target="#b2">Agarwal et al. (2020)</ref>.</p><p>As before, our objective is to maximize the expected value of the states from an initial state distribution ?:</p><formula xml:id="formula_20">J(?) = E S?? [v ? (S)] .<label>(29)</label></formula><p>It will be helpful to define the discounted state visitation distribution d ? (s) as:</p><formula xml:id="formula_21">d ? (s) = (1 ? ?)E S0?? ? t=0 ? t P (S t = s|?, S 0 ) ,<label>(30)</label></formula><p>where P (S t = s|?, S 0 ) is the probably of S t being s, if starting the episode from S 0 and following the policy ?. The scaling by (1 ? ?) ensures that d ? (s) sums to one.</p><p>From the policy gradient theorem <ref type="bibr" target="#b73">(Sutton et al., 2000)</ref> we know that the gradient of J(?) with respect to the policy parameters is</p><formula xml:id="formula_22">?J ?? = 1 1 ? ? s d ? (s) a ??(a|s) ?? q ? (s, a).<label>(31)</label></formula><p>In practice, we often train on data from an older policy ? prior . Training on such data maximizes a different function:</p><formula xml:id="formula_23">TotalAdv prior (?) = 1 1 ? ? s d ?prior (s) a ?(a|s) adv ?prior (s, a),<label>(32)</label></formula><p>where adv ?prior (s, a) = q ?prior (s, a) ? v ?prior (s) is an advantage. Notice that the states are sampled from d ?prior (s) and the policy is criticized by adv ?prior <ref type="figure">(s, a)</ref>. This happens often in the practice, if updating the policy multiple times in an episode, using a replay buffer or bootstrapping from a network trained on past data.</p><p>While maximization of TotalAdv prior (?) is more practical, we will see that unregularized maximization of TotalAdv prior (?) does not guarantee an improvement in our objective J. The J(?) ? J(? prior ) difference can be even negative, if we are not careful. <ref type="bibr" target="#b33">Kakade &amp; Langford (2002)</ref> stated a useful lemma for the performance difference:</p><p>Lemma C.1 (The performance difference lemma) For all policies ?, ? prior , J(?) ? J(? prior ) = 1 1 ? ? s d ? (s) a ?(a|s) adv ?prior (s, a).</p><p>We would like the J(?) ? J(? prior ) to be positive. We can express the performance difference as TotalAdv prior (?) plus an extra term: To get a positive J(?) ? J(? prior ) performance difference, it is not enough to maximize TotalAdv prior (?). We also need to make sure that the second term in (36) will not degrade the performance. The impact of the second term can be kept small by keeping the total variation distance between ? and ? prior small. For example, the performance can degrade, if ? is not trained at a state and that state gets a higher d ? (s) probability. The performance can also degrade, if a stochastic policy is needed and the adv ?prior advantages are for an older policy. The ? would become deterministic, if maximizing a ?(a|s) adv ?prior (s, a) without any regularization.</p><formula xml:id="formula_25">J(?) ? J(? prior ) = TotalAdv prior (?) ? TotalAdv prior (?) + 1 1 ? ? s d ? (s)</formula><p>C.1. Performance difference lower bound.</p><p>We will express a bound of the performance difference as a function of the total variation between ? and ? prior . Starting from Eq. 36, we can derive the TRPO lower bound for the performance difference. Let ? be the maximum total variation distance between ? and ? prior :</p><formula xml:id="formula_26">? = max s 1 2 a |?(a|s) ? ? prior (a|s)|.<label>(37)</label></formula><p>The d ? ? d ?prior 1 is then bounded (see <ref type="bibr" target="#b2">Agarwal et al., 2020</ref>, Similar policies imply similar state visitations):</p><formula xml:id="formula_27">d ? ? d ?prior 1 ? 2?? 1 ? ? .<label>(38)</label></formula><p>Finally, by plugging the bounds to Eq. 36, we can construct the lower bound for the performance difference:</p><formula xml:id="formula_28">J(?) ? J(? prior ) ? TotalAdv prior (?) ? 4? 2 ? max (1 ? ?) 2 ,<label>(39)</label></formula><p>where max = max s,a | adv ?prior (s, a)|. The same bound was derived in TRPO <ref type="bibr" target="#b61">(Schulman et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proof of Maximum CMPO total variation distance</head><p>We will prove the following theorem: For any clipping threshold c &gt; 0, we have: max ?prior,? dv,s D TV (? CMPO (?|s), ? prior (?|s)) = tanh( c 2 ).</p><p>Having 2 actions. We will first prove the theorem when the policy has 2 actions. To maximize the distance, the clipped advantages will be ?c and c. Let's denote the ? prior probabilities associated with these advantages as 1 ? p and p, respectively.</p><p>The total variation distance is then:</p><formula xml:id="formula_29">D TV (? CMPO (?|s), ? prior (?|s)) = p exp(c) p exp(c) + (1 ? p) exp(?c) ? p.<label>(40)</label></formula><p>We will maximize the distance with respect to the parameter p ? [0, 1].</p><p>The first derivative with respect to p is:</p><formula xml:id="formula_30">d D TV (? CMPO (?|s), ? prior (?|s)) dp = 1 (p exp(c) + (1 ? p) exp(?c)) 2 ? 1.<label>(41)</label></formula><p>The second derivative with respect to p is:</p><p>d 2 D TV (? CMPO (?|s), ? prior (?|s)) dp 2 = ?2(p exp(c) + (1 ? p) exp(?c)) ?3 (exp(c) ? exp(?c)).</p><p>Because the second derivative is negative, the distance is a concave function of p. We will find the maximum at the point where the first derivative is zero. The solution is:</p><formula xml:id="formula_32">p * = 1 ? exp(?c) exp(c) ? exp(?c) .<label>(43)</label></formula><p>max p D TV (? CMPO (?|s), ? prior (?|s)) = exp(c) ? 1 exp(c) + 1 = tanh( c 2 ).</p><p>This completes the proof when having 2 actions.</p><p>Having any number of actions. We will now prove the theorem when the policy has any number of actions. To maximize the distance, the clipped advantages will be ?c or c. Let's denote the sum of ? prior probabilities associated with these advantages as 1 ? p and p, respectively.</p><p>The total variation distance is again:</p><formula xml:id="formula_34">D TV (? CMPO (?|s), ? prior (?|s)) = p exp(c) p exp(c) + (1 ? p) exp(?c) ? p,<label>(45)</label></formula><p>and the maximum distance is again tanh( c 2 ). We also verified the theorem predictions experimentally, by using gradient ascent to maximize the total variation distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Extended related work</head><p>We used the desiderata to motivate the design of the policy update. We will use the desiderata again to discuss the related methods to satisfy the desiderata. For a comprehensive overview of model-based reinforcement learning, we recommend the surveys by <ref type="bibr" target="#b44">Moerland et al. (2020)</ref> and <ref type="bibr" target="#b20">Hamrick (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Observability and function approximation</head><p>1a) Support learning stochastic policies. The ability to learn a stochastic policy is one of the benefits of policy gradient methods.</p><p>1b) Leverage Monte-Carlo targets. Muesli uses multi-step returns to train the policy network and Q-values. MPO and MuZero need to train the Q-values, before using the Q-values to train the policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Policy representation</head><p>2a) Support learning the optimal memory-less policy. Muesli represents the stochastic policy by the learned policy network. In principle, acting can be based on a combination of the policy network and the Q-values. For example, one possibility is to act with the ? CMPO policy. ACER <ref type="bibr" target="#b83">(Wang et al., 2016)</ref> used similar acting based on ? MPO . Although we have not seen benefits from acting based on ? CMPO on Atari <ref type="figure" target="#fig_0">(Figure 15</ref>), we have seen better results on Go with a deeper search at the evaluation time.</p><p>2b) Scale to (large) discrete action spaces. Muesli supports large actions spaces, because the policy loss can be estimated by sampling. MCTS is less suitable for large action spaces. This was addressed by <ref type="bibr" target="#b16">Grill et al. (2020)</ref>, who brilliantly revealed MCTS as regularized policy optimization and designed a tree search based on MPO or a different regularized policy optimization. The resulting tree search was less affected by a small number of simulations. Muesli is based on this view of regularized policy optimization as an alternative to MCTS. In another approach, MuZero was recently extended to support sampled actions and continuous actions .</p><p>2c) Scale to continuous action spaces. Although we used the same estimator of the policy loss for discrete and continuous actions, it would be possible to exploit the structure of the continuous policy. For example, the continuous policy can be represented by a normalizing flow <ref type="bibr" target="#b48">(Papamakarios et al., 2019)</ref> to model the joint distribution of the multi-dimensional actions. The continuous policy would also allow to estimate the gradient of the policy regularizer with the reparameterization trick <ref type="bibr" target="#b36">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b52">Rezende et al., 2014)</ref>. Soft Actor-Critic <ref type="bibr" target="#b18">(Haarnoja et al., 2018)</ref> and <ref type="bibr">TD3 (Fujimoto et al., 2018)</ref> achieved great results on the Mujoco tasks by obtaining the gradient with respect to the action from an ensemble of approximate Q-functions. The ensemble of Q-functions would probably improve Muesli results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Robust learning</head><p>3a) Support off-policy and historical data. Muesli supports off-policy data thanks to the regularized policy optimization, Retrace  and policy gradients with clipped importance weights <ref type="bibr" target="#b12">(Gruslys et al., 2018)</ref>. Many other methods deal with off-policy or offline data <ref type="bibr" target="#b38">(Levine et al., 2020)</ref>. Recently MuZero Reanalyse  achieved state-of-the-art results on an offline RL benchmark by training only on the offline data.</p><p>3b) Deal gracefully with inaccuracies in the values/model. Muesli does not trust fully the Q-values from the model. Muesli combines the Q-values with the prior policy to propose a new policy with a constrained total variation distance from the prior policy. Without the regularized policy optimization, the agent can be misled by an overestimated Q-value for a rarely taken action. Soft Actor-Critic <ref type="bibr" target="#b18">(Haarnoja et al., 2018) and</ref><ref type="bibr">TD3 (Fujimoto et al., 2018)</ref> mitigate the overestimation by taking the minimum from a pair of Q-networks. In model-based reinforcement learning an unrolled one-step model would struggle with compounding errors <ref type="bibr" target="#b30">(Janner et al., 2019)</ref>. VPN <ref type="bibr" target="#b47">(Oh et al., 2017)</ref> and MuZero  avoid compounding errors by using multi-step predictions P (R t+k+1 |s t , a t , a t+1 , . . . , a t+k ), not conditioned on previous model predictions. While VPN and MuZero avoid compounding errors, these models are not suitable for planning a sequence of actions in a stochastic environment. In the stochastic environment, the sequence of actions needs to depend on the occurred stochastic events, otherwise the planning is confounded and can underestimate or overestimate the state value <ref type="bibr" target="#b53">(Rezende et al., 2020)</ref>. Other models conditioned on limited information from generated (latent) variables can face similar problems on stochastic environment (e.g. DreamerV2 ). Muesli is suitable for stochastic environments, because Muesli uses only one-step look-ahead. If combining Muesli with a deep search, we can use an adaptive search depth or a stochastic model sufficient for causally correct planning <ref type="bibr" target="#b53">(Rezende et al., 2020)</ref>. Another class of models deals with model errors by using the model as a part of the Q-network or policy network and trains the whole network end-to-end. These networks include VIN <ref type="bibr" target="#b75">(Tamar et al., 2016)</ref>, Predictron , I2A , IBP , <ref type="bibr">TreeQN, ATreeC (Farquhar et al., 2018)</ref> (with scores in <ref type="table" target="#tab_5">Table 3</ref>), ACE <ref type="bibr" target="#b90">(Zhang &amp; Yao, 2019)</ref>, UPN <ref type="bibr" target="#b69">(Srinivas et al., 2018)</ref> and implicit planning with DRC <ref type="bibr" target="#b13">(Guez et al., 2019)</ref>. 3c) Be robust to diverse reward scales. Muesli benefits from the normalized advantages and from the advantage clipping inside ? CMPO . Pop-Art <ref type="bibr" target="#b80">(van Hasselt et al., 2016)</ref> addressed learning values across many orders of magnitude. On Atari, the score of the games vary from 21 on Pong to 1M on Atlantis. The non-linear transformation by <ref type="bibr" target="#b50">Pohlen et al. (2018)</ref> is practically very helpful, although biased for stochastic returns.</p><p>3d) Avoid problem-dependent hyperparameters. The normalized advantages were used before in PPO <ref type="bibr" target="#b62">(Schulman et al., 2017)</ref>. The maximum CMPO total variation (Theorem 4.1) helps to explain the success of such normalization. If the normalized advantages are from [?c, c], they behave like advantages clipped to <ref type="bibr">[?c, c]</ref>. Notice that the regularized policy optimization with the popular ?H[?] entropy regularizer is equivalent to MPO with uniform ? prior (because ?H[?] = KL(?, ? uniform ) + const.). As a simple modification, we recommend to replace the uniform prior with ? prior based on a target network. That leads to the model-free direct MPO with normalized advantages, outperforming vanilla policy gradients (compare <ref type="figure" target="#fig_0">Figure 13</ref> to <ref type="figure" target="#fig_0">Figure 1a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4. Rich representation of knowledge</head><p>4a) Estimate values (variance reduction, bootstrapping). In Muesli, the learned values are helpful for bootstrapping Retrace returns, for computing the advantages and for constructing the ? CMPO . Q-values can be also helpful inside a search, as demonstrated by <ref type="bibr" target="#b21">Hamrick et al. (2020a)</ref>.</p><p>4b) Learn a model <ref type="bibr">(representation, composability)</ref>. Multiple works demonstrated benefits from learning a model. Like <ref type="bibr">VPN and MuZero, Gregor et al. (2019)</ref> learns a multi-step action-conditional model; they learn the distribution of observations instead of actions and rewards, and focus on the benefits of representation learning in model-free RL induced by modellearning; see also <ref type="bibr" target="#b15">(Guo et al., 2018;</ref><ref type="bibr" target="#b16">Guo et al., 2020)</ref>. <ref type="bibr" target="#b68">Springenberg et al. (2020)</ref> study an algorithm similar to MuZero with an MPO-like learning signal on the policy (similarly to <ref type="bibr">SAC and Grill et al. (2020)</ref>) and obtain strong results on Mujoco tasks in a transfer setting. <ref type="bibr" target="#b9">Byravan et al. (2020)</ref> use a multi-step action model to derive a learning signal for policies on continuous-valued actions, leveraging the differentiability of the model and of the policy. <ref type="bibr" target="#b32">Kaiser et al. (2019)</ref> show how to use a model for increasing data-efficiency on Atari (using an algorithm similar to Dyna <ref type="bibr" target="#b71">(Sutton, 1990)</ref>), but see also  for the relation between parametric model and replay. Finally, <ref type="bibr" target="#b22">Hamrick et al. (2020b)</ref> investigate drivers of performance and generalization in MuZero-like algorithms.   <ref type="bibr">(Espeholt et al., 2018)</ref>. Like the LASER agent , we increase the number of channels 4-times. Specifically, the numbers of channels are: <ref type="bibr">(64,</ref><ref type="bibr">128,</ref><ref type="bibr">128,</ref><ref type="bibr">64)</ref>, followed by a fully connected layer and LSTM <ref type="bibr" target="#b26">(Hochreiter &amp; Schmidhuber, 1997)</ref> with 512 hidden units. This LSTM inside of the IMPALA representation network is different from the second LSTM used inside the model dynamics function, described later. In the Atari experiments, the network takes as the input one RGB frame. Stacking more frames would help as evidenced in <ref type="figure" target="#fig_0">Figure 17</ref>.</p><p>Q-network and model architecture. The original IMPALA agent was not learning a Q-function. Because we train a MuZero-like model, we can estimate the Q-values by:</p><formula xml:id="formula_35">q(s, a) =r 1 (s, a) + ?v 1 (s, a),<label>(46)</label></formula><p>wherer 1 (s, a) andv 1 (s, a) are the reward model and the value model, respectively. The reward model and the value model are based on MuZero dynamics and prediction functions . We use a very small dynamics function, consisting of a single LSTM layer with 1024 hidden units, conditioned on the selected action ( <ref type="figure" target="#fig_0">Figure 10</ref>).</p><p>The decomposition ofq(s, a) to a reward model and a value model is not crucial. The Muesli agent obtained a similar score with a model of the q ? (s, a) action-values ( <ref type="figure" target="#fig_0">Figure 14)</ref>.</p><p>Value model and reward model losses. Like in MuZero , the value model and the reward model are trained by categorical losses. The target for the value model is the multi-step return estimate provided by Retrace . Inside of the Retrace, we useq ?prior (s, a) action-values provided by the target network.</p><p>Optimizer. We use the Adam optimizer <ref type="bibr" target="#b35">(Kingma &amp; Ba, 2014)</ref>, with the decoupled weight decay by <ref type="bibr" target="#b40">(Loshchilov &amp; Hutter, 2017)</ref>. The learning rate is linearly decayed to reach zero at the end of the training. We do not clip the norm of the gradient. Instead, we clip the parameter updates to [?1, 1], before multiplying them with the learning rate. In Adam's notation, the update rule is:</p><formula xml:id="formula_36">? t = ? t?1 + ? clip(m t ?v t + , ?1, 1),<label>(47)</label></formula><p>wherem t andv t are the estimated moments, not value functions.</p><p>Replay. As observed by , the LASER agent benefited from mixing replay data with on-policy data in each batch. Like LASER, we also use uniform replay and mix replay data with on-policy data. To obtain results comparable with other methods, we do not use LASER's shared experience replay and hence compare to the LASER version that did not share experience either.</p><p>Evaluation. On Atari, the human-normalized score is computed at 200M environment frames (including skipped frames). The episode returns are collected from last 200 training episodes that finished before the 200M environment frames. This is the same evaluation as used by MuZero. The replayed frames are not counted in the 200M frame limit. For example, if replayed frames form 95% of each batch, the agent is trained for 20-times more steps than an agent with no replay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Muesli policy update</head><p>The Muesli policy loss usage is summarized in Algorithm 1.</p><p>Prior policy. We use a target network to approximate v ?prior , q ?prior and ? prior . Like the target network in DQN <ref type="bibr" target="#b42">(Mnih et al., 2015)</ref>, the target network contains older network parameters. We use an exponential moving average to continuously update the parameters of the target network.</p><p>In general, the ? prior can be represented by a mixture of multiple policies. When forming ? CMPO , we represented ? prior by the target network policy mixed with a small proportion of the uniform policy (0.3%) and the behavior policy (3%). Mixing with these policies was not a significant improvement to the results <ref type="figure" target="#fig_0">(Figure 18</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. Hyperparameters</head><p>On Atari, the experiments used the Arcade Learning Environment <ref type="bibr" target="#b5">(Bellemare et al., 2013)</ref> with sticky actions. The environment parameters are listed in <ref type="table">Table 4</ref>.</p><p>The hyperparameters shared by all policy updates are listed in <ref type="table">Table 5</ref>. When comparing the clipped and unclipped advantages in <ref type="figure" target="#fig_4">Figure 4</ref>, we estimated the KL(? CMPO , ?) with exact KL. The unclipped advantages would have too large variance without the exact KL.</p><p>The hyperparameters for the large-scale Atari experiments are in <ref type="table" target="#tab_8">Table 6</ref>, hyperparameters for 9x9 Go self-play are in <ref type="table" target="#tab_9">Table 7</ref> and hyperparameters for continuous control on MuJoCo are in <ref type="table" target="#tab_10">Table 8</ref>. On Go, the discount ? = ?1 allows to train by self-play on the two-player perfect-information zero-sum game with alternate moves without modifying the reinforcement learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4. Policy losses</head><p>We will explain the other compared policy losses here. When comparing the different policy losses, we always used the same network architecture and the same reward model and value model training. The advantages were always normalized.</p><p>The hyperparameters for all policy losses are listed in <ref type="table">Table 9</ref>. We tuned the hyperparameters for all policy losses on 10 Atari games (alien, beam rider, breakout, gravitar, hero, ms pacman, phoenix, robotank, seaquest and time pilot). For each hyperparameter we tried multiples of 3 (e.g., 0.1, 0.3, 1.0, 3.0). For the PPO clipping threshold, we explored 0.2, 0.25, 0.3, 0.5, 0.8.</p><p>Policy gradients (PG). The simplest tested policy loss uses policy gradients with the entropy regularizer, as in (Mnih et al.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Agent with Muesli policy loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization:</head><p>Initialize the estimate of the variance of the advantage estimator: var := 0 ? product := 1.0 Initialize ? prior parameters with the ? parameters:</p><p>? ?prior := ? ? Data collection on an actor: For each step: Observe state s t and select action a t ? ? prior (?|s t ).</p><p>Execute a t in the environment. Append s t , a t , r t+1 , ? t+1 to the replay buffer. Append s t , a t , r t+1 , ? t+1 to the online queue. Compute the total loss: L total = ( L PG+CMPO (?, s) // Regularized policy optimization, Eq. 9. + L m (?, s) // Policy model loss, Eq. 13. + L v (v ? , s) + L r (r ? , s)) // MuZero value and reward losses. Use L total to update ? ? by one step of gradient descent. Use a moving average of ? parameters as ? prior parameters: ? ?prior := (1 ? ? target )? ?prior + ? target ? ? 2016). The loss is defined by</p><formula xml:id="formula_37">L PG (?, s) = ?E A??(?|s) ? dv(s, A) ? ? H H[?(?|s)].<label>(48)</label></formula><p>Policy gradients with the TRPO penalty. The next policy loss uses KL(? b (?|s), ?(?|s)) inside the regularizer. The ? b is the behavior policy. This policy loss is known to work as well as PPO <ref type="bibr" target="#b10">(Cobbe et al., 2020)</ref>.</p><p>L PG+TRPOpenalty (?, s) = ?E A??(?|s) ? dv(s, A) ? ? H H[?(?|s)] + ? TRPO KL(? b (?|s), ?(?|s)).</p><p>Proximal Policy Optimization (PPO). PPO <ref type="bibr" target="#b62">(Schulman et al., 2017)</ref> is usually used with multiple policy updates on the same batch of data. In our setup, we use a replay buffer instead. PPO then required a larger clipping threshold PPO . In our setup, the policy gradient with the TRPO penalty is a stronger baseline. </p><formula xml:id="formula_39">L PPO (?, s) = ?E A?? b (?|s) min( ?(A|s) ? b (A|s)? dv(s, A), clip( ?(A|s) ? b (A|s) , 1 ? PPO , 1 + PPO )? dv(s, A)) ? ? H H[?(?|s)].<label>(50)</label></formula><p>Maximum a Posteriori Policy Optimization (MPO). We use a simple variant of MPO <ref type="bibr" target="#b0">(Abdolmaleki et al., 2018)</ref> that is not specialized to Gaussian policies. Also, we use ? MPO (?|s t+k ) as the target for the policy model.</p><formula xml:id="formula_40">L MPO (?, s t ) = KL(? MPO (?|s t ), ?(?|s t )) + 1 K K k=1 KL(? MPO (?|s t+k ), ? k (?|s t , a &lt;t+k ))<label>(51)</label></formula><formula xml:id="formula_41">s.t. E S?d? b [KL(? MPO (?|S), ?(?|S))] &lt; MPO .<label>(52)</label></formula><p>Direct MPO. Direct MPO uses the MPO regularizer ? KL(?, ? prior ) as a penalty.</p><p>L DirectMPO (?, s) = ?E A??(?|s) ? dv(s, A) + ? KL(?(?|s), ? prior (?|s)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5. Go experimental details</head><p>The Go environment was configured using OpenSpiel <ref type="bibr" target="#b37">(Lanctot et al., 2019)</ref>. Games were scored with the Tromp-Taylor rules with a komi of 7.5. Observations consisted of the last 2 board positions, presented with respect to the player in three  9x9 planes each (player's stones, opponent's stones, and empty intersections), in addition to a plane indicating the player's color. The agents were evaluated against GnuGo v3.8 at level 10 <ref type="bibr" target="#b8">(Bump et al., 2005)</ref> and Pachi v11.99 <ref type="bibr" target="#b4">(Baudi? &amp; Gailly, 2011)</ref> with 10,000 simulations, 16 threads, and no pondering. Both were configured with the Chinese ruleset. <ref type="figure" target="#fig_0">Figure 11</ref> shows the results versus GnuGo. <ref type="table" target="#tab_11">Table 10</ref> lists the median and mean human-normalized score across 57 Atari games. The table also lists the differences in the number of stacked frames, the amount of replay and the probability of a sticky action. The environment with a non-zero probability of a sticky action is more challenging by being stochastic <ref type="bibr" target="#b41">(Machado et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Additional experiments</head><p>In <ref type="figure" target="#fig_0">Figure 15</ref> we compare the different ways to act and explore during training. Muesli (in blue) acts by sampling actions from the policy network. Acting proportionally to ? CMPO was not significantly different (in green). Acting based on the Q-values only was substantially worse (in red). This is consistent with our example from <ref type="figure" target="#fig_1">Figure 2</ref> where acting with Q-values would be worse. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HYPERPARAMETER VALUE</head><p>Replay proportion in a batch 95.8% <ref type="table">Table 9</ref>. Hyperparameters for the different policy losses.    <ref type="figure" target="#fig_0">Figure 15</ref>. Median score across 57 Atari games for different ways to act and explore. Acting with ?CMPO was not significantly different. Acting with softmax(q/temperature) was worse. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HYPERPARAMETER</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Median human normalized score across 57 Atari games.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>An episodic MDP with 4 states. State 1 is the initial state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>A comparison (on two Atari games) of the robustness of clipped and unclipped MPO agents to the scale of the advantages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>A comparison (on two Atari games) of direct and indirect optimization. Whether direct MPO (in green) or indirect CMPO (in yellow) perform best depends on the environment. Muesli, however, typically performs as well or better than either one of them. The aggregate score across the 57 games for Muesli, direct MPO and CMPO are reported inFigure 13of the appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Median score across 57 Atari games. (a) Return ablations: 1) Retrace or V-trace, 2) training the policy with multi-step returns or withq? prior (s, a) only (in red). (b) Different numbers of samples to estimate the KL(?CMPO, ?). The "1 sample, oracle" (pink) used the exact zCMPO(s) normalizer, requiring to expand all actions. The ablations were run with 2 random seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Median score across 57 Atari games. (a) Muesli ablations that train one-step models (in green), or drop the policy component of the model (in red). (b) Muesli and two MCTSbaselines that act sampling from ?MCT S and learn using ?MCT S as target; all use the IMPALA policy network and an LSTM model. single action (brown). Performance increased by sampling up to 16 actions, which was then comparable the exact KL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Win probability on 9x9 Go when training from scratch, by self-play, for 5B frames. Evaluating 3 seeds against Pachi with 10K simulations per move. (a) Muesli and other search-free baselines. (b) MuZero MCTS with 150 simulations and Muesli with and without the use of MCTS at the evaluation time only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>The episodic MDP from Figure 2, reproduced here for an easier reference. State 1 is the initial state. State 4 is terminal. The discount is 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>s (d ? (s) ? d ?prior (s))a (?(a|s) ? ? prior (a|s)) adv ?prior (s, a). (36)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>The model architecture when using the IMPALA-based representation network. Ther1(st, at) predicts the reward E[Rt+1|st, at]. Thev1(st, at) predicts the value E[v?(St+1)|st, at]. In general,r k (st, a &lt;t+k ) predicts the reward E[R t+k |st, a &lt;t+k ]. Andv k (st, a &lt;t+k ) predicts the value E[v?(S t+k )|st, a &lt;t+k ].F. Experimental details F.1. Common parts Network architecture. The large MuZero network is used only on the large scale Atari experiments (Figure 1b) and on Go. In all other Atari and MuJoCo experiments the network architecture is based on the IMPALA architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 .Figure 13 .Figure 14 .</head><label>111314</label><figDesc>Win probability on 9x9 Go when training from scratch, by self-play, for 5B frames. Evaluating 3 seeds against GnuGo (level 10). (a) Muesli and other search-free baselines. (b) MuZero MCTS with 150 simulations and Muesli with and without the use of MCTS at the evaluation time only. Median score of across 57 Atari games for different MPO variants. CMPO is MPO with clipped advantages and no constrained optimization. Median score of Muesli across 57 Atari games when modeling the reward and value or when modeling the q?(s, a) directly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 16 .Figure 17 .Figure 18 .</head><label>161718</label><figDesc>Median score across 57 Atari games when using or not using ?-LOO action dependent baselines. Median score across 57 Atari games for different numbers of stacked frames. Median score across 57 Atari games for different ?prior compositions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Median human-normalized score across 57 Atari games from the ALE, at 200M frames, for several published baselines. These results are sourced from different papers, thus the agents differ along multiple dimensions (e.g. network architecture and amount of experience replay). MuZero and Muesli both use a very similar network, the same proportion of replay, and both use the harder version of the ALE with sticky actions<ref type="bibr" target="#b41">(Machado et al., 2018)</ref>. The ? denotes the standard error over 2 random seeds. Ostrovski, G.,Silver, D., and Munos, R. Implicit  Quantile Networks for Distributional Reinforcement Learning. In International Conference on Machine Learning, pp. 1096-1105, 2018.</figDesc><table><row><cell>AGENT</cell><cell>MEDIAN</cell></row><row><cell>DQN (Mnih et al., 2015)</cell><cell>79%</cell></row><row><cell>DreamerV2 (Hafner et al., 2020)</cell><cell>164%</cell></row><row><cell>IMPALA (Espeholt et al., 2018)</cell><cell>192%</cell></row><row><cell>Rainbow (Hessel et al., 2018)</cell><cell>231%</cell></row><row><cell>Meta-gradient{?, ?} (Xu et al., 2018)</cell><cell>287%</cell></row><row><cell>STAC (Zahavy et al., 2020)</cell><cell>364%</cell></row><row><cell>LASER (Schmitt et al., 2020)</cell><cell>431%</cell></row><row><cell cols="2">MuZero Reanalyse (Schrittwieser et al., 2021) 1,047 ?40%</cell></row><row><cell>Muesli</cell><cell>1,041 ?40%</cell></row><row><cell>6. Conclusion</cell><cell></cell></row><row><cell cols="2">Starting from our desiderata for general policy optimiza-</cell></row><row><cell cols="2">tion, we proposed an update (Muesli), that combines policy</cell></row><row><cell cols="2">gradients with Maximum a Posteriori Policy Optimization</cell></row><row><cell cols="2">(MPO) and model-based action values. We empirically eval-</cell></row><row><cell cols="2">uated the contributions of each design choice in Muesli,</cell></row><row><cell cols="2">and compared the proposed update to related ideas from</cell></row><row><cell cols="2">the literature. Muesli demonstrated state of the art perfor-</cell></row><row><cell cols="2">mance on Atari (matching MuZero's most recent results),</cell></row><row><cell cols="2">without the need for deep search. Muesli even outperformed</cell></row><row><cell cols="2">MCTS-based agents, when evaluated in a regime of smaller</cell></row><row><cell cols="2">networks and/or reduced computational budgets. Finally,</cell></row><row><cell cols="2">we found that Muesli could be applied out of the box to self-</cell></row><row><cell cols="2">play 9x9 Go and continuous control problems, showing the</cell></row><row><cell cols="2">generality of the update (although further research is needed</cell></row><row><cell cols="2">to really push the state of the art in these domains). We hope</cell></row><row><cell cols="2">that our findings will motivate further research in the rich</cell></row><row><cell cols="2">space of algorithms at the intersection of policy gradient</cell></row><row><cell cols="2">methods, regularized policy optimization and planning.</cell></row></table><note>Fujimoto, S., Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. In Interna- tional Conference on Machine Learning, pp.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>The mean score from the last 100 episodes at 40M frames on games used by TreeQN and ATreeC. The agents differ along multiple dimensions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>B with sequences from the online queue and the replay buffer. Use Retrace to estimate each return G v (s, a), bootstrapping fromq ?prior . Estimate the variance of the advantage estimator:var := ? var var + (1 ? ? var ) 1</figDesc><table><row><cell cols="3">Training on a learner:</cell></row><row><cell cols="3">For each minibatch:</cell></row><row><cell cols="3">Form a minibatch |B|</cell><cell>(s,a)?B (G v (s, a) ?v ?prior (s)) 2</cell></row><row><cell cols="3">Compute the bias-corrected variance estimate in Adam's style:</cell></row><row><cell cols="3">? product := ? product ? var</cell></row><row><cell>var :=</cell><cell cols="2">var 1?? product</cell></row><row><cell cols="3">Prepare the normalized advantages:</cell></row><row><cell cols="2">adv(s, a) =q</cell><cell>? prior (s,a)?v? prior (s) ? var+ var</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Atari parameters. In general, we follow the recommendations by<ref type="bibr" target="#b41">Machado et al. (2018)</ref>. Hyperparameters shared by all experiments.</figDesc><table><row><cell>PARAMETER</cell><cell>VALUE</cell></row><row><cell>Random modes and difficulties</cell><cell>No</cell></row><row><cell>Sticky action probability ?</cell><cell>0.25</cell></row><row><cell>Start no-ops</cell><cell>0</cell></row><row><cell>Life information</cell><cell>Not allowed</cell></row><row><cell>Action set</cell><cell>18 actions</cell></row><row><cell>Max episode length</cell><cell>30 minutes (108,000 frames)</cell></row><row><cell>Observation size</cell><cell>96 ? 96</cell></row><row><cell>Action repetitions</cell><cell>4</cell></row><row><cell>Max-pool over last N action repeat frames</cell><cell>4</cell></row><row><cell cols="2">Total environment frames, including skipped frames 200M</cell></row><row><cell>HYPERPARAMETER</cell><cell>VALUE</cell></row><row><cell>Batch size</cell><cell>96 sequences</cell></row><row><cell>Sequence length</cell><cell>30 frames</cell></row><row><cell>Model unroll length K</cell><cell>5</cell></row><row><cell>Replay proportion in a batch</cell><cell>75%</cell></row><row><cell>Replay buffer capacity</cell><cell>6,000,000 frames</cell></row><row><cell>Initial learning rate</cell><cell>3 ? 10 ?4</cell></row><row><cell>Final learning rate</cell><cell>0</cell></row><row><cell>AdamW weight decay</cell><cell>0</cell></row><row><cell>Discount</cell><cell>0.995</cell></row><row><cell>Target network update rate ?target</cell><cell>0.1</cell></row><row><cell>Value loss weight</cell><cell>0.25</cell></row><row><cell>Reward loss weight</cell><cell>1.0</cell></row><row><cell cols="2">Retrace EA??[q? prior (s, A)] estimator 16 samples</cell></row><row><cell>KL(?CMPO, ?) estimator</cell><cell>16 samples</cell></row><row><cell>Variance moving average decay ?var</cell><cell>0.99</cell></row><row><cell>Variance offset var</cell><cell>10 ?12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Modified hyperparameters for large-scale Atari experiments. The network architecture, discount and replay proportion are based on MuZero Reanalyze.</figDesc><table><row><cell>HYPERPARAMETER</cell><cell>VALUE</cell></row><row><cell>Network architecture</cell><cell>MuZero net with 16 ResNet blocks</cell></row><row><cell>Stacked frames</cell><cell>16</cell></row><row><cell>Batch size</cell><cell>768 sequences</cell></row><row><cell cols="2">Replay proportion in a batch 95%</cell></row><row><cell>Replay buffer capacity</cell><cell>28,800,000 frames</cell></row><row><cell>AdamW weight decay</cell><cell>10 ?4</cell></row><row><cell>Discount</cell><cell>0.997</cell></row><row><cell>Retrace ?</cell><cell>0.95</cell></row><row><cell>KL(?CMPO, ?) estimator</cell><cell>exact KL</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>Modified hyperparameters for 9x9 Go self-play experiments.</figDesc><table><row><cell>HYPERPARAMETER</cell><cell>VALUE</cell></row><row><cell>Network architecture</cell><cell>MuZero net with 6 ResNet blocks</cell></row><row><cell>Batch size</cell><cell>192 sequences</cell></row><row><cell>Sequence length</cell><cell>49 frames</cell></row><row><cell>Replay proportion in a batch</cell><cell>0%</cell></row><row><cell>Initial learning rate</cell><cell>2 ? 10 ?4</cell></row><row><cell cols="2">Target network update rate ?target 0.01</cell></row><row><cell>Discount</cell><cell>-1 (self-play)</cell></row><row><cell>Multi-step return estimator</cell><cell>V-trace</cell></row><row><cell>V-trace ?</cell><cell>0.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Modified hyperparameters for MuJoCo experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 .</head><label>10</label><figDesc>Median and mean human-normalized score across 57 Atari games, after 200M environment frames. The agents differ in network size, amount of replay, the probability of a sticky action and agent training. The ? indicates the standard error across 2 random seeds. While DreamerV2 was not evaluated on defender and surround, DreamerV2 median score remains valid on 57 games, if we assume a high DreamerV2 score on defender.</figDesc><table><row><cell cols="3">AGENT</cell><cell></cell><cell></cell><cell>MEDIAN</cell><cell></cell><cell>MEAN</cell><cell cols="2">STACKED FRAMES REPLAY STICKY ACTION</cell></row><row><cell cols="4">DQN (Mnih et al., 2015)</cell><cell></cell><cell>79%</cell><cell></cell><cell>-</cell><cell></cell><cell>4</cell><cell>87.5%</cell><cell>0.0</cell></row><row><cell cols="5">IMPALA (Espeholt et al., 2018)</cell><cell>192%</cell><cell></cell><cell>958%</cell><cell></cell><cell>4</cell><cell>0%</cell><cell>0.0</cell></row><row><cell cols="4">IQN (Dabney et al., 2018)</cell><cell></cell><cell>218%</cell><cell></cell><cell>-</cell><cell></cell><cell>4</cell><cell>87.5%</cell><cell>0.0</cell></row><row><cell cols="5">Rainbow (Hessel et al., 2018)</cell><cell>231%</cell><cell></cell><cell>-</cell><cell></cell><cell>4</cell><cell>87.5%</cell><cell>0.0</cell></row><row><cell cols="5">Meta-gradient{?, ?} (Xu et al., 2018)</cell><cell>287%</cell><cell></cell><cell>-</cell><cell></cell><cell>4</cell><cell>0%</cell><cell>0.0</cell></row><row><cell cols="5">LASER (Schmitt et al., 2020)</cell><cell>431%</cell><cell></cell><cell>-</cell><cell></cell><cell>4</cell><cell>87.5%</cell><cell>0.0</cell></row><row><cell cols="5">DreamerV2 (Hafner et al., 2020)</cell><cell>164%</cell><cell></cell><cell>-</cell><cell></cell><cell>1</cell><cell>-</cell><cell>0.25</cell></row><row><cell cols="5">Muesli with IMPALA architecture</cell><cell>562 ?3%</cell><cell></cell><cell>1,981 ?66%</cell><cell></cell><cell>4</cell><cell>75%</cell><cell>0.25</cell></row><row><cell cols="5">Muesli with MuZero arch, replay=80%</cell><cell cols="2">755 ?27%</cell><cell>2,253 ?120%</cell><cell></cell><cell>16</cell><cell>80%</cell><cell>0.25</cell></row><row><cell cols="5">Muesli with MuZero arch, replay=95%</cell><cell cols="3">1,041 ?40% 2,524 ?104%</cell><cell></cell><cell>16</cell><cell>95%</cell><cell>0.25</cell></row><row><cell cols="8">MuZero Reanalyse (Schrittwieser et al., 2021) 1,047 ?40% 2,971 ?115%</cell><cell></cell><cell>16</cell><cell>95%</cell><cell>0.25</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell cols="2">9x9 Go vs GnuGo</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9x9 Go vs GnuGo</cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Win probability</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Muesli CMPO TRPO penalty PPO PG</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0 0.2</cell><cell>0</cell><cell>1000</cell><cell>2000 Millions of frames 3000 (a)</cell><cell>4000</cell><cell>5000</cell><cell>0</cell><cell>1000</cell><cell>2000 Millions of frames 3000 (b) MuZero[2021] 4000 MCTS[Act,Learn,Eval] 5000 Muesli/MCTS[Eval] Muesli</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Manuel Kroiss and Iurii Kemaev for developing the research platform we use to run and distribute experiments at scale. Also we thank Dan Horgan, Alaa Saade, Nat McAleese and Charlie Beattie for their excellent help with reinforcement learning environments. Joseph Modayil improved the paper by wise comments and advice. Coding was made fun by the amazing JAX library <ref type="bibr" target="#b6">(Bradbury et al., 2018)</ref>, and the ecosystem around it (in particular the optimisation library Optax, the neural network library Haiku, and the reinforcement learning library Rlax). We thank the MuZero team at DeepMind for inspiring us.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Maximum a Posteriori Policy Optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00261</idno>
		<title level="m">On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: Theory and Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://rltheorybook.github.io" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimal Control of Markov Processes with Incomplete State Information I</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Astr?m</surname></persName>
		</author>
		<idno>0022-247X</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="174" to="205" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">State of the art open source Go program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baudi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in computer games</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="24" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The Arcade Learning Environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>JAIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://github.com/google/jax" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Openai</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gym</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01540</idno>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bump</surname></persName>
		</author>
		<ptr target="http://www.gnu.org/software/gnugo/gnugo.html" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagined value gradients: Model-based policy optimization with tranferable latent dynamics models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Byravan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neunert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lampe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="566" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.04416</idno>
		<imprint>
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
	<note type="report_type">Phasic Policy Gradient. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Computers and Games, CG&apos;06</title>
		<meeting>the 5th International Conference on Computers and Games, CG&apos;06<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="72" to="83" />
		</imprint>
	</monogr>
	<note>ISBN 3540755373</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Reactor: A fast and sampleefficient Actor-Critic agent for Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Racani?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Orseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eccles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03559</idno>
		<title level="m">An investigation of model-free planning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Value-driven hindsight modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kapturowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gheshlaghi Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06407</idno>
		<title level="m">Neural Predictive Belief Representations. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bootstrap latent-predictive representations for multitask reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3875" to="3886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent world models facilitate policy evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2450" to="2462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hartikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05905</idno>
		<title level="m">Soft Actor-Critic Algorithms and Applications. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02193</idno>
		<title level="m">Mastering Atari with Discrete World Models. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analogues of mental simulation and imagination in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Behavioral Sciences</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8" to="16" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Combining q-learning and search with amortized value estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Behbahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Witherspoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04021</idno>
		<title level="m">On the role of planning in model-based deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rainbow</surname></persName>
		</author>
		<title level="m">Combining improvements in deep reinforcement learning. AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02908</idno>
		<title level="m">On Inductive Biases in Deep Reinforcement Learning. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kroiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Keck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<title level="m">Podracer architectures for scalable reinforcement learning</title>
		<imprint>
			<date type="published" when="2021-04" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning and Planning in Complex Action Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barekatain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-04" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reinforcement learning algorithm for partially observable Markov decision problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="345" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reinforcement learning with unsupervised auxiliary tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08253</idno>
		<title level="m">When to Trust Your Model: Model-Based Policy Optimization. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical task and motion planning in the now</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st AAAI Conference on Bridging the Gap Between Task and Motion Planning, AAAIWS&apos;10-01</title>
		<meeting>the 1st AAAI Conference on Bridging the Gap Between Task and Motion Planning, AAAIWS&apos;10-01</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Osinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Czechowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kozakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00374</idno>
		<title level="m">Model-Based Reinforcement Learning for Atari. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2019-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Approximately optimal approximate reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="267" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A natural policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1531" to="1538" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A Method for Stochastic Optimization. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-Encoding Variational Bayes. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>P?rolat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Timbers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Omidshafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hennes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kram?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Vylder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan-Davis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Openspiel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09453</idno>
		<title level="m">A Framework for Reinforcement Learning in Games</title>
		<imprint>
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
	<note>arXiv e-prints, art</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.01643</idno>
		<title level="m">Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-improving reactive agents based on reinforcement learning, planning and teaching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Lin</surname></persName>
		</author>
		<idno>0885-6125</idno>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="293" to="321" />
			<date type="published" when="1992-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled Weight Decay Regularization. arXiv e-prints, art</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Talvitie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="523" to="562" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Moerland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Broekens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Jonker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16712</idno>
		<title level="m">Modelbased Reinforcement Learning: A Survey. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Safe and efficient off-policy reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stepleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1054" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Actionconditional video prediction using deep networks in Atari games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2845" to="2853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Value prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">;</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Guyon, I</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02762</idno>
		<title level="m">Normalizing Flows for Probabilistic Modeling and Inference. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Racani?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06170</idno>
		<title level="m">Learning model-based planning from scratch. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gheshlaghi Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ve?er?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Look</forename><surname>Observe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Further</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11593</idno>
		<title level="m">Achieving Consistent Performance on Atari. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Imagination-augmented agents for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Racani?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Puigdom?nech Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">;</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Fergus, R., Vishwanathan, S., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Guyon, I</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<editor>Xing, E. P. and Jebara, T.</editor>
		<meeting>the 31st International Conference on Machine Learning<address><addrLine>Bejing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="22" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Merzic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mitrovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02836</idno>
		<title level="m">Causally Correct Partial Models for Reinforcement Learning. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Paper: Model predictive heuristic control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Testud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Papon</surname></persName>
		</author>
		<idno>0005-1098</idno>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="413" to="428" />
			<date type="published" when="1978-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Neural fitted Q iteration -first experiences with a data efficient neural reinforcement learning method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th European Conference on Machine Learning, ECML&apos;05</title>
		<meeting>the 16th European Conference on Machine Learning, ECML&apos;05<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="317" to="328" />
		</imprint>
	</monogr>
	<note>ISBN 3540292438</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">On-line Q-learning using connectionist systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Rummery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
		<idno>TR 166</idno>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Cambridge University Engineering Department, Cambridge, England</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">An on-line algorithm for dynamic reinforcement learning and planning in reactive environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/INNS International Joint Conference on Neural Networks</title>
		<meeting>IEEE/INNS International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="253" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Off-Policy Actor-Critic with Shared Experience Replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Mastering Atari, Go, chess and shogi by planning with a learned model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno>1476-4687</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">588</biblScope>
			<biblScope unit="issue">7839</biblScope>
			<biblScope unit="page" from="604" to="609" />
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Online and offline reinforcement learning by planning with a learned model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mandhane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barekatain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-04" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Trust Region Policy Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal Policy Optimization Algorithms. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Monte-Carlo Planning in Large POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2164" to="2172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deterministic policy gradient algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="387" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The predictron: End-toend learning and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dulac-Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, D. and Teh, Y. W.</editor>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning without state-estimation in partially observable Markovian decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Proceedings</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="284" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Byravan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Degrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buchli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05545</idno>
		<title level="m">Local Search for Policy Iteration in Continuous Control. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Universal planning networks: Learning generalizable representations for visuomotor control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4732" to="4741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Learning to predict by the methods of temporal differences. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Integrated architectures for learning, planning and reacting based on dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: Proceedings of the Seventh International Workshop</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Pilarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 10th International Conference on Autonomous Agents and Multiagent Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="761" to="768" />
		</imprint>
	</monogr>
	<note>International Foundation for Autonomous Agents and Multiagent Systems</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Value iteration networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Temporal difference learning and TD-Gammon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="58" to="68" />
			<date type="published" when="1995-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Efroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09814</idno>
		<title level="m">Mirror Descent Policy Optimization. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04582</idno>
		<title level="m">Learning to Predict Independent of Span. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2015-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Reinforcement learning in continuous action spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Wiering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barreto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03687</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007-07" />
			<biblScope unit="page" from="272" to="279" />
		</imprint>
	</monogr>
	<note>General non-linear Bellman equations. arXiv e-prints, art</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Learning values across many orders of magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">When to use parametric models in reinforcement learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aslanides</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="14322" to="14333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Leverage the Average: an Analysis of KL Regularization in RL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vieillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kozuno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01224</idno>
		<title level="m">Sample Efficient Actor-Critic with Experience Replay. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Learning from Delayed Rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>King&apos;s College, Cambridge, England</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Learning how the world works: Specifications for predictive networks in robots and brains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Systems, Man and Cybernetics</title>
		<meeting>IEEE International Conference on Systems, Man and Cybernetics<address><addrLine>N.Y.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Function optimization using connectionist reinforcement learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">241</biblScope>
			<date type="published" when="1991-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Meta-gradient reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2402" to="2413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">A self-tuning actorcritic algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zahavy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">ACE: An actor ensemble algorithm for continuous control with tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5789" to="5796" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
