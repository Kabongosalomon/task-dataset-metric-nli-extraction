<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FDGATII : Fast Dynamic Graph Attention with Initial Residual and Identity Mapping</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gayan</forename><forename type="middle">K</forename><surname>Kulatilleke</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Portmann</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Ko</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shekhar</forename><forename type="middle">S</forename><surname>Chandra</surname></persName>
						</author>
						<title level="a" type="main">FDGATII : Fast Dynamic Graph Attention with Initial Residual and Identity Mapping</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While Graph Neural Networks have gained popularity in multiple domains, graph-structured input remains a major challenge due to (a) oversmoothing, (b) noisy neighbours (heterophily), and (c) the suspended animation problem. To address all these problems simultaneously, we propose a novel graph neural network FDGATII, inspired by attention mechanism's ability to focus on selective information supplemented with two feature preserving mechanisms. FDGATII combines Initial Residuals and Identity Mapping with the more expressive dynamic self-attention to handle noise prevalent from the neighbourhoods in heterophilic data sets. By using sparse dynamic attention, FDGATII is inherently parallelizable in design, whist efficient in operation; thus theoretically able to scale to arbitrary graphs with ease. Our approach has been extensively evaluated on 7 datasets. We show that FDGATII outperforms GAT and GCN based benchmarks in accuracy and performance on fully supervised tasks, obtaining state-of-the-art results on Chameleon and Cornell datasets with zero domain-specific graph pre-processing, and demonstrate its versatility and fairness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, research on graphs has been receiving more and more attention due to the great expressive power and pervasiveness of graph structured data <ref type="bibr" target="#b29">(Zhu et al., 2020)</ref>. Many interesting irregular domain tasks such as 3D meshes, social networks, telecommunication networks and biological networks involve data that are not representable in gridlike structures <ref type="bibr" target="#b25">(Veli?kovi? et al., 2018)</ref>. As a unique noneuclidean data structure for machine learning <ref type="bibr" target="#b29">(Zhu et al., 2020)</ref>, graphs can be used to represent a diverse set of feature rich domains, from social and dark web forums <ref type="bibr" target="#b23">(Samtani et al., 2017)</ref> to cryptocurrency blockchains. <ref type="bibr" target="#b14">Kipf &amp; Welling (2016)</ref> predicted Facebook friend suggestions; <ref type="bibr" target="#b7">Chen et al. (2018)</ref> analysed large social attribute sets for node classification; and <ref type="bibr" target="#b23">Samtani et al. (2017)</ref> derived insights from social network structures and attribute sets to identify key players and malware exploits in dark web forums.</p><p>A Graph Neural Networks (GNN) performs neighbourhood structure aggregation along with node feature transformation to map each node to an embedding vector . GNN variants mostly differ in how each node aggregates the representations of its neighbours and combines them with its own representation <ref type="bibr" target="#b4">(Brody et al., 2021)</ref>.Graph Convolutional Networks (GCN) <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2016)</ref> generalize Convolutional Neural Networks (CNN) <ref type="bibr" target="#b16">(LeCun et al., 1995)</ref> to graph-structured data. Graph Attention Networks (GAT) <ref type="bibr" target="#b25">(Veli?kovi? et al., 2018)</ref> uses attention. Graph-Sage <ref type="bibr" target="#b10">(Hamilton et al., 2017)</ref> applies max pooling. The basic approach is to learn an aggregation of neighbour features into a low dimensional vector <ref type="bibr" target="#b28">(Zhou et al., 2020)</ref> for downstream tasks such as node classification, clustering, and link prediction <ref type="bibr" target="#b22">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b10">Hamilton et al., 2017)</ref>.</p><p>However, graph-structured inputs are one of the major challenges of machine learning <ref type="bibr" target="#b5">(Bronstein et al., 2017;</ref><ref type="bibr" target="#b10">Hamilton et al., 2017;</ref><ref type="bibr" target="#b2">Battaglia et al., 2021)</ref> due to (a) the oversmoothing problem which limits the depth and receptive field of models <ref type="bibr">(Chen et al., 2020)</ref>, (b) noise or heterophily in features and class label distribution (nodes with similar features but different in labels) <ref type="bibr" target="#b1">(Alon &amp; Yahav, 2020)</ref>, which necessitates attending to features without any prior information during the aggregation process, and (c) the suspended animation problem <ref type="bibr" target="#b26">(Wu et al., 2019)</ref>.</p><p>As most graphs require the interaction between nodes that are not directly connected this is achieved by stacking multiple GNN layers <ref type="bibr" target="#b1">(Alon &amp; Yahav, 2020)</ref>. In practice though, GCNs were observed not to benefit from more than few layers due to over-smoothing: node representations become arXiv:2110.11464v2 <ref type="bibr">[cs.</ref>LG] 25 Oct 2021 indistinguishable when the number of layers increases <ref type="bibr" target="#b26">(Wu et al., 2019)</ref>. As over-smoothing was mostly demonstrated in short-range tasks <ref type="bibr">(Chen et al., 2020)</ref>, models such as Graph Convolutional Network via Initial residual and Identity mapping (GCNII) <ref type="bibr">(Chen et al., 2020)</ref> were able to extend GCN with extra initial residual representations and identity information to achieve better results while still performing local aggregation.</p><p>However, due to unfocused uniform aggregation on the neighbourhood, most of these models, including GCNII, are more suitable only for homophily datasets where nodes linked to each other are more likely to belong in the same class, i.e. the neighbourhoods with very low noise, from an aggregation perspective. In practice, real-world graphs are also often noisy with connections between unrelated nodes resulting in poor performance in current GNNs. Also, GNNs in general are not able to handle tasks that depend on longrange information due to over-squashing: information from the exponentially growing receptive field being compressed into fixed-length node vectors <ref type="bibr" target="#b1">(Alon &amp; Yahav, 2020)</ref> due to its unfocused aggregation mechanism. As a result, whilst GCNII achieves state-of-the-art performance on homophilic datasets such as Cora, accuracy in heterophilic datasets such as Texas and Wisconsin is relatively poor ( <ref type="table" target="#tab_2">Table 2)</ref>  <ref type="bibr" target="#b29">(Zhu et al., 2020)</ref>.</p><p>Many popular GNN models implicitly assume homophily producing results that may be biased, unfair or erroneous <ref type="bibr" target="#b19">(Maurya et al., 2021)</ref>. This can result in the so-called 'filter bubble' phenomenon in a recommendation system (reinforcing existing beliefs/views, and downplaying the opposite ones), or make minority groups less visible in social networks <ref type="bibr" target="#b29">(Zhu et al., 2020)</ref>.</p><p>On the other hand, <ref type="bibr" target="#b24">Vaswani et al. (2017)</ref> showed that selfattention is sufficient for achieving state-of-the-art performance on machine translation tasks. GAT <ref type="bibr" target="#b25">(Veli?kovi? et al., 2018)</ref> generalized the attention mechanism for graphs using attention-based neighbourhood aggregation. Importantly, GAT improves on the simple averaging or max pooling of neighbours <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b10">Hamilton et al., 2017)</ref>, by allowing every node to now compute a weighted average of its neighbours <ref type="bibr" target="#b4">(Brody et al., 2021)</ref>, which is a form of selective aggregation. According to <ref type="bibr" target="#b15">Knyazev et al. (2019)</ref>, the generalization ability of attention mechanism helps GNNs generalize to larger and noisy graphs. By determining individual attention on each node neighbour, GAT is able to ignore the irrelevant neighbours and focus on the relevant neighbours <ref type="bibr" target="#b1">(Alon &amp; Yahav, 2020)</ref>. A refinement, GATv2 <ref type="bibr" target="#b4">(Brody et al., 2021)</ref>, uses a more expressive variant termed dynamic attention where the ranking of attended nodes is better conditioned on the query node by replacing the supposedly monotonic GAT attention function with a universal approximator attention function that is strictly more expres-sive. However, GAT or GATv2 alone, in its current form is not able to handle heterophilic data due to the still present essentially local aggregation operation .</p><p>Thus, it remains an open problem to design efficient GNN models that effectively prevents over-smoothing, suspended animation and noise. As observed by <ref type="bibr">Chen et al. (2020)</ref> it is even unclear whether the network depth is a resource or a burden when designing new GNNs. In this work we proposed a light weight, efficient and parallelizable model based on self attention that addresses these challenges simultaneously.</p><p>Our Fast Dynamic Graph Attention with Initial residual and Identity mapping (FDGATII), is an efficient shallow dynamic attention based model that overcomes over smoothing and noisy neighbours simultaneously by combining self attention integrated with two feature preserving mechanisms. At each layer, an initial residual constructs a 'skip connection' from the input layer, while 'identity mapping' preserves the node's identity via by adding an identity matrix to the weight matrix. Mainly, as demonstrated, nodes are aggregated based on individually focused sparse attention which is able to disregard noise and generalize well to homophilic and heterophilic datasets. FDGATII achieves comparable or state-of-the-art results on various full-supervised tasks and over an order of magnitude training and inference time efficiency with significantly low resources. In our work, we do not perform exhaustive hyper parameter tuning as <ref type="bibr" target="#b1">Alon &amp; Yahav (2020)</ref> shows that prior work with extensively tuned GNNs to real-world datasets suffer from over-squashing.</p><p>The contribution of this work can be summarized as follows:</p><p>? We introduce a novel GNN, namely FDGATII, which can help address the multiple challenges in in real world datasets effectively. FDGATII introduces an enhanced attention mechanism to handle noise and generalize to both homophilic and heterophilic (assortative and disassortative) datasets.</p><p>? We also show that FDGATII is robust to noise while able to solve the main graph challenges : heterophily, over-smoothing and suspended animation simultaneously.</p><p>? FDGATII is efficient due to using shallow selfattention. Its sparse implementation is also parallelizable across node neighbour pairs and can be split between multiple GPUs; (further, no eigen decompositions or similar costly matrix operations are required.)</p><p>? As FDGATII is based on GAT, it is also directly applicable to inductive learning problems, including tasks where the model must generalize to completely unseen graphs; and finally</p><p>? We test the effectiveness on a wide range of graph benchmark datasets and compare against both classic and state-of-the-art GNN models to demonstrate that FDGATII consumes over a magnitude less computational resources while maintaining or exceeding state-of-the-art GAT and GCN models; and</p><p>? By not assuming homophily, FDGATII minimizes its potential negative effects, i.e.: bias and unfairness, whist obtaining state-of-the-art results on Chameleon and Cornell benchmark data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries and Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Notations</head><p>We follow the same notations as <ref type="bibr">Chen et al. (2020)</ref> where G = (V, E) is a simple connected undirected graph with n nodes (denoted {1, . . . , n}) and m edges while? = (V,?) is its self-looped graph. d i and d i + 1 is the degree of node i in G and? respectively. Given A as the adjacency matrix and D the degree matrix of G, the adjacency matrix and degree matrix of? is? = A</p><formula xml:id="formula_0">+ I andD = D + I due to the presence of the self-loop. Every v ? V is represented by a d-dimensional feature vector X v where X ? R n?d is the feature matrix. The symmetric positive semi definite normalized graph Laplacian matrix is given by L = I n ? D ?1/2 AD ?1/2 whoes eigen decomposition is U ?U T . ? is the diagonal eigenvalue matrix of L. U ? R n?n is the unitary eigenvector matrix of L.</formula><p>Given signal x and filter g ? (?) = diag(?) the graph convolution operation is g ? (L) * x = U g ? (?)U T x where ? ? R n is the vector of spectral filter coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Homophily vs Heterophily</head><p>Node classification problem relies on the graph structure and features of the nodes to identify the labels of the node. Under homophily, nodes are assumed to have neighbours with similar features and labels , thus the cumulative aggregation of node's self-features with it's neighbours reinforces the signal corresponding to the label and helps to improve accuracy of the predictions. However, in case of heterophily, nodes are assumed to have dissimilar features and labels, thus the cumulative aggregation will reduce the signal and add more noise causing the neural network to learn poorly resulting in poor performance <ref type="bibr" target="#b19">(Maurya et al., 2021;</ref><ref type="bibr" target="#b29">Zhu et al., 2020)</ref>.</p><p>Since many existing GNNs assume strong homophily, they fail to generalize to networks with heterophily. Ego-and neighbour-embedding separation, higher-order neighbourhoods and combination of intermediate representations can help improve the performance of GNN models in heterophily settings <ref type="bibr" target="#b29">(Zhu et al., 2020)</ref>.</p><p>Homophily denotes the fraction of edges which connects two nodes of the same label . A higher value (closer to 1) indicates strong homophily, while a lower value (closer to 0) indicates strong heterophily in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Vanilla GCN</head><p>Hammond et al. <ref type="formula" target="#formula_1">(2011)</ref> suggested approximating g ? (?) by a truncated expansion in terms of K th order Chebyshev polynomial where ? ? R K+1 corresponds to a vector of polynomial coefficients.</p><formula xml:id="formula_1">Ug ? (?) U T x ? U K l=0 ? l ? l U T x = K l=0 ? l L l x<label>(1)</label></formula><p>The vanilla GCN <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2016</ref>) simplifies the convolution operation by setting K = 1, ? 0 = 2? and ? 1 = ?? to derive the convolution operation g ? * x = ?(I + D ?1/2 AD ?1/2 )x. GCN also applies the renormalization trick, i.e. use the normalized self-looped adjacency</p><formula xml:id="formula_2">matrixP =D ?1/2?D?1/2 = (D + I n ) ?1/2 (A + I n )(D + I n ) ?1/2 . Each convolutional layer (Equation 2</formula><p>) contains a nonlinear activation function ?, typically ReLU.</p><formula xml:id="formula_3">H l+1 = ? P H l W l<label>(2)</label></formula><p>However, since node embeddings are aggregated recursively from the neighbour embeddings layer by layer, the embedding in the final layer requires all embeddings of upper layers, resulting in high memory cost. Also, GCN gradient update in the full-batch training scheme requires storing all intermediate embeddings, which makes the training unable to extend to large graphs.</p><p>Unfortunately, in all above spectral approaches, the learned filters depend on the Laplacian eigen basis, which depends on the entire graph structure. As a result, a model trained on a specific structure cannot be directly applied to a graph with a different structure <ref type="bibr" target="#b25">(Veli?kovi? et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">GCNII</head><p>GCNII <ref type="bibr">(Chen et al., 2020)</ref> extends GCN to a deep model by enabling GCN to express a K order polynomial filter of arbitrary coefficients with two simple techniques: initial residual connection and identity mapping. Formally, we define the l-th layer of GCNII as:</p><formula xml:id="formula_4">H l+1 = ? (1 ? ? l )PH l + ? l H 0 (1 ? ? l ) I n + ? l W l (3)</formula><p>where ? l and ? l are hyperparameters.</p><p>In summary, GCNII 1) combines the smoothed representation PH l with an initial residual connection to the first layer H (0) ; and 2) adds an identity mapping I n to the l-th weight matrix W l . <ref type="bibr">Chen et al. (2020)</ref> shows that the alternative to the skip connection in ResNet <ref type="bibr" target="#b13">(He et al., 2016)</ref> the residual connection <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2016)</ref>, is only able to partially relieve the over-smoothing problem. By using a connection to the initial representation H 0 , GCNII ensures that the final representation of each node retains at least a ? l fraction from the input layer.</p><p>Further GCNII builds upon <ref type="bibr" target="#b12">Hardt &amp; Ma (2016)</ref> who showed that identity mapping of the form H l+1 = H l (W l + I n ) satisfies the following properties: 1) the optimal weight matrices W l have small norms; 2) the only critical point is the global minimum. The first property allows us to put strong regularization on W l to avoid over-fitting, while the later is desirable in semi-supervised tasks where training data is limited. <ref type="bibr" target="#b20">Oono &amp; Suzuki (2019)</ref> theoretically proved that a K-layer GCN's convergence rate depends on s K , where s is the maximum singular value of the weight matrices W l , l = 0, . . . , K ? 1. GCNII replaces W l with (1 ? ? l )I n + ? l W l with regularization on W l , resulting in singular values of (1 ? ? l )I n + ? l W l closer to 1, which implies that s K is large, and the information loss is relieved.</p><p>However, as GCNII combines neighbour embeddings by uniformly averaging, its heterophilic performance is relatively poor. Alternatively, a selective aggregation over the neighbourhood allows focusing on relevant nodes <ref type="bibr" target="#b29">(Zhu et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Attention Mechanism</head><p>The attention mechanism <ref type="bibr" target="#b24">(Vaswani et al., 2017)</ref> has been widely used in GNNs <ref type="bibr">(Chen et al., 2020;</ref><ref type="bibr" target="#b4">Brody et al., 2021;</ref><ref type="bibr" target="#b25">Veli?kovi? et al., 2018)</ref>. Attention essentially maps a query Q and a set of key-value pairs K, V to an output, where the query, keys, values, and output are all vectors <ref type="figure" target="#fig_0">(Figure 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">GAT</head><p>In contrast to GCN, which weighs all neighbours j ? N i with equal importance GAT <ref type="bibr" target="#b25">(Veli?kovi? et al., 2018)</ref> computes a learned weighted average of the representations of N i using attention. Compared to GCN, attention-based operators assign different weights for neighbours, and can alleviate noises and achieve better results <ref type="bibr" target="#b28">(Zhou et al., 2020)</ref> while being more robust <ref type="bibr" target="#b1">(Alon &amp; Yahav, 2020)</ref>.</p><p>Specifically, a scoring function e : R d ? R d ? R computes a score for every edge (j, i), which indicates the importance of the features of the neighbour j to the node i:</p><formula xml:id="formula_5">H l+1 = ?Ah l W l (4) H l+1 = ? j?N i a l i,j h l j W l (5) e (h i , h j ) = LeakyReLU a T ? [Wh i Wh j ]<label>(6)</label></formula><p>where attention scores a ? R 2d and weights W ? R d ?d are learned. denotes vector concatenation. Attention scores are normalized across all neighbours j ? N i using softmax, and the attention function is defined as:</p><formula xml:id="formula_6">? ij = sof tmax j (e (h i , h j )) = exp (e (h i , h j )) j ?Ni exp e h i , h j<label>(7)</label></formula><p>Finally, GAT computes a weighted average of the transformed features of the neighbour nodes (followed by a nonlinearity ?) as the new representation of i, using the normalized attention coefficients:</p><formula xml:id="formula_7">h i = ? ? ? j?Ni ? ij Wh j ? ?<label>(8)</label></formula><p>The motivation of GAT is to compute a representation for every node as a weighted average of its neighbours, by attending to its neighbours using its own representation as the query <ref type="bibr" target="#b25">(Veli?kovi? et al., 2018)</ref>. Ability to focus on the most relevant parts of the input to make decisions results in robustness in the presence of noisy irrelevant neighbours <ref type="bibr" target="#b1">(Alon &amp; Yahav, 2020)</ref>. <ref type="bibr">et al. (2021)</ref> notes that the main problem in the standard GAT scoring function, Equation 6, is that the learned layers W and a are applied consecutively, and thus can be collapsed into a single linear layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Dynamic attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Brody</head><p>GATv2 replaces the linear approximator with a universal approximator function.</p><formula xml:id="formula_8">e (h i , h j ) = a T ? LeakyReLU (W [h i h j ])<label>(9)</label></formula><p>Thus, GATv2 has been shown to perform better on noisy data <ref type="bibr" target="#b4">(Brody et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Proposed Architecture</head><p>Most GNN models use a simple graph convolution based aggregation scheme as the basic building block <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b10">Hamilton et al., 2017)</ref>. Recent studies point out that this leads to filter incompleteness which can be solved by using a more complex graph kernel (Abu-El-Haija et al., 2019). Further, majority of the GNN models are designed under the assumption of homophily, and cannot handle heterophily <ref type="bibr" target="#b29">(Zhu et al., 2020)</ref>. Despite the GATs effectiveness, GAT performs poorly given heterophilic data, despite the attention mechanism's inherent ability to focus on the most relevant nodes during the local aggregation process.</p><p>Motivated by this limitations, we propose a modified attention based model that adapts well to heterohilic data while also solving oversmoothing in an efficient manner.</p><p>Our proposed design ( <ref type="figure" target="#fig_2">Figure 2</ref>) is built upon a local embedding step that extracts local node embeddings from the node feature vectors using GATv2. However, to extend the attention model to handle heterophilic and noisy data we borrow two techniques from GCNII <ref type="bibr">(Chen et al., 2020)</ref> and H2GCN <ref type="bibr" target="#b29">(Zhu et al., 2020)</ref> with modifications, namely residual connections and identity mapping. We show, from a wide range of datasets, that this simple arrangement results in an efficient and fast model that generalizes well to homophilic and heterophilic datasets.</p><p>Essentially, we combine GATv2 (Equation 9) with Initial residual connection and Identity mapping as in Equation <ref type="formula">3</ref> to enhance local aggregation while ensuring robustness to heterophily. In Equation 3, ? and ? are the weights of Initial residual and the Identity respectively.</p><p>In addition to Equation 3, GCNII also uses a variant, GC-NII* with different weight matrices for the smoothed rep-resentationP H l and the initial residual H 0 . Formally, the (l + 1)-th layer of GCNII* is defined as :</p><formula xml:id="formula_9">H l+1 = ? (1 ? ? l )PH l (1 ? ? l ) I n + ? l W l 1 +? l H 0 (1 ? ? l ) I n + ? l W l 2<label>(10)</label></formula><p>We use both forms, ( Equation 3, Equation 10 ) of supplement methods in our model. <ref type="bibr">Chen et al. (2020)</ref> uses ? l is to ensure the decay of the weight matrix adaptively increases with more layers. While our model, FDGATII is essentially a shallow model (typically 2 to 4 layers), we adopt the same settings of ? l = log ? l + 1 ? ? l , where ? is a hyperparameter. Further, we do not fine tune ? for any dataset during model comparisons. <ref type="bibr" target="#b29">Zhu et al. (2020)</ref>  FDGATII differs from existing models with respect to its use of modified attention mechanism. Notably FDGATII is able to augment self attention with the use of a proportion of initial representation and identity resulting in an efficient shallow yet well genializing architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the performance of FDGATII against the state-of-the-art graph neural network models on a wide variety of open graph datasets for fully supervised classification.</p><p>Following <ref type="bibr" target="#b21">Pei et al. (2019)</ref> and <ref type="bibr">Chen et al. (2020)</ref> , we use 7 datasets <ref type="table" target="#tab_0">(Table 1)</ref>. Cora, Citeseer, and Pubmed are homophilic citation network datasets where nodes correspond to documents, and edges correspond to citations; each node feature corresponds to the bag-of-words representation of the document and belongs to one of the academic topics. The remaining four (in <ref type="table" target="#tab_0">Table 1</ref>) are heterophilic datasets of web networks, where nodes and edges represent web pages and hyperlinks, respectively. The feature vector of each node is the bag-of-words representation of the corresponding page.</p><p>Identical to <ref type="bibr" target="#b21">Pei et al. (2019)</ref> and <ref type="bibr">Chen et al. (2020)</ref> we use the same data splits, same mix of 60%, 20%, and 20% for training, validation and testing, same pre-processing and measure the average model performance on the 10 test splits for each dataset. We fix the learning rate to 0.01, dropout rate to 0.5 and the number of hidden units to 64 on all datasets. Addressing observations by <ref type="bibr" target="#b1">Alon &amp; Yahav (2020)</ref> we do not perform hyper-parameter fine tuning and use the benchmark, i.e.: GCNII, settings for comparability. For efficient computation, adjacency matrices are stored and used as sparse matrices.</p><p>We choose GCNII <ref type="bibr">(Chen et al., 2020)</ref> as our performance preprint 2.4 Fast Dynamic Graph Attention with Initial residual and Identity mapping (FDGATII) In with ? and ? proportions respectively at each layer. The attention module concatenates source (row) and destination (column) features of each edge from the adjacency matrix A (with self-edges) projected via W n H , applies a non-linearity (leaky-relu) followed by an exp() to obtain the edgewise attentions which is reshaped to a matrix suitable for the final softmax with the query. FDGATII can have multiple layers, followed by a final W1 projection (layer) and log softmax that provides the node classification. and accuracy benchmark as it is (a) more current; (b) the most similar to our work in the use of initial representation and identity; (c) it actively attempts to solve the over smoothing problem and (d) as it is the current state-of-the-art in Cora dataset, a prominent data set for GNN model comparison. We also compare FDGATII with H2GCN <ref type="bibr" target="#b29">(Zhu et al., 2020)</ref> which is the current state-of-the-art on Cornel,Texas and Wisconsin; hightly heterophelic datasets. We further carryout training and inference time measurements with GPU warmup and proper synchronization prior to measurements. Additionally, for inference, we take the average time for 1000 inferences to lower any possibility of errors and to be more reflective of real-world use of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and discussion</head><p>5.1. Full-Supervised Node Classification <ref type="table" target="#tab_2">Table 2</ref> reports the mean classification accuracy of each model. We reuse the metrics already reported by <ref type="bibr">Chen et al. (2020)</ref> and <ref type="bibr" target="#b29">(Zhu et al., 2020)</ref>. We observe that FDGATII demonstrates state-of-the-art results on heterophilic datasets while still being competitive on the homophilic datasets. Further FDGATII exhibits significant accuracy increases over its attention based predecessor, GAT. This result suggests that dynamic attention with initial residuals and identity improves the predictive power whilst keeping the layer count (and hence the model parameters and computational requirements) low.   <ref type="bibr" target="#b29">Zhu et al. (2020)</ref>, <ref type="formula">(3)</ref>: GCNII best results from our tests with same pre-processing and 10 splits averaged as in <ref type="bibr">Chen et al. (2020)</ref>, (4): our FDGATII, with same pre-processing and 10 splits averaged as in <ref type="bibr">Chen et al. (2020)</ref>   isons for our FDGATII with the benchmark GCNII model. FDGATII demonstrates up to 18x faster training speeds and up to 7x faster inference speeds. <ref type="figure" target="#fig_3">Figure 3</ref> plots accuracy vs efficiency of FDGATII against the benchmark for all datasets, clearly indicating its superior performance mix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study</head><p>In this section, along with <ref type="table">Table 4</ref>, we consider the effect of various proposed design strategies on the performance of the model. Our 1 or 2-layer models, without Initial residual and Identity mapping (II), is theoretically equivalent to GAT/GATv2. The ablation study indicates that the addition of the II mechanism results in significant improvements on the heterophilic dataset performance. This result suggests that both II and dynamic attention techniques are needed to solve the problem of over-smoothing and data heterophily.   FDGATT exhibits higher accuracy and/or lower training and inference times. The proposed architecture performs consistently better across noisy and diverse datasets with comparable or better accuracy results to state-of-theart (Table 2) while exhibiting superiority in training and inference times, specifically 18x faster training speeds and up to 7x faster inference speeds over our chosen state-of-theart benchmarks, GCNII <ref type="bibr">(Chen et al., 2020)</ref> and H2GCN <ref type="bibr" target="#b29">(Zhu et al., 2020)</ref>. FDGATII's dynamic attention is able to achieve higher expressive power using less layers and parameters while still paying selective attention to important nodes, while the II mechanism supplements self-node features in highly heterophilic datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Suspended animation and over smoothing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Broader issues related to hetophily</head><p>Many popular GNN models implicitly assume homophily producing results that may be biased, unfair or erroneous <ref type="bibr" target="#b29">(Zhu et al., 2020)</ref>. This can result in the so-called 'filter bubble' phenomenon in a recommendation system (reinforcing existing beliefs/views, and downplaying the opposite ones), or make minority groups less visible in social networks creating ethical implications <ref type="bibr" target="#b9">(Chitra &amp; Musco, 2020</ref>). FDGATI's novel self-attention mechanism, where dynamic attention is supplemented with II for self-node feature preservation, reduces filter bubble phenomena and its potential negative consequences ensuring fairness and less bias.</p><p>This offers new possibilities for research into dataset where 'opposites attract' , i.e.: majority of linked nodes are different, such as social and dating networks (majority of gender connects with opposite gender), chemistry and biology (amino acids bond with dissimilar types in protein structures), e-commerce (sellers with promoters and influencers) and dark web and other cybercrime related activities <ref type="bibr" target="#b29">(Zhu et al., 2020)</ref>. In a typical dark web, fraudsters are more likely to connect to accomplices and prospective victims and not to fellow fraudsters, to facilitate efficiency while maintaining security, i.e.: illicit actors will form ties with other actors who play different roles <ref type="bibr" target="#b3">(Bright et al., 2019)</ref>, resulting in heterophilic characteristics. <ref type="figure">Figure 4</ref> shows a typical set of nodes seen in a dark web marketplace with illicit actors. ? 1 <ref type="figure">Figure 4</ref>. Typical structure of a dark web market with illicit actors. Fraudsters would exhibit minimal links to fellow fraudsters and form form ties with other actors who play different roles <ref type="bibr" target="#b3">(Bright et al., 2019)</ref>. Simple aggregation of neighbours in such a scenario can be misleading <ref type="bibr" target="#b1">(Alon &amp; Yahav, 2020)</ref>.</p><p>Interesting future directions include combining FDGATII with fourier based self-attention mechanisms (Lee- <ref type="bibr" target="#b17">Thorp et al., 2021)</ref> and analysing the behaviour of FDGATII with novel attention mechanisms that integrate non-local information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This work investigates the use of modified self-attention models for full-supervised node classification. We propose FDGATII, a novel efficient dynamic attention-based model that handles over-smoothing as well as robustness to noise (and heterophily) simultaneously by combining attentional aggregation with multiple feature preserving mechanisms based on initial residual connection and identity mapping. As a result, our model exceeds popular mainstream GAT and GCN benchmarks. Extensive experiments on wide spectrum of benchmark datasets shows that DFGATII achieves well-balanced state-of-the-art or second-best performance on various benchmark full-supervised tasks whilst exhibiting exceptional accuracy to efficiency ratios and parallelizability while, simultaneously addressing over smoothing, suspended animation problem and heterophily prevalent in real world datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The Vaswani et al. (2017) 'Scaled Dot-Product Attention' (left). Multi-Head Attention consists of several attention layers running in parallel (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>observations that (a) higher-order neighbourhoods; and (b) combination of intermediate representations improve the performance of models in heterophily settings and following MixHop (Abu-El-Haija et al., 2019), which explicitly models 1-hop and 2-hop neighbourhoods, we use 2 to 4 layers. Following<ref type="bibr" target="#b27">Xu et al. (2018)</ref> we add skip connections in the form of initial representations H 0 as inChen et al. (2020).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>FDGATII uses an initial representation obtained from the node features via f c0 projection combined with attention and Identity</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>summarises the high accuracy to computational time efficiency ratio of FDGATII. For every dataset,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Training efficiency vs Accuracy (top); and Inference efficiency vs Accuracy (bottom). Ef f iciency = log(1/time) where training time is GPU time for 10 samples averaged with warm-up with GPU warm-up. Inference time is average for 1000 inferences on the GPU with warm-up.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the node classification datasets. H% indicates the hormophily percentage.</figDesc><table><row><cell>Dataset</cell><cell cols="2">H% Cls. Nodes</cell><cell cols="2">Edges Features</cell></row><row><cell>Cora</cell><cell>0.81 7</cell><cell>2,708</cell><cell>5,429</cell><cell>1,433</cell></row><row><cell>Citeseer</cell><cell>0.74 6</cell><cell>3,327</cell><cell>4,732</cell><cell>3,703</cell></row><row><cell>Pubmed</cell><cell>0.80 3</cell><cell cols="2">19,717 44,338</cell><cell>500</cell></row><row><cell cols="2">Chameleon 0.23 4</cell><cell cols="2">2,277 36,101</cell><cell>2,325</cell></row><row><cell>Cornell</cell><cell>0.30 5</cell><cell>183</cell><cell>295</cell><cell>1,703</cell></row><row><cell>Texas</cell><cell>0.11 5</cell><cell>183</cell><cell>309</cell><cell>1,703</cell></row><row><cell>Wisconsin</cell><cell>0.21 5</cell><cell>251</cell><cell>499</cell><cell>1,703</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>shows the training and inference time compar-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Mean classification accuracy of full-supervised node classification. (1): Metrics reported by Chen et al. (2020), (2): Metrics reported by</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>for comparison. (5) Accuracy gain = (GCN IIacc-F DGAT IIacc) * 100/GCN IIacc. SOTA performance for each dataset is marked as bold and second-best performance is underlined for comparison. Parenthesis indicates the number of layers.</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Cite.</cell><cell>Pumb.</cell><cell>Cham.</cell><cell>Corn.</cell><cell>Texa.</cell><cell>Wisc.</cell></row><row><cell>GCN 1</cell><cell>85.77</cell><cell>73.68</cell><cell>88.13</cell><cell>28.18</cell><cell>52.70</cell><cell>52.16</cell><cell>45.88</cell></row><row><cell>GAT 1</cell><cell>86.37</cell><cell>74.32</cell><cell>87.62</cell><cell>42.93</cell><cell>54.32</cell><cell>58.38</cell><cell>49.41</cell></row><row><cell>Geom-GCN-I 1</cell><cell>85.19</cell><cell>77.99</cell><cell>90.05</cell><cell>60.31</cell><cell>56.76</cell><cell>57.58</cell><cell>58.24</cell></row><row><cell>Geom-GCN-P 1</cell><cell>84.93</cell><cell>75.14</cell><cell>88.09</cell><cell>60.90</cell><cell>60.81</cell><cell>67.57</cell><cell>64.12</cell></row><row><cell>Geom-GCN-S 1</cell><cell>85.27</cell><cell>74.71</cell><cell>84.75</cell><cell>59.96</cell><cell>55.68</cell><cell>59.73</cell><cell>56.67</cell></row><row><cell>APPNP 1</cell><cell>87.87</cell><cell>76.53</cell><cell>89.40</cell><cell>54.3</cell><cell>73.51</cell><cell>65.41</cell><cell>69.02</cell></row><row><cell>JKNet 1</cell><cell cols="2">85.25(16) 75.85(8)</cell><cell cols="3">88.94(64) 60.07(32) 57.30(4)</cell><cell cols="2">56.49(32) 48.82(8)</cell></row><row><cell>JKNet(Drop) 1</cell><cell cols="2">87.46(16) 75.96(8)</cell><cell cols="3">89.45(64) 62.08(32) 61.08(4)</cell><cell cols="2">57.30(32) 50.59(8)</cell></row><row><cell>Incep(Drop) 1</cell><cell>86.86(8)</cell><cell>76.83(8)</cell><cell>89.18(4)</cell><cell>61.71(8)</cell><cell cols="2">61.62(16) 57.84(8)</cell><cell>50.20(8)</cell></row><row><cell>GraphSAGE 2</cell><cell>86.90</cell><cell>76.04</cell><cell>88.45</cell><cell>58.73</cell><cell>81.18</cell><cell>82.43</cell><cell>75.95</cell></row><row><cell>MixHop 2</cell><cell>87.61</cell><cell>76.26</cell><cell>85.31</cell><cell>60.50</cell><cell>75.88</cell><cell>77.84</cell><cell>73.51</cell></row><row><cell>H2GCN-1 2</cell><cell>86.92</cell><cell>77.07</cell><cell>89.40</cell><cell>57.11</cell><cell>82.16</cell><cell>84.86</cell><cell>86.67</cell></row><row><cell>GCNII 1</cell><cell cols="4">88.49(64) 77.08(64) 89.57(64) 60.61(8)</cell><cell cols="3">74.86(16) 69.46(32) 74.12(16)</cell></row><row><cell>GCNII* 1</cell><cell cols="4">88.01(64) 77.13(64) 90.30(64) 62.48(8)</cell><cell cols="3">76.49(16) 77.84(32) 81.57(16)</cell></row><row><cell>GCNII 3</cell><cell>88.2696</cell><cell>76.9325</cell><cell>90.3499</cell><cell>63.75</cell><cell>77.2973</cell><cell>78.3784</cell><cell>79.8039</cell></row><row><cell>FDGATII 4</cell><cell>87.7867</cell><cell>75.6434</cell><cell>90.3524</cell><cell>65.1754</cell><cell>82.4324</cell><cell>80.5405</cell><cell>86.2745</cell></row><row><cell cols="2">Accuracy Gain % 5 -0.55</cell><cell>-1.68</cell><cell>0.00</cell><cell>2.24</cell><cell>6.64</cell><cell>2.76</cell><cell>8.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Training and inference time comparison. In case of variants, we use the lowest average time taken to run all 10 standard splits. Efficiency = GCN IItime/F DGAT IItime. We have used our time results of GCNII to eliminate any hardware-based effects. All timing is from running on Wiener supercomputer with CPU :Intel Xeon Gold 6132 GPU :V100/cuda/11.3.0 Memory:32G DDR4 allocation. Ablation study with and without II (Initial residual and Identity mapping). * Indicates the variant defined in Equation 3 and ** is from Equation 10. Each dataset is compared with the same hyperparameter settings (dropout, learning rate, decay) as inChen et al.  (2020). L1 and L2 identifies 1 and 2 layers respectively.</figDesc><table><row><cell>Metric</cell><cell>Cora</cell><cell>Cite.</cell><cell>Pumb.</cell><cell cols="3">Cham. Corn. Texa.</cell><cell>Wisc.</cell></row><row><cell>Training Time</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FDGATII (ms)</cell><cell>7489</cell><cell>1121</cell><cell>54247</cell><cell>3456</cell><cell>2892</cell><cell>4779</cell><cell>1842</cell></row><row><cell>GCNII (ms)</cell><cell cols="4">79264 20070 246477 8540</cell><cell cols="3">13933 40224 11843</cell></row><row><cell>Training Efficiency</cell><cell cols="3">10.6x 17.9x 4.5x</cell><cell>2.5x</cell><cell>4.8x</cell><cell>8.4x</cell><cell>6.4x</cell></row><row><cell>Inference Time</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FDGATII (ms)</cell><cell>4.96</cell><cell>4.92</cell><cell>6.97</cell><cell>3.65</cell><cell>3.62</cell><cell>4.70</cell><cell>3.74</cell></row><row><cell>GCNII (ms)</cell><cell cols="3">35.06 35.37 52.64</cell><cell>6.01</cell><cell>8.90</cell><cell cols="2">16.32 8.86</cell></row><row><cell cols="4">Inference Efficiency 7.07x 7.19x 7.56x</cell><cell>1.65x</cell><cell cols="3">2.46x 3.47x 2.37x</cell></row><row><cell>Metric</cell><cell>Cora</cell><cell>Cite.</cell><cell cols="5">Pumb. Cham. Corn. Texa. Wisc.</cell></row><row><cell cols="5">Without II, L1 86.90 75.65 87.01 65.18</cell><cell cols="3">65.95 62.16 54.51</cell></row><row><cell cols="5">Without II, L2 86.74 74.45 86.19 49.78</cell><cell cols="3">58.92 57.30 51.76</cell></row><row><cell>With II*, L1</cell><cell cols="4">87.06 75.07 89.96 61.34</cell><cell cols="3">76.76 70.00 81.96</cell></row><row><cell>With II*, L2</cell><cell cols="4">87.79 75.30 90.35 57.13</cell><cell cols="3">79.19 79.73 83.53</cell></row><row><cell cols="5">With II**, L1 84.91 75.28 89.48 49.12</cell><cell cols="3">80.27 78.65 84.12</cell></row><row><cell cols="5">With II**, L2 86.52 75.14 90.12 57.83</cell><cell cols="3">80.81 80.54 84.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>shows FDGATII performance for 2 selected datasets</cell></row><row><cell>under increasing layer depth. As model depth increases,</cell></row><row><cell>FDGATII does not exhibit performance degradation from</cell></row><row><cell>suspended animation problem or over smoothing and per-</cell></row><row><cell>forms well up to 32 layers of test depth.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Model accuracy with increasing layer depth shows no presence of over smoothing or suspended animation problem. We used a hidden dimension of 64 and Equation 3 variant for all tests. Model layers and variant of II used for provided results. All other parameters are identical toChen et al. (2020).</figDesc><table><row><cell cols="3">Num of Layers Wisc. Cite.</cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>81.96 75.07</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>83.53 74.34</cell><cell></cell></row><row><cell></cell><cell>4</cell><cell>82.16 74.57</cell><cell></cell></row><row><cell></cell><cell>8</cell><cell>82.16 74.68</cell><cell></cell></row><row><cell></cell><cell>16</cell><cell>81.76 74.86</cell><cell></cell></row><row><cell></cell><cell>32</cell><cell>82.55 74.82</cell><cell></cell></row><row><cell>Dataset</cell><cell>Variant</cell><cell cols="2">Dimensions Layers</cell></row><row><cell>cora</cell><cell>Equation 3</cell><cell>64</cell><cell>2</cell></row><row><cell>citeseer</cell><cell>Equation 10</cell><cell>128</cell><cell>1</cell></row><row><cell>pubmed</cell><cell>Equation 3</cell><cell>64</cell><cell>2</cell></row><row><cell cols="2">chameleon None</cell><cell>64</cell><cell>1</cell></row><row><cell>cornell</cell><cell>Equation 10</cell><cell>128</cell><cell>1</cell></row><row><cell>texas</cell><cell>Equation 10</cell><cell>64</cell><cell>2</cell></row><row><cell>wisconsin</cell><cell>Equation 10</cell><cell>128</cell><cell>1</cell></row><row><cell cols="3">5.4. Performance and Efficiency</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixhop</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
	<note>In international conference on machine learning</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05205</idno>
		<title level="m">On the bottleneck of graph neural networks and its practical implications</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S R</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Illicit network dynamics: The formation and evolution of a drug trafficking network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koskinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Quantitative Criminology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="258" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">How attentive are graph attention networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahav</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14491</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Measurement and analysis of the swarm social network with tens of millions of nodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="4547" to="4559" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">preprint 2.4 Fast Dynamic Graph Attention with Initial residual and Identity mapping (FDGATII)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analyzing the impact of filter bubbles on social network polarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Chitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Musco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Identity matters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04231</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4202" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03824</idno>
		<title level="m">Mixing tokens with fourier transforms</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14612</idno>
		<title level="m">Non-local graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving graph neural networks with simple architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Maurya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Murata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07634</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepwalk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploring emerging hacker assets and key hackers for proactive cyber threat intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samtani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Nunamaker</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Management Information Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1023" to="1053" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
