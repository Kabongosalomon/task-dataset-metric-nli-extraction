<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Homophily with Graph Echo State Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-27">27 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domenico</forename><surname>Tortorella</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Largo B. Pontecorvo 3</orgName>
								<orgName type="institution">University of Pisa</orgName>
								<address>
									<postCode>56127</postCode>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Largo B. Pontecorvo 3</orgName>
								<orgName type="institution">University of Pisa</orgName>
								<address>
									<postCode>56127</postCode>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Homophily with Graph Echo State Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 30th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN 2022)</title>
						<meeting>the 30th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN 2022)						</meeting>
						<imprint>
							<biblScope unit="page" from="491" to="496"/>
							<date type="published" when="2022-10-27">27 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.14428/esann/2022.ES2022-58</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Echo State Networks (GESN) have already demonstrated their efficacy and efficiency in graph classification tasks. However, semi-supervised node classification brought out the problem of oversmoothing in end-to-end trained deep models, which causes a bias towards high homophily graphs. We evaluate for the first time GESN on node classification tasks with different degrees of homophily, analyzing also the impact of the reservoir radius. Our experiments show that reservoir models are able to achieve better or comparable accuracy with respect to fully trained deep models that implement ad hoc variations in the architectural bias, with a gain in terms of efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs provide a useful structure to represent relations between entities, such as paper citations or web page networks. A plethora of neural models have been proposed to solve graph-, edge-, and node-level tasks <ref type="bibr" target="#b0">[1]</ref>, most of them sharing an architecture structured in layers that perform local aggregations of node features. This architectural bias, where node features are progressively smoothed in deeper layers via local aggregation <ref type="bibr" target="#b1">[2]</ref>, is the source of most of the issues that graph neural models are facing. This bias towards locally homogeneous graphs is more apparent in node classification tasks, where graphs presenting a significant number of inter-class edges, i.e. a low homophily degree, present a challenge to convolutive models. Graph Echo State Network (GESN) <ref type="bibr" target="#b2">[3]</ref> is an efficient model within the reservoir computing (RC) paradigm. In RC, input data is encoded via a randomly-initialized reservoir, while only a linear readout requires training. GESN has already been successfully applied to graph-level classification tasks <ref type="bibr" target="#b3">[4]</ref>.</p><p>In this paper, we analyze for the first time its application to node classification tasks, focusing in particular on the efficacy in tackling low-homophily graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Node classification and homophily</head><p>Let G = (V, E) denote a graph with node feature vectors u v ? R U for each node v ? V. We also denote by N r (v) the set of nodes within r hops of node v, and by A the graph adjacency matrix. The goal of a semi-supervised node classification task is to learn a model from a subset of graph nodes with known target labels {(u v , y v )} v?Vtrain , in order to infer the node labels y v ? {1, ..., C} for the remaining nodes V \ V train using the network structure and input features u v . Most common graph convolutional models are structured in L layers, where Proceedings of the 30th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN 2022), pp. 491-496 doi:10.14428/esann/2022.ES2022-58 each layer learns an embedding for each node based on an increasingly large receptive field. These layers can be formalized as <ref type="bibr" target="#b4">[5]</ref> </p><formula xml:id="formula_0">h (?) v = combine h (??1) v , aggregate({h (??1) v ? : v ? ? N 1 (v)}) ,<label>(1)</label></formula><p>where node embeddings h (?) v ? R H of layer ? are obtained by aggregating the previous embeddings h</p><formula xml:id="formula_1">(??1) v ?</formula><p>of node v's 1-hop neighbors via aggregate(?), and then combined with the node's previous embeddings h</p><formula xml:id="formula_2">(??1) v via combine(?); for ? = 1, h (0) v = u v .</formula><p>The final layer L either directly predicts the one-hot encoding of target label y v , or is followed by an MLP that serves this purpose. The whole model is trained end-to-end by typically minimizing the cross-entropy loss.</p><p>The choice of functions in (1) determines the architectural bias of the model. For example, GCN <ref type="bibr" target="#b5">[6]</ref> layers are defined as h (?) = relu(?h (??1) ? (?) ), wher? A is the normalized adjacency matrix, ? (?) are learnable weights, and h (?) is the row stack of node features for layer ?. It has been shown that stacking more than three or four layers of graph convolution causes a degradation in accuracy <ref type="bibr" target="#b1">[2]</ref>, since representations h (?) v converge asymptotically to a fixed point of? as ? increases, or more generally, to a low-frequency subspace of the graph spectrum. This problem is known as oversmoothing. Indeed, by acting as a lowpass filter, GCNs are biased in favor of tasks whose graphs present a high degree of homophily, that is nodes in the same neighborhood mostly share the same class <ref type="bibr" target="#b6">[7]</ref>. Formally, homophily in a graph can be quantified <ref type="bibr" target="#b6">[7]</ref> as the intra-class edges ratio</p><formula xml:id="formula_3">h G = |{(v, v ? ) ? E : y v = y v ? }| / |E| .<label>(2)</label></formula><p>Changes in the model architectural bias have been proposed to improve classification on low homophily graphs. Some solutions individuated by <ref type="bibr" target="#b6">[7]</ref> are: v computed at each intermediate layer ? &lt; L to make predictions, e.g. as in Jumping-Knowledge networks <ref type="bibr" target="#b8">[9]</ref>. H2GCN <ref type="bibr" target="#b6">[7]</ref> incorporates all three architectural solutions. Alternative solutions include altering the graph structure to improve the homophily degree, in order to increase the ratio of intra-class edges in node neighborhoods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reservoir computing for graphs</head><p>Reservoir computing is a paradigm for the efficient design of recurrent neural networks. Input data is encoded by a randomly initialized reservoir, while only the task prediction layer requires training. Graph Echo State Networks (GESNs) extended the reservoir computing paradigm to graph-structured data <ref type="bibr" target="#b2">[3]</ref>, and have already demonstrated their effectiveness in graph classification tasks <ref type="bibr" target="#b3">[4]</ref>. Node embeddings are recursively computed by the dynamical system</p><formula xml:id="formula_4">x (k) v = tanh W in u v + v ? ?N1(v)? x (k?1) v ? , x (0) v = 0,<label>(3)</label></formula><p>where W in ? R H?U and? ? R H?H are the input-to-reservoir and the recurrent weights, respectively (input bias is omitted). Equation <ref type="formula" target="#formula_4">(3)</ref> is iterated over k until the system state converges to fixed point x</p><formula xml:id="formula_5">(?) v</formula><p>, which is used as the embedding. The existence of a fixed point is guaranteed by the Graph Embedding Stability (GES) property <ref type="bibr" target="#b3">[4]</ref>, which also guarantees independence from the system's initial state x (0) v . A necessary condition <ref type="bibr" target="#b11">[12]</ref> for the GES property is ?(?) &lt; 1/?, where ?(?) denotes the spectral radius of a matrix, i.e. its largest absolute eigenvalue, and ? = ?(A) is the graph spectral radius. This condition also provides the best estimate of the system bifurcation point, i.e. the threshold beyond which (3) becomes asymptotically unstable. Reservoir weights are randomly initialized from a uniform distribution in [?1, 1], and then rescaled to the desired input scaling and reservoir spectral radius, without requiring any training. While in graph-level task node features are aggregated to provide global embeddings, for node classification tasks we directly apply a linear readout to node embeddings</p><formula xml:id="formula_6">y v = W out x (?) v + b out , where the weights W out ? R C?H , b out ? R C are trained by ridge regression on one-hot encodings of target classes y v .</formula><p>The contractivity of (3) is a sufficient condition for the GES property <ref type="bibr" target="#b11">[12]</ref>. However, the contractivity of graph convolution layers has also been linked to the degradation of representativeness in deep models <ref type="bibr" target="#b10">[11]</ref>. Graph rewiring solutions to the homophily bias, such as <ref type="bibr" target="#b9">[10]</ref>, greatly increase the edges of a graph, which in turn leads to an increase of ? and a decrease in contractivity. Therefore, in our experiments we will explore also values of the reservoir radius beyond the stability threshold, in this case by arbitrarily fixing the number of iterations of (3) to K. Indeed, we can interpret the K iterations of (3) as equivalent to K graph convolution layers with weights shared among layers and input skip connections. While in deep GCNs convergence to a fixed point of the graph convolution operator, due to stacking too many layers, has been linked to the oversmoothing issue <ref type="bibr" target="#b1">[2]</ref>, GESNs can in principle avoid that by selecting a reservoir radius ? ? 1/?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and discussion</head><p>We evaluate GESN on six node classification tasks with low homophily degree (? 0.3) and three tasks with high homophily degree (&gt; 0.7). We adopt the same 10 scaffold splits 48%/32%/20% of <ref type="bibr" target="#b6">[7]</ref>, averaging results in each fold over 10 different reservoir initializations. We explore a number of units ranging from 2 4 to 2 12 , input scaling factors from 1 to 1 320 , readout regularization values from 10 ?5 to 10 2 , and reservoir radii ?? ? [0.1, 9.5] with steps of 0.2 (up to 35 with larger steps for Squirrel and Chameleon). Embeddings are computed with at most K = 100 iterations of equation <ref type="formula" target="#formula_4">(3)</ref>.     <ref type="table">Table 1</ref>: Node classification accuracy on low and high homophily graphs (average and standard deviation; results of fully trained models reported from <ref type="bibr" target="#b6">[7]</ref>; results within one standard deviation of the best accuracy are highlighted).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 30th European Symposium on</head><p>Accuracy results are reported in <ref type="table">Table 1</ref>, while <ref type="figure" target="#fig_0">Fig. 1</ref> shows the reservoir radii selected in the 10 splits. We can observe three different behaviors, exemplified in <ref type="figure" target="#fig_1">Fig. 2 and 3 (top)</ref>. The number of reservoir units plays a significant role, offering best results when it is closer to the number of input features. For Texas, Wisconsin, Actor, and Cornell, the performances of GESN are closer to the accuracies of MLP, which uses only node features u v , and H2GCN, with reservoir radii ?? &lt; 1: in this case, the graph connectivity appears to be of no use. While Squirrel and Chameleon present a low homophily degree, graph convolution models fare better than MLP: in this case graph connectivity needs to be taken into account. On these two tasks, GESN improves upon the best model accuracy by 27.3% and 12.8%, respectively, with reservoir radii selected in the range 33-35. Finally, on high homophily tasks (Citeseer, Pubmed, Cora) GESN performs generally in line with graph convolution models, which in turn do better than MLP; reservoir radii are selected in the range 4-6.</p><p>We observe how the best accuracy results are for reservoir radii well above the stability threshold, which are required when the graph connectivity needs to be leveraged in classifying nodes. To support our conclusion, in <ref type="figure" target="#fig_2">Fig. 3 (bottom)</ref> we report the accuracy on Squirrel and Cora where input features have been removed. We observe that for stable embeddings (?? &lt; 1), accuracy significantly drops below the level reached by having input features, while it reaches almost the same levels of accuracy for the values of ?? selected with features, which are well beyond the region where GESN stability is guaranteed.</p><p>Finally, we underline the efficiency of GESN. Only the linear readout's C(H + 1) parameters require training, against the additional O(H 2 L) parameters of models that need to be trained end-to-end through many gradient descent epochs (for further time comparisons, see <ref type="bibr" target="#b3">[4]</ref>). The time required to compute node embeddings and train the readout for a model of 4096 units takes from 0.87 to 1.58 seconds on a GPU Nvidia Tesla V100, depending on graph size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>For the first time, we have applied Graph Echo State Networks to the task of node classification. Experiments on nine graphs with different degrees of homophily have shown a classification accuracy generally in line with most fully trained models, with extraordinary improvements over two low homophily tasks. Furthermore, contrary to the theory and experiments that demonstrated the crucial role of system stability in applying GESNs to graph-level tasks, our experiments have shown that node embeddings computed in regions well beyond the theoretical stability threshold are better suited to represent the graph structure. Future work will analyze more in-depth the embedding space structure, the role of reservoir radius in conditioning the filtering properties of GESN, and the impact of reservoir spectrum.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN 2022), pp. 491-496 doi:10.14428/esann/2022.ES2022-58 T e x a s W i s c o n s i n A c t o r S q u i r r e l C Reservoir radii selected on each task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Impact of reservoir radius and units on classification accuracy for Texas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Impact of reservoir radius and units on classification accuracy for Squirrel (h G = 0.22) and Cora (h G = 0.81), with and without input features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Proceedings of the 30th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN 2022), pp. 491-496 doi:10.14428/esann/2022.ES2022-58</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Proceedings of the 30th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN 2022), pp. 491-496 doi:10.14428/esann/2022.ES2022-58 ?5.3 59.8 ?7.0 30.3 ?0.8 36.9 ?1.3 59.8 ?2.6 57.0 ?4.7 76.7 ?1.6 87.4 ?0.7 87.3 ?1.3 +JK 66.5 ?6.6 74.3 ?6.4 34.2 ?0.9 40.5 ?1.6 63.4 ?2.0 64.6 ?8.7 74.5 ?1.8 88.4 ?0.5 85.8 ?0.9 +Cheby 77.3 ?4.1 79.4 ?4.5 34.1 ?1.1 43.9 ?1.6 55.2 ?2.8 74.3 ?7.5 75.8 ?1.5 88.7 ?0.6 86.8 ?1.0 H2GCN 84.9 ?6.8 86.7 ?4.7 35.9 ?1.0 36.4 ?1.9 57.1 ?1.6 82.2 ?4.8 77.1 ?1.6 89.4 ?0.3 86.9 ?1.4 MLP 81.9 ?4.8 85.3 ?3.6 35.8 ?1.0 29.7 ?1.8 46.4 ?2.5 81.1 ?6.4 72.4 ?2.2 86.7 ?0.4 74.8 ?2.2 GESN 84.3 ?4.4 83.3 ?3.8 34.5 ?0.8 71.2 ?1.5 76.2 ?1.2 81.1 ?6.0 74.5 ?2.1 89.2 ?0.3 86.0 ?1.0</figDesc><table><row><cell></cell><cell cols="8">Texas Wisconsin Actor Squirrel Chameleon Cornell Citeseer Pubmed</cell><cell>Cora</cell></row><row><cell>Homo.</cell><cell>0.11</cell><cell>0.21</cell><cell>0.22</cell><cell>0.22</cell><cell>0.23</cell><cell>0.30</cell><cell>0.74</cell><cell>0.80</cell><cell>0.81</cell></row><row><cell>Nodes</cell><cell>183</cell><cell>251</cell><cell>7,600</cell><cell>5,201</cell><cell>2,277</cell><cell>183</cell><cell>3,327</cell><cell>19,717</cell><cell>2,708</cell></row><row><cell>Edges</cell><cell>295</cell><cell>466</cell><cell cols="3">26,752 198,493 31,421</cell><cell>280</cell><cell>9,104</cell><cell cols="2">88,648 10,556</cell></row><row><cell cols="2">Radius 2.56</cell><cell>2.88</cell><cell>9.99</cell><cell>138.60</cell><cell>61.90</cell><cell>2.68</cell><cell>13.74</cell><cell>23.24</cell><cell>14.39</cell></row><row><cell cols="2">Featur. 1,703</cell><cell>1,703</cell><cell>932</cell><cell>2,089</cell><cell>2,089</cell><cell>1,703</cell><cell>3,703</cell><cell>500</cell><cell>1,433</cell></row><row><cell>Classes</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>6</cell><cell>3</cell><cell>7</cell></row><row><cell>GCN</cell><cell>59.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Proceedings of the 30th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN 2022), pp. 491-496 doi:10.14428/esann/2022.ES2022-58</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A gentle introduction to deep learning for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Podda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="203" to="221" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph echo state networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gallicchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2010 International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3967" to="3974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast and deep graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gallicchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7793" to="7804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gasteiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13298" to="13310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding over-squashing and bottlenecks on graphs via curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Topping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Di</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spectral bounds for graph echo state network stability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tortorella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gallicchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2022 International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
