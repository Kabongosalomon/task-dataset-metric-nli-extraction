<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pay Attention When Required</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swetha</forename><surname>Mandava</surname></persName>
							<email>smandava@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Migacz</surname></persName>
							<email>smigacz@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Fit-Florea</surname></persName>
							<email>afitflorea@nvidia.com</email>
						</author>
						<title level="a" type="main">Pay Attention When Required</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer-based models consist of interleaved feed-forward blocks -that capture content meaning, and relatively more expensive self-attention blocks -that capture context meaning. In this paper, we explored trade-offs and ordering of the blocks to improve upon the current Transformer architecture and proposed PAR Transformer. It needs 35% lower compute time than Transformer-XL achieved by replacing 63% of the self-attention blocks with feed-forward blocks and retains the perplexity on WikiText-103 language modeling benchmark. We further validated our results on text8 and enwiki8 datasets, as well as on the BERT model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The seminal work in <ref type="bibr" target="#b25">(Vaswani et al., 2017)</ref> introduced the Transformer architecture. Since its introduction, it profoundly influenced algorithms for Question Answering, Text Classification, Translation, Language Modeling, and practically all of the Natural Language Processing tasks. A transformer layer consists of interleaved self attention and feed forward blocks and is used in state of the art models, like Transformer-XL , BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>, Megatron <ref type="bibr" target="#b19">(Shoeybi et al., 2020)</ref>, and other large-scale language models.</p><p>As corresponding model sizes and compute requirements continue to become increasingly more demanding, it becomes important to optimize the Transformer-based architectures, for both financial as well as environmental reasons <ref type="bibr" target="#b22">(Strubell et al., 2019)</ref>. Several optimization approaches, that used pruning <ref type="bibr" target="#b15">(Michel et al., 2019)</ref>, and distillation <ref type="bibr" target="#b18">(Sanh et al., 2020;</ref><ref type="bibr">Jiao et al., 2020;</ref><ref type="bibr" target="#b29">Wang et al., 2020b)</ref>, were able to achieve better run-time performance for an accuracy trade-off.</p><p>Our optimization approach investigates the tradeoff between the self-attention and feed-forward building blocks. We start with the intuition that attention blocks provides context meaning while being comparatively more expensive, and feedforward blocks provide content meaning. We then ask the fundamental questions of what are the saturation points when using one block versus the other, and how accuracy depends on the relative number of blocks of each type as well as on their ordering. To answer these questions, we employed architecture search.</p><p>While recent works such as <ref type="bibr" target="#b30">(Wu et al., 2019;</ref><ref type="bibr" target="#b26">Wan et al., 2020;</ref><ref type="bibr" target="#b10">Liu et al., 2019)</ref> explored using differential neural architecture search for designing ConvNets automatically to significantly improve accuracies and/or latencies, similar work for transformer models is limited. Recent works, however, explored using random search <ref type="bibr" target="#b16">(Press et al., 2020)</ref> and evolutionary search <ref type="bibr" target="#b28">(Wang et al., 2020a;</ref><ref type="bibr" target="#b20">So et al., 2019)</ref> for designing transformer models. However, even with a search space of three options per layer (Self Attention, Feed Forward, Identity), the design space becomes intractable for 32 layers as it is combinatorial (=3 32 ). For this reason, we explored the use of differential neural architecture search that has linear complexity to redesign the transformer architecture in this paper.</p><p>In order to analyze the transformer architecture, we studied the search on Transformer-XL Base with the WikiText-103 dataset. The analysis of the resulting optimal architectures, highlights two fundamental rules: 1. Self-attention layers are necessary only among the former two-thirds layers of the network 2. The total number of layers to self-attention layers ratio of p:1 is sufficient, with p=5 being optimal for Transformer-XL.</p><p>We propose Pay Attention when Required Transformer (or PAR Transformer), a new family of models based on the above two design rules, that uses 63% fewer self-attention blocks while retaining test accuracies. Further, we validated that our hypothesis generalizes to different datasets (text8, enwiki8) as well as transformer models (PAR BERT) for different tasks (Question Answering, Sentiment Analysis, Semantic Textual Similarity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Optimal Design Rules for Transformers</head><p>Our baseline, the Transformer-XL model, has equal number of self-attention and feed forward blocks in an interleaved design pattern as visualized in <ref type="figure">Figure 5</ref>. Sandwich transformers <ref type="bibr" target="#b16">(Press et al., 2020)</ref>, also keeps an equal number of self-attention and feed forward blocks but are designed using a sandwich coefficient k instead of having a simple interleaved design pattern. They have the first k sublayers consisting of self-attention, the last k sublayers consisting of feed forward layers with both sandwiched between the classic interleaving pattern of self-attention and feed forward blocks. This design pattern was found by conducting a series of random search experiments with constraints to keep the number of parameters constant. In this section, we attempt to optimize the transformer architecture by relaxing the above search constraints. We employ differential neural architecture search and allow it to select one of the three options -Self Attention, Feed Forward, or Identity -for each of the layers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Search Space</head><p>Is interleaved attention and feed forward layers in a transformer really the optimal design pattern? Can we get the same results for smaller, faster or imbalanced networks? In order to test these questions, we used a very simple search space that would allow us to do so, consisting of identity block, feed forward block and self-attention block that modify the input sequence X as follows:</p><formula xml:id="formula_0">LN (X) = LayerN orm(X)<label>(1)</label></formula><p>F attn (X) = Self -Attention(LN (X)) + X (2)</p><formula xml:id="formula_1">F F F (X) = F eed-F orward(LN (X)) + X (3) F identity (X) = X<label>(4)</label></formula><p>The output of each layer l can be computed using equation 6 where i is the block choice and m l,i is a probability distribution computed by a Gumbel Softmax function <ref type="bibr" target="#b7">(Jang et al., 2017;</ref><ref type="bibr" target="#b11">Maddison et al., 2017)</ref> on all the choices in a layer from the search space. Once trained, m l,i allows us to study the optimal models. For example, if identity block is the most possible block on a layer, we can hypothesize that there is no benefit from a deeper network. Similarly, the model can also pick different design patterns and faster models.</p><formula xml:id="formula_2">i?(attn,F F,identity) m l,i = 1 (5) X l = i?(attn,F F,identity) m l,i .F l i (X l?1 )<label>(6)</label></formula><p>Since the output at each layer is a linear combination of individual search blocks in that layer, the search cost is linear with respect to the number of blocks in the search space. Since the search also consists of training only one supernet consisting of all the search blocks, it is orders of magnitude faster than RL based search algorithms <ref type="bibr" target="#b33">(Zoph and Le, 2017;</ref><ref type="bibr" target="#b34">Zoph et al., 2018)</ref> that rely on training individual combinations of search blocks. For our choice of supernet, the search cost was &lt; 2? the training cost of the baseline. All of our experiments use the same model architecture parameters as the baseline from <ref type="table" target="#tab_4">Table A1</ref> unless otherwise mentioned. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Search Algorithm and Experiments</head><p>In order to explore design paradigms of a transformer architecture, we use differential neural architecture search, similar to FBNet Algorithm <ref type="bibr" target="#b30">(Wu et al., 2019)</ref>, formulated as a two stage search shown in equation 7 where the goal is to find the architecture a within a search space A, and weights w a that minimizes the loss function L a,wa or the cross entropy loss. Within the architecture phase, architecture parameters m l,i are tuned and within the weight phase, weight parameters of the individual search blocks are tuned, to minimize the loss.</p><p>a, w a = min a?A min wa E a?P ? {L a,wa }</p><p>We run the neural architecture search described above on 16 ? 2 = 32 layers for the WikiText-103 <ref type="bibr" target="#b14">(Merity et al., 2016)</ref> dataset. On each layer, the search algorithm is able to choose between a feedforward, self-attention or identity blocks. We run the search algorithm with batch size=128, architecture update lr=1e-2, weight decay for architecture parameters=5e-4, weight update lr=1e-2, weight decay for block weights=1e-4. We initialize the architecture parameters uniformly and keep architecture parameters constant for the first 10k iterations and then perform architecture update for 20% of an epoch from there on for 40k iterations. We train till the architecture converges i.e. does not change in 75% of the architecture tuning stage. The differential architecture search produces probability distributions m l,i representing the likelihood of block i being the optimal choice for layer l. At each layer, the most probable block is selected. We repeated this search process for 6 random seeds, and retrained the searched models from scratch. Analyzing these search architectures and their performance revealed interesting properties.</p><p>We first observe that total number of layers to self-attention layers ratio is much higher than 2:1. In <ref type="figure" target="#fig_1">Figure 2</ref>, we see that we are able to achieve lower perplexities than the baseline by architectures that use fewer self-attention blocks. We then analyzed where these self-attention blocks are located within the model network. To do this, we split the model into three slices and counted the number of selfattention or feed forward blocks in each of these slices. In <ref type="figure" target="#fig_2">Figure 3</ref>, we see that the majority of selfattention blocks are in the former two-third layers of the network ie the mean number of self-attention blocks in the final third of the layers is &lt; 1.</p><p>Previous research <ref type="bibr" target="#b9">(Kovaleva et al., 2019;</ref><ref type="bibr">Cordonnier et al., 2020)</ref> on transformer layers also indicate that attention layers are severely over parametrized and that they are more prevalent in the beginning of a network <ref type="bibr" target="#b16">(Press et al., 2020)</ref>. Our aim is to quantify how much and when they are needed and to formalize that in a set of rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Formalizing Design Rules For Transformers</head><p>While this process of searching for an architecture and re-training the optimal architecture from scratch for a particular dataset and a particular model can be employed, it is expensive. The scope <ref type="figure">Figure 4</ref>: Perplexities on WikiText-103 dev set with respect to PAR coefficient p, for total number of layers = 32. Horizontal line indicates mean +-std perplexity of our baseline, Transformer-XL Base, from 6 random seeds of this paper is to understand generalizable design rules that can be applied to different transformer models and datasets. To this end, we attempt to hypothesize optimal design rules based on the observations in section 2.2 and validate them in the following sections. Our observations in previous sections motivate us to design a family of PAR Transformer models that have fewer self-attention blocks that are positioned in the former two-third layers of the network. A PAR transformer is now, formalized as being based on the following two design rules: 1. Self-attention layers are only needed among the former two-third layers of the network 2. Total number of layers to self-attention layers ratio of p:1 is sufficient.</p><p>We can now attempt to design optimized transformer models manually based on these design rules. For example, to design a transformer architecture with 32 layers, for a PAR Coefficient of p=5, we use 6 ? (32/5) self-attention layers. These self-attention layers are placed uniformly within the first 21 ? (2 * 32/3) layers.</p><p>We train PAR transformers for various p &gt; 2 (to use fewer self-attention blocks than our baseline). <ref type="figure">Figure 4</ref> shows the performance as a function of PAR coefficient. Of those models, PAR coefficient of 5 is sufficient to match the accuracy of our baseline. We identify this 32 layer, p=5 PAR model as PAR Transformer Base.  We also observe that while self-attention blocks are essential for contextual meaning, the need is saturated fairly quickly. The advantage of replacing self-attention blocks with feed-forward blocks is the significant latency benefit we obtain, as shown in <ref type="figure">Figure 5</ref>. The benefits are even more pronounced with higher sequence lengths as self-attention and feed forward blocks have O(N 2 ) and O(N ) complexities per-layer with respect to sequence lengths, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we review our results on PAR Transformer Base with WikiText-103 with respect to state of the art transformer architectures. We further validate that the PAR design rules generalize to other Transformer-XL models (Large, 24B) and to other datasets (enwiki8, text8). We also validate our design rules on BERT models with PAR BERT.</p><p>All the models are based on the same code base for training, for an apples-to-apples comparison. We used NVIDIA A100 40GB Tensor Core GPUs for our experiments.   <ref type="table">Table 3</ref>: Experimental results of PAR BERT in comparison to BERT Base and DistilBERT. F1 score for SQuAD v1.1 and accuracy for SST-2 and MRPC reported from a median of 5 runs on dev sets.</p><p>Reported Latency for SQuAD inference. * indicates originally published results. + indicates estimated latency as 61% of Bert Base based on DistilBERT paper inference latencies for batch size 1 as is standard. We see similar performance benefits while training as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PAR Transformer</head><p>We compare the performance of our PAR Transformer on WikiText 103 dataset in <ref type="table">Table 1</ref>. Wiki-Text 103 language modelling dataset consists of over 100 million tokens from articles on Wikipedia. It is well suited for testing long term dependencies as it is composed of full articles and with original case, punctuation and numbers. The only difference in the model architectures is the ordering and composition of layers, as visualized in <ref type="figure">Figure 5</ref> and listed under the Architecture column.</p><p>The Transformer-XL Base code is based on the code published by the authors from the Transformer-XL paper but modifies hyper parameters as described in <ref type="table" target="#tab_4">Table A1</ref> for better hardware utilization in base model. Inference latencies are computed using tgt_len = 64, mem_len = 640 and clamp_len = 400. We validate that we are able to obtain the same perplexities with 0.65x the cost in terms of latency.</p><p>We further validate that our hypothesis generalizes to the PAR Transformer Large Model in <ref type="table" target="#tab_2">Table  2</ref>, by maintaining the perplexities with 0.7x the cost. The Large model uses 36 layers, d_model = 1024, d_head = 64, n_head = 16, d_inner = 4096, tgt_len = mem_len =384, batchsize = 128 for training and tgt_len = 128, mem_len = 1600, clamp_len = 1000 for evaluation.</p><p>In order to showcase generalizability over different datasets, we validate our results on enwiki8 and text8 datasets <ref type="bibr" target="#b12">(Mahoney, 2009)</ref> in <ref type="table" target="#tab_2">Table 2</ref>. Enwiki8 consists of 100M bytes of unprocessed Wikipedia text whereas text8 contains 100M characters of preprocessed Wikipedia text. We reuse the same model hyperparameters as in <ref type="table" target="#tab_4">Table A1</ref> with 24 layers. In addition, we use tgt_len = mem_len = 512 for training and tgt_len = 128, mem_len = 2052, clamp_len = 820 for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PAR BERT</head><p>We further study the effect of PAR design rules on BERT models by pre-training on Wikipedia+Books  <ref type="bibr" target="#b32">(Zhu et al., 2015)</ref> using the NVLAMB optimizer  in two phases. Phase 1 is trained with a sequence length of 128, with a batch size of 64k for 7038 steps and phase 2 is trained with a sequence length of 512, with a batch size of 32k for 1563 steps.</p><p>Our pre-training loss curves in <ref type="figure" target="#fig_4">Figure 6</ref> highlight the on-par performance of PAR BERT and BERT Base with a fraction of the self-attention blocks. We see in <ref type="table">Table 3</ref> that using the same architectural design rules results in a 1% accuracy drop on SQuAD v1.1 fine-tuning task even though the pre-training loss and accuracy on MRPC and SST-2 are on track. We hypothesize that tuning PAR coefficient specifically might help bridge the gap. However, incorporating the two stage optimization process (pre-training followed by fine-tuning) that is inherent to BERT and other such language models into architecture tuning is a future research problem.</p><p>It is, however, interesting to note that PAR BERT has comparable latencies with respect to Distil-BERT even though it uses twice as many layers. PAR BERT also outperforms DistilBERT while having a much simpler training paradigm. Nevertheless, we note that pruning, quantization and distillation are orthogonal to the present work and could be used in conjunction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We used differential neural architecture search to study patterns in the ordering of transformer model sub-layers and made two key observations. One, that we only need attention layers in the former part of the network and two, that we need 63% fewer attention layers to retain the model accuracy. Even though we studied the search results specifically on Transformer-XL Base for the WikiText-103 dataset, the same observations were valid for other transformer models and datasets as well.</p><p>We proposed PAR Transformer that achieves 35% lower latency and validated accuracy on en-wiki8 and text8 datasets as well as on 24B + Large variations. We also validated our results on SQuAD v1.1, MRPC and SST-2 with PAR BERT Base model.</p><p>In this paper, we used differential neural architecture search to make optimal composition of transformer architectures explainable. It provides an avenue for automatic design of optimized model families.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Hyper parameter changes to Model</head><p>Our Transformer-XL (Base, 24B) baselines are based on the code base published by the authors of the Transformer-XL paper but uses a modified set of model hyper parameters. Our modifications were made to achieve better hardware utilization and to take advantage of Tensor Cores, most commonly by aligning certain hyper parameters with powers of two. They are described in <ref type="table" target="#tab_4">Table A1</ref>.   Even though literature generally reports number of parameters to estimate efficiency of a model, it is too simplistic, often obscuring performance issues rather than illuminating them. We can see from <ref type="table" target="#tab_2">Table A2</ref> that #Parameters don't actually reflect the latency. While #FLOP count is hardware independent to its merit, latency is less abstract and more indicative of actual performance.  <ref type="table">Table A3</ref> lists test perplexities at 40k and 140k iterations with a global batch size of 256. As we can see, there is little benefit in training much further after 40k iterations for the base model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Composition of Super Net as a linear combination of search blocks along with the block cost. GFLOPS computed for inference with tgt_len = 64, mem_len = 640, batch_size = 1. Latency complexity with respect to sequence length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Perplexities on WikiText-103 dev set as a function of number of self-attention blocks, for Total Number of Layers = 32. Architectures from search are obtained from 6 random seeds, and re-trained from scratch for 40k iterations. Transformer-XL base indicates mean +-std perplexity from 6 random seeds</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Analysis of number of blocks within each slice of the model for architectures from search, with 6 random seeds</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure</head><label></label><figDesc>Figure 5: Comparison of Model Architecture and Latency on A100</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Pretraining Loss curve using NVLamb Optimizer for BERT Base and PAR BERT Base models datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Bits Per Character (bpc) on enwiki8 and text8 and Perplexity (PPL) on WikiText-103 for Transformer-XL models.</figDesc><table><row><cell>Model</cell><cell cols="5">Architecture Latency on A100 (ms) SQuAD v1.1 SST-2 MRPC</cell></row><row><cell>DistilBERT*</cell><cell>(sf)x6</cell><cell>5.3 +</cell><cell>86.9</cell><cell>91.3</cell><cell>87.5</cell></row><row><cell>BERT Base</cell><cell>(sf)x12</cell><cell>8.6</cell><cell>88.4</cell><cell>91.5</cell><cell>88.7</cell></row><row><cell cols="2">PAR BERT Base (sff)x5 (f)x9</cell><cell>5.7</cell><cell>87.4</cell><cell>91.6</cell><cell>89.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table A1 :</head><label>A1</label><figDesc>Hyper parameter modifications made to Transformer-XL Base</figDesc><table><row><cell>Model</cell><cell cols="2">Architecture #Params #GFLOPs Latency</cell></row><row><cell></cell><cell></cell><cell>on A100 (ms)</cell></row><row><cell>Transformer-XL Base</cell><cell>(sf)x16 192M</cell><cell>27 15.2</cell></row><row><cell cols="2">Sandwich Transformer Base (s)x6 (sf)x10 (f)x6 192M</cell><cell>27 15.2</cell></row><row><cell>PAR Transformer Base</cell><cell>(sfff)x6 (f)x8 200M</cell><cell>17 9.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table A2 :</head><label>A2</label><figDesc>Flops and Parameters with respect to Latency for Base Models A.2 #Parameters #Flops with respect to Latency</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table A3: Latency and Valid Perplexity on WikiText-103 dataset with respect to training steps A.3 Accuracy with respect to training steps</figDesc><table><row><cell>Model</cell><cell cols="2">Valid PPL Valid PPL</cell></row><row><cell></cell><cell>@ 40k</cell><cell>@ 140k</cell></row><row><cell>Transformer-XL Base</cell><cell>23.3</cell><cell>22.2</cell></row><row><cell>Sandwich Transformer Base</cell><cell>23.4</cell><cell>22.4</cell></row><row><cell>PAR Transformer Base</cell><cell>23.3</cell><cell>22.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Character-level language modeling with deeper self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dokook</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Electra: Pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-head attention: Collaborate instead of concatenate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Andreas Loukas, and Martin Jaggi. 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>length context</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thor</forename><surname>Johnsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swetha</forename><surname>Mandava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Sharath Turuvekere Sreenivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allison</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Puri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Bert meets gpus</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Fang Wang, and Qun Liu. 2020. Tinybert: Distilling bert for natural language understanding</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Revealing the dark secrets of bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Large text compression benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Mahoney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pruning a bert-based question answering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Mccarley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Are sixteen heads really better than one?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improving transformer models by reordering their sublayers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<title level="m">The evolved transformer</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pretraining bert with layer wise adaptive learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharath</forename><surname>Sreenivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swetha</forename><surname>Mandava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Energy and policy considerations for deep learning in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Augmenting self-attention with persistent memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><forename type="middle">E</forename><surname>Sainbayar Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayden</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Sung</forename><surname>Ferng</surname></persName>
		</author>
		<title level="m">Hyung Won Chung, and Jason Riesa. 2020. Finding fast transformers: One-shot neural architecture search by component composition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<title level="m">Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Hat: Hardware-aware transformers for efficient natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Large batch optimization for deep learning: Training bert in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
