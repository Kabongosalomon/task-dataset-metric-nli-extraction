<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Heavy-tailed Representations, Text Polarity Classification &amp; Data Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Jalalzai</surname></persName>
							<email>hamid.jalalzai@telecom-paris.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Colombo</surname></persName>
							<email>pierre.colombo@telecom-paris.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chlo?</forename><surname>Clavel</surname></persName>
							<email>chloe.clavel@telecom-paris.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
							<email>eric.gaussier@imag.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanna</forename><surname>Varni</surname></persName>
							<email>giovanna.varni@telecom-paris.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vignon</surname></persName>
							<email>emmanuel.vignon@fr.ibm.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Sabourin</surname></persName>
							<email>anne.sabourin@telecom-paris.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LTCI</orgName>
								<orgName type="institution" key="instit2">T?l?com Paris Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">IBM France LTCI</orgName>
								<address>
									<settlement>T?l?com Paris Institut Polytechnique de Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">LTCI, T?l?com Paris Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Grenoble Alpes, CNRS, Grenoble INP, LIG</orgName>
								<orgName type="institution">Univ</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">LTCI, T?l?com Paris Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">IBM</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">LTCI, T?l?com Paris Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Heavy-tailed Representations, Text Polarity Classification &amp; Data Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The dominant approaches to text representation in natural language rely on learning embeddings on massive corpora which have convenient properties such as compositionality and distance preservation. In this paper, we develop a novel method to learn a heavy-tailed embedding with desirable regularity properties regarding the distributional tails, which allows to analyze the points far away from the distribution bulk using the framework of multivariate extreme value theory. In particular, a classifier dedicated to the tails of the proposed embedding is obtained which exhibits a scale invariance property exploited in a novel text generation method for label preserving dataset augmentation. Experiments on synthetic and real text data show the relevance of the proposed framework and confirm that this method generates meaningful sentences with controllable attributes, e.g. positive or negative sentiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representing the meaning of natural language in a mathematically grounded way is a scientific challenge that has received increasing attention with the explosion of digital content and text data in the last decade. Relying on the richness of contents, several embeddings have been proposed <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b18">19]</ref> with demonstrated efficiency for the considered tasks when learnt on massive datasets. * Both authors contributed equally 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. However, none of these embeddings take into account the fact that word frequency distributions are heavy tailed <ref type="bibr">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b40">41]</ref>, so that extremes are naturally present in texts (see also <ref type="figure">Fig. 6a</ref> and 6b in the supplementary material). Similarly, <ref type="bibr">[3]</ref> shows that, contrary to image taxonomies, the underlying distributions for words and documents in large scale textual taxonomies are also heavy tailed. Exploiting this information, several studies, as <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39]</ref>, were able to improve text mining applications by accurately modeling the tails of textual elements. In this work, we rely on the framework of multivariate extreme value analysis, based on extreme value theory (EVT) which focuses on the distributional tails. EVT is valid under a regularity assumption which amounts to a homogeneity property above large thresholds: the tail behavior of the considered variables must be well approximated by a power law, see Section 2 for a rigorous statement. The tail region (where samples are considered as extreme) of the input variable x ? R d is of the kind { x ? t}, for a large threshold t. The latter is typically chosen such that a small but non negligible proportion of the data is considered as extreme, namely 25% in our experiments. A major advantage of this framework in the case of labeled data <ref type="bibr" target="#b30">[31]</ref> is that classification on the tail regions may be performed using the angle ?(x) = x ?1 x only, see <ref type="figure" target="#fig_0">Figure 1</ref>. The main idea behind the present paper is to take advantage of the scale invariance for two tasks regarding sentiment analysis of text data: (i) Improved classification of extreme inputs, (ii) Label preserving data augmentation, as the most probable label of an input x is unchanged by multiplying x by ? &gt; 1. EVT in a machine learning framework has received increasing attention in the past few years. Learning tasks considered so far include anomaly detection <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b53">54]</ref>, anomaly clustering <ref type="bibr" target="#b8">[9]</ref>, unsupervised learning <ref type="bibr" target="#b22">[23]</ref>, online learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">1]</ref>, dimension reduction and support identification <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30]</ref>. The present paper builds upon the methodological framework proposed by Jalalzai et al. <ref type="bibr" target="#b30">[31]</ref> for classification in extreme regions. The goal of Jalalzai et al. <ref type="bibr" target="#b30">[31]</ref> is to improve the performance of classifiers g(x) issued from Empirical Risk Minimization (ERM) on the tail regions { x &gt; t} Indeed, they argue that for very large t, there is no guarantee that g would perform well conditionally to { X &gt; t}, precisely because of the scarcity of such examples in the training set. They thus propose to train a specific classifier dedicated to extremes leveraging the probabilistic structure of the tails. Jalalzai et al. <ref type="bibr" target="#b30">[31]</ref> demonstrate the usefulness of their framework with simulated and some real world datasets. However, there is no reason to assume that the previously mentioned text embeddings satisfy the required regularity assumptions. The aim of the present work is to extend <ref type="bibr" target="#b30">[31]</ref>'s methodology to datasets which do not satisfy their assumptions, in particular to text datasets embedded by state of the art techniques. This is achieved by the algorithm Learning a Heavy Tailed Representation (in short LHTR) which learns a transformation mapping the input data X onto a random vector Z which does satisfy the aforementioned assumptions. The transformation is learnt by an adversarial strategy <ref type="bibr" target="#b26">[27]</ref>.</p><p>In Appendix C we propose an interpretation of the extreme nature of an input in both LHTR and BERT representations. In a word, these sequences are longer and are more difficult to handle (for next token prediction and classification tasks) than non extreme ones.</p><p>Our second contribution is a novel data augmentation mechanism GENELIEX which takes advantage of the scale invariance properties of Z to generate synthetic sequences that keep invariant the attribute of the original sequence. Label preserving data augmentation is an effective solution to the data scarcity problem and is an efficient pre-processing step for moderate dimensional datasets <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>. Adapting these methods to NLP problems remains a challenging issue. The problem consists in constructing a transformation h such that for any sample x with label y(x), the generated sample h(x) would remain label consistent: y h(x) = y(x) <ref type="bibr" target="#b46">[47]</ref>. The dominant approaches for text data augmentation rely on word level transformations such as synonym replacement, slot filling, swap deletion <ref type="bibr" target="#b56">[57]</ref> using external resources such as wordnet <ref type="bibr" target="#b42">[43]</ref>. Linguistic based approaches can also be combined with vectorial representations provided by language models <ref type="bibr" target="#b32">[33]</ref>. However, to the best of our knowledge, building a vectorial transformation without using any external linguistic resources remains an open problem. In this work, as the label y h(x) is unknown as soon as h(x) does not belong to the training set, we address this issue by learning both an embedding ? and a classifier g satisfying a relaxed version of the problem above mentioned, namely ?? ? 1 g h ? (?(x)) = g ?(x) .</p><p>(1)</p><p>For mathematical reasons which will appear clearly in Section 2.2, h ? is chosen as the homothety with scale factor ?, h ? (x) = ?x. In this paper, we work with output vectors issued by BERT <ref type="bibr" target="#b18">[19]</ref>. BERT and its variants are currently the most widely used language model but we emphasize that the proposed methodology could equally be applied using any other representation as input. BERT embedding does not satisfy the regularity properties required by EVT (see the results from statistical tests performed in Appendix B.5) Besides, there is no reason why a classifier g trained on such embedding would be scale invariant, i.e. would satisfy for a given sequence u, embedded as x, g(h ? (x)) = g(x) ?? ? 1. On the classification task, we demonstrate on two datasets of sentiment analysis that the embedding learnt by LHTR on top of BERT is indeed following a heavy-tailed distribution. Besides, a classifier trained on the embedding learnt by LHTR outperforms the same classifier trained on BERT. On the dataset augmentation task, quantitative and qualitative experiments demonstrate the ability of GENELIEX to generate new sequences while preserving labels.</p><p>The rest of this paper is organized as follows. Section 2 introduces the necessary background in multivariate extremes. The methodology we propose is detailed at length in Section 3. Illustrative numerical experiments on both synthetic and real data are gathered in sections 4 and 5. Further comments and experimental results are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Extreme values, heavy tails and regular variation</head><p>Extreme value analysis is a branch of statistics whose main focus is on events characterized by an unusually high value of a monitored quantity. A convenient working assumption in EVT is regular variation. A real-valued random variable X is regularly varying with index ? &gt; 0, a property denoted as RV (?), if and only if there exists a function b(t) &gt; 0, with b(t) ? ? as t ? ?, such that for any fixed x &gt; 0:</p><formula xml:id="formula_0">tP {X/b(t) &gt; x} ???? t?? x ?? . In the multivariate case X = (X 1 , . . . , X d ) ? R d ,</formula><p>it is usually assumed that a preliminary component-wise transformation has been applied so that each margin X j is RV (1) with b(t) = t and takes only positive values. X is standard multivariate regularly varying if there exists a positive Radon measure ? on [0, ?] d \{0}</p><formula xml:id="formula_1">tP t ?1 X ? A ???? t?? ?(A),<label>(2)</label></formula><p>for any Borelian set A ? [0, ?] d which is bounded away from 0 and such that the limit measure ? of the boundary ?A is zero. For a complete introduction to the theory of Regular Variation, the reader may refer to <ref type="bibr" target="#b47">[48]</ref>. The measure ? may be understood as the limit distribution of tail events.</p><formula xml:id="formula_2">In (2), ? is homogeneous of order ?1, that is ?(tA) = t ?1 ?(A), t &gt; 0, A ? [0, ?] d \ {0}.</formula><p>This scale invariance is key for our purposes, as detailed in Section 2.2. The main idea behind extreme value analysis is to learn relevant features of ? using the largest available data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classification in extreme regions</head><p>We now recall the classification setup for extremes as introduced in <ref type="bibr" target="#b30">[31]</ref>. Let (X, Y ) ? R d + ? {?1, 1} be a random pair. Authors of <ref type="bibr" target="#b30">[31]</ref> assume standard regular variation for both classes, that is tP {X ? tA | Y = ?1} ? ? ? (A), where A is as in <ref type="bibr">(2)</ref>. Let ? be any norm on R d and consider the risk of a classifier g : R d + ? {?1} above a radial threshold t, L t (g) = P {Y = g(X) | X &gt; t} .</p><p>The goal is to minimize the asymptotic risk in the extremes L ? (g) = lim sup t?? L t (g). Using the scale invariance property of ?, under additional mild regularity assumptions concerning the regression function, namely uniform convergence to the limit at infinity, one can prove the following result (see <ref type="bibr" target="#b30">[31]</ref>, Theorem 1): there exists a classifier g ? depending on the pseudo-angle ?(x) = x ?1 x only, that is g ? (x) = g ? ?(x) , which is asymptotically optimal in terms of classification risk, i.e. L ? (g ? ) = inf g measurable L ? (g). Notice that for x ? R d + \ {0}, the angle ?(x) belongs to the positive orthant of the unit sphere, denoted by S in the sequel. As a consequence, the optimal classifiers on extreme regions are based on indicator functions of truncated cones on the kind { x &gt; t, ?(x) ? B}, where B ? S, see <ref type="figure" target="#fig_0">Figure 1</ref>. We emphasize that the labels provided by such a classifier remain unchanged when rescaling the samples by a factor ? ? 1 (i.e. g(x) = g(?(x)) = g(?(?x)), ?x ? {x, x ? t}). The angular structure of the optimal classifier g ? is the basis for the following ERM strategy using the most extreme points of a dataset. Let G S be a class of angular classifiers defined on the sphere S with finite VC dimension V G S &lt; ?. By extension, for any</p><formula xml:id="formula_4">x ? R d + and g ? G S , g(x) = g ?(x) ? {?1, 1}. Given n training data {(X i , Y i )} n i=1 made of i.i.d copies of (X, Y )</formula><p>, sorting the training observations by decreasing order of magnitude, let X (i) (with corresponding sorted label Y (i) ) denote the i-th order statistic, i.e. X (1) ? . . . ? X (n) . The empirical risk for the k largest observations L k (g) = 1 k k i=1 1{Y (i) = g(?(X (i) ))} is an empirical version of the risk L t(k) (g) as defined in <ref type="formula" target="#formula_3">(3)</ref> where t(k) is a (1 ? k/n)-quantile of the norm, P { X &gt; t(k)} = k/n. Selection of k is a bias-variance compromise, see Appendix B for further discussion. The strategy promoted by <ref type="bibr" target="#b30">[31]</ref> is to use g k = argmin g?G S L k (g), for classification in the extreme region {x ? R d + :</p><p>x &gt; t(k)}. The following result provides guarantees concerning the excess risk of g k compared with the Bayes risk above level t = t(k), L t = inf g measurable L t (g). </p><formula xml:id="formula_5">L t(k) ( g k ) ? L t(k) ? 1 ? k 2(1 ? k/n) log(2/?) + C V G S log(1/?) + 1 k 5 + 2 log(1/?) + log(1/?)(C V G S + ? 2) + inf g?G S L t(k) (g) ? L t(k) ,</formula><p>where C is a universal constant.</p><p>In the present work we do not assume that the baseline representation X for text data satisfies the assumptions of Theorem 1. Instead, our goal is is to render the latter theoretical framework applicable by learning a representation which satisfies the regular variation condition given in (2), hereafter referred as Condition (2) which is the main assumption for Theorem 1 to hold. Our experiments demonstrate empirically that enforcing Condition (2) is enough for our purposes, namely improved classification and label preserving data augmentation, see Appendix B.3 for further discussion.</p><p>3 Heavy-tailed Text Embeddings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning a heavy-tailed representation</head><p>We now introduce a novel algorithm Learning a heavy-tailed representation (LHTR) for text data from high dimensional vectors as issued by pre-trained embeddings such as BERT. The idea behind is to modify the output X of BERT so that classification in the tail regions enjoys the statistical guarantees presented in Section 2, while classification in the bulk (where many training points are available) can still be performed using standard models. Stated otherwise, LHTR increases the information carried by the resulting vector Z = ?(X) ? R d regarding the label Y in the tail regions of Z in order to improve the performance of a downstream classifier. In addition LHTR is a building block of the data augmentation algorithm GENELIEX detailed in Section 3.2. LHTR proceeds by training an encoding function ? in such a way that (i) the marginal distribution q(z) of the code Z be close to a user-specified heavy tailed target distribution p satisfying the regularity condition (2); and (ii) the classification loss of a multilayer perceptron trained on the code Z be small.</p><p>A major difference distinguishing LHTR from existing auto-encoding schemes is that the target distribution on the latent space is not chosen as a Gaussian distribution but as a heavy-tailed, regularly varying one. A workable example of such a target is provided in our experiments (Section 4). As the Bayes classifier (i.e. the optimal one among all possible classifiers) in the extreme region has a potentially different structure from the Bayes classifier on the bulk (recall from Section 2 that the optimal classifier at infinity depends on the angle ?(x) only), LHTR trains two different classifiers, g ext on the extreme region of the latent space on the one hand, and g bulk on its complementary set on the other hand. Given a high threshold t, the extreme region of the latent space is defined as the set {z : z &gt; t}. In practice, the threshold t is chosen as an empirical quantile of order (1 ? ?) (for some small, fixed ?) of the norm of encoded data Z i = ?(X i ) . The classifier trained by LHTR is thus of the kind g(z) = g ext (z)1{ z &gt; t} + g bulk (z)1{ z ? t}. If the downstream task is classification on the whole input space, in the end the bulk classifier g bulk may be replaced with any other classifier g trained on the original input data X restricted to the non-extreme samples (i.e. {X i , ?(X i ) ? t}). Indeed training g bulk only serves as an intermediate step to learn an adequate representation ?.</p><p>Remark 1 Recall from Section 2.2 that the optimal classifier in the extreme region as t ? ? depends on the angular component ?(x) only, or in other words, is scale invariant. One can thus reasonably expect the trained classifier g ext (z) to enjoy the same property. This scale invariance is indeed verified in our experiments (see Sections 4 and 5) and is the starting point for our data augmentation algorithm in Section 3.2. An alternative strategy would be to train an angular classifier, i.e. to impose scale invariance. However in preliminary experiments (not shown here), the resulting classifier was less efficient and we decided against this option in view of the scale invariance and better performance of the unconstrained classifier.</p><p>The goal of LHTR is to minimize the weighted risk</p><formula xml:id="formula_6">R(?, g ext , g bulk ) =? 1 P Y = g ext (Z), Z ? t + ? 2 P Y = g bulk (Z), Z &lt; t + ? 3 D(q(z), p(z))</formula><p>where Z = ?(X), D is the Jensen-Shannon distance between the heavy tailed target distribution p and the code distribution q, and ? 1 , ? 2 , ? 3 are positive weights. Following common practice in the adversarial literature, the Jensen-Shannon distance is approached (up to a constant term) by the empirical proxy L(q, p) = sup D?? L(q, p, D), with L(q, p,</p><formula xml:id="formula_7">D) = 1 m m i=1 log D(Z i ) + log 1 ? D(Z i ) ,</formula><p>where ? is a wide class of discriminant functions valued in [0, 1], and where independent samples Z i ,Z i are respectively sampled from the target distribution and the code distribution q. Further details on adversarial learning are provided in Appendix A.1. The classifiers g ext , g bulk are of the form g ext (z) = 21{C ext (z) &gt; 1/2) ? 1, g bulk (z) = 21{C bulk (z) &gt; 1/2) ? 1 where C ext , C bulk are also discriminant functions valued in [0, 1]. Following common practice, we shall refer to C ext , C bulk as classifiers as well. In the end, LHTR solves the following min-max problem</p><formula xml:id="formula_8">inf C ext ,C bulk ,? sup D R(?, C ext , C bulk , D) with R(?, C ext , C bulk , D) = ? 1 k k i=1 (Y (i) , C ext (Z (i) )) + ? 2 n ? k n?k i=k+1 (Y (i) , C bulk (Z (i) )) + ? 3L (q, p, D),</formula><p>where {Z (i) = ?(X (i) ), i = 1, . . . , n} are the encoded observations with associated labels Y (i) sorted by decreasing magnitude of Z (i.e. Z (1) ? ? ? ? ? Z (n) ), k = ?n is the number of extreme samples among the n encoded observations and (y,</p><formula xml:id="formula_9">C(x)) = ?(y log C(x) + (1 ? y) log(1 ? C(x)), y ? {0, 1}</formula><p>is the negative log-likelihood of the discriminant function C(x) ? (0, 1). A summary of LHTR and an illustration of its workflow are provided in Appendices A.2 and A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A heavy-tailed representation for dataset augmentation</head><p>We now introduce GENELIEX (Generating Label Invariant sequences from Extremes), a data augmentation algorithm, which relies on the label invariance property under rescaling of the classifier for the extremes learnt by LHTR. GENELIEX considers input sentences as sequences and follows the seq2seq approach <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7]</ref>. It trains a Transformer Decoder <ref type="bibr" target="#b54">[55]</ref> G ext on the extreme regions.</p><p>For an input sequence U = (u 1 , . . . , u T ) of length T , represented as X U by BERT with latent code Z = ?(X U ) lying in the extreme regions, GENELIEX produces, through its decoder G ext M sequences U j where j ? {1, . . . , M }. The M decoded sequences correspond to the codes</p><formula xml:id="formula_10">{? j Z, j ? {1, . . . , M }} where ? j &gt; 1.</formula><p>To generate sequences, the decoder iteratively takes as input the previously generated word (the first word being a start symbol), updates its internal state, and returns the next word with the highest probability. This process is repeated until either the decoder generates a stop symbol or the length of the generated sequence reaches the maximum length (T max ). To train the decoder G ext : R d ? 1, . . . , |V| Tmax where V is the vocabulary on the extreme regions, GENELIEX requires an additional dataset D gn = (U 1 , . . . , U n ) (not necessarily labeled) with associated representation via BERT (X U,1 , . . . , X U,n ). Learning is carried out by optimising the classical negative log-likelihood of individual tokens gen . The latter is defined as</p><formula xml:id="formula_11">gen U, G ext (?(X)) def = Tmax t=1 v?V 1{u t = v} log p v,t ,</formula><p>where p v,t is the probability predicted by G ext that the t th word is equal to v. A detailed description of the training step of GENELIEX is provided in Algorithm 2 in Appendix A.3, see also Appendix A.2 for an illustrative diagram.</p><p>Remark 2 Note that the proposed method only augments data on the extreme regions. A general data augmentation algorithm can be obtained by combining this approach with any other algorithm on the original input data X whose latent code Z = ?(X U ) does not lie in the extreme regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments : Classification</head><p>In our experiments we work with the infinity norm. The proportion of extreme samples in the training step of LHTR is chosen as ? = 1/4. The threshold t defining the extreme region { x &gt; t} in the test set is t = Z ( ?n ) as returned by LHTR. We denote by T test and T train respectively the extreme test and train sets thus defined. Classifiers C bulk , C ext involved in LHTR are Multi Layer Perceptrons (MLP), see Appendix B.6 for a full description of the architectures. Heavy-tailed distribution. The regularly varying target distribution is chosen as a multivariate logistic distribution with parameter ? = 0.9, refer to Appendix B.4 for details and an illustration with various values of ?. This distribution is widely used in the context of extreme values analysis <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b23">24]</ref> and differ from the classical logistic distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Toy example: about LHTR</head><p>We start with a simple bivariate illustration of the heavy tailed representation learnt by LHTR. Our goal is to provide insight on how the learnt mapping ? acts on the input space and how the transformation affects the definition of extremes (recall that extreme samples are defined as those samples which norm exceeds an empirical quantile). Labeled samples are simulated from a Gaussian mixture distribution with two components of identical weight. The label indicates the component from which the point is generated. LHTR is trained on 2250 examples and a testing set of size 750 is shown in <ref type="figure" target="#fig_3">Figure 2</ref>. The testing samples in the input space ( <ref type="figure" target="#fig_3">Figure 2a</ref>) are mapped onto the latent space via ? ( <ref type="figure" target="#fig_3">Figure 2c</ref>) In <ref type="figure" target="#fig_3">Figure 2b</ref>, the extreme raw observations are selected according to their norm after a component-wise standardisation of X i , refer to Appendix B for details. The extreme threshold t is chosen as the 75% empirical quantile of the norm on the training set in the input space. Notice in the latter figure the class imbalance among extremes. In <ref type="figure" target="#fig_3">Figure 2c</ref>, extremes are selected as the 25% samples with the largest norm in the latent space. <ref type="figure" target="#fig_3">Figure 2d</ref> is similar to <ref type="figure" target="#fig_3">Figure 2b</ref> except for the selection of extremes which is performed in the latent space as in <ref type="figure" target="#fig_3">Figure 2c</ref>. On this toy example, the adversarial strategy appears to succeed in learning a code which distribution is close to the logistic target, as illustrated by the similarity between <ref type="figure" target="#fig_3">Figure 2c</ref> and <ref type="figure" target="#fig_8">Figure 5a</ref> in the supplementary. In addition, the heavy tailed representation allows a more balanced selection of extremes than the input representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Application to positive vs. negative classification of sequences</head><p>In this section, we dissect LHTR to better understand the relative importance of: (i) working with a heavy-tailed representation, (ii) training two independent classifiers: one dedicated to the bulk and the second one dedicated to the extremes. In addition, we verify experimentally that the latter classifier is scale invariant, which is neither the case for the former, nor for a classifier trained on BERT input. Experimental settings. We compare the performance of three models. The baseline NN model is a MLP trained on BERT. The second model LHTR 1 is a variant of LHTR where a single MLP (C) is trained on the output of the encoder ?, using all the available data, both extreme and non extreme ones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X(2)</head><p>labeled -1 labeled +1</p><p>(a) Comparing LHTR 1 with NN model assesses the relevance of working with heavy-tailed embeddings.</p><formula xml:id="formula_12">2 1 0 1 2 X(1) 4 3 2 1 0 1 2 3 X(2) bulk -1 bulk +1 extreme -1 extreme +1 (b) 0 1 2 3 4 5 Z(1) 0 1 2 3 4 5 6 7 Z(2) bulk -1 bulk +1 extreme -1 extreme +1 (c) 2 1 0 1 2 X(1)</formula><p>Since LHTR 1 is obtained by using LHTR with C ext = C bulk , comparing LHTR 1 with LHTR validates the use of two separate classifiers so that extremes are handled in a specific manner. As we make no claim concerning the usefulness of LHTR in the bulk, at the prediction step we suggest working with a combination of two models: LHTR with C ext for extreme samples and any other off-the-shelf ML tool for the remaining samples (e.g. NN model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets.</head><p>In our experiments we rely on two large datasets from Amazon (231k reviews) <ref type="bibr" target="#b41">[42]</ref> and from Yelp (1,450k reviews) <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b36">37]</ref>. Reviews, (made of multiple sentences) with a rating greater than or equal to 4 / 5 are labeled as +1, while those with a rating smaller or equal to 2 / 5 are labeled as ?1.</p><p>The gap in reviews' ratings is designed to avoid any overlap between labels of different contents. Results. <ref type="figure">Figure 3</ref> gathers the results obtained by the three considered classifiers on the tail regions of the two datasets mentioned above. To illustrate the generalization ability of the proposed classifier in the extreme regions we consider nested subsets of the extreme test set T test ,</p><formula xml:id="formula_13">T ? = {z ? T test , z ? ?t}, ? ? 1. For all factor ? ? 1, T ? ? T test .</formula><p>The greater ?, the fewer the samples retained for evaluation and the greater their norms. On both datasets, LHTR 1 outperforms the baseline NN model. This shows the improvement offered by the heavy-tailed embedding on the extreme region. In addition, LHTR 1 is in turn largely outperformed by the classifier LHTR, which proves the importance of working with two separate classifiers. The performance of the proposed model respectively on the bulk region, tail region and overall, is reported in <ref type="table" target="#tab_0">Table 1</ref>, which shows that using a specific classifier dedicated to extremes improves the overall performance.  Comparison with existing work. We compare GENELIEX with two state of the art methods for dataset augmentation, Wei and Zou <ref type="bibr" target="#b56">[57]</ref> and Kobayashi <ref type="bibr" target="#b32">[33]</ref>. Contrarily to these works which use heuristics and a synonym dictionary, GENELIEX does not require any linguistic resource. To ensure that the improvement brought by GENELIEX is not only due to BERT, we have updated the method in <ref type="bibr" target="#b32">[33]</ref> with a BERT language model (see Appendix B.7 for details and <ref type="table">Table 7</ref> for hyperparameters). Evaluation Metrics. Automatic evaluation of generative models for text is still an open research problem. We rely both on perceptive evaluation and automatic measures to evaluate our model through four criteria (C1, C2, C3,C4). C1 measures Cohesion <ref type="bibr" target="#b16">[17]</ref> (Are the generated sequences grammatically and semantically consistent?). C2 (named Sent. in <ref type="table">Table 3</ref>) evaluates label conservation (Does the expressed sentiment in the generated sequence match the sentiment of the input sequence?). C3 measures the diversity <ref type="bibr" target="#b35">[36]</ref> (corresponding to dist1 or dist2 in <ref type="table">Table 3</ref> 2 ) of the sequences (Does the augmented dataset contain diverse sequences?). Augmenting the training set with very diverse sequences can lead to better classification performance. C4 measures the improvement in terms of F1 score when training a classifier (fastText <ref type="bibr" target="#b31">[32]</ref>) on the augmented training set (Does the augmented dataset improve classification performance?). Datasets. GENELIEX is evaluated on two datasets, a medium and a large one (see <ref type="bibr" target="#b50">[51]</ref>) which respectively contains 1k and 10k labeled samples. In both cases, we have access to D gn a dataset of 80k unlabeled samples. Datasets are randomly sampled from Amazon and Yelp. Experiment description. We augment extreme regions of each dataset according to three algorithms: GENELIEX (with scaling factor ? ranging from 1 to 1.5), Kobayashi <ref type="bibr" target="#b32">[33]</ref>, and Wei and Zou <ref type="bibr" target="#b56">[57]</ref>. For each train set's sequence considered as extreme, 10 new sequences are generated using each algorithm. Appendix B.7 gathers further details. For experiment C4 the test set contains 10 4 sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Automatic measures. The results of C3 and C4 evaluation are reported in <ref type="table" target="#tab_2">Table 2</ref>. Augmented data with GENELIEX are more diverse than the one augmented with Kobayashi <ref type="bibr" target="#b32">[33]</ref> and Wei and Zou <ref type="bibr" target="#b56">[57]</ref>. The F1-score with dataset augmentation performed by GENELIEX outperforms the aforementioned methods on Amazon in medium and large dataset and on Yelp for the medium dataset. It equals</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amazon</head><p>Yelp   <ref type="table">Table 3</ref>: Qualitative evaluation with three turkers. Sent. stands for sentiment label preservation. The Krippendorff Alpha for Amazon is ? = 0.28 on the sentiment classification and ? = 0.20 for cohesion. The Krippendorff Alpha for Yelp is ? = 0.57 on the sentiment classification and ? = 0.48 for cohesion.</p><formula xml:id="formula_14">Medium Large Medium Large F1 dist1/dist2 F1 dist1/dist2 F1 dist1/dist2 F1 dist1/dist2</formula><p>state of the art performances on Yelp for the large dataset. As expected, for all three algorithms, the benefits of data augmentation decrease as the original training dataset size increases. Interestingly, we observe a strong correlation between more diverse sequences in the extreme regions and higher F1 score: the more diverse the augmented dataset, the higher the F1 score. More diverse sequences are thus more likely to lead to better improvement on downstream tasks (e.g. classification). Perceptive Measures. To evaluate C1, C2, three turkers were asked to annotate the cohesion and the sentiment of 100 generated sequences for each algorithm and for the raw data. F1 scores of this evaluation are reported in <ref type="table">Table 3</ref>. Grammar evaluation confirms the findings of <ref type="bibr" target="#b56">[57]</ref> showing that random swaps and deletions do not always maintain the cohesion of the sequence. In contrast, GENELIEX and Kobayashi <ref type="bibr" target="#b32">[33]</ref>, using vectorial representations, produce more coherent sequences.</p><p>Concerning sentiment label preservation, on Yelp, GENELIEX achieves the highest score which confirms the observed improvement reported in <ref type="table" target="#tab_2">Table 2</ref>. On Amazon, turker annotations with data from GENELIEX obtain a lower F1-score than from Kobayashi <ref type="bibr" target="#b32">[33]</ref>. This does not correlate with results in <ref type="table" target="#tab_2">Table 2</ref> and may be explained by a lower Krippendorff Alpha 3 on Amazon (? = 0.20) than on Yelp (? = 0.57).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Broader Impact</head><p>In this work, we propose a method resulting in heavy-tailed text embeddings. As we make no assumption on the nature of the input data, the suggested method is not limited to textual data and can be extended to any type of modality (e.g. audio, video, images). A classifier, trained on aforementioned embedding is dilation invariant (see Equation 1) on the extreme region. A dilation invariant classifier enables better generalization for new samples falling out of the training envelop. For critical application ranging from web content filtering (e.g. spam <ref type="bibr" target="#b27">[28]</ref>, hate speech detection <ref type="bibr" target="#b17">[18]</ref>, fake news <ref type="bibr" target="#b43">[44]</ref> or multi-modal classification <ref type="bibr" target="#b21">[22]</ref>) to medical case reports to court decisions it is crucial to build classifiers with lower generalization error. The scale invariance property can also be exploited to automatically augment a small dataset on its extreme region. For application where data collection requires a huge effort both in time and cost (e.g. industrial factory design, classification for rare language <ref type="bibr">[4]</ref>), beyond industrial aspect, active learning problems involving heavy-tailed data may highly benefit from our data augmentation approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgement</head><p>Anne Sabourin was partly supported by the Chaire Stress testing from Ecole Polytechnique and BNP Paribas. Concerning Eric Gaussier, this project partly fits within the MIAI project (ANR-19-P3IA-0003).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIAL : HEAVY-TAILED REPRESENTATIONS, TEXT POLARITY CLASSIFICATION &amp; DATA AUGMENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Background on Adversarial Learning</head><p>Adversarial networks, introduced in [26], form a system where two neural networks are competing. A first model G, called the generator, generates samples as close as possible to the input dataset. A second model D, called the discriminator, aims at distinguishing samples produced by the generator from the input dataset. The goal of the generator is to maximize the probability of the discriminator making a mistake. Hence, if P input is the distribution of the input dataset then the adversarial network intends to minimize the distance (as measured by the Jensen-Shannon divergence) between the distribution of the generated data P G and P input . In short, the problem is a minmax game with value function V (D, G)</p><formula xml:id="formula_15">min G max D V (D, G) =E x?Pinput [log D(x)] + E z?P G [log 1 ? D(G(z)) ].</formula><p>Auto-encoders and derivatives <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21]</ref> form a subclass of neural networks whose purpose is to build a suitable representation by learning encoding and decoding functions which capture the core properties of the input data. An adversarial auto-encoder (see <ref type="bibr" target="#b39">[40]</ref>) is a specific kind of auto-encoders where the encoder plays the role of the generator of an adversarial network. Thus the latent code is forced to follow a given distribution while containing information relevant to reconstructing the input.</p><p>In the remaining of this paper, a similar adversarial encoder constrains the encoded representation to be heavy-tailed. <ref type="figure" target="#fig_5">Figure 4</ref> provides an overview of the different algorithms proposed in the paper. <ref type="figure" target="#fig_5">Figure 4a</ref> describes the pipeline for LHTR detailed in Algorithm 1. <ref type="figure" target="#fig_5">Figure 4b</ref> describes the pipeline for the comparative baseline LHTR 1 where C ext = C bulk . <ref type="figure" target="#fig_5">Figure 4c</ref> illustrates the pipeline for the baseline classifier trained on BERT. <ref type="figure" target="#fig_5">Figure 4d</ref> describes GENELIEX described in Algorithm 2, note that the hatched components are inherited from LHTR and are not used in the workflow. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Models Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 LHTR and GENELIEX algorithm</head><p>This subsection provides detailed algorithm for both models LHTR and GENELIEX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 LHTR</head><p>INPUT: Weighting coef. ? 1 , ? 2 , ? 3 &gt; 0, Training dataset D n = {(X 1 , Y 1 ), . . . , (X n , Y n )}, batch size m, proportion of extremes ?, heavy tailed prior P Z . Initialization: parameters (?, ?, ? , ?) of the encoder ? ? , classifiers C ext ? , C bulk ? and discriminator D ? Optimization:</p><formula xml:id="formula_16">while (?, ?, ? , ?) not converged do Sample {(X 1 , Y 1 ) . . . , (X m , Y m )} from D n and defineZ i = ?(X i ), i ? m. Sample {Z 1 , . . . , Z m } from the prior P Z .</formula><p>Update ? by ascending:</p><formula xml:id="formula_17">? 3 m m i=1 log D ? (Z i ) + log(1 ? D ? (Z i )).</formula><p>Sort {Z i } i?{1,...,m} by decreasing order of magnitude ||Z (1) || ? . . . ? ||Z (m) ||. Update ? by descending:</p><formula xml:id="formula_18">L ext (?, ? ) def = ? 1 ?m ?m i=1 Y (i) , C ext ? (Z (i) ) .</formula><p>Update ? by descending:</p><formula xml:id="formula_19">L bulk (? , ? ) def = ? 2 m ? ?m m i= ?m +1 Y (i) , C bulk ? (Z (i) ) .</formula><p>Update ? by descending:</p><formula xml:id="formula_20">1 m m i=1 ?? 3 log D ? (Z i ) + L ext (?, ? ) + L bulk (? , ? ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>end while</head><p>Compute {Z i } i?{1,...,n} = ?(X i ) i?{1,...,n} Sort {Z i } i?{1,...,n} by decreasing order of magnitude ||Z (1) || ? . . . ||Z ( ?n ) || ? . . . ? ||Z (n) ||. OUTPUT: encoder ?, classifiers C ext for {x : ||?(x)|| ? t := ||Z ( ?n ) ||} and C bulk on the complementary set.  To the best of our knowledge, selection of k in extreme value analysis (in particular in Algorithm 1 and Algorithm 2) is still a vivid problem in EVT for which no absolute answer exists. As k gets large the number of extreme points increases including samples which are not large enough and deviates from the asymptotic distribution of extremes. Smaller values of k increase the variance of the classifier/generator. This bias-variance trade-off is beyond the scope of this paper.</p><formula xml:id="formula_21">L ext g (?) def = ? 1 ?m ?m i=1 gen. U (i) , G ext ? (Z (i) ) . end while Compute {Z i } i?{1,...,n} = ?(X i ) i?{1,...,n} Sort {Z i } i?{1,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Preliminary standardization for selecting extreme samples</head><p>In <ref type="figure" target="#fig_3">Figure 2b</ref> selecting the extreme samples on the input space is not a straightforward step as the two components of the vector are not on the same scale, componentwise standardisation is a natural and necessary preliminary step. Following common practice in multivariate extreme value analysis it was decided to standardise the input data (X i ) i?{1,...,n} by applying the rank-transformation: </p><formula xml:id="formula_22">T (x) = 1/ 1 ? F j (x) j=1,...,d for all x = (x 1 , . . . , x d ) ? R d where F j (x) def = 1 n+1 n i=1 1{X j i ? x} is</formula><formula xml:id="formula_23">{V i , V i ? V ( ?n ) }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Enforcing regularity assumptions in Theorem 1</head><p>The methodology in the present paper consists in learning a representation Z for text data via LHTR satisfying the regular variation condition (2). This condition is weaker than the assumptions from Theorem 1 for two reasons: first, it does not imply that each class (conditionally to the label Y ) is regularly varying, only that the distribution of Z (unconditionally to the label) is. Second, in Jalalzai et al. <ref type="bibr" target="#b30">[31]</ref>, it is additionally required that the regression function ?(z) = P {Y = +1 | Z = z} converges uniformly as z ? ?. Getting into details, one needs to introduce a limit random pair (Z ? , Y ? ) which distribution is the limit of P Y = ? , t ?1 Z ? ? Z &gt; t as t ? ?. Denote by ? ? the limiting regression function, ? ? (z) = P {Y ? = +1 | Z ? = z}. The required assumption is that sup</p><formula xml:id="formula_24">{z?R d + : z &gt;t} ?(z) ? ? ? (z) ???? t?? 0.<label>(4)</label></formula><p>Uniform convergence <ref type="formula" target="#formula_24">(4)</ref> is not enforced in LHTR and the question of how to enforce it together with regular variation of each class separately remains open. However, our experiments in sections 4 and 5 demonstrate that enforcing Condition <ref type="formula" target="#formula_1">(2)</ref> is enough for our purposes, namely improved classification and label preserving data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Logistic distribution</head><p>The logistic distribution with dependence parameter ?</p><formula xml:id="formula_25">? (0, 1] is defined in R d by its c.d.f. F (x) = exp ? ( d j=1 x (j) 1 ? ) ? .</formula><p>Samples from the logistic distribution can be simulated according to the algorithm proposed in Stephenson <ref type="bibr" target="#b51">[52]</ref>. <ref type="figure" target="#fig_8">Figure 5</ref> illustrates this distribution with various values of ?. Values of ? close to 1 yield non concomitant extremes, i.e. the probability of a simultaneous excess of a high threshold by more than one vector component is negligible. Conversely, for small values of ?, extreme values tend to occur simultaneously. These two distinct tail dependence structures are respectively called 'asymptotic independence' and 'asymptotic dependence' in the EVT terminology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Scale invariance comparison of BERT and LHTR</head><p>In this section, we compare LHTR and BERT and show that the latter is not scale invariant. For this preliminary experiment we rely on labeled fractions of both Amazon and Yelp datasets respectively denoted as Amazon small dataset and Yelp small dataset detailed in <ref type="bibr" target="#b33">[34]</ref>, each of them containing 1000 sequences from the large dataset. Both datasets are divided at random in a train set T train and T test . The train set represents 3 /4 of the whole dataset while the remaining samples represent the test set. We use the hyperparameters reported in <ref type="table">Table 4</ref>  <ref type="table">Table 4</ref>: Network architectures for Amazon small dataset and Yelp small dataset . The weight decay is set to 10 5 , the learning rate is set to 5 * 10 ?4 , the number of epochs is set to 500 and the batch size is set to 64.</p><p>BERT is not regularly varying. In order to show that X is not regularly varying, independence between X and a margin of ?(X) can be tested <ref type="bibr" target="#b13">[14]</ref>, which is easily done via correlation tests. Pearson correlation tests were run on the extreme samples of BERT and LHTR embeddings of Amazon small dataset and Yelp small dataset. The statistical tests were performed between all margins of ?(X i ) 1?i?n and X i 1?i?n .  <ref type="figure">Figure 6</ref>: Histograms of the p-values for the non-correlation test between ?(X i ) 1?i?n and X i 1?i?n on embeddings provided by BERT <ref type="figure">(Figure 6a</ref> and <ref type="figure">Figure 6b</ref>) or LHTR <ref type="figure">(Figure 6c</ref> and <ref type="figure">Figure 6d</ref>). <ref type="figure">Figure 6</ref> displays the distribution of the p-values of the correlation tests between the margins X j and the angle ?(X) for j ? {1, . . . d}, in a given representation (BERT or LHTR) for a given dataset. For both Amazon small dataset and Yelp small dataset the distribution of the p-values is shifted towards larger values in the representation of LHTR than in BERT, which means that the correlations are weaker in the former representation than in the latter. This phenomenon is more pronounced with Yelp small dataset than with Amazon small dataset. Thus, in BERT representation, even the largest data points exhibit a non negligible correlation between the radius and the angle and the regular variation condition does not seem to be satisfied. As a consequence, in a classification setup such as binary sentiment analysis detailed in Section 4.2), classifiers trained on BERT embedding are not guaranteed to be scale invariant. In other words for a representation X of a sequence U with a given label Y , the predicted label g(?X) is not necessarily constant for varying values of ? ? 1. <ref type="figure" target="#fig_10">Figure 7</ref> illustrates this fact on a particular example taken from Yelp small dataset. The color (white or black respectively) indicates the predicted class (respectively ?1 and +1). For values of ? close to 1, the predicted class is ?1 but the prediction shifts to class +1 for larger values of ?.  Scale invariance of LHTR. We provide here experimental evidence that LHTR's classifier g ext is scale invariant (as defined in Equation <ref type="formula">(1)</ref>). <ref type="figure">Figure 8</ref> displays the predictions g ext (?Z i ) for increasing values of the scale factor ? ? 1 and Z i belonging to T test , the set of samples considered as extreme in the learnt representation. For any such sample Z, the predicted label remains constant as ? varies, i.e. it is scale invariant, g ext (?Z) = g ext (Z), for all ? ? 1.  <ref type="figure">Figure 8</ref>: Scale invariance of g ext trained on LHTR: evolution of the predicted label g ext (?Z i ) (white or black for ?1/ + 1) for increasing values of ?, for samples Z i from the extreme test set T test from Amazon small dataset ( <ref type="figure">Figure 8a)</ref> and Yelp small dataset <ref type="figure">(Figure 8b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Each histogram in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Experimental settings (Classification): additional details</head><p>Toy example. For the toy example, we generate 3000 points distributed as a mixture of two normal distributions in dimension two. For training LHTR, the number of epochs is set to 100 with a dropout rate equal to 0.4, a batch size of 64 and a learning rate of 5 * 10 ?4 . The weight parameter ? 3 in the loss function (Jensen-Shannon divergence from the target) is set to 10 ?3 . Each component ?, C bulk and C ext is made of 3 fully connected layers, the sizes of which are reported in <ref type="table">Table 5</ref>. Datasets. For Amazon, we work with the video games subdataset from http://jmcauley.ucsd. edu/data/amazon/. For Yelp <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b36">37]</ref>, we work with 1,450,000 reviews after that can be found at https://www.yelp.com/dataset. <ref type="table">Table 5</ref>: Sizes of the successive layers in each component of LHTR used in the toy example.</p><formula xml:id="formula_26">Layers' sizes ? [2,4,2] C bulk ? [2,8,1] C ext ? [2,8,1]</formula><p>BERT representation for text data. We use BERT pretrained models and code from the library Transformers 4 . All models were implemented using Pytorch and trained on a single Nvidia P100. The output of BERT is a R 768 vector. All parameters of the models have been selected using the same grid search.</p><p>Network architectures. Tables 6 report the architectures (layers sizes) chosen for each component of the three algorithms considered for performance comparison (Section 4), respectively for the moderate and large datasets used in our experiments. We set ? 1 = (1 ?P(||Z|| ? ||Z ( ?n ) ||)) ?1 and ? 2 =P(||Z|| ? ||Z ( ?n ) ||) ?1 . 0.01 <ref type="table">Table 7</ref>: For Amazon and Yelp, we follow <ref type="bibr" target="#b57">[58]</ref> the weight decay is set to 10 5 , the learning rate is set to 1 * 10 ?4 , the number of epochs is set to 100 and the batch size is set to 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NN model</head><p>the Transformer Decoder we use 2 layers with 8 heads, the dimension of the key and value is set to 64 <ref type="bibr" target="#b54">[55]</ref> and the inner dimension is set to 512. The architectures for the models proposed by Wei and Zou <ref type="bibr" target="#b56">[57]</ref> and Kobayashi <ref type="bibr" target="#b32">[33]</ref> are chosen according to the original papers. For a fair comparison with <ref type="bibr">Kobayashi [33]</ref>, we update the language model with a BERT model, the labels are embedded in R 10 and fed to a single MLP layer (this dimension is chosen using the same procedure as in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref>). The new model is trained using AdamW <ref type="bibr" target="#b37">[38]</ref>. B.7.2 Influence of the scaling factor on the linguistic content <ref type="table">Table 8</ref> gathers some extreme sequences generated by GENELIEX for ? ranging from 1 to 1.5. No major linguistic change appears when ? varies. The generated sequences are grammatically correct and share the same polarity (positive or negative sentiment) as the input sequence. Note that for greater values of ?, a repetition phenomenon appears. The resulting sequences keep the label and polarity of the input sequence but repeat some words <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Extremes in Text</head><p>Aim of the experiments The aim of this section is double: first, to provide some intuition on what characterizes sequences falling in the extreme region of LHTR. Second, to investigate the hypothesis that extremes from LHTR are input sequences which tend to be harder to model than non extreme ones Regarding the first aim ( (i) Are there interpretable text features correlated with the extreme nature of a text sample?, since we characterize extremes by their norm in LHTR representation, in practice the question boils down to finding text features which are positively correlated with the norm of the text samples in LHTR, which we denote by ?(X) and referred to as the 'LHTR norm' in the sequel. Preliminary investigations did not reveal semantic features (related to the meaning or the sentiment expressed in the sequence ) displaying such correlation. However we have identified two features which are positively correlated both together and with the norm in LHTR, namely the sequence length |U | as measured by the number of tokens of the input (recall that in our case an input sequence U is a review composed of multiple sequences ), and the norm of the input in BERT representation ('BERT norm', denoted by X ).</p><p>As for the second question ( (ii) Are LHTR's extremes harder to model? ) we consider the next token prediction loss <ref type="bibr">[5]</ref> ('LM loss' in the sequel) obtained by training a language model on top of BERT. The next token prediction loss can be seen as a measure of hardness to model the input sequence. The question is thus to determine whether this prediction loss is correlated with the norm in LHTR (or in BERT, or with the sequence length).</p><p>Results <ref type="figure" target="#fig_12">Figure 9</ref> displays pairwise scatterplots for the four considered variables on Yelp dataset (left) and Amazon dataset (right). These scatterplot suggest strong dependence for all pairs of variables. For a more quantitative assessment, <ref type="figure" target="#fig_0">Figure 10</ref> displays the correlation matrices between the four quantities ?(X) , X , |U | and 'LM Loss' described above on Amazon and Yelp datasets. Pearson and Spearman two-sided correlation tests are performed on all pairs of variables, both tests having as null hypothesis that the correlation between two variables is zero. For all tests, p-values are smaller than 10 ?16 , therefore null hypotheses are rejected for all pairs.</p><p>These results prove that the four considered variables are indeed significantly positively correlated, which answers questions (i) and (ii) above. Input very sloppy and slow service. when we arrived they told us to sit anywhere but all the tables were still dirty and haven't been cleaned. they didn't bother to ask if we wanted refills on our drinks. we needed an extra plate and didn't get one so my nephew decides to go up to the counter and ask for one because he's hungry. they gave our check when we were still eating. the list can go on and on. i wouldn't recommend this place. go somewhere else for faster and better service. very disappointed ? = 1.1 very sloppy and sluggish service. when we got there, they told us to sit anywhere but all the tables were empyt full of dishes and were not cleaned at all. they didn't bother to ask if our drinks would be added. we needed an extra dish and didn't get one, so my cousin decided to go to the counter and ask one because he's hungry. they were going to watch while we were still eating. the list could go on and on. i would not recommend this place. go elsewhere for faster and better service. very very disappointed ? = 1.2 services and survivors. when he got there, he told us we were sitting everywhere but all the tables were full of dishes and we didn't wash everything. he never bothered to ask if our drinks would be added. we needed extra food and didn't get one, so my brother decided to go to the locker and ask because he was thirsty. they want to watch it while we eat. the list can be continuous and active. i would not recommend this place. go elsewhere for faster and better service. very disappointed ? = 1.3 services and survivors. when he got there, he told us that we were sitting everywhere, but all the tables were full of dishes and we didn't wash everything. he never bothered to ask if our drinks would be added. We needed more food and we didn't get it, so my brother decided to go to the locker and ask because he was thirsty. they want to watch it when we eat. the list can be continuous and active. i would not recommend this place. go faster and faster for better service. very disappointed Input visited today with my husband. we were in the firearms section. there were 3 employees in attendance with one customer. my husband ask a question and was ignored. he waited around for another 10 minutes or so. if it had been busy i could understand not receiving help. we left and went elsewhere for our purchases. ? = 1.1 visited today with my husband. we were in the firearms section. together with one customer there were 3 employees.</p><p>my husband asked and was ignored. waited about another 10 minutes. if it was busy, i would understand that i wouldn't get help. we left and went somewhere else because of our purchases. ? = 1.2 today she visited with her husband. we were in the gun department. there were 3 employees together with one customer. my husband asked and was ignored. waited another 10 minutes. if he was busy, i would understand that i would not receive help. we went and went somewhere else because of our shopping. ? = 1.3 today, she went with her husband. we are in the gun department. there are 3 employees and one customer. my husband rejected me and ignored him. wait another minute. if he has a job at hand, i will understand that i will not get help. we went somewhere else because of our business.</p><p>Input walked in on a friday and got right in. it was exactly what i expected for a thai massage. the man did a terrific job. he was very skilled, working on the parts of my body with the most tension and adjusting pressure as i needed throughout the massage. i walked out feeling fantastic and google eyed. ? = 1.1 walked in on a friday and got right in. it was exactly what i expected for a thai massage. the man did a terrific job. he was very skilled, working on the parts of my body with the most tension and adjusting pressure as needed throughout the massage. i walked out feeling fantastic and google eyed. ? = 1.2 climb up the stairs and get in. the event that i was expecting a thai massage. the man did a wonderful job. he was very skilled, dealing with a lot of stress and stress on my body parts. i walked out feeling lightly happy and tired. ? = 1.3 go up and up. this was the event i was expecting a thai massage. the man did a wonderful job. what this was was an expert, with a lot of stress and stress on my body parts. i walked out feeling lightly happy and tired.</p><p>Input i came here four times during a 3 -day stay in madison. the first two was while i was working -from -home. this place is awesome to plug in, work away at a table, and enjoy a great variety of coffee. the other two times, i brought people who wanted good coffee, and this place delivered. awesome atmosphere. awesome awesome awesome. ? = 1.1 i came here four times during a 3-day stay in henderson. the first two were while i was working -from home. this place is great for hanging out, working at tables and enjoying the best variety of coffee. the other two times, i brought in people who wanted a good coffee, and it delivered a place. better environment. really awesome awesome. ? = 1.2 i came here four times during my 3 days in the city of henderson. the first two were while i was working -at home. this place is great for trying, working tables and enjoying the best variety of coffee. the other two times, i brought people who wanted good coffee, and it brought me somewhere. good environment. really amazing. ? = 1.3 i came here four times during my 3 days in the city of henderson. the first two are when i'm working -at home. this place is great for trying, working tables and enjoying a variety of the best coffees. the other two times, i bring people who want good coffee, and that brings me somewhere. good environment. very amazing. <ref type="table">Table 8</ref>: Sequences generated by GENELIEX for extreme embeddings implying label (sentiment polarity) invariance for generated Sequence. ? is the scale factor. Two first reviews are negatives, two last reviews are positive. <ref type="figure" target="#fig_0">Figure 11</ref> provides additional insight about the magnitude of the shift in sequence length between extremes in the LHTR representation and non extreme samples. Even though the histograms overlap (so that two different sequences of same length may be regarded as extreme or not depending on other factors that are not understood yet), there is a visible shift in distribution for both Yelp and Amazon datasets, both for the positive and negative class in the classification framework for sentiment analysis. Kolmogorov-Smirnoff tests between the length distributions of the two considered classes for each label were performed, which allows us to reject the null hypothesis of equality between distributions, as the maximum p-values is less than 0.05.   <ref type="figure" target="#fig_0">Figure 11</ref>: Histograms of the samples' sequence length for Yelp dataset <ref type="figure" target="#fig_0">(Figure 11a</ref> and <ref type="figure" target="#fig_0">Figure 11b</ref>) and Amazon <ref type="figure" target="#fig_0">(Figure 11c</ref> and <ref type="figure" target="#fig_0">Figure 11d</ref>). The number of sequences in the bulk is approximately 3 times the number of extreme sequences for each dataset 10000 sequences are considered and extreme region contains approximately 3000 sequences .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental conclusions</head><p>We summarize the empirical findings of this section:</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>arXiv:2003.11593v2 [stat.ML] 25 Mar 2021 t t Illustration of angular classifier g dedicated to extremes {x, x ? ? t} in R 2 + . The red and green truncated cones are respectively labeled as +1 and ?1 by g.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 1 (</head><label>1</label><figDesc><ref type="bibr" target="#b30">[31]</ref>, Theorem 2) If each class satisfies the regular variation assumption(2), under an additional regularity assumption concerning the regression function ?(x) = P {Y = +1 | x} (see Equation(4)in Appendix B.3), for ? ? (0, 1), ?n ? 1, it holds with probability larger than 1 ? ? that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2a: Bivariate samples X i in the input space.Figure 2b: X i 's in the input space with extremes from each class selected in the input space.Figure 2c: Latent space representation Z i = ?(X i ). Extremes of each class are selected in the latent space.Figure 2d: X i 's in the input space with extremes from each class selected in the latent space.The third model (LHTR) trains two separate MLP classifiers C ext and C bulk respectively dedicated to the extreme and bulk regions of the learnt representation ?. All models take the same training inputs, use BERT embedding and their classifiers have identical structure, see Appendix A.2 and B.6 for a summary of model workflows and additional details concerning the network architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Scale invariance. On all datasets, the extreme classifier g ext verifies Equation (1) for each sample of the test set, g ext (?Z) = g ext (Z) with ? ranging from 1 to 20, demonstrating scale invariance of g ext on the extreme region. The same experiments conducted both with NN model and a MLP classifier trained on BERT and LHTR 1 show label changes for varying values of ?: none of them are scale invariant. Appendix B.5 gathers additional experimental details. The scale invariance property will be exploited in the next section to perform label invariant generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Illustrative pipelines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>...,n} by decreasing order of magnitude Z (1) ? . . . Z (k) ? . . . ? Z (n) . OUTPUT: encoder ?, decoder G ext applicable on the region {x : ?(x) ? Z ( ?n ) } B Extreme Value Analysis: additional material B.1 Choice of k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>the j th empirical marginal distribution. Denoting by V i the standardized variables, ?i ? {1, . . . , n}, V i =T (X i ). The marginal distributions of V i are well approximated by standard Pareto distribution, the approximation error comes from the fact that the empirical c.d.f 's are used in T instead of the genuine marginal c.d.f.'s F j . After this standardization step, the selected extreme samples are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Illustration of the distribution of the angle ?(X) obtained with bivariate samples X generated from a logistic model with different coefficients of dependence ranging from near asymptotic independence Figure 5a (? = 0.9) to high asymptotic dependence Figure 5c (? = 0.1) including moderate dependence Figure 5b (? = 0.5). Non extreme samples are plotted in gray, extreme samples are plotted in black and the angles ?(X) (extreme samples projected on the sup norm sphere) are plotted in red. Note that not all extremes are shown since the plot was truncated for a better visualization. However all projections on the sphere are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Lack of scale invariance of the classifier trained on BERT: evolution of the predicted label g(?X) from ?1 to +1 for increasing values of ?, for one particular example X.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Scatterplots of the four variables 'BERT norm', 'LHTR norm', 'LM loss' and 'sequence length' on Yelp dataset (top) and Amazon dataset (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Non diagonal entries of the correlation matrices of the four variables 'BERT norm', 'LHTR norm', 'LM loss' and 'sequence length' for Yelp dataset (left) and Amazon dataset (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification loss of LHTR, LHTR 1 and NN model on the extreme test set {x ? T , ||x|| ? ?t} for increasing values of ? (X-axis), on Yelp and Amazon. Classification losses on Amazon and Yelp. 'Proposed Model' results from using NN model model for the bulk and LHTR for the extreme test sets. The extreme region contains 6.9k samples for Amazon and 6.1k samples for Yelp, both corresponding roughly to 25% of the whole test set size.</figDesc><table><row><cell>0.12 0.13 0.14 0.15 0.16 Classification loss</cell><cell>Yelp</cell><cell>0.06 0.08 0.10 0.12 0.14</cell><cell cols="2">Amazon</cell><cell></cell><cell>LHTR LHTR 1 NN model</cell></row><row><cell cols="2">1.00 1.02 1.04 1.06 1.08 1.10</cell><cell></cell><cell cols="3">1.00 1.02 1.04 1.06 1.08 1.10</cell><cell></cell></row><row><cell>Figure 3: Model</cell><cell cols="6">Amazon Bulk Extreme Overall Bulk Extreme Overall Yelp</cell></row><row><cell>NN model</cell><cell>0.085</cell><cell>0.135</cell><cell>0.098</cell><cell>0.098</cell><cell>0.148</cell><cell>0.111</cell></row><row><cell>LHTR 1</cell><cell>0.104</cell><cell>0.091</cell><cell>0.101</cell><cell>0.160</cell><cell>0.139</cell><cell>0.155</cell></row><row><cell>LHTR</cell><cell>0.105</cell><cell>0.08</cell><cell cols="2">0.0988 0.162</cell><cell>0.1205</cell><cell>0.152</cell></row><row><cell cols="2">Proposed Model 0.085</cell><cell>0.08</cell><cell>0.084</cell><cell>0.097</cell><cell>0.1205</cell><cell>0.103</cell></row><row><cell cols="4">5 Experiments : Label Invariant Generation</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">5.1 Experimental Setting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative Evaluation. Algorithms are compared according to C3 and C4. dist1 and dist2 respectively stand for distinct 1 and 2, it measures the diversity of new sequences in terms of unigrams and bigrams. F1 is the F1-score for FastText classifier trained on an augmented labelled training set.</figDesc><table><row><cell>Model</cell><cell cols="4">Amazon Sent. Cohesion Sent. Cohesion Yelp</cell></row><row><cell>Raw Data</cell><cell>83.6</cell><cell>78.3</cell><cell>80.6</cell><cell>0.71</cell></row><row><cell>Kobayashi [33]</cell><cell>80.0</cell><cell>84.2</cell><cell>82.9</cell><cell>0.72</cell></row><row><cell cols="2">Wei and Zou [57] 69.0</cell><cell>67.4</cell><cell>80.0</cell><cell>0.60</cell></row><row><cell>GENELIEX</cell><cell>78.4</cell><cell>73.2</cell><cell>85.7</cell><cell>0.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Algorithm 2 GENELIEX: training stepINPUT: input of LHTR, D gn = {U 1 , . . . , U n } Initialization: parameters of ? ? , C ext ? , C bulk ? , D ? and decoder G ext ext , C bulk = LHTR(? 1 , ? 2 , ? 3 , D n , ?, m) while ? not converged do Sample {U 1 .. . , U m } from the training set D gn and defineZ i = ?(X U,i ) for i ? {1, . . . , m}. Sort {Z i } i?{1,...,m} by decreasing order of magnitude Z (1) ? . . . ? Z (m) .</figDesc><table><row><cell>?</cell></row><row><cell>Optimization:</cell></row><row><cell>?, C Update ? by descending:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>:</head><label></label><figDesc>Network architectures for Amazon dataset and Yelp dataset. The weight decay is set to 10 5 , the learning rate is set to 1 * 10 ?4 , the number of epochs is set to 500 and the batch size is set to 256.As mentioned in Section 5.1, hyperparameters for dataset augmentation are detailed inTable 7. For</figDesc><table><row><cell></cell><cell></cell><cell>LHTR</cell><cell></cell></row><row><cell cols="2">Sizes of the layers ?</cell><cell>[768,384,200,150]</cell><cell></cell></row><row><cell cols="2">Sizes of the layers of C bulk ? Sizes of the layers of C ext ? ? 3</cell><cell>[150,75,8,1] [150,75,8,1]</cell><cell></cell></row><row><cell></cell><cell></cell><cell>LHTR 1</cell><cell>LHTR</cell></row><row><cell>Sizes of the layers ?</cell><cell cols="3">[768,384,200,50,8,1] [768,384,200,100] [768,384,200,150]</cell></row><row><cell>Sizes of the layers of C bulk ? Sizes of the layers of C ext ? ? 3</cell><cell>[150,75,8,1] X X</cell><cell>[100,50,8,1] X X</cell><cell>[150,75,8,1] [150,75,8,1] 0.01</cell></row><row><cell cols="2">Table 6B.7 Experiments for data generation</cell><cell></cell><cell></cell></row><row><cell>B.7.1 Experimental setting</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">dist n is obtained by calculating the number of distinct n-grams divided by the total number of generated tokens to avoid favoring long sequences.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">measure of inter-rater reliability in [0, 1]: 0 is perfect disagreement and 1 is perfect agreement.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/huggingface/transformers</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Max k-armed bandit: On the extremehunter algorithm and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mastane</forename><surname>Achab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Cl?men?on</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aur?lien</forename><surname>Garivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Vernade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="389" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Word frequency distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Baayen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On power law distributions in large-scale taxonomies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Babbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Metzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massih-Reza</forename><surname>Amini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multimix: A robust data augmentation strategy for cross-lingual nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Tasnim Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joty</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13240</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?jean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extreme bandits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carpentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1089" to="1097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Hierarchical pre-training for sequence labelling in spoken dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emile</forename><surname>Chapuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Labeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Clavel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11152</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Feature clustering for extreme events analysis, with application to extreme stream-flow data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma?l</forename><surname>Chiapino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Sabourin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on New Frontiers in Mining Complex Patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="132" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A multivariate extreme value theory approach to anomaly clustering and visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma?l</forename><surname>Chiapino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phan</forename><surname>Cl?men?on</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Feuillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Sabourin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics</title>
		<imprint>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identifying groups of variables with the potential of being large simultaneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma?l</forename><surname>Chiapino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Segers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Extremes</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="222" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Poisson mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William A</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="190" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Novelty detection with multivariate extreme value statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hugueny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tarassenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Signal Process Syst</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="371" to="389" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Information-based models for ad hoc ir</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 33rd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical methods for multivariate extremes: an application to structural design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">A</forename><surname>Coles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tawn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Witon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02793</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Affect-driven dialog generation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Guiding attention in sequence-to-sequence models for dialogue act prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emile</forename><surname>Chapuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanna</forename><surname>Varni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Clavel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08801</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cohesion, coherence, and expert evaluations of writing proficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Crossley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Mcnamara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automated hate speech detection and the problem of offensive language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Warmsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Macy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh international aaai conference on web and social media</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The importance of fillers for text representations of speech transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanvi</forename><surname>Dinkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Labeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chlo?</forename><surname>Clavel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11340</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Maziar Moradi Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Thonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaussier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10069</idno>
	</analytic>
	<monogr>
		<title level="m">Jointly clustering with k-means and learning representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">From the token to the review: A hierarchical multimodal approach to opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slim</forename><surname>Essid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Florence D&amp;apos;alch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chlo?</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clavel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11216</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning the dependence structure of rare events: a non-asymptotic study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cl?men?on</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sparse representation of multivariate extremes with applications to anomaly ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cl?men?on</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="75" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sparse representation of multivariate extremes with applications to anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cl?men?on</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="12" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A review of machine learning approaches to spam filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guzella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Walmir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caminhas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="10206" to="10222" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09751</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Informative clusters for multivariate extremes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Jalalzai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Leluc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07365</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On binary classification in extreme regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Jalalzai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Cl?men?on</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Sabourin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3092" to="3100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06201</idno>
		<title level="m">Contextual augmentation: Data augmentation by words with paradigmatic relations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">From group to individual labels using deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kotzias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="597" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Autoencoding any data through kernel autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Laforgue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Cl?men?on</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florence D&amp;apos;alch?</forename><surname>Buc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11028</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03055</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mining quality phrases from massive text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2015 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1729" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modeling word burstiness using the dirichlet distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rasmus E Madsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Kauchak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Adversarial autoencoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">An informational theory of the statistical structure of language. Communication theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Mandelbrot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1953" />
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="486" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hidden factors and hidden topics: understanding rating dimensions with review text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM conference on Recommender systems</title>
		<meeting>the 7th ACM conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Automatic detection of fake news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ver?nica</forename><surname>P?rez-Rosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bennett</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Lefevre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07104</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning to compose domain-specific transformations for data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeshan</forename><surname>Ehrenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3236" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Extreme values, regular variation and point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sidney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Novelty detection using extreme value statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE P-VIS IMAGE SIGN</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="124" to="129" />
			<date type="published" when="1999-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Extreme value statistics for novelty detection in biomedical data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE P-SCI MEAS TECH</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="363" to="367" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Data augmentation for morphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miikka</forename><surname>Silfverberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Wiemerslage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingshuang Jack</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection</title>
		<meeting>the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="90" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Simulating multivariate extreme value distributions of logistic type</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stephenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Extremes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="59" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Anomaly detection in extreme regions via empirical mv-sets on the sphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Cl?men?on</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Sabourin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1011" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The effectiveness of data augmentation in image classification using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Convolutional Neural Networks Vis. Recognit</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Eda: Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6383" to="6389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Disney at iest 2018: Predicting emotions using an ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Witon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting>the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="248" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Personalized entity recommendation: A heterogeneous information network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Sturt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM international conference on Web search and data mining</title>
		<meeting>the 7th ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="283" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">An &apos;extreme&apos; text sequence in LHTR representation is more likely to have a greater length (number of tokens) than a non extreme one</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Positive correlation between the BERT norm and the LHTR norm indicates that a large sample in the BERT representation is likely to have a large norm in the LHTR representation as well: the learnt representation LHTR taking BERT as input keeps invariant</title>
		<imprint/>
	</monogr>
	<note>in probability) the ordering implied by the norm</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">A consequence of the two above points is that long sequences tend to have a large norm in BERT</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Extreme text samples (regarding the BERT norm or the LHTR norm) tend to be harder to model than non-extreme ones</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Since extreme texts are harder to model and also somewhat harder to classify in view of the BERT classification scores reported in Table 1, there is room for improvement in their analysis and it is no wonder that a method</title>
		<imprint/>
	</monogr>
	<note>dedicated to extremes i.e. relying on EVT such as LHTR outperforms the baseline</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
