<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TEACHTEXT: CrossModal Generalized Distillation for Text-Video Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioana</forename><surname>Croitoru</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">Univ. of Oxford</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Inst. of Mathematics of the Romanian Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simion-Vlad</forename><surname>Bogolin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">Univ. of Oxford</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Inst. of Mathematics of the Romanian Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Inst. of Mathematics of the Romanian Academy</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Univ. Politehnica of Bucharest</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">Univ. of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">Univ. of Oxford</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Dept. of Engineering</orgName>
								<orgName type="institution">Univ. of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">Univ. of Oxford</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Wangxuan Inst. of Computer Technology</orgName>
								<orgName type="institution">Peking Univ</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TEACHTEXT: CrossModal Generalized Distillation for Text-Video Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, considerable progress on the task of text-video retrieval has been achieved by leveraging largescale pretraining on visual and audio datasets to construct powerful video encoders. By contrast, despite the natural symmetry, the design of effective algorithms for exploiting large-scale language pretraining remains under-explored. In this work, we are the first to investigate the design of such algorithms and propose a novel generalized distillation method, TEACHTEXT, which leverages complementary cues from multiple text encoders to provide an enhanced supervisory signal to the retrieval model. Moreover, we extend our method to video side modalities and show that we can effectively reduce the number of used modalities at test time without compromising performance. Our approach advances the state of the art on several video retrieval benchmarks by a significant margin and adds no computational overhead at test time. Last but not least, we show an effective application of our method for eliminating noise from retrieval datasets. Code and data can be found at https://www.robots.ox.ac.uk/?vgg/ research/teachtext/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The focus of this work is text-video retrieval-the task of identifying which video among a pool of candidates best matches a natural language query describing its content. Video search has a broad range of applications across domains such as wildlife monitoring, security, industrial process monitoring and entertainment. Moreover, as humanity continues to produce video at ever-increasing scale, the ability to perform such searches effectively and efficiently takes on critical commercial significance to video hosting platforms such as YouTube.</p><p>A central theme of recently proposed retrieval methods has been the investigation of how to best use multiple video * Equal contribution. ? Corresponding authors. <ref type="figure">Figure 1</ref>. Distilling the knowledge from multiple text encoders for stronger text-video retrieval. Prior works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref> have shown the considerable benefit of transitioning from video encoders that ingest a single modality (left) to multi-modal video encoders (centre). In this work, we show that retrieval performance can be further significantly enhanced by learning from multiple text encoders through the TEACHTEXT algorithm which imposes no additional cost during inference. Text-to-video retrieval performance gain (geometric mean of R1-R5-R10) is reported for a <ref type="bibr" target="#b35">[36]</ref> model as well as for our method on the MSR-VTT <ref type="bibr" target="#b71">[72]</ref> dataset. modalities to improve performance. In particular, architectures based on mixtures-of-experts <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b41">42]</ref> and multimodal transformers <ref type="bibr" target="#b19">[20]</ref> have shown the benefit of making use of diverse sets of pre-trained models for related tasks (such as image classification, action recognition and ambient sound classification) as a basis for video encoding during training and testing.</p><p>In this work, we explore whether commensurate gains could be achieved by leveraging multiple text embeddings learned on large-scale written corpora. Different from video embeddings using multiple modalities and pretraining tasks, it is less obvious that there is sufficient diversity among collections of text embeddings to achieve a meaningful boost in performance. In fact, our inspiration stems from a careful investigation of the performance of different text embeddings across a range of retrieval benchmarks <ref type="figure">(Fig. 2)</ref>. Strikingly, we observe not only that there is considerable variance in performance across text embeddings, but also that their ranking is not consistent, strongly supporting the idea of using multiple text embeddings.</p><p>Motivated by this finding, we propose a simple algorithm, TEACHTEXT, to effectively exploit the knowledge captured by collections of text embeddings. Our approach requires a "student" model to learn from a single or multiple "teacher" retrieval models with access to different text embeddings by distilling their text-video similarity matrices into an enhanced supervisory signal. As shown in <ref type="figure">Fig. 1</ref>, TEACHTEXT is capable of delivering a significant performance gain. Moreover, this gain is complementary to that of adding more video modalities to the video encoder but importantly, unlike the addition of video modalities, does not incur additional computational cost during inference.</p><p>Our main contributions can be summarised as follows: <ref type="bibr" target="#b0">(1)</ref> We propose the TEACHTEXT algorithm, which leverages the additional information given by the use of multiple text encoders; <ref type="bibr" target="#b1">(2)</ref> We show that directly learning the retrieval similarity matrix between the joint query video embeddings, which to the best of our knowledge is novel, is an effective generalized distillation technique for this task (and we compare our approach to alternatives among prior work such as uni-modal relationship distillation <ref type="bibr" target="#b46">[47]</ref>); <ref type="bibr" target="#b2">(3)</ref> We show an application of our approach in eliminating noise from modern training datasets for the text-video retrieval task; <ref type="bibr" target="#b3">(4)</ref> We demonstrate the effectiveness of our approach empirically, achieving state of the art performance on six text-video retrieval benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video retrieval methods. The task of indexing video content to enable retrieval has a rich history in computer vision-sophisticated systems have been developed to find specific objects <ref type="bibr" target="#b55">[56]</ref>, actions <ref type="bibr" target="#b33">[34]</ref>, predefined semantic categories <ref type="bibr" target="#b27">[28]</ref>, irregularities <ref type="bibr" target="#b3">[4]</ref> and near-duplicates <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b53">54]</ref>. In this work, we focus on the task of retrieving content that matches a given natural language description. For this particular task, there has been considerable interest in developing cross-modal methods that employ a joint-embedding space for text queries and video content <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b73">74]</ref>. These joint video-text embeddings, which aim to map videos and text descriptions into a common space such that matching video and text pairs are close together, form an attractive computational model for tackling this problem, since they allow for efficient indexing (although hierarchical embeddings have also been investigated <ref type="bibr" target="#b13">[14]</ref>). Recently, two key themes have emerged towards improving the quality of these embeddings. First, large-scale weakly supervised pretraining methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref> have sought to expand their training data by exploiting the speech contained in the videos themselves as a supervisory signal. Second, the integration of multiple modalities (which has long been considered important for semantic indexing <ref type="bibr" target="#b56">[57]</ref>) has been shown to yield significant gains in performance <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49]</ref>. We focus on candidates from this latter theme as a basis for investigating our approach.</p><p>Text embeddings. The representation of language through learned embeddings has been widely studied <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref> and applied in a variety of natural language processing applications. Several works have demonstrated that even with large-scale pretraining, there still are benefits to finetuning the models on the target task <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b49">50]</ref> and that larger models (often employing multiple attention heads) yield higher performance <ref type="bibr" target="#b15">[16]</ref>. Recently, <ref type="bibr" target="#b7">[8]</ref> provided a detailed comparisons on the importance of language features for vision applications and proposes a word embedding that is specifically designed for vision tasks. In this work, we first study how various pretrained language embeddings affect the performance for text-video retrieval and then propose a method to take advantage of the benefits of combining multiple text embeddings. Knowledge Distillation/Privileged Information. The purpose of knowledge distillation is to transfer knowledge from one model (teacher) to another model (student). This idea was originally introduced in the context of decision tree simplification <ref type="bibr" target="#b5">[6]</ref> and model compression <ref type="bibr" target="#b6">[7]</ref>, and later extended by <ref type="bibr" target="#b23">[24]</ref> who formalised this knowledge transfer as the temperature-parameterised process of knowledge distillation. The concept was further generalised in the unifying framework of generalized distillation <ref type="bibr" target="#b38">[39]</ref> for learning with privileged information <ref type="bibr" target="#b63">[64]</ref> (via similarity control and knowledge transfer <ref type="bibr" target="#b62">[63]</ref>), together with knowledge distillation <ref type="bibr" target="#b23">[24]</ref>. Our approach distills knowledge of the similarities between video and text samples into the student and therefore represents a form of generalized distillation. While most knowledge distillation methods train the student with the teacher's outputs as targets, more recent methods propose different approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b74">75]</ref>. Of most relevance to our approach, <ref type="bibr" target="#b46">[47]</ref> transfer mutual relations of data examples and propose distance-wise and angle-wise distillation losses that penalize structural differences in relations instead of training the student to mimic the output of the teacher-we compare to their approach in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motivation and intuition</head><p>Recently, <ref type="bibr" target="#b50">[51]</ref> points out that even though language representation learning systems (such as <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">50]</ref>) are pretrained on vast amounts of data, they are still sensitive to slight changes in the data distribution and task specification. In this way, most systems can be viewed as narrow experts rather than competent generalists.</p><p>Consequently, in <ref type="figure">Fig. 2</ref> we investigate how the usage of different off-the-shelf pre-trained text embeddings affects the retrieval performance. We observe that there is significant variance both within and across datasets, suggesting that each embedding captures different types of information. Our intuition is that this information comes from the diversity of architectures, pretraining datasets and pretraining objectives, which differs across the text embeddings.</p><p>Next, we give details about the used text embeddings <ref type="figure">Figure 2</ref>. Influence of varying the text embedding. Different text embeddings are presented on the x axis: w2v <ref type="bibr" target="#b43">[44]</ref>, mt grovle <ref type="bibr" target="#b7">[8]</ref>, openai-gpt <ref type="bibr" target="#b49">[50]</ref>, roberta <ref type="bibr" target="#b36">[37]</ref>, albert <ref type="bibr" target="#b32">[33]</ref>, gpt2-large <ref type="bibr" target="#b50">[51]</ref>, gpt2-xl <ref type="bibr" target="#b50">[51]</ref>, gpt2-xl-F along with their performance in geometric mean of R1-R5-R10 on five datasets. For each experiment, we report the mean (diamond) and standard deviation (error bar) of three randomly seeded runs.This study is performed using the CE retrieval architecture <ref type="bibr" target="#b35">[36]</ref>: each model differs only in its use of pre-trained text embedding at input. We observe a significant variance in performance when changing the text embedding, both across and within datasets. The difference in rankings across datasets suggests the presence of additional information among different text embeddings. <ref type="figure">Figure 3</ref>. Share of correctly retrieved samples based on the used pre-trained text embedding on MSR-VTT. We observe that each embedding has a considerable share of sample retrieved correctly only by itself (in terms of R1 left and R5 right), further justifying our approach. Best viewed in color. and summarise the key differences between them in relationship with our findings. Word2vec (w2v) <ref type="bibr" target="#b43">[44]</ref> is a lightweight text embedding that is widely used for vision tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b67">68]</ref>. Multi-task GrOVLE (mt grovle) <ref type="bibr" target="#b7">[8]</ref>, is an extension of w2v that is specially designed for visionlanguage tasks (in our experiments, however, we find that it slightly under-performs w2v). The finetuned transformer language model (openai-gpt) <ref type="bibr" target="#b49">[50]</ref> embedding is trained on a book corpus containing long stretches of contiguous text. We observe that it performs well on datasets that have longer text queries such as ActivityNet. RoBERTa and AL-BERT <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37]</ref> are based on the BERT architecture <ref type="bibr" target="#b15">[16]</ref> and are trained on the same data which consists of unpublished books and Wikipedia articles. RoBERTa <ref type="bibr" target="#b36">[37]</ref> focuses on hyperparameter optimization and shows that greater model capacity leads to better performance while ALBERT <ref type="bibr" target="#b32">[33]</ref> proposes some parameter-reduction techniques to reduce memory consumption and increase training speed. In our experiments, we observe a high variation in performance when comparing the two. In contrast to the other embeddings, gpt2 <ref type="bibr" target="#b50">[51]</ref> is trained on a crawled dataset that was designed to be as diverse as possible. We observe that gpt2 performs most robustly in our experiments, especially on smaller datasets such as MSR-VTT and MSVD. However, it nevertheless exhibits a domain gap to each corpus (highlighted by the fact that performance increases when finetuning gpt2-xl, termed gpt2-xl-F throughout the paper, on queries from the text-video retrieval datasets). Additionally, in <ref type="figure">Fig. 3</ref> we show how many correctly retrieved queries are shared between three text embeddings on MSR-VTT: gpt2-xl, gpt2-xl-F and w2v. Only around 19% (R1), respectively 42% (R5) queries are correctly retrieved by all the three considered text embeddings. This means that a significantly number of queries are sensitive to the used text embedding, consolidating our intuition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>Motivated by the findings from Sec. 3, our work aims to study the influence of using multiple text embeddings for text-video retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Problem description and learning setup</head><formula xml:id="formula_0">Let D = {(v i , c i )} n i=1</formula><p>be a dataset of paired videos and captions. Following the multi-modal experts approach of <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>, for each video we have access to a collection of video embeddings (sometimes referred to as "experts") x i extracted from the various modalities of video v i using a pretrained video encoder (VE) in addition to a text embedding t i (extracted using a text encoder, TE) for each caption/query c i 1 . The objective of the text-video retrieval task is to learn a model M (x i , t j ) which assigns a high similarity value to pairings (x i , t j ) of video and text embeddings that are in correspondence (i.e. i = j) and a low similarity otherwise. As is common in the literature <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42]</ref>, we parameterise the model as a dual-encoder that produces joint-embeddings in a shared space such that they can be compared directly M (x i , t j ) = F (x i ) T Q(t j ) ? R where F and Q represent the learnt video and text encoder respectively. To train the video and text encoder for the task of <ref type="figure">Figure 4</ref>. TEACHTEXT teacher-student framework overview. Given a batch of input videos and queries in natural language during training, the student model, M (left) and teacher models T1, . . . , TN (right) each produce similarity matrices (visualised as square grids). The similarity matrix produced by M is encouraged to match the aggregated matrices of the teachers through the distillation loss L d in addition to the retrieval loss Lr. Note that both the student and teachers ingest the same video embeddings (VE), but employ different text embeddings (TES for the student, TE1, . . . ,TEN for the teachers). At test time, the teacher models are discarded. retrieval, we adopt a contrastive ranking loss <ref type="bibr" target="#b57">[58]</ref>:</p><formula xml:id="formula_1">L r = 1 B B i=1 i? =j [max(0, s ij ? s ii + m)+ max(0, s ji ? s ii + m)]<label>(1)</label></formula><p>where B represents the batch size used during training,</p><formula xml:id="formula_2">s ij = F (x i ) T Q(t j )</formula><p>is the similarity score between the encoded video F (x i ) and query Q(t j ) while m is the margin.</p><p>The key idea behind our approach is to learn a retrieval model, M , that, in addition to the loss described above, also has access to information provided by a collection of pretrained "teacher" retrieval models which are trained on the same task but ingest different text embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">TEACHTEXT algorithm</head><p>To enhance the retrieval performance of model M , we propose the TEACHTEXT algorithm which aims to exploit cues from multiple text embeddings. An overview of our approach is provided in <ref type="figure">Fig. 4</ref>. Initially, we train a collection of teacher models {T k : k ? {1, . . . , N }} for the text-video retrieval task using the approach described in Sec. 4.1. The teachers share the same architecture but each model T k uses a different text embedding as input (extracted using a pre-trained text encoder TE k ). In the second phase the parameters of the teachers are frozen. We then proceed by sampling a batch of B pairs of videos and captions and computing a corresponding similarity matrix S k ? R B?B for each teacher T k <ref type="figure">(Fig. 4 right)</ref>. These N similarity matrices are then combined with an aggregation function, ? : R N ?B?B ? R B?B , to form a single supervisory similarity matrix ( <ref type="figure">Fig. 4</ref>, centre-right). Concurrently, the batch of videos and captions are likewise processed by the student model, M , which produces another similarity matrix, S s ? R B?B . Finally, in addition to the standard retrieval loss (Eq. 1), a distillation loss, L d , encourages the S s to lie close to the aggregate ?(S 1 , . . . , S N ). The algorithm is summarized in Alg. 1. During inference, the teacher models are discarded and the student model M requires only a single text embedding. Next, we give details of the distillation loss used for the similarity matrix learning. For each pair (v i , c i ) extract video experts and text embedding pairs (x i , t i ) using VE and T E S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Compute student similarity matrix S s where S s (i, j) = F (x i ) T Q(t j ) for i, j ? {1, . . . , B}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Compute the loss L r via Eqn. 1 using S s . <ref type="bibr">8:</ref> for teacher T k , k = 1, . . . , N do <ref type="bibr">9:</ref> For each pair (v i , c i ) extract the video experts and text embedding pairs (x i , t k i ) using V E and T E k . <ref type="bibr" target="#b9">10</ref>: Compute aggregate teacher matrix ?(S 1 , . . . , S N ).</p><formula xml:id="formula_3">Compute the similarity matrix S k where S k (i, j) = F k (x i ) T Q k (t k i ) for i, j ? {1, . . . , B}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>Compute the loss L d between S s and ?(S 1 , . . . , S N ) via Eqn. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>Update M with gradients computed from the composite loss L = L r + L d . <ref type="bibr">15:</ref> end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Learning the similarity matrix</head><p>As noted in Sec. 4.1, the essence of the retrieval task is to create a model that is able to establish cross-modal correspondences between videos and texts/queries, assigning a high similarity value to a pairing in which a query accurately describes a video, and a low similarity otherwise. This renders the similarity matrix a rich source of information about the knowledge held by the model. In order to be able to transfer knowledge from the teachers to the student, we encourage the student to produce a similarity matrix that matches an aggregate of those produced by the teachers. In this way, we convey information about texts and video correspondences without strictly forcing the student to produce exactly the same embeddings as the teachers. To this end, we define the similarity matrix distillation loss as:</p><formula xml:id="formula_4">L d = 1 B B i=1 B j=1 [l(?(i, j), S s (i, j))]<label>(2)</label></formula><p>where B represents the batch size, ? = ?(S 1 , . . . , S N ) represents the aggregate of the teacher similarity matrices and S s represents the similarity matrix of the student. Finally, inspired from other distillation works such as <ref type="bibr" target="#b46">[47]</ref>, l represents the Huber loss and is defined as</p><formula xml:id="formula_5">l(x, y) = 1 2 (x ? y) 2 if |x ? y| ? 1, |x ? y| ? 1 2 otherwise (3)</formula><p>We explored several forms of aggregation function and found that a simple element-wise mean,</p><formula xml:id="formula_6">?(S 1 , . . . , S N ) = 1 N N k=1 S k , worked well in practice.</formula><p>The idea of learning directly the cross-modal similarity matrix is, to the best of our knowledge novel. It draws inspiration from the work of relational knowledge distillation <ref type="bibr" target="#b46">[47]</ref> which considered the idea of learning from relationships and introduced two algorithms to implement this concept in a uni-modal setting through pairwise and triplet distance sampling. We compare our matrix learning approach with theirs in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Student model</head><p>A key advantage of our approach is that it is agnostic to the architectural form of the student and teachers, and thus the student (and teachers) can employ any method from the current literature. We test our TEACHTEXT algorithm using three different recent works MoEE <ref type="bibr" target="#b41">[42]</ref>, CE <ref type="bibr" target="#b35">[36]</ref>, MMT <ref type="bibr" target="#b19">[20]</ref> as the student and teacher base architectures. All these works employ multi-modal video encoders for the text-video retrieval task. For more details, please consult the original paper of each method. Establishing a stronger baseline. In addition to these models, we also investigate our approach on a model which shares the CE architecture of <ref type="bibr" target="#b35">[36]</ref> but includes a series of small technical improvements to provide a stronger baseline against which we also test the TEACHTEXT algorithm. Starting from this base architecture, we refine the input embedding selection, finding that the face and OCR video modalities employed by <ref type="bibr" target="#b35">[36]</ref> do not consistently produce improvement so we remove them as inputs to the video encoder. We update the model to use the more powerful gpt2-xl text embedding of <ref type="bibr" target="#b50">[51]</ref> and following <ref type="bibr" target="#b19">[20]</ref>, we finetune this text embedding on captions from the target dataset to bring additional improvement. Combining all of these changes (ablations provided in Sec. 5.3 and <ref type="figure">Fig. 5a</ref>) results in the CE+ model which we include as an additional baseline. Thus, in summary we use four ( <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref> and CE+) different base architectures for the student model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Teacher models</head><p>The teacher models use the same architecture as the student model. Concretely, for each of the four base architectures described in Sec. 4.4, we create a pool of multiple teachers, each using a different pre-trained text embedding as input. The candidate text embeddings we consider are: mt grovle <ref type="bibr" target="#b7">[8]</ref>, openai-gpt <ref type="bibr" target="#b49">[50]</ref>, gpt2-large <ref type="bibr" target="#b50">[51]</ref>, gpt2xl <ref type="bibr" target="#b50">[51]</ref>, w2v <ref type="bibr" target="#b43">[44]</ref>. So, we obtain a set of up to five models that form the teachers T k , k = 1..5 used by TEACHTEXT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Training and implementation details</head><p>In order to train our final student, we combine the retrieval loss and the proposed distillation loss L = L r + L d . Our model is trained in Pytorch <ref type="bibr" target="#b47">[48]</ref> using the Adam <ref type="bibr" target="#b28">[29]</ref> optimizer. TEACHTEXT does not add any additional trainable parameters or modalities to the final model. Moreover, when training the student using TEACHTEXT, only the additional loss term L d is added, all other hyper-parameters remaining the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets description</head><p>To provide an extensive comparison we test our approach on seven video datasets that have been explored in recent works as benchmarks for the task of text-video retrieval: <ref type="bibr" target="#b68">[69]</ref> and QuerYD <ref type="bibr" target="#b45">[46]</ref>. We follow the same experimental setup as prior works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49]</ref>.</p><formula xml:id="formula_7">LSMDC [52], DiDeMo [1], MSVD [13], MSRVTT [72], ActivityNet [9], VaTeX</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Metrics</head><p>To assess performance, we follow prior work (e.g <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b73">74]</ref>) and report standard retrieval metrics, including R@K (recall at rank K, where higher is better) and MdR (median rank where lower is better). For certain analyses, to maintain conciseness we report the geometric mean of R@1, R@5 and R@10 rather than individual metrics (this statistic aims to be representative of overall retrieval performance). The numbers are reported for the task of retrieving a video given text queries t2v which is more common in real world applications. The numbers for the reverse task v2t and the number of parameters for each model are reported in the Suppl. Mat. For each experi-ment, we report the mean and standard deviation of three randomly seeded runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablations</head><p>In this section we present an extensive study of our proposed approach. Following the setup used in prior works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref> we conduct ablations on the MSR-VTT dataset <ref type="bibr" target="#b71">[72]</ref>, except where otherwise stated.</p><p>Baseline improvements. We propose CE+ as an additional baseline which consists of a series of technical improvements to the model of <ref type="bibr" target="#b35">[36]</ref>. As seen in <ref type="figure">Fig. 5a</ref> each modification described in Sec. 4.4 brings additional gain over the base architecture. We observe in particular that finetuning the text embedding on the target dataset has a high influence, further highlighting the critical role played by text embeddings and justifying their study. In addition to other changes we found that certain video embedding expert features were highly sensitive to compression choices used in video pre-processing, which we correct accordingly (more details in Suppl. Mat.). Please note that for a fair comparison, in Sec. 5.4 we report the numbers of re-training the methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b41">42]</ref> using these embeddings extracted with the updated pre-processing which yields a higher performance than the ones reported in the original papers.</p><p>Using multiple text embeddings during inference. TEACHTEXT makes no use of additional information at test time. However, it is natural to ask whether the additional text embeddings can be trivially included as part of the model architecture. In <ref type="figure">Fig. 5</ref>(b) we compare our <ref type="figure">Figure 5</ref>. (a) Baseline improvements. The y-axes (scaled for clarity) denotes retrieval performance on MSR-VTT. We begin by presenting the performance of the original CE <ref type="bibr" target="#b35">[36]</ref>. Firstly, we correct compression artefacts in the pre-processing used for embedding extraction (CE HQ, more details in Suppl. Mat.). Secondly, we refine the used video modalities and text modalities (Mod ref and Text ref ). Finally, we finetune the text embedding (F) and change the optimizer to Adam <ref type="bibr" target="#b28">[29]</ref>, thus obtaining the CE+ baseline. (b) Use additional text embeddings at inference time. All experiments were performed with the same architecture <ref type="bibr" target="#b35">[36]</ref>, but with different text embeddings: gpt2-xl (first bullet), gpt2-xl-F (second bullet), the concatenation of gpt2-xl and gpt2-xl-F (third bullet), the mean of gpt2-xl and gpt2-xl-F (fourth bullet) and using TEACHTEXT (last bullet). By using multiple text embeddings at test time, which introduces an overhead, a boost in performance is obtained. However, by using TEACHTEXT there is no additional overhead at test time and the performance is superior. <ref type="figure">Figure 6</ref>. (a) Teacher study. We show the influence of learning from different number of teachers on the MSR-VTT dataset (all students share the same CE+ model, y-axes scaled for clarity). The teachers were added in the following order: gpt2-xl, w2v, gpt2xl-F, mt grovle, openai-gpt, gpt2-large. The performance of the combined teachers grows as more teachers are added, however it reaches a plateau after the first 3 teachers. The trend is similar for student performance. (b) Distillation type. Presenting various alternatives for distilling the information from the teacher: relational distillation <ref type="bibr" target="#b46">[47]</ref> which preserves intra-text and intra-video relationships, pairwise distance distillation (Pdist -adapting <ref type="bibr" target="#b46">[47]</ref> for cross modal relationships), ranking distillation inspired by <ref type="bibr" target="#b58">[59]</ref> at Rank 1 and Rank 10 and TEACHTEXT. The first bullet represents the student without distillation. approach with some relatively simple text embedding aggregation techniques, which require access to multiple text embeddings during both training and inference. We observe that TEACHTEXT outperforms these aggregation techniques such as direct concatenation or mean of the text embeddings, suggesting that the proposed method is effective in capturing the additional information given by multiple text embeddings. Moreover, the text encoder of existing systems <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref> typically employs many parameters, so adding multiple text embeddings to the architecture adds a significant number of parameters (100M+). For example, the concatenation of two text embeddings (provided that they have the same size) almost doubles the total number of parameters for CE+. In contrast, when employing TEACHTEXT, no parameters are added.</p><p>Teacher variation. The teacher models share the same architecture with the student, but use a different text embedding. We next conduct an ablation on the influence of the number of used teachers. We observe in <ref type="figure">Fig. 6a</ref> that performance increases with the addition of more teachers. Since the combined performance of the teachers after adding more than 3 remains about the same, we do not obtain a further improvement. Thus, for our final experiments presented in Sec. 5.4 we use a combination of three teachers, having the following text embeddings: w2v <ref type="bibr" target="#b43">[44]</ref>, gpt2-xl <ref type="bibr">[</ref>  <ref type="bibr" target="#b41">[42]</ref>, CE <ref type="bibr" target="#b35">[36]</ref>, MMT <ref type="bibr" target="#b19">[20]</ref> (on available datasets) and CE+. We present in bold cases where TEACHTEXT brings an improvement over the base architecture. We observe that our method improves the performance for all underlying base models and on all datasets. <ref type="figure">Figure 7</ref>. (a) Denoising. We present the effect of denoising on retrieval performance on MSR-VTT (y-axes scaled for clarity). Some of the captions available in datasets with multiple captions per video may be noisy and actively harm the training process. We estimate the degree of noise present in a caption by looking at the teacher rank and drop the caption if necessary. We observe the effectiveness of denoising when applied in isolation (CE+ vs CE+ Denoise) and in conjunction with the full TEACHTEXT method.</p><p>(b) TEACHVIDEO -Extension to video side modalities. We observe that our method can be effective in taking advantage of the additional information brought by using multiple video side modalities, without incurring computational overhead at test time.</p><p>ing a teacher with lower performance (w2v), the student has a significant boost in performance. Distillation ablation. We compare the proposed learning of the similarity matrix with other distillation alternatives. As seen in <ref type="figure">Fig.6b</ref>, our proposed approach is effective in capturing the relationships between video and text. We first provide comparisons between TEACHTEXT and several possible instantiations of relational distillation <ref type="bibr" target="#b46">[47]</ref>. Indeed, given the highly general nature of <ref type="bibr" target="#b46">[47]</ref>, TEACHTEXT can be interpreted within this framework as a particular relational configuration that employs cross-modal distillation through batches of similarity matrices. Since the original work of <ref type="bibr" target="#b46">[47]</ref> considered single-modality applications, we explore two variations of <ref type="bibr" target="#b46">[47]</ref> as baselines for the text-video retrieval task. The first one (Relational), preserves the same intra-text and intra-video relationships independently. We use the same cost function as in <ref type="bibr" target="#b46">[47]</ref> and enforce it on both video and text embeddings. The second approach (Pdist), uses the cross modal pairwise distances as a relation measure between text and video as opposed to the similarity matrix. While these methods indeed bring a gain, we observe that TEACHTEXT is more effective.</p><p>We also provide a baseline inspired by the work of <ref type="bibr" target="#b58">[59]</ref> which highlights the importance of looking only at the top K predictions given by the teacher. To do so, we enforce the same similarities using TEACHTEXT only for the top K ranks given by the teacher rather than for the whole minibatch. We show the performance for K=1 and K=10 (Rank 1 and Rank 10 presented in <ref type="figure">Fig.6b</ref>). Restricting to only top K predictions when distilling the similarity matrix results in a slight drop in performance.</p><p>Method generality. To demonstrate the generality of TEACHTEXT, we test it against three state of the art methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref> in addition to the proposed CE+ baseline. In Tab. 1 we observe a consistent gain in performance, independent of the base architecture. Moreover, a gain is achieved across all the datasets that we tested, having over 5% absolute gain on DiDeMo and ActivityNet datasets for MoEE, CE and CE+ models. Note that for MMT <ref type="bibr" target="#b19">[20]</ref> we report results on the datasets included in the public implementation provided by the authors 2 .</p><p>Method application -Denoising. One immediate application of our method is data denoising. Existing realworld text-video datasets for the retrieval task suffer from label noise which can harm training. More concretely, in crowd-sourced datasets such as MSR-VTT there are some captions that are highly ambiguous/generic (e.g "A tutorial is presented", "Clip showing different colours", "A man is writing") and can describe multiple videos from the dataset. We therefore propose to use TEACHTEXT teachers to filter out such cases. For this scenario, we simply remove lowranked predictions given by teachers and re-train the student using only the new samples. Specifically, we remove all sentences for which the correct video is not ranked in top 40 from the training set. This method is best-suited for datasets where multiple captions per video are available, ensuring that we can remove noisy captions without removing the video itself from training. Following this, we apply the denoising on MSR-VTT and MSVD datasets with the CE+ model. As seen in <ref type="figure">Fig. 7a</ref>, this can be an effective way of further improving the results. Please note, denoising is not used in any other ablations.</p><p>TEACHVIDEO -Extension to video modalities. While the focus of this work is the use of multiple text embeddings, it is natural to consider whether this approach can be extended to the video encoder modalities. Thus, we introduce the TEACHVIDEO algorithm which follows the same    setup as the original TEACHTEXT, but now the teacher has access to multiple video modalities instead of multiple text modalities. In this study, all students and all teachers use the same text embedding, so we can assess the gains due to TEACHVIDEO. By employing TEACHVIDEO we retain the computational advantage of requiring fewer video modalities during inference. As it can be seen from our experiments presented in <ref type="figure">Fig. 7b</ref>, the method is effective and brings a boost over the original student. We believe this extension may be useful in scenarios in which limited computational resources are available during inference. Qualitative examples and other ablation studies are presented in Suppl. Mat. <ref type="bibr" target="#b2">3</ref> Please note that the numbers reported are higher than in the original paper due to compression artefacts correction.   <ref type="table">Table 9</ref>. QuerYD: Comparison to state of the art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison to prior work</head><p>As it can be seen in Tab.2,3,4,5,6,7,8,9 our approach is effective and achieves state of the art results on six datasets. All methods are trained for the retrieval task using only the samples from the target datasets. In order to be as fair as possible, we included the results of our TEACHTEXT (abbreviated TT) applied also to the best existing method for each dataset. So, the architecture and the used features are identical during inference (e.g. TT-CE has the same architecture and uses the same video and text embeddings as CE). We highlight in bold the best performing method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we present a novel algorithm TEACHTEXT for the text-video retrieval task. We use a teacher-student paradigm where a student learns to leverage the additional information given by one or multiple teachers, sharing the architecture, but each using a different pre-trained text embedding at input. In this way, we achieve state of the art results on six benchmarks. Finally, we present an application of our approach for denoising video retrieval datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Video embeddings (experts) description</head><p>In this work, we used the set of pretrained experts considered by the authors of <ref type="bibr" target="#b35">[36]</ref>. For completeness, we summarise here the manner in which these experts were extracted.</p><p>? Two form of action experts are used: Action(KN) and</p><p>Action(IG). The former is an I3D architecture trained on Kinetics <ref type="bibr" target="#b11">[12]</ref>, which produces 1024-dimensional embeddings from frame clips extracted at 25fps and center cropped to 224 pixels. The Action(IG) model is a 34-layer R(2+1)D model <ref type="bibr" target="#b61">[62]</ref> that has ben trained on IG-65m <ref type="bibr" target="#b20">[21]</ref>: it operates on frames extracted at 30 fps in clips of 8 at 112 ? 112 pixel resolution.</p><p>? Two forms of object experts are used, named Obj(IN) and Obj(IG). They are produced from frame-level embeddings extracted at 25fps. The Obj(IN) model consists of an SENet-154 backbone <ref type="bibr" target="#b24">[25]</ref> which has been trained on ImageNet for image classification. Obj(IG) is formed from a ResNext-101 <ref type="bibr" target="#b70">[71]</ref> extractor which was trained on Instagram data that was weakly labelled with hashtags <ref type="bibr" target="#b39">[40]</ref>. For both models, frames are resized to 224?224 pixels.</p><p>? The face expert uses a ResNet50 <ref type="bibr" target="#b21">[22]</ref> that has been trained for task of face classification on the VGGFace2 dataset <ref type="bibr" target="#b9">[10]</ref>, producing a 512-dimensional embedding for each detected face following detection.</p><p>? The audio expert is produced using the VGGish model, trained for audio classification on the YouTube-8m dataset and described by <ref type="bibr" target="#b22">[23]</ref>.</p><p>? The scene expert is a 2208-dim embedding that is extracted frames (at 25 fps) for a center crop of 224?224 pixels. The model, which is pretrained on Places365 <ref type="bibr" target="#b77">[78]</ref>, uses a DenseNet-161 <ref type="bibr" target="#b25">[26]</ref> architecture.</p><p>? The speech expert is produced using the Google Cloud API (to transcribe the speech content).</p><p>? The OCR expert is a word2vec encoding <ref type="bibr" target="#b43">[44]</ref> of text detected in frames using <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b54">55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Experts refinement -Modifying the Kinetics action recognition model</head><p>Apart from dropping the OCR and face experts as described in the main paper, one small modification we propose to the expert selection made by <ref type="bibr" target="#b35">[36]</ref> is to replace the Action(IG) from an I3D model <ref type="bibr" target="#b11">[12]</ref> to an R2P1D model <ref type="bibr" target="#b61">[62]</ref> (matching the architecture Action(IG)) which has also been pretrained on IG-65m <ref type="bibr" target="#b20">[21]</ref> and then finetuned on the Kinetics dataset <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Text embeddings description</head><p>We use several text embeddings. In addition to the Sec. 3 from the main paper, further technical details about each of them are given below:</p><p>? mt grovle <ref type="bibr" target="#b7">[8]</ref> is a "vision-sensitive" language embedding which is adapted from w2v using WordNet and an original vision-language graph built from Visual Genome <ref type="bibr" target="#b31">[32]</ref>. The size of the final pre-trained embedding is 300.</p><p>? OpenAI-GPT [50] is a pre-trained text embedding which uses transformers <ref type="bibr" target="#b64">[65]</ref> and language modeling on a large corpus (the Toronto Book Corpus) (the final model has 110M params). The size of the final pretrained embedding is 768.</p><p>? RoBERTa <ref type="bibr" target="#b36">[37]</ref> is a BERT-based embedding <ref type="bibr" target="#b15">[16]</ref>. The model is trained longer with bigger batch size on more data, having 125M params. The size of the final pretrained embedding is 768.</p><p>? ALBERT <ref type="bibr" target="#b32">[33]</ref> is a lightweight modification to BERT <ref type="bibr" target="#b15">[16]</ref> which overcomes some memory limitations, having 11M params. The size of the final pretrained embedding is 768.</p><p>? GPT2-large <ref type="bibr" target="#b50">[51]</ref> is a transformer-based [65] model trained on even more data (40 Gb of text) without any supervision, having 774M params. The size of the final pre-trained embedding is 1280.</p><p>? GPT2-xl <ref type="bibr" target="#b50">[51]</ref> is similar to GPT2-large, but has more parameters (1558M params). The size of the final pretrained embedding is 1600.</p><p>? W2V <ref type="bibr" target="#b43">[44]</ref> is one of the most popular text embeddings used in vision tasks. It uses a neural network model to learn word representations. The size of the final pretrained embedding is 300.</p><p>In <ref type="figure">Fig.1</ref> from the main paper we highlight that the gain for a model that uses multiple text embeddings (last bar) is comparable with the gain of a model that uses multiple video modalities (middle bar), having as comparison a model that uses only one video modality (first bar). The first bar represents the CE <ref type="bibr" target="#b35">[36]</ref> model trained with one video embedding, namely Obj(IG) (the performance of the model is 19.8?0.1 in geometric mean of R1-R5-R10). The second bar represents a CE model using 7 video modalities both for inference and training (the performance of the model is 24.4?0.1 in geometric mean of R1-R5-R10). In the third and final bar of the chart we present the performance of using three different text embeddings with TEACHTEXT at training, while using only one text embedding at inference time (the performance of the model is 30.4?0.0 in geometric mean of R1-R5-R10). All the numbers are presented after the modification of the pre-processing pipeline (please see Sec. E for further details). All the experts used by CE <ref type="bibr" target="#b35">[36]</ref> are described in Sec.A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Optimization setup</head><p>CE+ models are trained in Pytorch <ref type="bibr" target="#b47">[48]</ref> using the Adam optimizer <ref type="bibr" target="#b28">[29]</ref>. We use a learning rate of 0.001 and weight decay of 1E-5. When using a base architecture different to the proposed CE+, we use the same hyper-parameters as in the public codebase for the underlying method (CE 4 and MMT 5 ). For MoEE, we use the re-implementation provided by the authors of the CE method <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Modification to pre-processing pipeline</head><p>During our preliminary analysis, we found out that some pretrained expert models produce embeddings that are fairly sensitive to jpeg compression artifacts. To address this, we re-extracted features from video frames densely extracted with minimal jpeg compression (corresponding to the use of ffmpeg <ref type="bibr" target="#b59">[60]</ref> and the -qscale:v 2 flag). In order to be fair in our comparisons, we apply this corrections everywhere. Due to this factor, we re-train MoEE <ref type="bibr" target="#b41">[42]</ref> and CE <ref type="bibr" target="#b35">[36]</ref> and report higher numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Dataset details</head><p>To provide an extensive comparison we test our approach on seven video datasets that have been explored in recent works as benchmarks for the task of text-video retrieval. Next, we give details about all the datasets used.</p><p>MSRVTT <ref type="bibr" target="#b71">[72]</ref> contains 10k videos, each having 20 captions. In order to test the retrieval performance, we report results on the official split which contains 2990 videos for the test split and 497 for validation, following the setup used in <ref type="bibr" target="#b35">[36]</ref>. We perform most of our ablations on this split. To enable comparison with as many other methods as possible, we also report results on the 1k-A split as used in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49]</ref>. For this split, we report the performance after training 100 epochs. The split contains 1000 video candidates for testing and 9000 for training. We use the same candidates as defined in <ref type="bibr" target="#b35">[36]</ref> which are used by all the other works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b73">74]</ref>, using each of the 20 captions associated to each video independently during evaluation and averaging performance across them.</p><p>MSVD <ref type="bibr" target="#b12">[13]</ref> contains 80k English descriptions for a total of 1970 videos. We use the standard split of 1200 (training), 100 (validation) and 670 (testing) as used in other works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b72">73]</ref>. The videos from MSVD do not have audio streams.</p><p>DiDeMo <ref type="bibr" target="#b0">[1]</ref> contains 10464 videos sourced from a large-scale creative commons collection <ref type="bibr" target="#b60">[61]</ref> and features moments of unedited, diverse content (concerts, sports, pets etc.). The dataset comprises 3-5 pairs of descriptions per video. We adopt the paragraph-video retrieval protocols used by <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b75">76]</ref> and use splits corresponding to 8392 train, 1065 validation and 1004 test videos.</p><p>LSMDC <ref type="bibr" target="#b51">[52]</ref> contains 118081 short video clips extracted from 202 movies. Each clip is described by a caption that is either extracted from the movie script or from transcribed DVS (descriptive video services) for the visually impaired. There are 7408 clips in the validation set and the testing is performed on 1000 videos from movies that are disjoint from the training and val sets as described in the Large Scale Movie Description Challenge (LSMDC) <ref type="bibr" target="#b5">6</ref> .</p><p>ActivityNet <ref type="bibr" target="#b8">[9]</ref> contains 20k videos extracted from YouTube and has around 100K descriptive sentences. We follow the same paragraph-video retrieval setup as used in prior works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b75">76]</ref> and report results on the val1 split. So, we use 10009 videos for training and 4917 videos for testing.</p><p>VaTeX <ref type="bibr" target="#b68">[69]</ref> contains 34911 videos with multilingual captions (Chinese and English). There are 10 captions per video for each language. We follow the same protocol as in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">49]</ref> and split the validation set equally (1500 validation and 1500 testing videos). In this work, we only use the English annotations.</p><p>QuerYD <ref type="bibr" target="#b45">[46]</ref> contains 1815 videos in the training split and 388 and 390 for validation and testing. The videos are sourced from YouTube and cover a diverse range of visual content. The dataset contains 31441 descriptions, from which 13019 are precisely localized in the video content (having start time and end time annotations) and the other 18422 are coarsely localized. For this work, we do not use the localization annotations and report results for the official splits. <ref type="figure">Figure 8</ref>. (a) Batch size variation. We vary the batch size for the MSR-VTT dataset to see how this affects the performance. We observe that batch size influences performance. The underlying architecture used for this experiment is CE+. (b) Similarity matrix aggregation. We present a comparison of different similarity matrix aggregation: min, max and average. As can be seen, the average aggregation has the best results (both when evaluating the teacher standalone or in conjunction with our TEACHTEXT algorithm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Ablations</head><p>In this section, we present additional ablations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Batch size variation</head><p>In <ref type="figure">Fig. 8a</ref> we vary the batch size for the MSR-VTT dataset in order to see how the performance is affected. As can be seen, we obtain the best value using the same batch size as for the method without applying TEACHTEXT algorithm (64 in this case).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Similarity matrix aggregation study</head><p>In <ref type="figure">Fig. 8b</ref> we present several similarity matrix aggregation possibilities: min, max and average. We observe that using the mean of the similarity matrices is more effective. Because of this, we use the mean as the final aggregation technique in our TEACHTEXT algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3. Denoising</head><p>In <ref type="figure">Fig. 9a</ref> we vary the threshold used to filter out sentences from the training set. As can be seen, this denoising method is effective and it can provide a significant gain in performance. In this experiment we have found out that the best threshold for MSRVTT is rank 40. Additionally, we present denoising results in <ref type="figure">Fig. 9b</ref> for the MSVD dataset using the 100 threshold. This method turns out to be effective in reducing noise for retrieval datasets. Denoising is not use in any other ablation studies. The final results when comparing with other state of the art methods are presented using denoising on MSRVTT and MSVD datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4. Distillation setup</head><p>As stated in the main paper, the distillation setup admits a number of variants. In addition to the methods presented <ref type="figure">Figure 9</ref>. (a) Rank variation for denoising. The denoising involves dropping captions that are assigned a low ranking by the teacher for the training set. In this experiment, we vary the rank below which we drop sentences. Please note that for a rank of 5 (on the training set) the amount of dropped sentences is approximately 46%. Note that MSR-VTT has 20 captions per video, so after applying this filter we keep on average 10 captions per video. (b) Denoising. We present the effect of denoising on retrieval performance on MSVD. Some of the captions available in datasets with multiple captions per video may be noisy and actively harm the training process. We estimate the degree of noise present in a caption by looking at the teacher rank and drop the caption if necessary. We observe the effectiveness of denoising when applied in isolation (CE+ vs CE+ Denoise) and in conjunction with the full TEACHTEXT method. The experiment is presented for dropping sentences that rank higher than rank 100.</p><p>in <ref type="figure">Fig. 6b</ref> from the main paper, in <ref type="figure">Fig. 10a</ref> we present several additional comparisons. More exactly, we test our approach against a more classical distillation setup where we directly regress the embeddings given by the teacher (Embd regress). This setup does not follow the idea of relational distillation. Additionally, we also apply the angle distillation as introduced by <ref type="bibr" target="#b46">[47]</ref> (Relational angle) where we use exactly the loss as in the public code <ref type="bibr" target="#b6">7</ref> . Please note that the drop in performance as opposed to the student without distillation can be explained by some technical challenges that we encountered in order to make the angle loss compatible with the ranking loss used for this task. Last but not least, we also show that a small improvement can be obtained by using a self learning technique, where the teacher has the exact same architecture and inputs as the student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5. Loss study</head><p>In the main paper, we follow recent literature <ref type="bibr" target="#b46">[47]</ref> and use the Huber loss for distillation. However, we wanted to see how various losses affect the performance. We test with L1 and L2 losses. As can be seen in <ref type="figure">Fig. 10b</ref>, the Huber loss performs better than L1 loss and a bit better than L2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.6. Mixture of architectures</head><p>Our TEACHTEXT assumes that the only difference between the teacher and the student is the used pre-trained text embedding fed to the model. However, our method is not limited to this constraint. In this section, we show <ref type="figure">Figure 10</ref>. (a) Distillation type. The first bar represents the student performance without distillation (CE+). In addition to the methods presented in the main paper, here we test other distillation approaches: Embd regress which is a classical approach where the query and video joint embeddings are directly regressed based on the embeddings given by the teacher, Relational angle where we apply the angle relationships as introduced by <ref type="bibr" target="#b46">[47]</ref>. In addition, we present results of our method in a self-learning setup where the teacher is the student from a previous run (Self learn). The last bar represent the performance of the TEACHTEXT approach. (b) Loss study. In this picture, we show how various distillation losses (L1, L2, Huber) affect the performance. <ref type="figure">Figure 11</ref>. Mixture of architectures. We perform some preliminary experiments to see if the method may benefit from learning from teachers that do not share the same architecture. The x axis corresponds to the models which are used as teachers. In cases labeled with 3 text each, we used three different variations of each architecture as teachers, accounting for a total number of no. methods * 3 teachers. As can be seen, the results suggest that there is no clear benefit in using multiple architectures as teachers.</p><p>how having multiple teachers, that now have a different underlying architecture affect the performance of our method. Please note that in all other ablations, the architecture is shared between student and teacher. This is the only exception. Our preliminary results shown in <ref type="figure">Fig. 11</ref> suggest that there isn't much improvement that may be achieved by using a mixture of architectures as teachers. This is somehow expected, since these methods usually share the same video modalities so there isn't much additional information that may be captured by the combination of multiple architectures. However, we expect to get a further boost if we diversify the set of used modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.7. Architecture extension</head><p>In addition to the main paper, we also introduce a new CE-L base architecture. This is similar to the CE <ref type="bibr" target="#b35">[36]</ref>  We study the influence of teachers with different text embeddings at input: w2v and gpt2-xl. The first point represents the performance of the student without using TEACHTEXT. We observe a boost in performance independent of the nature of the teacher. We can see that the model with TEACHTEXT, preserves most of the knowledge from the student without TEACHTEXT, but also acquires new information from the teacher (yellow area). Best viewed in color. and CE+, but uses w2v as the text embedding. In this way, the number of parameters are greatly reduced, making this the most lightweight architecture in terms of number of parameters that we can create. In Tab. 10, you can see that our method TEACHTEXT is effective even when using this lightweight architecture. This architecture also has the lowest numbers of parameters when compared to other state of the art methods as can be seen in Tab. <ref type="bibr">11,12,13,14,15,16,17,18.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.8. Model complexity</head><p>Changes in the pretrained text embedding strongly affect the number of parameters. Because of this factor, using more text embeddings at test time may strongly affect the total number of learnable parameters available to the model (in addition to adding the requirement to extract additional text embeddings during inference). While the simple 'Mean' aggregation from <ref type="figure">Fig.5b</ref>   <ref type="table" target="#tab_0">Table 10</ref>. Method generality. Retrieval performance on various datasets when applying TEACHTEXT on top of different base models. In addition to the main paper, we present results on the CE-L architecture which has a significant drop in the number of used parameters as compared to the other models. We present in bold cases where TEACHTEXT brings an improvement over the base architecture. As can be seen, our method is effective and brings a consistent boost independent of the base architecture. <ref type="figure">Figure 14</ref>. Qualitative results. We present the top 3 video retrievals for each query, given by the TEACHTEXT method used on top of a CE+ architecture. Moreover, we show the rank and similarity for the teacher, as well as for the student without using TEACHTEXT for the ground truth video. We mark in green cases where the retrieval is correct in terms of R1 and with red cases where is incorrect. For each of the cases shown, the model learns from the teacher to correct its prediction.</p><p>adds a significant quantity (approx 240M learnable parameters, yielding total model sizes of 503.98M vs 262.73M for CE+). The proposed TEACHTEXT approach leaves the number of parameters untouched.</p><p>Since changing the text embedding to CE+ results in an increase in number of learnable parameters, we also study a CE-L architecture as a lightweight alternative in this Suppl. Mat. (described in Sec. G.7), which demonstrates that the gain from the proposed TEACHTEXT approach is not limited to models with many learnable parameters. Please check Tab. <ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18</ref> for the exact number of params for every used architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.9. Amount of training data vs performance.</head><p>We next study how training data quantity influences the proposed method. In <ref type="figure" target="#fig_2">Fig. 12a</ref> we observe that by using the TEACHTEXT with more and more data, the performance gap increases, suggesting that its benefit may prove to be useful even in larger scale dataset scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.10. Teacher study</head><p>In <ref type="figure" target="#fig_2">Fig. 12b</ref>, we study how each embedding affects the final performance. We observe that even though the model ingesting w2v embeddings has a significant lower performance than the student model without using TEACHTEXT, there is a significant gain when learning from the teacher which uses w2v. This again indicates that there is additional information captured by using a different text embedding which can be exploited by TEACHTEXT.</p><p>G.11. Influence of distillation over the correctly retrieved samples</p><p>In <ref type="figure" target="#fig_3">Fig. 13</ref> we present the shares of correctly retrieved samples in terms of R1 on the test set of the MSR-VTT dataset for the student with and without TEACHTEXT and for the teacher. In <ref type="figure" target="#fig_3">Fig. 13a</ref> we present results when we learn from the three teachers and in <ref type="figure" target="#fig_3">Fig. 13b</ref> we considered the case when we learn only from one teacher (namely w2v). There is a significant share of correctly retrieved sample between the student using TEACHTEXT and the teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Comparison to prior work</head><p>In Tab. <ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18</ref> we make an extensive comparison of our method with other methods from the literature. In addition to the numbers reported in the main paper, we also report results for the v2t task. Moreover, we present the number of parameters of each method where available. As can be seen, our TEACHTEXT algorithm brings a clear improvement and the total number of parameters remains the same as for the base architecture. In addition to the main paper, we also introduce a new CE-L  base architecture. This is similar to the CE <ref type="bibr" target="#b35">[36]</ref> and CE+, but uses w2v as text embedding. In this way, the number of parameters is greatly reduced. As can be seen from the tables, this lightweight architecture combined with our TEACHTEXT algorithm has very good results showcasing the effectiveness of TEACHTEXT across different parame-ter regimes. Moreover, some qualitative results can be seen in <ref type="figure">Fig.14</ref> </p><formula xml:id="formula_8">Model Task R@1 ? R@5 ? R@10 ? M dR ? Task R@1 ? R@5 ? R@10 ? M dR</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 TEACHTEXT algorithm 1 : 3 : 4 :</head><label>134</label><figDesc>Phase 1: Learn teacher models 2:Train N teacher models T k = (F k , Q k ), k ? {1, . . . , N } using the training pairs (x i , t k j ) where t k jrepresents the text modality used by teacher T k in a standard retrieval training setup (Sec. 4.1). Phase 2: Learn the student model, M = (F, Q) for minibatch of B paired samples {(v i , c i )} do 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 12 .</head><label>12</label><figDesc>(a) Amount of training data vs performance. As it can be seen, with the increase of training data, the improvement brought by TEACHTEXT increases. (b) Performance vs teacher type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 13 .</head><label>13</label><figDesc>Share of samples correctly retrieved samples in terms of R1 when using TEACHTEXT on the MSR-VTT test set. In sub-fig (a) we show the case where we learn from 3 teachers, while in sub-fig (b) you can find the single teacher case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>51] and gpt2-xl-F (gpt2-xl finetuned on the captions from the target dataset). A study of how each individual text embedding affects the final performance can be found in the Suppl. Mat. section Teacher study, where we observe that even when us-?0.1 25.8 ?0.1 41.6 ?0.4 43.4 ?0.6 41.8 ?0.3 43.2 ?0.5 33.2 ?1.4 40.2 ?0.7 23.8 ?0.4 26.0 ?0.5 40.1 ?0.3 45.2 ?0.1 CE 24.4 ?0.1 25.9 ?0.1 42.0 ?0.8 43.8 ?0.3 42.3 ?0.6 42.6 ?0.4 34.2 ?0.4 39.5 ?0.5 23.7 ?0.3 25.5 ?0.5 40.4 ?0.3 45.0 ?0.6 ?0.7 25.9 ?0.6 44.0 ?0.4 47.9 ?0.4 CE+ 29.2 ?0.2 30.4 ?0.0 50.3 ?0.2 50.9 ?0.4 46.5 ?1.0 46.6 ?0.5 35.8 ?0.4 40.4 ?0.4 28.1 ?0.3 30.7 ?0.3 39.7 ?0.0 46.3 ?0.2 Method generality. Retrieval performance (geometric mean of R1-R5-R10) on various datasets when applying TEACHTEXT on top of different base models: MoEE</figDesc><table><row><cell>Model</cell><cell>Base</cell><cell>MSRVTT TEACHTEXT</cell><cell cols="2">MSRVTT 1k-A Base TEACHTEXT</cell><cell>Base</cell><cell>MSVD TEACHTEXT</cell><cell>Base</cell><cell>DiDeMo TEACHTEXT</cell><cell>Base</cell><cell>LSMDC TEACHTEXT</cell><cell>ActivityNet Base TEACHTEXT</cell></row><row><cell cols="2">MoEE 24.4 MMT -</cell><cell>-</cell><cell>44.7 ?0.4</cell><cell>45.6 ?0.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>24.6</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>MoEE[42] 3 11.1 ?0.1 30.7 ?0.1 42.9 ?0.1 15.0 ?0.0 CE[36] 3 11.0 ?0.0 30.8 ?0.1 43.3 ?0.3 15.0 ?0.0 TT-CE 11.8 ?0.1 32.7 ?0.1 45.3 ?0.1 13.0 ?0.0 TT-CE+ 15.0 ?0.1 38.5 ?0.1 51.7 ?0.1 10.0 ?0.0 MSR-VTT full split: Comparison to state of the art. 21.6 ?1.0 50.8 ?1.1 65.6 ?0.7 5.3 ?0.6 CE[36] 3 21.7 ?1.3 51.8 ?0.5 65.7 ?0.6 5.0 ?0.0 ?0.2 55.9 ?0.7 68.5 ?1.0 4.3 ?0.5 TT-CE+ 29.6 ?0.3 61.6 ?0.5 74.2 ?0.3 3.0 ?0.0</figDesc><table><row><cell>Model</cell><cell>R@1 ?</cell><cell>R@5 ?</cell><cell cols="2">R@10 ? M dR ?</cell></row><row><cell>Dual[18]</cell><cell>7.7</cell><cell>22.0</cell><cell>31.8</cell><cell>32.0</cell></row><row><cell>HGR[14]</cell><cell>9.2</cell><cell>26.2</cell><cell>36.5</cell><cell>24.0</cell></row><row><cell>Model</cell><cell>R@1 ?</cell><cell>R@5 ?</cell><cell cols="2">R@10 ? M dR ?</cell></row><row><cell>MoEE[42] 3 MMT[20]</cell><cell cols="4">24.6 ?0.4 54.0 ?0.2 67.1 ?0.5 4.0 ?0.0</cell></row><row><cell>SSB[49]</cell><cell>27.4</cell><cell>56.3</cell><cell>67.7</cell><cell>3.0</cell></row><row><cell>TT-MMT</cell><cell>24.8</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>MSR-VTT 1k-A split[74]: Comparison with others. MoEE[42] 3 21.1 ?0.2 52.0 ?0.7 66.7 ?0.2 5.0 ?0.0 CE[36] 3 21.5 ?0.5 52.3 ?0.8 67.5 ?0.7 5.0 ?0.0 TT-CE 22.1 ?0.4 52.2 ?0.5 67.2 ?0.6 5.0 ?0.</figDesc><table><row><cell>Model</cell><cell>R@1 ?</cell><cell>R@5 ?</cell><cell cols="2">R@10 ? M dR ?</cell></row><row><cell>VSE++[19]</cell><cell>15.4</cell><cell>39.6</cell><cell>53.0</cell><cell>9.0</cell></row><row><cell>M-Cues[45]</cell><cell>20.3</cell><cell>47.8</cell><cell>61.1</cell><cell>6.0</cell></row></table><note>0 TT-CE+ 25.4 ?0.3 56.9 ?0.4 71.3 ?0.2 4.0 ?0.0</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>MSVD: Comparison to state of the art methods. MoEE[42] 3 16.1 ?1.0 41.2 ?1.6 55.2 ?1.6 8.3 ?0.5 CE[36] 3 17.1 ?0.9 41.9 ?0.2 56.0 ?0.5 8.0 ?0.0 TT-CE 21.0 ?0.6 47.5 ?0.9 61.9 ?0.5 6.0 ?0.0 TT-CE+ 21.6 ?0.7 48.6 ?0.4 62.9 ?0.6 6.0 ?0.0 DiDeMo: Comparison to state of the art methods. MoEE[42] 3 12.1 ?0.7 29.4 ?0.8 37.7 ?0.2 23.2 ?0.8 CE[36] 3 12.4 ?0.7 28.5 ?0.8 37.9 ?0.6 21.7 ?0.6 MMT[20] 13.2 ?0.4 29.2 ?0.8 38.8 ?0.9 21.0 ?1.4 TT-MMT 13.6 ?0.5 31.2 ?0.4 40.8 ?0.5 17.7 ?0.5 TT-CE+ 17.2 ?0.4 36.5 ?0.6 46.3 ?0.3 13.7 ?0.5</figDesc><table><row><cell>Model</cell><cell>R@1 ?</cell><cell>R@5 ?</cell><cell>R@10 ?</cell><cell>M dR ?</cell></row><row><cell>S2VT[67]</cell><cell>11.9</cell><cell>33.6</cell><cell>?</cell><cell>13.0</cell></row><row><cell>FSE[77]</cell><cell cols="2">13.9 ?0.7 36.0 ?0.8</cell><cell>?</cell><cell>11.0 ?0.0</cell></row><row><cell>Model</cell><cell>R@1 ?</cell><cell>R@5 ?</cell><cell>R@10 ?</cell><cell>M dR ?</cell></row><row><cell>JSFus[74]</cell><cell>9.1</cell><cell>21.2</cell><cell>34.1</cell><cell>36.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>LSMDC: Comparison to state of the art methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>MoEE[42] 3 19.7 ?0.3 50.0 ?0.5 92.0 ?0.2 5.3 ?0.5 CE[36] 3 19.9 ?0.3 50.1 ?0.7 92.2 ?0.6 5.3 ?0.5 ?0.2 54.2 ?1.0 93.2 ?0.4 5.0 ?0.0 ?0.3 58.7 ?0.4 95.6 ?0.2 4.0 ?0.0 TT-CE+ 23.5 ?0.2 57.2 ?0.5 96.1 ?0.1 4.0 ?0.0 ActivityNet: Comparison to state of the art methods. ?0.1 84.2 ?0.1 91.3 ?0.1 2.0 ?0.0 TT-CE 49.7 ?0.1 85.6 ?0.1 92.4 ?0.1 2.0 ?0.0 TT-CE+ 53.2 ?0.2 87.4 ?0.1 93.3 ?0.0 1.0 ?0.0</figDesc><table><row><cell>Model</cell><cell>R@1 ?</cell><cell>R@5 ?</cell><cell cols="2">R@50 ? M dR ?</cell></row><row><cell>HSE[76]</cell><cell>20.5</cell><cell>49.3</cell><cell>?</cell><cell>?</cell></row><row><cell cols="2">MMT[20] 22.7 SSB[49] 26.8</cell><cell>58.1</cell><cell>93.5</cell><cell>3.0</cell></row><row><cell cols="2">TT-MMT 25.0 Model R@1 ?</cell><cell>R@5 ?</cell><cell cols="2">R@10 ? M dR ?</cell></row><row><cell>VSE[30]</cell><cell>28.0</cell><cell>64.3</cell><cell>76.9</cell><cell>3.0</cell></row><row><cell>Dual[18]</cell><cell>31.1</cell><cell>67.4</cell><cell>78.9</cell><cell>3.0</cell></row><row><cell>VSE++[19]</cell><cell>33.7</cell><cell>70.1</cell><cell>81.0</cell><cell>2.0</cell></row><row><cell>HGR[14]</cell><cell>35.1</cell><cell>73.5</cell><cell>83.5</cell><cell>2.0</cell></row><row><cell>SSB[49]</cell><cell>44.6</cell><cell>81.8</cell><cell>89.5</cell><cell>1.0</cell></row><row><cell>CE[36]</cell><cell>47.9</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>VaTeX: Comparison to state of the art methods. ?1.3 30.2 ?3.0 43.2 ?3.1 14.2 ?1.</figDesc><table><row><cell>Model</cell><cell>R@1 ?</cell><cell>R@5 ?</cell><cell>R@10 ?</cell><cell>M dR ?</cell></row><row><cell cols="2">MoEE[42] 11.6</cell><cell></cell><cell></cell><cell></cell></row></table><note>6 CE[36] 13.9 ?0.8 37.6 ?1.2 48.3 ?1.4 11.3 ?0.6 TT-CE 14.2 ?1.4 36.6 ?2.0 51.1 ?2.1 9.7 ?1.2 TT-CE+ 14.4 ?0.5 37.7 ?1.7 50.9 ?1.6 9.8 ?1.0</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>in the main paper, does not change the number of parameters, the 'Concat' aggregation ?0.1 25.8 ?0.1 41.6 ?0.4 43.4 ?0.6 41.8 ?0.3 43.2 ?0.5 33.2 ?1.4 40.2 ?0.7 23.8 ?0.4 26.0 ?0.5 40.1 ?0.3 45.2 ?0.1 CE 24.4 ?0.1 25.9 ?0.1 42.0 ?0.8 43.8 ?0.3 42.3 ?0.6 42.6 ?0.4 34.2 ?0.4 39.5 ?0.5 23.7 ?0.3 25.5 ?0.5 40.4 ?0.3 45.0 ?0.6 ?0.7 25.9 ?0.6 44.0 ?0.4 47.9 ?0.4 CE+ 29.2 ?0.2 30.4 ?0.0 50.3 ?0.2 50.9 ?0.4 46.5 ?1.0 46.6 ?0.5 35.8 ?0.4 40.4 ?0.4 28.1 ?0.3 30.7 ?0.3 39.7 ?0.0 46.3 ?0.2 CE-L 25.5 ?0.1 26.9 ?0.1 45.7 ?0.2 46.5 ?0.8 41.3 ?0.5 42.6 ?0.7 36.4 ?0.5 41.5 ?0.4 24.1 ?0.2 25.9 ?0.3 39.6 ?0.5 45.7 ?0.2</figDesc><table><row><cell>Model</cell><cell>Base</cell><cell>MSRVTT TEACHTEXT</cell><cell cols="2">MSRVTT 1k-A Base TEACHTEXT</cell><cell>Base</cell><cell>MSVD TEACHTEXT</cell><cell>Base</cell><cell>DiDeMo TEACHTEXT</cell><cell>Base</cell><cell>LSMDC TEACHTEXT</cell><cell>ActivityNet Base TEACHTEXT</cell></row><row><cell cols="2">MoEE 24.4 MMT -</cell><cell>-</cell><cell>44.7 ?0.4</cell><cell>45.6 ?0.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>24.6</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>t2v 11.1 ?0.1 30.7 ?0.1 42.9 ?0.1 15.0 ?0.0 v2t 16.5 ?0.1 43.1 ?0.5 57.3 ?0.6 7.7 ?0.5 400.41M CE[36] 3 t2v 11.0 ?0.0 30.8 ?0.1 43.3 ?0.3 15.0 ?0.0 v2t 17.0 ?0.5 43.5 ?0.4 57.8 ?0.5 7.2 ?0.2 183.45M TT-CE t2v 11.8 ?0.1 32.7 ?0.1 45.3 ?0.1 13.0 ?0.0 v2t 19.3 ?0.4 47.0 ?0.7 60.0 ?0.4 6.7 ?0.5 183.45M TT-CE-L t2v 13.0 ?0.0 34.6 ?0.1 47.3 ?0.2 12.0 ?0.0 v2t 22.4 ?0.3 50.4 ?0.6 63.8 ?0.3 5.3 ?0.5 66.72M TT-CE+ t2v 15.0 ?0.1 38.5 ?0.1 51.7 ?0.1 10.0 ?0.0 v2t 25.3 ?0.1 55.6 ?0.0 68.6 ?0.4 4.0 ?0.0 262.73M Table 11. MSR-VTT full split: Comparison to state of the art. t2v 21.6 ?1.0 50.8 ?1.1 65.6 ?0.7 5.3 ?0.6 v2t 22.4 ?0.5 51.2 ?1.0 66.1 ?0.4 5.2 ?0.3 400.41M CE[36] 3 t2v 21.7 ?1.3 51.8 ?0.5 65.7 ?0.6 5.0 ?0.0 v2t 22.7 ?0.4 51.8 ?0.4 65.7 ?0.2 5.0 ?0.0 183.45M MMT[20] t2v 24.6 ?0.4 54.0 ?0.2 67.1 ?0.5 4.0 ?0.0 v2t 24.4 ?0.5 56.0 ?0.9 67.8 ?0.3 4.0 ?0.0 133.36M ?0.2 55.9 ?0.7 68.5 ?1.0 4.3 ?0.5 v2t 25.1 ?1.0 57.1 ?0.8 69.9 ?1.1 4.0 ?0.0 133.36M TT-CE-L t2v 26.5 ?0.4 58.0 ?0.8 71.1 ?0.4 4.0 ?0.0 v2t 27.6 ?0.8 58.0 ?0.6 70.0 ?0.5 4.0 ?0.0 66.72M TT-CE+ t2v 29.6 ?0.3 61.6 ?0.5 74.2 ?0.3 3.0 ?0.0 v2t 32.1 ?0.5 62.7 ?0.5 75.0 ?0.2 3.0 ?0.0 262.73M Table 12. MSR-VTT 1k-A split[74]: Comparison with others. t2v 21.1 ?0.2 52.0 ?0.7 66.7 ?0.2 5.0 ?0.0 v2t 27.3 ?0.9 55.1 ?1.2 65.0 ?0.8 4.3 ?0.5 131.37M CE[36] 3 t2v 21.5 ?0.5 52.3 ?0.8 67.5 ?0.7 5.0 ?0.0 v2t 26.3 ?1.4 53.7 ?0.4 65.3 ?1.1 4.8 ?0.2 84.04M TT-CE t2v 22.1 ?0.4 52.2 ?0.5 67.2 ?0.6 5.0 ?0.0 v2t 26.0 ?0.4 53.3 ?0.4 63.9 ?0.1 4.9 ?0.1 84.04M TT-CE-L t2v 22.5 ?0.0 53.7 ?0.3 68.7 ?0.5 5.0 ?0.0 v2t 25.6 ?0.2 55.7 ?0.9 65.9 ?0.5 4.0 ?0.0 27.78M TT-CE+ t2v 25.4 ?0.3 56.9 ?0.4 71.3 ?0.2 4.0 ?0.0 v2t 27.1 ?0.4 55.3 ?1.0 67.1 ?0.2 4.0 ?0.0 87.79M Table 13. MSVD: Comparison to state of the art methods. ?0.7 36.0 ?0.8 -78.9 ?1.6 11.0 ?0.0 v2t 13.1 ?0.5 33.9 ?0.4 -78.0 ?0.8 12.0 ?0.0 -MoEE[42] 3 t2v 16.1 ?1.0 41.2 ?1.6 55.2 ?1.6 81.7 ?1.4 8.3 ?0.5 v2t 16.0 ?1.5 41.7 ?1.9 54.6 ?1.7 81.0 ?1.4 8.7 ?0.9 107.26M CE[36] 3 t2v 17.1 ?0.9 41.9 ?0.2 56.0 ?0.5 83.4 ?0.7 8.0 ?0.0 v2t 17.1 ?0.1 41.8 ?0.9 55.2 ?1.0 83.0 ?0.8 7.7 ?0.5 79.29M TT-CE t2v 21.0 ?0.6 47.5 ?0.9 61.9 ?0.5 86.4 ?0.8 6.0 ?0.0 v2t 20.3 ?0.6 46.6 ?0.6 59.8 ?1.2 85.7 ?0.6 6.7 ?0.5 79.29M TT-CE-L t2v 22.3 ?0.2 50.1 ?0.9 64.3 ?0.5 86.9 ?0.4 5.3 ?0.5 v2t 21.3 ?0.4 48.3 ?0.5 62.5 ?0.3 86.6 ?0.1 6.0 ?0.0 43.51M TT-CE+ t2v 21.6 ?0.7 48.6 ?0.4 62.9 ?0.6 86.8 ?0.3 6.0 ?0.0 v2t 21.1 ?0.2 47.3 ?0.2 61.1 ?0.4 86.7 ?0.2 6.3 ?0.5 99.51M</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>Params</cell></row><row><cell>Dual[18]</cell><cell cols="2">t2v</cell><cell>7.7</cell><cell></cell><cell>22.0</cell><cell>31.8</cell><cell>32.0</cell><cell>v2t</cell><cell>13.0</cell><cell>30.8</cell><cell></cell><cell></cell><cell>43.3</cell><cell>15.0</cell><cell>-</cell></row><row><cell>HGR[14]</cell><cell cols="2">t2v</cell><cell>9.2</cell><cell></cell><cell>26.2</cell><cell>36.5</cell><cell>24.0</cell><cell>v2t</cell><cell>15.0</cell><cell>36.7</cell><cell></cell><cell></cell><cell>48.8</cell><cell>11.0</cell><cell>-</cell></row><row><cell>MoEE[42] 3 Model</cell><cell>Task</cell><cell cols="3">R@1 ?</cell><cell>R@5 ?</cell><cell cols="3">R@10 ? M dR ? Task</cell><cell>R@1 ?</cell><cell cols="2">R@5 ?</cell><cell cols="3">R@10 ? M dR ?</cell><cell>Params</cell></row><row><cell>MoEE[42] 3 SSB[49]</cell><cell>t2v</cell><cell></cell><cell>27.4</cell><cell></cell><cell>56.3</cell><cell>67.7</cell><cell>3.0</cell><cell>v2t</cell><cell>26.6</cell><cell>55.1</cell><cell></cell><cell></cell><cell>67.5</cell><cell>3.0</cell><cell>?</cell></row><row><cell cols="5">TT-MMT t2v 24.8 Model Task R@1 ?</cell><cell>R@5 ?</cell><cell cols="3">R@10 ? M dR ? Task</cell><cell>R@1 ?</cell><cell cols="2">R@5 ?</cell><cell cols="3">R@10 ? M dR ?</cell><cell>Params</cell></row><row><cell cols="3">VSE++[19] t2v</cell><cell cols="2">15.4</cell><cell>39.6</cell><cell>53.0</cell><cell>9.0</cell><cell>v2t</cell><cell>21.2</cell><cell>43.4</cell><cell></cell><cell></cell><cell>52.2</cell><cell>9.0</cell><cell>-</cell></row><row><cell cols="3">M-Cues[45] t2v</cell><cell cols="2">20.3</cell><cell>47.8</cell><cell>61.1</cell><cell>6.0</cell><cell>v2t</cell><cell>31.5</cell><cell>51.0</cell><cell></cell><cell></cell><cell>61.5</cell><cell>5.0</cell><cell>-</cell></row><row><cell>MoEE[42] 3 Model</cell><cell>Task</cell><cell cols="2">R@1 ?</cell><cell>R@5 ?</cell><cell>R@10 ?</cell><cell>R@50 ?</cell><cell>M dR ?</cell><cell>Task</cell><cell>R@1 ?</cell><cell>R@5 ?</cell><cell cols="2">R@10 ?</cell><cell>R@50 ?</cell><cell>M dR ?</cell><cell>Params</cell></row><row><cell>S2VT[67]</cell><cell>t2v</cell><cell>11.9</cell><cell></cell><cell>33.6</cell><cell>-</cell><cell>76.5</cell><cell>13.0</cell><cell>v2t</cell><cell>13.2</cell><cell>33.6</cell><cell>-</cell><cell></cell><cell>76.5</cell><cell>15.0</cell><cell>-</cell></row><row><cell>FSE[77]</cell><cell cols="2">t2v 13.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 14 .</head><label>14</label><figDesc>DiDeMo: Comparison to state of the art methods. t2v 12.1 ?0.7 29.4 ?0.8 37.7 ?0.2 23.2 ?0.8 v2t 11.9 ?0.5 28.0 ?0.5 37.4 ?0.5 25.5 ?1.5 159.78M CE[36] 3 t2v 12.4 ?0.7 28.5 ?0.8 37.9 ?0.6 21.7 ?0.6 v2t 11.4 ?0.4 28.4 ?0.7 36.5 ?0.5 25.0 ?0.8 116.86M MMT[20] t2v 13.2 ?0.4 29.2 ?0.8 38.8 ?0.9 21.0 ?1.4 v2t 12.1 ?0.1 29.3 ?1.1 37.9 ?1.1 22.5 ?0.4 133.16M TT-MMT t2v 13.6 ?0.5 31.2 ?0.4 40.8 ?0.5 17.7 ?0.5 v2t 12.5 ?0.3 31.3 ?0.6 41.0 ?1.1 18.7 ?0.5 133.16M TT-CE-L t2v 14.2 ?0.2 30.6 ?0.3 40.0 ?0.5 20.3 ?0.5 v2t 13.6 ?0.3 30.8 ?0.9 38.9 ?0.8 21.5 ?0.4 87.22M TT-CE+ t2v 17.2 ?0.4 36.5 ?0.6 46.3 ?0.3 13.7 ?0.5 v2t 17.5 ?0.6 36.0 ?1.2 45.0 ?0.5 14.3 ?0.9 388.24M Table 15. LSMDC: Comparison to state of the art methods.</figDesc><table><row><cell>Model</cell><cell>Task</cell><cell>R@1 ?</cell><cell>R@5 ?</cell><cell>R@10 ?</cell><cell>M dR ?</cell><cell>Task</cell><cell>R@1 ?</cell><cell>R@5 ?</cell><cell>R@10 ?</cell><cell>M dR ?</cell><cell>Params</cell></row><row><cell>JSFus[74]</cell><cell>t2v</cell><cell>9.1</cell><cell>21.2</cell><cell>34.1</cell><cell>36.0</cell><cell>v2t</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MoEE[42] 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 16 .Table 17 .Table 18 .</head><label>161718</label><figDesc>. R@50 ? M dR ? Params MoEE[42] 3 t2v 19.7 ?0.3 50.0 ?0.5 92.0 ?0.2 5.3 ?0.5 v2t 18.3 ?0.5 48.3 ?0.8 92.0 ?0.2 6.0 ?0.0 330.42M CE[36] 3 t2v 19.9 ?0.3 50.1 ?0.7 92.2 ?0.6 5.3 ?0.5 v2t 18.6 ?0.3 48.6 ?0.7 92.0 ?0.2 6.0 ?0.0 260.68M ?0.2 54.2 ?1.0 93.2 ?0.4 5.0 ?0.0 v2t 22.9 ?0.8 54.8 ?0.4 93.1 ?0.2 4.3 ?0.5 127.35M ?0.3 58.7 ?0.4 95.6 ?0.2 4.0 ?0.0 v2t 24.4 ?0.1 58.2 ?0.3 95.7 ?0.1 4.0 ?0.0 127.35M TT-CE-L t2v 23.3 ?0.1 56.3 ?0.1 95.5 ?0.1 4.0 ?0.0 v2t 20.7 ?0.2 52.8 ?0.2 94.4 ?0.0 5.0 ?0.0 103M TT-CE+ t2v 23.5 ?0.2 57.2 ?0.5 96.1 ?0.1 4.0 ?0.0 v2t 23.0 ?0.3 56.1 ?0.2 95.8 ?0.0 4.0 ?0.0 376.02M ActivityNet: Comparison to state of the art methods. ?0.1 84.2 ?0.1 91.3 ?0.1 2.0 ?0.0 v2t 60.7 ?1.0 89.0 ?0.4 94.9 ?0.2 1.0 ?0.0 115.56M TT-CE t2v 49.7 ?0.1 85.6 ?0.1 92.4 ?0.1 2.0 ?0.0 v2t 62.1 ?0.2 90.0 ?0.1 95.3 ?0.1 1.0 ?0.0 115.56M TT-CE-L t2v 51.5 ?0.1 86.5 ?0.1 92.6 ?0.1 1.0 ?0.0 v2t 65.0 ?0.5 90.3 ?0.5 95.9 ?0.2 1.0 ?0.0 55.07M TT-CE+ t2v 53.2 ?0.2 87.4 ?0.1 93.3 ?0.0 1.0 ?0.0 v2t 64.7 ?0.3 91.5 ?0.3 96.2 ?0.1 1.0 ?0.0 223.1M VaTeX: Comparison to state of the art methods. t2v 11.6 ?1.3 30.2 ?3.0 43.2 ?3.1 14.2 ?1.6 v2t 13.0 ?3.1 30.9 ?2.0 43.0 ?2.8 14.5 ?1.8 57.75M CE[36] t2v 13.9 ?0.8 37.6 ?1.2 48.3 ?1.4 11.3 ?0.6 v2t 13.7 ?0.7 35.2 ?2.7 46.9 ?3.2 12.3 ?1.5 30.82M TT-CE t2v 14.2 ?1.4 36.6 ?2.0 51.1 ?2.1 9.7 ?1.2 v2t 14.1 ?0.5 34.8 ?3.0 49.1 ?0.3 11.3 ?0.5 30.82M TT-CE+ t2v 14.4 ?0.5 37.7 ?1.7 50.9 ?1.6 9.8 ?1.0 v2t 14.3 ?0.6 36.3 ?0.9 48.3 ?1.2 11.3 ?0.5 30.82M QuerYD: Comparison to state of the art methods.</figDesc><table><row><cell>Model</cell><cell>Task</cell><cell>R@1 ?</cell><cell>R@5 ?</cell><cell cols="3">R@50 ? M dR ? Task</cell><cell>R@1 ?</cell><cell>R@5 ?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HSE[76]</cell><cell>t2v</cell><cell>20.5</cell><cell>49.3</cell><cell>?</cell><cell>?</cell><cell>v2t</cell><cell>18.7</cell><cell>48.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">MMT[20] t2v 22.7 SSB[49] t2v 26.8</cell><cell>58.1</cell><cell>93.5</cell><cell>3.0</cell><cell>v2t</cell><cell>25.5</cell><cell>57.3</cell><cell>93.5</cell><cell>3.0</cell><cell>-</cell></row><row><cell cols="3">TT-MMT t2v 25.0 Model Task R@1 ?</cell><cell>R@5 ?</cell><cell cols="3">R@10 ? M dR ? Task</cell><cell>R@1 ?</cell><cell>R@5 ?</cell><cell cols="2">R@10 ? M dR ?</cell><cell>Params</cell></row><row><cell>VSE[30]</cell><cell>t2v</cell><cell>28.0</cell><cell>64.3</cell><cell>76.9</cell><cell>3.0</cell><cell>v2t</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>Dual[18]</cell><cell>t2v</cell><cell>31.1</cell><cell>67.4</cell><cell>78.9</cell><cell>3.0</cell><cell>v2t</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell cols="2">VSE++[19] t2v</cell><cell>33.7</cell><cell>70.1</cell><cell>81.0</cell><cell>2.0</cell><cell>v2t</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>HGR[14]</cell><cell>t2v</cell><cell>35.1</cell><cell>73.5</cell><cell>83.5</cell><cell>2.0</cell><cell>v2t</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>SSB[49]</cell><cell>t2v</cell><cell>44.6</cell><cell>81.8</cell><cell>89.5</cell><cell>1.0</cell><cell>v2t</cell><cell>58.1</cell><cell>83.8</cell><cell>90.9</cell><cell>1.0</cell><cell>?</cell></row><row><cell cols="3">CE[36] t2v 47.9 Model Task R@1 ?</cell><cell>R@5 ?</cell><cell>R@10 ?</cell><cell>M dR ?</cell><cell>Task</cell><cell>R@1 ?</cell><cell>R@5 ?</cell><cell>R@10 ?</cell><cell>M dR ?</cell><cell>Params</cell></row><row><cell>MoEE[42]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">These embeddings are produced by models that have been trained on relevant tasks (such as action recognition for the video encoder and language modelling for the text encoder)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/gabeur/mmt</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://shorturl.at/ksxIS 5 https://github.com/gabeur/mmt</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://shorturl.at/cdrI6</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/lenscloth/RKD</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by EPSRC Programme Grants Seebibyte EP/M013774/1 and VisualAI EP/T028572/1, and a gift from Adobe. M.L. was supported by UEFISCDI, under project EEA-RO-2018-0496. The authors would like to thank Gyungin Shin and Iulia Duta for assistance. S.A. would like to acknowledge the support of Z. Novak and S. Carlson in enabling his contribution.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this supplementary material, we provide additional details on the video embeddings (Sec. A) and text embeddings (Sec. B) used in the main submission. We provide further details of <ref type="figure">Fig. 1</ref> from the main paper (Sec. C) as well as details on optimization (Sec. D), modifications to the embedding pre-processing pipeline used in prior work (Sec. E) and summaries of the datasets used (Sec. F). Finally, we include additional ablations (Sec. G) and a more comprehensive set of metrics for comparison with previous work, along with qualitative results (Sec. H).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5803" to="5812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Utilizing semantic word similarity measures for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">G?l Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00650</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting irregularities in images and in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Boiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="31" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly-supervised alignment of video with text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4462" to="4470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Born again trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Shang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Berkeley, Berkeley, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucilu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language features matter: Effective language representations for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuben</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan A</forename><surname>Plummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7474" to="7483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">VGGFace2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep visual-semantic quantization for efficient image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1328" to="1337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fine-grained video-text retrieval with hierarchical graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10638" to="10647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalable near identical image and shot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIVR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Word2visualvec: Image and video to sentence matching by visual feature prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06838</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dual dense encoding for zero-example video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multi-modal transformer for video retrieval. European Conference on Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Largescale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12046" to="12055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cnn architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jort</forename><forename type="middle">F</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plakal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Devin Platt, Rif A. Saurous, Bryan Seybold, Malcolm Slaney, Ron Weiss, and Kevin Wilson</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Like what you like: Knowledge distill via neuron selectivity transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01219</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards optimal bag-of-features for object categorization and semantic video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th ACM international conference on Image and video retrieval</title>
		<meeting>the 6th ACM international conference on Image and video retrieval</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="494" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Video understanding as machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07203</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Retrieving actions in movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hoi analysis: Integrating and decomposing human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5011" to="5022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13487</idno>
		<title level="m">Use what you have: Video retrieval using representations from collaborative experts</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Synthetically supervised feature learning for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Wassell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="435" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Unifying distillation and privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06430</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning a text-video embedding from incomplete and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02516</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning joint embedding with multimodal cues for cross-modal video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Niluthpol Chowdhury Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Queryd: a video dataset with high-quality textual and audio narrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea-Maria</forename><surname>Oncescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11071</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02824</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="120" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Real-time large scale near-duplicate web video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwok-Ping</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page">1470</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Multimodal video indexing: A review of the state-of-the-art. Multimedia tools and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Worring</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="5" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Ranking distillation: Learning compact ranking models with high performance for recommender system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2289" to="2298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ffmpeg Team</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ffmpeg</surname></persName>
		</author>
		<ptr target="https://www.ffmpeg.org/" />
		<imprint>
			<biblScope unit="page" from="2020" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Yfcc100m: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning using privileged information: similarity control and knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rauf</forename><surname>Izmailov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2023" to="2049" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A new learning paradigm: Learning using privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Vashist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="544" to="557" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4729</idno>
		<title level="m">Translating videos to natural language using deep recurrent neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Describing like humans: on diversity in image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingzhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4195" to="4203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Vatex: A large-scale, highquality multilingual dataset for video-and-language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4581" to="4591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Fine-grained action retrieval through multiple partsof-speech embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="450" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Jointly modeling deep video and compositional text to bridge vision and language in a unified framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="471" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03928</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Cross-modal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="374" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Lookahead optimizer: k steps forward, 1 step back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9597" to="9608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
