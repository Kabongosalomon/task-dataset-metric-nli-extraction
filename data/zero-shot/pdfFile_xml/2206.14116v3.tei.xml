<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SSL-Lanes: Self-Supervised Learning for Motion Forecasting in Autonomous Driving Baseline Pretext Task 1 (lane masking) Pretext Task 4 (success/failure classification) Pretext Task 3</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prarthana</forename><surname>Bhattacharyya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Huang</surname></persName>
							<email>c.huang@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Czarnecki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SSL-Lanes: Self-Supervised Learning for Motion Forecasting in Autonomous Driving Baseline Pretext Task 1 (lane masking) Pretext Task 4 (success/failure classification) Pretext Task 3</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>maneuver classification) Other agents Lane centerline Past trajectory GT trajectory Pred trajectory Pretext Task 2 (distance to intersection) Ego vehicle Figure 1. Motion forecasting on Argoverse [8] validation.</p><p>We show four challenging scenarios at intersections. The baseline [28] misses all the predictions. In the first row, our proposed lane masking successfully captures the right-turn. For the second row, predicting distance to intersection helps the most in capturing the left turn. In the third row, acceleration at an intersection is best captured by the model that is made to classify maneuvers of traffic agents. Finally, in the fourth row, classifying successful final goal states is the most effective at capturing the left turn. These tasks are trained with pseudo-labels which are obtained for free from data. Please refer to Sec. 7.2 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Self-supervised learning (SSL) is an emerging technique that has been successfully employed to train convolutional neural networks (CNNs) and graph neural networks (GNNs) for more transferable, generalizable, and robust representation learning. However its potential in motion forecasting for autonomous driving has rarely been explored. In this study, we report the first systematic exploration and assessment of incorporating self-supervision into motion forecasting. We first propose to investigate four novel self-supervised learning tasks for motion forecasting with theoretical rationale and quantitative and qualitative comparisons on the challenging large-scale Argoverse dataset. Secondly, we point out that our auxiliary SSL-based learning setup not only outperforms forecasting methods which use transformers, complicated fusion mechanisms and sophisticated online dense goal candidate opti-mization algorithms in terms of performance accuracy, but also has low inference time and architectural complexity. Lastly, we conduct several experiments to understand why SSL improves motion forecasting. Code is open-sourced at https://github.com/AutoVision-cloud/ SSL-Lanes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Motion forecasting in a real-world urban environment is an important task for autonomous robots. It involves predicting the future trajectories of traffic agents including vehicles and pedestrians. This is absolutely crucial in the self-driving domain for safe, comfortable and efficient operation. However, this is a very challenging problem. Difficulties include inherent stochasticity and multimodality of driving behaviors, and that future motion can involve complicated maneuvers such as yielding, nudging, lane-changing, turning and acceleration or deceleration.</p><p>The motion prediction task has traditionally been based on kinematic constraints and road map information with handcrafted rules. These approaches however fail to capture long-term behavior and interactions with map structure and other traffic agents in complex scenarios. Tremendous progress has been made with data-driven methods in motion forecasting <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b25">24,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b53">52]</ref>. Recent methods use a vector representation for HD maps and agent trajectories, including approaches like Lane-GCN <ref type="bibr" target="#b29">[28]</ref>, Lane-RCNN <ref type="bibr" target="#b50">[49]</ref>, Vector-Net <ref type="bibr" target="#b12">[12]</ref>, TNT <ref type="bibr" target="#b53">[52]</ref> and Dense-TNT <ref type="bibr" target="#b16">[16]</ref>. More recently, the enormous success of transformers <ref type="bibr" target="#b43">[42]</ref> has been leveraged for forecasting in mm-Transformer <ref type="bibr" target="#b33">[32]</ref>, Scene transformer <ref type="bibr" target="#b36">[35]</ref>, Multimodal transformer <ref type="bibr" target="#b21">[20]</ref> and Latent Variable Sequential Transformers <ref type="bibr" target="#b15">[15]</ref>. Most of these methods however are extremely complex in terms of architecture and have low inference speeds, which makes them unsuitable for real-world settings.</p><p>In this work, we extend ideas from self-supervised learning (SSL) to the motion forecasting task. Self-supervision has seen huge interest in both natural language processing and computer vision <ref type="bibr" target="#b4">[5]</ref> to make use of freely available data without the need for annotations. It aims to assist the model to learn more transferable and generalized representation from pseudo-labels via pretext tasks. Given the recent success of self-supervision with CNNs, transformers, and GNNs, we are naturally motivated to ask the question: Can self-supervised learning improve accuracy and generalizability of motion forecasting, without sacrificing inference speed or architectural simplicity?</p><p>Contributions: Our work, SSL-Lanes, presents the first systematic study on how to incorporate self-supervision in a standard data-driven motion forecasting model. Our contributions are:</p><p>? We demonstrate the effectiveness of incorporating self-supervised learning in motion forecasting.</p><p>Since this does not add extra parameters or compute during inference, SSL-Lanes achieves the best accuracy-simplicity-efficiency trade-off on the challenging large-scale Argoverse <ref type="bibr" target="#b8">[8]</ref> benchmark. ? We propose four self-supervised tasks based on the nature of the motion forecasting problem. The key idea is to leverage easily accessible map/agent-level information to define domain-specific pretext tasks that encourage the standard model to capture more superior and generalizable representations for forecasting, in comparison to pure supervised learning.</p><p>? We further design experiments to explore why forecasting benefits from SSL. We provide extensive results to hypothesize that SSL-Lanes learns richer features from the SSL training as compared to a model trained with vanilla supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Motion Forecasting: Traditional methods for motion forecasting primarily use Kalman filtering <ref type="bibr" target="#b24">[23]</ref> with a prior from HD-maps to predict future motion states <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b46">45]</ref>. With the huge success of deep learning, recent works use datadriven approaches for motion forecasting. These methods explore different architectures involving rasterized images and CNNs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b40">39]</ref>, vectorized representations and GNNs <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b25">24,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b50">49]</ref>, point-cloud representations <ref type="bibr" target="#b47">[46]</ref>, transformers <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b36">35]</ref> and sophisticated fusion mechanisms <ref type="bibr" target="#b29">[28]</ref>, to generate features that predict final output trajectories. While the focus of these works is to find more effective ways of feature extraction from HD-maps and interacting agents, they need huge model capacity, heavy parameterization, and extensive augmentations or large amounts of data to converge to a general solution. Other works <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b41">40,</ref><ref type="bibr" target="#b51">50,</ref><ref type="bibr" target="#b53">52]</ref> build on them to incorporate prior knowledge in the form of predefined candidate trajectories from sampling or clustering strategies from training data. However the disadvantage of these methods is that their performance is highly related to the quality of the trajectory proposals, which becomes an extra dependency. End-to-end solutions for optimizing end-points of these candidates trajectories are proposed by Dense-TNT <ref type="bibr" target="#b16">[16]</ref> and HOME <ref type="bibr" target="#b14">[14]</ref>. Dense-TNT has state-ofthe-art accuracy with a reasonable parameter budget, but its online dense goal candidate optimization strategy is computationally very expensive, which is unrealistic for realtime operations like autonomous driving. Lately, ensembling techniques like MultiPath++ <ref type="bibr" target="#b42">[41]</ref> and DCMS <ref type="bibr" target="#b48">[47]</ref> have been proposed. While they have high forecasting performance, a major disadvantage is their high memory cost for training and heavy computational cost at inference.</p><p>Self-supervised Learning: SSL is a rapidly emerging learning framework that generates additional supervised signals to train deep learning models through carefully designed pretext tasks. In the image domain, various selfsupervised learning techniques have been developed for learning high-level image representations, including predicting the relative locations of image patches <ref type="bibr" target="#b10">[10]</ref>, jigsaw puzzle <ref type="bibr" target="#b37">[36]</ref>, image rotation <ref type="bibr" target="#b13">[13]</ref>, image clustering <ref type="bibr" target="#b3">[4]</ref>, image inpainting <ref type="bibr" target="#b39">[38]</ref>, image colorization <ref type="bibr" target="#b52">[51]</ref> and segmentation prediction <ref type="bibr" target="#b38">[37]</ref>. In the domain of graphs and graph neural networks, pretext tasks include graph partitioning, node clustering, context prediction and graph completion <ref type="bibr" target="#b20">[19,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b49">48]</ref>. To the best of our knowledge, this is the first principled approach that explores motion forecasting for autonomous driving with self-supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Formulation</head><p>We are given the past motion of N actors. The ith actor is denoted as a set of its center locations over the past L time-steps. We pre-process it to represent GT Predicted </p><formula xml:id="formula_0">P i = {?p ?L+1 i , ..., ?p ?1 i , ?p 0 i }, where p l i is the 2D displace- ment from time step l ? 1 to l.</formula><p>We are also given a highdefinition (HD) map, which contains lanes and semantic attributes. Each lane is composed of several consecutive lane nodes, with a total of M nodes. X ? R M ?F denotes the lane node feature matrix, where x j = X[j, :] T is the Fdimensional lane node vector. Following the connections between lane centerlines (i.e., predecessor, successor, left neighbour and right neighbour), we represent the connectivity among the lane nodes with four adjacency matrices {A f } f ?{pre,suc,left,right} , with A f ? R M ?M . This implies that if A f,gh = 1, then node h is an f -type neighbor of node g. Our goal is to forecast the future motions of all actors in the scene O 1:</p><formula xml:id="formula_1">T GT = {(x 1 i , y 1 i ), ..., (x T i , y T i )|i = 1, ..., N }, where T is our prediction horizon.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Background</head><p>In this section, we first briefly introduce a standard datadriven motion forecasting framework.</p><p>Feature Encoding: We first encode the agent and map inputs similar to Lane-GCN <ref type="bibr" target="#b29">[28]</ref>. The agent encoder includes a 1D convolution with a feature pyramid network, parameterized by g enc , as given by Eq. (1). For mapencoding, we adopt two Lane-Conv residual blocks, parameterized by ? = {W 0 , W left , W right , W pre,k , W suc,k }, where k ? {1, 2, 4, 8, 16, 32}, as given by Eq. <ref type="bibr" target="#b1">(2)</ref>.</p><formula xml:id="formula_2">p i = g enc (P i ) (1) Y = XW0 + j?{left,right} AjXWj + k A k pre XWpre,k + A k suc XWsuc,k<label>(2)</label></formula><p>Modeling Interactions: Since the behavior of agents depends on map topology and social consistency, each encoded agent i subsequently aggregates context from the surrounding map features and its neighboring agent features, via spatial attention <ref type="bibr" target="#b44">[43]</ref> as given by Eq. Here, y j is the feature of the j-th node,p i is the feature of the i-th agent, ? the composition of layer normalization and ReLU, and</p><formula xml:id="formula_3">? ij = MLP(v j ? v i ),</formula><p>where v denotes the (x, y) 2-D BEV location of the agent or the lane node. The parameters for map and agent feature aggregation is</p><formula xml:id="formula_4">represented by ? = {W M2A , W 1 , W 2 , W A2A , W 3 , W 4 }.</formula><p>Trajectory Prediction: Finally, we decode the future trajectories from the features? i corresponding to the agents of interest as given by: O 1:T pred = {g dec (? i )|i = 1, ..., N }, where g dec is the parameterized trajectory decoder. The parameters for the motion forecasting model are learned by minimizing the supervised loss (L sup ) calculated between the predicted output and the ground-truth future trajectories (O 1:T GT ), as given by Eq. (4):</p><p>g enc , ? , ? , g dec = arg min genc,?,?,g dec </p><formula xml:id="formula_5">Lsup(O 1:T pred , O 1:T GT )<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SSL-Lanes</head><p>The goal of our proposed SSL-Lanes framework is to improve the performance of the primary motion forecasting baseline by learning simultaneously with various selfsupervised tasks. <ref type="figure">Fig. 2</ref> shows the pipeline of our proposed approach, and Tab. 1 summarizes the self-supervised tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Self-Supervision meets Motion Forecasting</head><p>Before we discuss designing pretext tasks to generate self-supervisory signals, we consider a scheme that will allow combined training for self-supervised pretext tasks and our standard framework.</p><p>How to combine motion forecasting and SSL? Selfsupervision can be combined with motion forecasting in various ways. In one scheme we could pre-train the forecasting encoder with pretext tasks (which can be viewed as an initialization for the encoder's parameters) and then fine-tune the pre-trained encoder with a downstream decoder as given by Eq. (4). In another scheme, we could choose to freeze the encoder and only train the decoder. In a third scheme, we could optimize our pretext task and primary task jointly, as a kind of multi-task learning setup. Inspired by relevant discussions in GNNs, we choose the third-scheme, i.e., multi-task learning, which is the most general framework among the three and is also experimentally verified to be the most effective <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b49">48]</ref>.</p><p>Joint Training: Considering our motion forecasting task and a self-supervised task, the output and the training process can be formulated as:</p><formula xml:id="formula_6">? , ? , ? ss = arg min ?,?,?ss ? 1 L sup (?, ?) + ? 2 L ss (?, ? ss ) (5)</formula><p>where, L ss (?, ?) is the loss function of the self-supervised task, ? ss is the corresponding linear transformation parameter, and ? 1 , ? 2 ? R &gt;0 are the weights for the supervised and self-supervised losses. If the pretext task only focuses on the map encoder, then ? = {?} and ? = {g enc , ?, g dec }. Otherwise, ? = {g enc , ?, ?} and ? = {g dec }. Henceforth, we also define the following representations. We will represent the primary task encoder as function f ? , parameterized by ?. Furthermore, given a pretext task, which we will design in the next section, the pretext decoder p ?ss is a function that predicts pseudolabels and is parameterized by ? ss .</p><p>Benefit of SSL-Lanes: In Eq. (5), the self-supervised task as a regularization term throughout network training. It acts as the regularizer learned from unlabeled data under the minor guidance of human prior (design of pretext task). Therefore, a properly designed task would introduce data-driven prior knowledge that improves model generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Pretext tasks for Motion Forecasting</head><p>At the core of our SSL-Lanes approach is defining pretext tasks based upon self-supervised information from the underlying map structure and the overall temporal prediction problem itself. Our proposed prediction-specific selfsupervised tasks are summarized in Tab. 1, and assign different pseudo-labels from unannotated data to solve Eq. (5). Our core approach is simple in contrast to state-of-the-art that rely on complex encoding architectures <ref type="bibr" target="#b21">[20,</ref><ref type="bibr" target="#b25">24,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b53">52]</ref>, ensembling forecasting heads <ref type="bibr" target="#b42">[41,</ref><ref type="bibr" target="#b48">47]</ref>, involved final goal-set optimization algorithms <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b41">40]</ref> or heavy fusion mechanisms <ref type="bibr" target="#b29">[28]</ref>, to improve prediction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Lane-Masking</head><p>Motivation: The goal of the Lane-Masking pretext task is to encourage the map encoder ? = {?} to learn local structure information in addition to the forecasting task that is being optimized. In this task, we learn by recovering feature information from the perturbed lane graphs. Vector-Net <ref type="bibr" target="#b12">[12]</ref> is the only other motion forecasting work that proposes to randomly mask out the input node features belonging to either scene context or agent trajectories, and ask the model to reconstruct the masked features. Their intuition is to encourage the graph networks to better capture the interactions between agent dynamics and scene context. However, our motivation differs from VectorNet in two respects: (a) We propose to use masking to learn local map-structure better, as opposed to learning interactions between map and the agent. This is an easier optimization task, and we outperform VectorNet. (b) A lane is made up of several nodes. We propose to randomly mask out a certain percentage of each lane. This is a much stronger prior as compared to randomly masking out any node and ensures that the model pays attention to all parts of the map.</p><p>Formulation: Formally, we randomly mask (i.e., set equal to zero) the features of m a percent of nodes per lane and then ask the self-supervised decoder to reconstruct </p><formula xml:id="formula_7">1 ma ma i=1 Lmse p ?ss ([f ? (X, A f )]v i ), Xi<label>(6)</label></formula><p>Here,X is the node feature matrix corrupted with random masking, i.e., some rows of X corresponding to nodes v i are set to zero. p ?ss is a fully connected network that maps the representations to the reconstructed features. L mse is the mean squared error (MSE) loss function penalizing the distance between the reconstructed map features</p><formula xml:id="formula_8">p ?ss ([f ? (X, A f )] vi ) for node v i and its GT features X i .</formula><p>Benefit of Lane-Masking: Since Argoverse <ref type="bibr" target="#b8">[8]</ref> has imbalanced data with respect to maneuvers, there are cases when right/left turns, lane-changes, acceleration/deceleration are missed by the baseline even with multi-modal predictions. We hypothesize that stronger map-features can help the multi-modal prediction header to infer that some of the predictions should also be aligned with map topology. For example, even if an agent is likely to go straight at an intersection, some of the possible futures should also cover acceleration/deceleration or right/left turns guided by the local map structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Distance to Intersection</head><p>Motivation: The Lane-Masking pretext task is from a local structure perspective based on masking and trying to predict local attributes of the vectorized HD-map. We further develop the Distance-to-Intersection pretext task to guide the map-encoder, ? = {?}, to maintain global topology information by predicting the distance (in terms of shortest path length) from all lane nodes to intersection nodes. Datasets like Argoverse <ref type="bibr" target="#b8">[8]</ref> provide lane attributes which describe whether a lane node is located within an intersection. This will force the representations to learn a global positioning vector of each of the lane nodes.</p><p>Formulation: We aim to regress the distances from each lane node to pre-labeled intersection nodes annotated as part of the dataset. Given K labeled intersection nodes V intersection = {v intersection,k |k = 1, ...K}, we first generate reliable pseudo labels using breadth-first search (BFS). Specifically, BFS calculates the shortest distance d i ? R for every lane node v i from the given set V intersection . The target of this task is to predict the pseudo-labeled distances using a pretext decoder.</p><formula xml:id="formula_9">If p ?ss ([f ? (X, A f )] vi )</formula><p>is the prediction of node v i , and L mse is the mean-squared error loss function for regression, then the loss formulation for this SSL pretext task is as follows:</p><formula xml:id="formula_10">? , ? ss = arg min ?,?ss 1 M M i=1 L mse p ?ss ([f ? (X, A f )] vi ), d i (7)</formula><p>Benefit of Distance to Intersection Task: We hypothesize that since change of speed, acceleration, primary direction of movement etc. for an agent can change far more dramatically as an agent approaches or moves away from an intersection, it is beneficial to explicitly incentivize the model to pick up the geometric structure near an intersection and compress the space of possible map-feature encoders, thereby effectively simplifying inference. We also expect this to improve drivable area compliance nearby an intersection, which is often a problem for current motion forecasting models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Maneuver Classification</head><p>Motivation: The Lane-Masking and Distance to Intersection pretext tasks are both based on extracting feature and topology information from a HD-map. However, pretext tasks can also be constructed from the overall forecasting task itself. Thus we propose to obtain free pseudo-labels in the form of a 'maneuver' the agent-of-interest intends to execute, and define a set of 'intentions' to represent common semantic modes (e.g. change lane, speed up, slow down, turn-right, turn-left etc.) We call this pretext task Maneuver Classification, and we expect it to provide prior regularization to ? = {g enc , ?, ?}, based on driving modes.</p><p>Formulation: We aim to construct pseudo label to divide agents into different clusters according to their driving behavior and explore unsupervised clustering algorithms to acquire the maneuver for each agent. We find that using naive k-Means (on agent end-points) or DBSCAN (on Hausdorff distance between entire trajectories <ref type="bibr" target="#b0">[1]</ref>) leads to noisy clustering. We find that constrained k-means <ref type="bibr" target="#b45">[44]</ref> on agent end-points works best to divide trajectory samples into C clusters equally. We define C = {maintain-speed, accelerate, decelerate, turn-left, turn-right,  lane-change} and the clustering function as ?.</p><p>If</p><formula xml:id="formula_11">p ?ss (f ? (P i , X, A f )) is the prediction of agent i's intention and E i = (x T i,GT , y T i,GT )</formula><p>is its ground-truth endpoint, then the learning objective is to classify each agent maneuver into its corresponding cluster using cross-entropy loss L ce as:</p><formula xml:id="formula_12">? , ? ss = arg min ?,?ss L ce p ?ss (f ? (P i , X, A f )), ?(E i ) (8)</formula><p>Benefit of Maneuver Classification Task: We hypothesize if one can identify the intention of a driver, the future motion of the vehicle will match that maneuver, thereby reducing the set of possible end-points for the agent. We also expect that agents with similar maneuvers will tend to have consistent semantic representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Forecasting Success/Failure Classification</head><p>Motivation: In contrast to maneuver classification, which provides coarse-grained prediction of the future, selfsupervision mechanisms can also offer a strong learning signal through goal-reaching tasks which are generated from the agent's trajectories. We propose a pretext task called Success/Failure Classification, which trains an agent specialized at achieving end-point goals which directly lead to the forecasting-task solution. We expect this to constrain ? = {g enc , ?, ?} to predict trajectories distance away from the correct final end-point. Conceptually, the more examples of successful goal states we collect, the better understanding of the target goal of the forecasting task we have.</p><p>Formulation: Similar to maneuver classification, we wish to create pseudo-labels for our data samples. We la-bel trajectory predictions as successful</p><formula xml:id="formula_13">(c = 1) if the final prediction (x T i,pred , y T i,pred ) is within &lt; 2m of the final end- point E i ,</formula><p>and as failure (c = 0) otherwise. We choose 2m as our threshold because it is also used for miss-rate calculation (Sec. 6). In this case, c ? C = {0, 1} is the pseudolabel which belongs to label set C. If the pretext decoder predicts agent i's final-endpoint as p ?ss (f ? (P i , X, A f )), and given ground-truth end-point E i its success or failure label is c i , then the pretext loss can be formulated as:</p><formula xml:id="formula_14">? , ? ss = arg min ?,?ss L ce p ?ss (f ? (P i , X, A f )), c i<label>(9)</label></formula><p>Benefit of Success/Failure Classification Task: We hypothesize that this task will especially provide stronger gains for cases where the final end-point is not aligned with the general direction of agent movement for majority of samples given in the dataset, and is thus not well captured by average displacement based supervised loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Learning</head><p>As all the modules are differentiable, we can train the model in an end-to-end way. We use the sum of classification, regression and self-supervised losses to train the model. Specifically, we use:</p><formula xml:id="formula_15">L = L cls + L reg + L terminal + L ss<label>(10)</label></formula><p>For classification and regression loss design, we adopt the formulation proposed in <ref type="bibr" target="#b29">[28]</ref>. L2 loss that minimizes the distance between predicted final-endpoints and the ground-truth. This is because L reg is averaged across all time-points 1 : T , and from a practical end user perspective, minimizing the endpoint loss is much more important than weighting loss from all time-steps equally. Our proposed pretext tasks contributes to L ss . During evaluation, we study each pretext task separately, and their corresponding loss formulations defined in Eq. <ref type="formula" target="#formula_7">(6)</ref>, Eq. <ref type="formula">(7)</ref>, Eq. <ref type="formula">(8)</ref>, Eq. (9) are used for joint training.</p><formula xml:id="formula_16">L terminal = 1 N N i=1 L2 (x T i,pred , y T i,pred ), (x T i,GT , y T i,GT ) is a simple</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>Dataset: Argoverse provides a large-scale dataset <ref type="bibr" target="#b8">[8]</ref> for the purpose of training, validating and testing models, where the task is to forecast 3 seconds of future motions, given 2 seconds of past observations. This dataset has more than 30K real-world driving sequences collected in Miami (MIA) and Pittsburgh (PIT). Those sequences are further split into train, validation, and test sets, without any geographical overlap. Each of them has 205,942, 39,472, and 78,143 sequences respectively. In particular, each sequence contains the positions of all actors in a scene within the past 2 seconds history, annotated at 10Hz. It also specifies one actor of interest in the scene, with type 'agent', whose future 3 seconds of motion are used for the evaluation. The train and validation splits additionally provide future locations of all actors within 3 second horizon labeled at 10Hz, while annotations for test sequences are withheld from the public and used for the leaderboard evaluation. HD map information is available for all sequences.</p><p>We have two main requirements for the dataset: (a) Scale of Data: Modern motion forecasting methods and selfsupervised learning systems require a large amount of training data to imitate human maneuvers in complex real-world scenarios. Thus, the dataset should be large-scale and diverse, such that it has a wide range of behaviors and trajec-tory shapes across different geometries represented in the data. (b) Interesting Scenarios for Forecasting Evaluation: The dataset should be collected for interesting behaviours by biasing sampling towards complex observed behaviours (e.g., lane changes, turns) and road features (e.g., intersections), since we wish to focus on these cases. We find that on the basis of these requirements, as well as its popularity in the the motion forecasting community, Argoverse <ref type="bibr" target="#b8">[8]</ref> is the best candidate to showcase our method. Please refer to the supplementary for more details regarding why we choose to focus on it in comparison to other motion forecasting benchmarks.</p><p>Metrics: ADE is defined as the average displacement error between ground-truth trajectories and predicted trajectories over all time steps. FDE is defined as displacement error between ground-truth trajectories and predicted trajectories at the final time step. We compute K likely trajectories for each scenario with the ground truth label, where K = 1 and K = 6 are used. Therefore, minADE and minFDE are minimum ADE and FDE over the top K predictions, respectively. Miss rate (MR) is defined as the percentage of the best-predicted trajectories whose FDE is within a threshold (2 m). Brier-minFDE is the minFDE plus (1?p) 2 , where p is the corresponding trajectory probability.</p><p>Experimental Details: To normalize the data, we translate and rotate the coordinate system of each sequence so that the origin is at current position t = 0 of 'agent' actor and x-axis is aligned with its current direction, i.e., orientation from the agent location at t = ?1 to the agent location at t = 0 is the positive x axis. We use all actors and lanes whose distance from the agent is smaller than 100 meters as the input. We train the model on 4 TITAN-X GPUs using a batch size of 128 with the Adam <ref type="bibr" target="#b26">[25]</ref> optimizer with an initial learning rate of 1 ? 10 ?3 , which is decayed to 1 ? 10 ?4 at 100,000 steps. The training process finishes at 128,000 steps and takes about 10 hours to complete. For our final test-set submission, we use success/failure classification as the pretext task, and initialize the map-encoder with the parameters from a model trained with the lane-masking pretext task. To avoid overfitting to the general directions that agents move, we augment the data from each scene for the test-set submission. We rotate all trajectories in a scene around the scene's origin by ?, where ? varies from 0?to 360?in 30?intervals. We provide more implementation details in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Ablation Studies</head><p>Effectiveness of Pretext tasks: We first examine the effect of incorporating our proposed pretext tasks (Sec. 5) with the standard data-driven motion forecasting baseline (Sec. 4). While evaluating the importance of our proposed pretext tasks, we wish to underline that motion prediction for autonomous driving is a safety-critical task, especially at intersections where most of our data is collected, and most accidents also happen. We thus posit that in this situation, even a small error in predicting final locations (FDE) for a given agent can lead to dangerous potential collisions.</p><p>Results in Tab. 2 show that all proposed pretext tasks improve motion forecasting performance for Argoverse. Specifically, the lane-masking pretext task improves min-FDE by 8.9% and MR@2m by 20.3%. distance to intersection improves min-FDE by 7.1% and 19.3%. Maneuver classification improves min-FDE by 6.3% and MR@2m by 15.4%. We expect that improving the quality of clustering for maneuvers and thus creating better pseudo-labels will improve this further. Finally, success/failure classification improves min-FDE by 9.8% and MR@2m by 22.4%. Moreover, since pretext tasks are not used for inference and only for training, they also do not add any extra parameters or FLOPs to the baseline, thereby increasing accuracy but at no cost to computational efficiency or architectural complexity. We present qualitative results with the different pretext tasks on several hard cases in <ref type="figure">Fig. 1</ref>.</p><p>Similarity in feature space: We analyze the CKA similarity <ref type="bibr" target="#b27">[26]</ref> between the representations learnt by: a model trained with pretext task 'D2I' (refers to distance to intersection task) and baseline; two models trained with different pretext tasks. In <ref type="figure" target="#fig_2">Fig. 4</ref>, Base(M2A) refers top i , Base(A2A) refers to? i (see Eq. (3)), 'Mask' refers to lane-masking, 'success/fail' refers to success or failure classification task and 'intention' suggests maneuver classification.</p><p>Our main questions are: (a) how much does the pretext task feature differ from the baseline? (b) do the features from different pretext tasks collapse to the same feature? First we note that representation learned by D2I does not collapse to the same representation learned by Mask or Suc- cess/Fail or Intention. Secondly we note that D2I features are quite different from Base-M2A featuresp i and Base-A2A features? i , which suggests that task-specific regularization has indeed resulted in different parameters.</p><formula xml:id="formula_17">&amp;.$6LPLODULW\ %DVHOLQH0$ %DVHOLQH$$ /DQH0DVNLQJ 6XFF)DLO&amp;OV 0DQHXYHU&amp;OV )HDWXUH3DLUV ',</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Comparison with State-of-the-Art</head><p>Performance: We compare our approach with top entries on Argoverse motion forecasting leaderboard <ref type="bibr" target="#b8">[8]</ref> in Tab. 3. SSL-Lanes improves the metrics for K = 1 convincingly and outperforms existing approaches w.r.t. min-ADE 1 , min-FDE 1 and MR 1 . We are also strongly competitive w.r.t. min-ADE 6 , min-FDE 6 and MR 6 against top approaches, with a relatively simple architecture.</p><p>Trade-off between min-FDE and Miss-Rate: min-FDE 6 and MR 6 are both important for autonomous robots to optimize. Ideally we wish for both of these metrics to be low. However, there exists a frequent trade-off between them. We compare this trade-off in <ref type="figure" target="#fig_1">Fig. 3(a)</ref> with six other popular motion forecasting models (in terms of citations and GitHub stars), namely: Lane-GCN <ref type="bibr" target="#b29">[28]</ref>, Lane-RCNN <ref type="bibr" target="#b29">[28]</ref>, MultiPath <ref type="bibr" target="#b7">[7]</ref>, mm-Transformer <ref type="bibr" target="#b33">[32]</ref>, TNT <ref type="bibr" target="#b53">[52]</ref> and Dense-TNT <ref type="bibr" target="#b16">[16]</ref> on the Argoverse validation set. We are on the lowest-left of meaning we optimize both min-FDE 6 and MR 6 successfully in comparison to other top models.</p><p>Trade-off between accuracy, efficiency and complexity: We are the first to point out a trade-off that exists for current state-of-the-art motion forecasting models between forecasting performance, architectural complexity and inference speed. This is illustrated in <ref type="figure" target="#fig_1">Fig. 3(b)</ref>-(c). NN+Map <ref type="bibr" target="#b8">[8]</ref>  <ref type="table">(see Tab.</ref> 3) is a simple nearest-neighbor based approach that also uses map-features, and while it has advantages in terms of fast inference and low model complexity, the forecasting performance is very low. MultiPath <ref type="bibr" target="#b7">[7]</ref> is a very popular approach that has reasonable accuracy and inference speed but is parametrically heavy due to it's use of convolutional kernels. Lane-GCN is a vector based approach <ref type="bibr" target="#b29">[28]</ref>   <ref type="table">Table 4</ref>. Different experimental settings to provide evidence for why SSL-based training helps motion forecasting to problems with over-smoothing for map-encoders <ref type="bibr" target="#b28">[27]</ref> and also has a complicated four-stage fusion mechanism. Lane-RCNN <ref type="bibr" target="#b50">[49]</ref> proposes to capture interactions between agents and map using not just a single vector, but a local interaction graph per agent -this adds huge number of hyper-parameters to the model and makes it very complex. Transformer-based models <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b36">35]</ref> also suffer in this regard. Scene-transformer for example has 15M parameters and uses heavy augmentation to prevent overfitting. A light high-performing model is Dense-TNT <ref type="bibr" target="#b16">[16]</ref>. However, Dense-TNT's inference speed on average is 50ms per agent, because it proposes a time-intensive optimization algorithm to find a dense goal set that minimizes the expected error of the given set. In contrast to these popular models, our approach has high accuracy (min-FDE: 1.25m, MR: 13.3%) while also having low architectural complexity (1.84M parameters) and high inference speed (3.30 ms).</p><p>Thus it provides a great balance for application to real-time safety-critical autonomous robots.</p><p>Qualitative Results: We present some multi-modal prediction trajectories on several hard cases shown in <ref type="figure">Fig. 1</ref>. The yellow trajectory represents the observed 2s. Red represents ground truth for the next 3s and green represents the multiple forecasted trajectories for those 3s. In Row 1, the agent turns right at the intersection. The baseline misses this mode completely, despite having access to the map. The model trained with lane-masking successfully predicts this right turn within 2m of the ground-truth end-point. In Row 2, the agent has a noisy past history and accelerates while turning left at the intersection. The pretext task distanceto-intersection can correctly capture this, while the baseline has only one trajectory covering this mode but vastly overshoots the ground-truth. Interestingly, we note that the success/failure pretext task is unable to capture this mode. We believe this is due to a stronger prior imposed by the model during learning. In Row 3, we have an agent accelerating while going straight at an intersection. We find that the maneuver classification pretext task is the only model that correctly predicts trajectories aligned with the groundtruth. In Row 4, we have an agent turning left at an intersection. Most of the predictions of other models predicts that the agent will go straight. The success/failure pretext task however picks up on the left-turn, possibly due to the priors imposed upon it by end-point conditioning. Overall, SSL-Lanes can capture left and right turns better, while also being able to discern acceleration at intersections. Our pretext tasks provide priors for the model and provides data-regularization for free. We believe this can improve forecasting through better understanding of map topology, agent context with respect to the map, and generalization with respect to imbalance implicitly present in data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">When does SSL help Motion Forecasting?</head><p>Hypotheses: We hypothesize that training with SSL pretext tasks probably helps motion forecasting as following: (a) Topology-based context prediction assumes feature similarity or smoothness in small neighborhoods of maps. Such a context-based feature representation can greatly improve prediction performance, especially when the neighborhoods are small. (b) Clustering and classification assumes that feature similarity implies target-label similarity and can group distant nodes with similar features together, leading to better generalization. (c) Supervised learning with imbalanced datasets sees significant degradation in performance. Although most of the data samples in Argoverse are at an inter-section, a significantly large number involve driving straight while maintaining speed. Recent studies <ref type="bibr" target="#b31">[30]</ref> have shown that SSL tends to learn richer features from more frequent classes which also allows it to generalize to to other classes better. Experiments: In order to provide evidence for our hypotheses, we propose to design 6 different training and testing setups as shown in Tab. 4. We use success/failure classification as the pretext task, and all models are trained for 50,000 steps. We initialize the map-encoder with the parameters from a model trained with the lane-masking pretext task. Our first setting is to train with 25% of the total data available for training and testing on the full validation set. We expect the SSL-based task to capture richer features and generalize better than the baseline. Our second setting assumes that SSL also generalizes to topology from different cities and trains on 100% of data from Pittsburgh (PIT) but only 20% of data from Miami (MIA). For evaluation, we only test on data examples taken from the city of MIA. For our third setting, we assume that SSL learns superior features and can thus perform better in difficult cases like lane-changes and turning cases. For evaluation, we only test on data examples which involves these difficult cases. In our fourth setting, we choose to explicitly train with data that contains 2? 'straight-with-same-speed' maneuver and 1? all other maneuvers. We test only on lane-changes and turning cases from validation. Finally in order to test the effect of noise on motion forecasting performance, we take two models already trained on full data. We now take the full validation set, randomly select agent trajectories or map nodes with probability p = 0.25 and p = 0.5, and then add Gaussian noise with zero mean and 0.2 variance to their features. We expect this to have the most impact on forecasting performance as compared to all other settings since this is the most aggressive form of corruption. But we also expect SSL-based pretext task training to provide robustness to noise for free due to better generalization capabilities. Our takeaway from these experiments is that there is strong evidence SSL-based tasks do provide better generalization capabilities and can thus prove to be more effective than pure supervised training based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We propose SSL-Lanes to leverage supervisory signals generated from data for free in the form of pseudo-labels and integrate it with a standard motion forecasting model. We design four pretext tasks that can take advantage of map-structure and similarities between agent dynamics to generate these pseudo-labels, namely: lane masking, distance to intersection prediction, maneuver classification and success/failure classification. We validate our proposed approach by achieving competitive results on the challenging large-scale Argoverse benchmark. The main advantage of SSL-Lanes is that it has high accuracy combined with low architectural complexity and high inference speed. We further demonstrate that each proposed SSL pretext task improves upon the baseline, especially in difficult cases like left/right turns and acceleration/deceleration. We also provide hypotheses and experiments on why SSL-Lanes can improve motion forecasting.</p><p>Limitations: A limitation of our framework is that it uses the different losses for our formulation only in a 1:1 ratio without tuning them. We also use only one pretext task at a time and do not explore the combination of these different tasks. For our future work, we plan to incorporate meta-learning <ref type="bibr" target="#b22">[21]</ref> to identify an effective combination of pretext tasks and automatically balance them-we expect that this will lead to more gains in terms of forecasting performance. Another limitation is that we report improvements with SSL-pretext tasks in scenarios without specifically considering multiple heavily interacting agents. In the future we would like to explore how the interactions between road agents can influence our SSL losses on the interaction split of the Waymo Open Motion dataset (WOMD) <ref type="bibr" target="#b11">[11]</ref>. Finally, we explore generalization in terms of implicit data imbalance only in comparison to pure supervised training on the same dataset from which training samples are derived. We would like to study the generalization of our work to other datasets without re-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Detailed Network Architecture for Baseline</head><p>We provide the detailed network architecture of our baseline in <ref type="figure">Fig. 5</ref>. For the agent feature extractor, the architecture is similar to <ref type="bibr" target="#b29">[28]</ref>. We use an 1D CNN to process the trajectory input. The output is a temporal feature map, whose element at t = 0 is used as the agent feature. The network has three groups/scales of 1D convolutions. Each group consists of two residual blocks <ref type="bibr" target="#b18">[17]</ref>, with the stride of the first block as 2. Feature Pyramid Network (FPN) <ref type="bibr" target="#b30">[29]</ref> fuses the multiscale features, and applies another residual block to obtain the output tensor. For all layers, the convolution kernel size is 3 and the number of output channels is 128. Layer normalization <ref type="bibr" target="#b1">[2]</ref> and Rectified Linear Unit (ReLU) are used after each convolution. The map feature extractor has two LaneConv residual <ref type="bibr" target="#b18">[17]</ref> blocks which are the stack of a LaneConv <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b33">32)</ref> and a linear layer, as well as a shortcut. All layers have 128 feature channels. Layer normalization <ref type="bibr" target="#b1">[2]</ref> and ReLU are used after each LaneConv and linear layer. For the map-aware agent feature (M2A) module, the distance threshold is 12m. It is 100m for the agent-to-agent (A2A) interaction module. The two interaction modules have two residual blocks, which consist of a stack of an attention layer and a linear layer, as well as a residual connection. All layers have 128 output feature channels. Taking the interaction-aware actor features as input, our trajectory decoder is a multi-modal prediction header that outputs the final motion forecasting. For each agent, it predicts K possible future trajectories and confidence scores. The header has two branches, a regression branch to predict the trajectory of each mode and a classification branch to predict the confidence score of each mode. Key differences with Lane-GCN <ref type="bibr" target="#b29">[28]</ref>: Our main difference is we use two Lane-Conv blocks instead of four as map-feature extractor in order to prevent over-smoothing in GNNs <ref type="bibr" target="#b28">[27]</ref>. We also do not use the four-way fusion proposed by Lane-GCN and do away with the agent to map (A2M) and the map to map (M2M) interaction blocks, which saves compute and memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Implementation of Pretext Tasks</head><p>In this section, we discuss various design decisions for the proposed pretext tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Lane-Masking</head><p>For this pretext task, we mask m a percent of every lane and reconstruct its features. In Tab. 5, we study the influence of masking ratio on the final forecasting performance. Random masking refers to masking out m a percent random  <ref type="table">Table 5</ref>. Effect of masking ratio (ma) on forecasting performance for lane-masking task map nodes and lane-masking refers to masking out m a percent of lanes in the map. We finally choose m a = 0.4 as the most effective parameter for the lane-masking pretext task, which outperforms random masking. The model infers missing lane-nodes to produce plausible outputs during reconstruction. We hypothesize that this reasoning is linked to learning useful representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Distance to Intersection</head><p>For this pretext task, we explore two different options for framing the problem of predicting the distance to the nearest intersection node in Tab. 6. We first explore predicting this distance as a classification task. We group the lengths into four categories: d ij = 1, d ij = 2, d ij = 3, d ij = 4 and d ij &gt;= 5. We however find that this is harder to optimize than the regression loss proposed in Eq. <ref type="formula">(7)</ref>, which we finally choose as our loss for the distance to intersection pretext task.  <ref type="table">Table 6</ref>. Effect of pretext loss type on forecasting performance for distance to intersection task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Maneuver Classification</head><p>For this pretext task, we first divide the lateral and longitudinal maneuvers by choosing a threshold angle of 20?f rom the vertical. We next find that constrained k-means <ref type="bibr" target="#b45">[44]</ref> on agent end-points for lateral and longitudinal maneuvers works best to separate the trajectory samples into different clusters. This is illustrated in <ref type="figure">Fig. 6</ref>. For differentiating the longitudinal maneuvers from the lane-change maneuver, we check a combination of the distance from the lane centerlines for start and stop positions and the orientations of the nearest centerline for start and stop positions.  the points in the scene can be considered as failure examples. We consider first setting = 3m, i.e. a wider area for success examples, and then reducing it to = 2m linearly over the total number of training steps. We find that this can actually harm the final forecasting performance. We thus follow <ref type="bibr" target="#b54">[53]</ref> to use focal loss to train our auxiliary classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Success/Failure Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Qualitative Results</head><p>We first present some multi-modal prediction trajectories on several hard cases shown in <ref type="figure">Fig. 1</ref>  priors for the model and provides data-driven regularization for free. This can improve forecasting because of better understanding of map topology, agent context with respect to the map, and also improve generalization for maneuver im-balance implicitly present in data. We next provide more visual results of our proposed SSL-Lanes on the Argoverse validation set in <ref type="figure" target="#fig_5">Fig. 7</ref>. Generally, these qualitative results demonstrate the effectiveness of our proposed pretext tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion: SSL-Lanes vs. State-of-the-Art</head><p>We use this section to distinguish our work from methods that we believe have similar intuition but very different construction, in order to highlight its novelty and value.</p><p>? SSL-Lanes vs. VectorNet <ref type="bibr" target="#b12">[12]</ref>: Vector-Net is the only other motion forecasting work that proposes to randomly mask out the input node features belonging to either scene context or agent trajectories, and ask the model to reconstruct the masked features. Their intuition is to encourage the graph networks to better capture the interactions between agent dynamics and scene context. However, our motivation differs from VectorNet in two respects: (a) We propose to use masking to learn local map-structure better, as opposed to learning interactions between map and the agent. This is an easier optimization task, and we out-perform VectorNet. (b) A lane is made up of several nodes. We propose to randomly mask out a certain percentage of each lane. This is a much stronger prior as compared to randomly masking out any node (which may correspond to either a moving agent or map) and ensures that the model pays attention to all parts of the map.</p><p>? SSL-Lanes vs. CS-LSTM <ref type="bibr" target="#b9">[9]</ref>: CS-LSTM appends the encoder context vector with a one-hot vector corresponding to the lateral maneuver class and a onehot vector corresponding to the longitudinal maneuver class. Subsequently, the added maneuver context allows the decoder LSTM to generate maneuver specific probability distributions. This construction however is quite different from our work because it is not auxiliary in nature -it always outputs and appends a maneuver to the decoder, even during inference. This we believe is too strong of a bias for the prediction model, especially given the fact that the maneuvers are generated using very simple velocity profiles and not from careful mining of the data. In our conditioning, the maneuvers are mined from data and the final motion prediction does not depend directly on them. We believe this design is much more flexible since it allows to generate more supervisory signals in the form of maneuvers during training, but at the same time does not require an explicit maneuver to condition the final future forecast trajectory output during inference.</p><p>? SSL-Lanes vs. MultiPath <ref type="bibr" target="#b7">[7]</ref>: MultiPath is also not auxiliary in nature: it factorizes motion uncertainty into intent uncertainty and control uncertainty; models the uncertainty over a discrete set of intents with a softmax distribution; and then outputs control uncertainty as a Gaussian distribution dependent on each waypoint state of the anchor trajectory (corresponding to the intent). While this construction is highly intuitive and effective by design, it is very different from our SSL-based construction. Ours is an auxiliary task which provides supervision during training, and effectively functions as a regularizer, while being general enough to be used with any other data-driven motion forecasting model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion: Choice of Dataset</head><p>We now compare the commonly used motionforecasting datasets, i.e., nuScenes [?], Waymo-Open-Motion-Dataset (WOMD) <ref type="bibr" target="#b11">[11]</ref> and Argoverse <ref type="bibr" target="#b8">[8]</ref>. We individually discuss why Argoverse is best positioned to bring out the benefits of our proposed work.</p><p>? Scale of Data: We first compare the dataset size and diversity. We note that Argoverse is not only larger and more diverse than nuScenes, but also has greater number of training samples and unique trajectories compared to WOMD. ? Interesting Scenarios for Forecasting Evaluation: We next compare if the datasets specifically mines for interesting scenarios, which is the area we want to improve the current baseline. nuScenes was not collected to capture a wide diversity of complex and interesting driving scenarios. WOMD on the other hand specifically mines for pairwise interaction scenarios, where the main objective is to improve forecasting for interacting agents. However, the scope of our study is to primarily focus on motion at intersections undergoing lane-changes and turns. We expect the SSL-losses to improve understanding of the context/environment, trajectory embeddings and address data-imbalance w.r.t. maneuvers. We leave heavy interaction-based use cases for future work. Finally, Argoverse mines for interesting motion patterns at intersections, which involve lane-changes, acceleration/deceleration, and turns. We thus find this dataset best suited to showcase our proposed method.</p><p>? Community focus on Argoverse: We also find that many popular motion forecasting methods published by the robotics community have also included evaluations only on the Argoverse dataset including: Lane-GCN, Lane-RCNN, PRIME, DCMS, TPCN, mm-Transformer, HiVT, Multi-modal Transformer, DSP etc. This makes it easier for us to position our work with respect to these approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion: Potential of this Work</head><p>We expect this work to influence real world deployment of SSL forecasting methods for autonomous driving. Another use case for this work is realistic behavior generation in traffic simulation. The general construction of the prediction problem, inspired by <ref type="bibr" target="#b29">[28]</ref>, enables a generic understanding of how an object moves in a given environment without memorizing the training data. A neural network may learn to associate particular areas of a scene with certain motion patterns. To prevent this, we centre around the agent of interest and normalize all other trajectory and map coordinates with respect to it. We predict relative motion as opposed to absolute motion for the future trajectory. This helps to learn general motion patterns. Reconstructing the map or predicting distances from map elements are conducted in a frame-of-reference relative to the agent of interest. This helps in learning general map connectivity. Following work in pedestrian trajectory prediction, we also additionally add random rotations to the training trajectories to reduce directional bias. Furthermore, we provide strong evidence that SSL-based tasks provide better generalization compared to pure supervised training, thereby having the ability to effectively reuse the same prediction model across different scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(pi, ?i,j, yj)W1)W2 pi =piWA2A + j ?(concat(pi, ?i,j,pj)W3)W4(3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2XUVFigure 3 .</head><label>3</label><figDesc>Left: min-FDE6 -Miss-Rate6 trade-off on Argoverse Validation. Lower-left is better. We optimize both successfully in comparison to other popular approaches. Right: We plot min-FDE on Argoverse Test Set against number of model parameters (in millions) and inference time (in milli-seconds). We find that there is a trade-off between min-FDE performance, architectural complexity (as measured by number of parameters) and computational efficiency (as measured by inference time). Our work achieves the best trade-off (lower-left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>CKA Feature similarity between feature pairs of baseline and different pretext tasks. Similarity score is 1 for completely overlapping features and 0 for completely divergent features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>For</head><label></label><figDesc>this pretext task, the primary bottleneck is the fact that the number of positive examples if far fewer than the number of negative examples. This is because there are only</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Architecture of the baseline model Modes of driving from unsupervised clustering of data a few success examples in a 2m area near the end-point of a single recorded ground-truth trajectory, while the rest of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results for our proposed SSL-Lanes pretext tasks on the Argoverse<ref type="bibr" target="#b8">[8]</ref> validation set. We outperform the baseline on several difficult cases at intersections and lane-changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Motion forecasting performance on Argoverse validation with our proposed pretext tasks these features.</figDesc><table><row><cell cols="7">Method minADE1 minFDE1 MR1 minADE6 minFDE6 MR6</cell></row><row><cell>Baseline</cell><cell>1.42</cell><cell>3.18</cell><cell>51.35</cell><cell>0.73</cell><cell>1.12</cell><cell>11.07</cell></row><row><cell>Lane-Masking</cell><cell>1.36</cell><cell>2.96</cell><cell>49.45</cell><cell>0.70</cell><cell>1.02</cell><cell>8.82</cell></row><row><cell>Distance to Intersection</cell><cell>1.38</cell><cell>3.02</cell><cell>49.53</cell><cell>0.71</cell><cell>1.04</cell><cell>8.93</cell></row><row><cell>Maneuver Classification</cell><cell>1.33</cell><cell>2.90</cell><cell>49.26</cell><cell>0.72</cell><cell>1.05</cell><cell>9.36</cell></row><row><cell>Success/Failure Classification</cell><cell>1.35</cell><cell>2.93</cell><cell>48.54</cell><cell>0.70</cell><cell>1.01</cell><cell>8.59</cell></row></table><note>? , ? ss = arg min?,?ss</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>MethodminADE 1 minFDE 1 MR 1 minADE 6 minFDE 6 MR 6 b-FDE 6</figDesc><table><row><cell>NN + Map [8]</cell><cell>3.65</cell><cell>8.12</cell><cell>94.0</cell><cell>2.08</cell><cell>4.02</cell><cell>58.0</cell><cell>-</cell></row><row><cell>Jean [33]</cell><cell>1.74</cell><cell>4.24</cell><cell>68.56</cell><cell>0.98</cell><cell>1.42</cell><cell>13.08</cell><cell>2.12</cell></row><row><cell>Lane-GCN [28]</cell><cell>1.71</cell><cell>3.78</cell><cell>58.77</cell><cell>0.87</cell><cell>1.36</cell><cell>16.20</cell><cell>2.05</cell></row><row><cell>LaneRCNN [49]</cell><cell>1.68</cell><cell>3.69</cell><cell>56.85</cell><cell>0.90</cell><cell>1.45</cell><cell>12.32</cell><cell>2.15</cell></row><row><cell>TNT [52]</cell><cell>1.77</cell><cell>3.91</cell><cell>59.70</cell><cell>0.94</cell><cell>1.54</cell><cell>13.30</cell><cell>2.14</cell></row><row><cell>DenseTNT [16]</cell><cell>1.68</cell><cell>3.63</cell><cell>58.43</cell><cell>0.88</cell><cell>1.28</cell><cell>12.58</cell><cell>1.97</cell></row><row><cell>PRIME [40]</cell><cell>1.91</cell><cell>3.82</cell><cell>58.67</cell><cell>1.22</cell><cell>1.55</cell><cell>11.50</cell><cell>2.09</cell></row><row><cell>WIMP [24]</cell><cell>1.82</cell><cell>4.03</cell><cell>62.88</cell><cell>0.90</cell><cell>1.42</cell><cell>16.69</cell><cell>2.11</cell></row><row><cell>TPCN [46]</cell><cell>1.66</cell><cell>3.69</cell><cell>58.80</cell><cell>0.87</cell><cell>1.38</cell><cell>15.80</cell><cell>1.92</cell></row><row><cell>HOME [14]</cell><cell>1.70</cell><cell>3.68</cell><cell>57.23</cell><cell>0.89</cell><cell>1.29</cell><cell>8.46</cell><cell>1.86</cell></row><row><cell>mmTransformer [32]</cell><cell>1.77</cell><cell>4.00</cell><cell>61.78</cell><cell>0.87</cell><cell>1.34</cell><cell>15.40</cell><cell>2.03</cell></row><row><cell>MultiModalTransformer [20]</cell><cell>1.74</cell><cell>3.90</cell><cell>60.23</cell><cell>0.84</cell><cell>1.29</cell><cell>14.29</cell><cell>1.94</cell></row><row><cell>LatentVariableTransformer [15]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.89</cell><cell>1.41</cell><cell>16.00</cell><cell>-</cell></row><row><cell>SceneTransformer [35]</cell><cell>1.81</cell><cell>4.06</cell><cell>59.21</cell><cell>0.80</cell><cell>1.23</cell><cell>12.55</cell><cell>1.88</cell></row><row><cell>SSL-Lanes (Ours)</cell><cell>1.63</cell><cell>3.56</cell><cell>56.71</cell><cell>0.84</cell><cell>1.25</cell><cell>13.26</cell><cell>1.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparison of our (best) proposed model and top approaches on the Argoverse Test. The best results are in bold and underlined, and the second best is also underlined.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>has comparatively fast inference time and high accuracy, but uses multiple GNN layers which can lead</figDesc><table><row><cell>Description</cell><cell cols="2">Experimental Setup Training Validation</cell><cell cols="4">Method minADE 6 minFDE 6 MR 6</cell></row><row><cell>Effects of limited training data</cell><cell>25% of train</cell><cell>All</cell><cell>Baseline Ours</cell><cell>0.82 0.78</cell><cell>1.33 1.22</cell><cell>14.66 12.63</cell></row><row><cell>Effects of new domain</cell><cell>100% PIT + 20% MIA</cell><cell>MIA val</cell><cell>Baseline Ours</cell><cell>0.88 0.85</cell><cell>1.46 1.34</cell><cell>17.21 14.96</cell></row><row><cell>Performance on difficult maneuvers</cell><cell>All</cell><cell>Turning &amp; lane changing</cell><cell>Baseline Ours</cell><cell>0.90 0.84</cell><cell>1.53 1.34</cell><cell>19.90 14.93</cell></row><row><cell>Effects of</cell><cell>2x straight</cell><cell>Turning &amp;</cell><cell>Baseline</cell><cell>0.94</cell><cell>1.65</cell><cell>21.53</cell></row><row><cell>imbalanced data</cell><cell>1x other maneuvers</cell><cell>lane changing</cell><cell>Ours</cell><cell>0.90</cell><cell>1.49</cell><cell>17.97</cell></row><row><cell>Effects of noisy data</cell><cell>All</cell><cell>Gaussian noise (? = 0.2) with p = 0.25</cell><cell>Baseline Ours</cell><cell>1.01 0.96</cell><cell>1.37 1.24</cell><cell>15.59 11.98</cell></row><row><cell>Effects of noisy data</cell><cell>All</cell><cell>Gaussian noise (? = 0.2) with p = 0.5</cell><cell>Baseline Ours</cell><cell>1.19 1.13</cell><cell>1.56 1.40</cell><cell>20.64 15.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Method m a minADE 6 minFDE 6 MR 6</figDesc><table><row><cell>Baseline</cell><cell>-</cell><cell>0.73</cell><cell>1.12</cell><cell>11.07</cell></row><row><cell cols="2">Random Masking 0.4</cell><cell>0.71</cell><cell>1.03</cell><cell>9.11</cell></row><row><cell cols="2">Lane-Masking 0.3</cell><cell>0.71</cell><cell>1.04</cell><cell>9.02</cell></row><row><cell cols="2">Lane-Masking 0.4</cell><cell>0.70</cell><cell>1.02</cell><cell>8.84</cell></row><row><cell cols="2">Lane-Masking 0.5</cell><cell>0.71</cell><cell>1.05</cell><cell>9.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Method Pretext Loss minADE 6 minFDE 6 MR 6</figDesc><table><row><cell>Baseline</cell><cell>-</cell><cell>0.73</cell><cell>1.12</cell><cell>11.07</cell></row><row><cell cols="2">Distance to Intersection Classification</cell><cell>0.72</cell><cell>1.06</cell><cell>9.64</cell></row><row><cell>Distance to Intersection</cell><cell>Regression</cell><cell>0.71</cell><cell>1.04</cell><cell>8.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>. SSL-Lanes can capture left and right turns better, while also being able to discern acceleration at intersections. Our pretext tasks provide</figDesc><table><row><cell>Pretext Task 1 (lane masking)</cell><cell>Pretext Task 2 (distance to intersection)</cell><cell>Pretext Task 3 (maneuver classification)</cell><cell>Pretext Task 4 (success/failure classification)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ego vehicle</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Other agents</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Lane centerline</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Past trajectory</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GT trajectory</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Pred trajectory</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was funded by the Mitacs Accelerate Program and Gatik Inc. This article solely reflects the opinions and conclusions of its authors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-supervised simultaneous multi-step prediction of road dynamics and cost map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elmira</forename><surname>Amirloo Abolfathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Rohani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ershad</forename><surname>Banijamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
		<idno>2021. 5</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8494" to="8503" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Layer normalization. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><forename type="middle">S</forename><surname>Ogale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems XV</title>
		<editor>Antonio Bicchi, Hadas Kress-Gazit, and Seth Hutchinson</editor>
		<meeting><address><addrLine>Freiburg im Breisgau, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>University of Freiburg</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11218</biblScope>
			<biblScope unit="page" from="139" to="156" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIV</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision, ICCV 2021</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9630" to="9640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Intentnet: Learning to predict intention from raw sensor data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">2nd Annual Conference on Robot Learning</title>
		<meeting><address><addrLine>Z?rich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31" />
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="947" to="956" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">3rd Annual Conference on Robot Learning</title>
		<editor>Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura</editor>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-10-30" />
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Fang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patsorn</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagjeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-modal trajectory prediction of surrounding vehicles with maneuver based lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nachiket</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium</title>
		<meeting><address><addrLine>IV; Changshu, Suzhou, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06-26" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large scale interactive motion forecasting for autonomous driving : The waymo open motion dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabeek</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoey</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mccauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vectornet: Encoding HD maps and agent dynamics from vectorized representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congcong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE, 2020. 2, 4</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">HOME: heatmap output for future motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Sabatini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Tsishkou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Stanciulescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Moutarde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th IEEE International Intelligent Transportation Systems Conference, ITSC 2021</title>
		<meeting><address><addrLine>Indianapolis, IN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Latent variable sequential set transformers for joint multi-agent motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Girgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Golemo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Aldon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D&amp;apos;</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Conference on Learning Representations, ICLR 2022, Conference Track Proceedings</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Densetnt: End-toend trajectory prediction from dense goal sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision, ICCV 2021</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vehicle trajectory prediction based on motion model and maneuver recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Houenou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Bonnifait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V?ronique</forename><surname>Cherfaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="4363" to="4369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multi-modal motion prediction with transformer-based neural network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lv</surname></persName>
		</author>
		<idno>abs/2109.06446</idno>
		<imprint>
			<date type="published" when="2004" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-supervised auxiliary learning with meta-paths for heterogeneous graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dasol</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunyoung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Min</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Self-supervised learning on graphs: Deep insights and new direction. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Others</forename><surname>Kalman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of basic Engineering</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">What-if motion prediction for autonomous driving. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhesh</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagjeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Similarity of neural network representations revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="3519" to="3529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<editor>Sheila A. McIlraith and Kilian Q. Weinberger</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA, February 2-7</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning lane graph representations for motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Self-supervised learning is more robust to dataset imbalance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">Z</forename><surname>Haochen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno>abs/2110.05025</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Graph self-supervised learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/2103.00111</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multimodal motion prediction with stacked transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangji</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinhong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE, 2021. 2, 4, 6</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-head attention for multi-modal joint vehicle motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Mercat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><forename type="middle">El</forename><surname>Zoghby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Sandou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beauvois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo Pita</forename><surname>Gil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-08-31" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Social-stgcnn: A social spatiotemporal graph convolutional neural network for human trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abduallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><forename type="middle">G</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Claudel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="14412" to="14420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Scene transformer: A unified multi-task model for behavior prediction and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Tien Lewis</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Venugopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>CoRR, abs/2106.08417, 2021. 2, 4</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9910</biblScope>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6024" to="6033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Covernet: Multimodal behavior prediction using trajectory sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung</forename><surname>Phan-Minh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><forename type="middle">Corina</forename><surname>Grigore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freddy</forename><forename type="middle">A</forename><surname>Boulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Wolff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="14062" to="14071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to predict vehicle trajectories with model-based planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenchao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<editor>Aleksandra Faust, David Hsu, and Gerhard Neumann</editor>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-11-11" />
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>PMLR, 2021. 2, 4</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Multipath++: Efficient information fusion and trajectory aggregation for behavior prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Hefny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avikalp</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><forename type="middle">S</forename><surname>Refaat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigamaa</forename><surname>Nayakanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Cornman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Pang</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<idno>abs/2111.14973</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Constrained k-means clustering with background knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiri</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schr?dl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<editor>Carla E. Brodley and Andrea Pohoreckyj Danyluk</editor>
		<meeting>the Eighteenth International Conference on Machine Learning<address><addrLine>Williams College, Williamstown, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001-06-28" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Vehicle trajectory prediction by integrating physics-and maneuver-based approaches using interactive multiple models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guotao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="5999" to="6008" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">TPCN: temporal point cloud networks for motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongyi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">DCMS: motion forecasting with dual consistency and multi-pseudo-target supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiamiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunnong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongyi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/2204.05859</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">When does self-supervision help graph convolutional networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Distributed representations for graph-centric motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><forename type="middle">Urtasun</forename><surname>Lanercnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2021</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">End-to-end interpretable neural motion planner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Sadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8660" to="8669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9907</biblScope>
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">TNT: target-driven trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congcong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th Conference on Robot Learning</title>
		<editor>Jens Kober, Fabio Ramos, and Claire J. Tomlin</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-11" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="895" to="904" />
		</imprint>
	</monogr>
	<note>Virtual Event. PMLR, 2020. 2, 4, 6, 8</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Objects as points. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resblock</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
	<note>stride 1</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resblock</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resblock</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
	<note>stride 1</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resblock</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resblock</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
	<note>stride 1</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
