<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GLAMpoints: Greedily Learned Accurate Match points</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
							<email>prune.truong@vision.ee.ethz.chstefanos</email>
							<affiliation key="aff0">
								<orgName type="institution">RetinAI Medical AG</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Apostolopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RetinAI Medical AG</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Mosinska</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RetinAI Medical AG</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Stucky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RetinAI Medical AG</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Ciller</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RetinAI Medical AG</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>De Zanet</surname></persName>
							<email>sandro@retinai.com</email>
							<affiliation key="aff0">
								<orgName type="institution">RetinAI Medical AG</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GLAMpoints: Greedily Learned Accurate Match points</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a novel CNN-based feature point detector -Greedily Learned Accurate Match Points (GLAMpoints) -learned in a semi-supervised manner. Our detector extracts repeatable, stable interest points with a dense coverage, specifically designed to maximize the correct matching in a specific domain, which is in contrast to conventional techniques that optimize indirect metrics. In this paper, we apply our method on challenging retinal slitlamp images, for which classical detectors yield unsatisfactory results due to low image quality and insufficient amount of low-level features. We show that GLAMpoints significantly outperforms classical detectors as well as state-of-the-art CNN-based methods in matching and registration quality for retinal images. Our method can also be extended to other domains, such as natural images. Training code and model weights are available at https://github.com/ PruneTruong/GLAMpoints_pytorch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Digital fundus images of the human retina are widely used to diagnose variety of eye diseases, such as Diabetic Retinopathy (DR), glaucoma, and Age-related Macular Degeneration (AMD) <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b53">54]</ref>. For retinal images acquired during the same session and presenting small overlaps, image registration can be used to create mosaics depicting larger areas of the retina. Through image mosaicking, ophthalmologists can display the retina in one large picture, which is helpful during diagnosis and treatment planning. Besides, mosaicking of retinal images taken at different time points has been shown to be important for monitoring the progression or identification of eye diseases. More importantly, fundus image registration has been explored in eye laser treatment for DR. It allows real-time tracking of * Work produced during an internship at RetinAI Medical AG a) SIFT b) GLAMpoints <ref type="figure">Figure 1</ref>: Keypoints detected by SIFT and GLAMpoints and resulting matches for a pair of pre-processed (top) and raw (bottom) slitlamp images. Detected points are in white, green matches are true positive, red represents false positive. Our GLAMpoints detector produces more reliable keypoints even without additional pre-processing. the vessels during surgical operations to ensure accurate application of the laser on the retina and minimal damage to the healthy tissues.</p><p>Mosaicking usually relies on extracting repeatable interest points from the images, matching the correspondences and searching for transformations relating them. As a result, the keypoint detection is the first and the most crucial stage of this pipeline, as it conditions all further steps and therefore the success of the registration.</p><p>At the same time, classical feature detectors are generalpurpose and manually optimized for outdoor, in-focus, lownoise images with sharp edges and corners. They usually fail to work with medical images, which can be distorted, noisy, have no guarantee of focus and depict soft tissue with no sharp edges (see <ref type="figure" target="#fig_1">Figure 3</ref>). Traditional methods perform sub-optimally on such images, making more sophisticated optimization necessary at a later step in the registration, such as Random Sampling Consensus (RanSaC) <ref type="bibr" target="#b21">[23]</ref>, bundle adjustment <ref type="bibr" target="#b43">[44]</ref> and Simultaneous Localization and Mapping (SLAM) <ref type="bibr" target="#b16">[18]</ref> techniques. Besides, supervised learning methods for keypoint detection fail or are not applicable, due to missing ground truths for feature points.</p><p>In this paper we present a method for learning feature points in a semi-supervised manner. Learned feature detectors were shown to outperform the heuristics-based methods, but they are usually optimized for repeatability, which is a proxy for the matching quality and as a result they may underperform during the final matching. On the contrary, our keypoints -GLAMpoints -are trained for the final matching accuracy and when associated with Scale-Invariant Feature Transform (SIFT) <ref type="bibr" target="#b0">[2]</ref> descriptor they outperform state-of-the-art in matching performance and registration quality on retinal images. As shown in <ref type="figure">Figure 1</ref>, GLAMpoints produces significantly more correct matches than SIFT detector.</p><p>Registration based on feature points is inherently nondifferentiable due to point matching and transformation estimations. We take inspiration from the loss formulation in Reinforcement Learning (RL) using a reward to compute the suitability of the detected keypoints based on the final registration quality. It makes it possible to use the key performance measure, i.e. matching power, to directly train a Convolutional Neural Network (CNN). Our contribution is therefore a formulation for keypoint detection that is directly optimized for the final matching performance in an image domain. Both training code and model weights are available at <ref type="bibr" target="#b45">[46]</ref> for the Tensorflow version and <ref type="bibr" target="#b44">[45]</ref> for the PyTorch variant.</p><p>The remainder of this paper is organized as follows: we introduce the current state-of-the-art feature detection methods in section 2, our training procedure and loss in section 3, followed by experimental comparison of previous methods in section 4 and conclusion in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Existing registration algorithms can be classified as areabased and feature-based approaches. The former typically rely on a similarity metric such as cross-correlation <ref type="bibr" target="#b14">[16]</ref>, mutual information <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b29">31]</ref> or phase correlation <ref type="bibr" target="#b27">[29]</ref> to compare the intensity patterns of an image pair and estimate the transformation. However, in the case of changes in illumination or small overlapping areas, the application of area-based approaches becomes challenging or infeasible. Conversely, feature-based methods extract corresponding points on pairs of images along with a set of features and search for a transformation that minimizes the distance between the detected key points. Compared with area-based registration techniques, they are more robust to changes of intensity, scale and rotation and therefore, they are considered more appropriate for problems such as medical image registration.</p><p>Typically, feature extraction and matching of two images comprise four steps: detection of interest points, computing feature descriptor for each of them, matching of corresponding keypoints and estimation of a transformation between the images using the matches. As can be seen, the detection step influences every further step and is therefore crucial for a successful registration. It requires a high image coverage and stable key points in low contrasts images.</p><p>In the literature, local interest point detectors have been thoroughly studied. SIFT <ref type="bibr" target="#b32">[34]</ref> is probably the most well known detector/descriptor in computer vision. It computes corners and blobs on different scales to achieve scale invariance and extracts descriptors using the local gradients. Speeded-Up Robust Features (SURF) <ref type="bibr" target="#b10">[12]</ref> is a faster alternative, using Haar filters and integral images, while KAZE <ref type="bibr" target="#b3">[5]</ref> exploits non-linear scale space for more accurate keypoint detection.</p><p>In the field of fundus imaging, a widely used technique relies on vascular trees and branch point analysis <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b23">25]</ref>. However, accurate segmentation of the vascular trees is challenging and registration often fails on images with few vessels. Alternative registration techniques are based on matching repeatable local features; Chen et al. <ref type="bibr" target="#b13">[15]</ref> detected Harris corners <ref type="bibr" target="#b24">[26]</ref> on low quality multi-modal retinal images and assigned them a partial intensity invariant feature (Harris-PIIFD) descriptor. They achieved good results on low quality images with an overlapping area greater than 30%, but the method is characterised by low repeatability. Wang et al. <ref type="bibr" target="#b48">[49]</ref> used SURF features to increase the repeatability and introduced a new method for point matching to reject a large number of outliers, but the success rate drops significantly when the overlapping area diminishes below 50%. Cattin et al. <ref type="bibr" target="#b11">[13]</ref> also demonstrated that SURF can be efficiently used to create mosaics of retina images even for cases with no discernible vascularisation. However this technique only appeared successful in the case of highly self-similar images. D-saddle detector/descriptor <ref type="bibr" target="#b37">[39]</ref> was shown to outperform the previous methods in terms of rate of successful registration on the Fundus Image Registration (FIRE) Dataset <ref type="bibr" target="#b25">[27]</ref>, enabling the detection of interest points on low quality regions.</p><p>Recently, with the advent of deep learning, learned detectors based on CNN architectures were shown to outperform state-of-the-art computer vision detectors <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b8">10]</ref>. Learned Invariant Feature Transform (LIFT) <ref type="bibr" target="#b51">[52]</ref> uses patches to train a fully differentiable deep CNN for interest point detection, orientation estimation and descriptor computation based on supervision from classical Structure from Motion (SfM) systems. SuperPoint <ref type="bibr" target="#b18">[20]</ref> introduced a self-supervised framework for training interest point detectors and descriptors. It rises to state-of-the-art homography estimation results on HPatches <ref type="bibr" target="#b9">[11]</ref> when compared to SIFT, LIFT and Oriented Fast and Rotated Brief (ORB) <ref type="bibr" target="#b39">[41]</ref>. The training procedure is, however, complicated and their self-supervision implies that the network can only find corner points. Altwaijry et al. <ref type="bibr" target="#b5">[7]</ref> proposed a twostep CNN for matching aerial image patches, which is a particularly challenging task due to ultra-wide baseline. Altwaijry et al. <ref type="bibr" target="#b6">[8]</ref> also introduced a method to detect keypoint locations on different scales, utilizing high activations in recursive network feature maps. KCNN <ref type="bibr" target="#b19">[21]</ref> was shown to emulate hand-crafted detectors by training small networks using keypoints detected by other methods as ground-truth. Local Feature Network (LF-NET) <ref type="bibr" target="#b35">[37]</ref> is the closest to our method: a keypoint detector and descriptor is trained endto-end in a two branch set-up, one being differentiable and feeding on the output of the other non-differentiable branch. However, they optimized their detector for repeatability between image pairs, not taking into account the matching performance.</p><p>Truong et al. <ref type="bibr" target="#b46">[47]</ref> presented an evaluation of SURF, KAZE, ORB, Binary Robust Invariant Scalable Keypoints (BRISK) <ref type="bibr" target="#b30">[32]</ref>, Fast Retina Keypoint (FREAK) <ref type="bibr" target="#b2">[4]</ref>, LIFT, SuperPoint and LF-NET both in terms of image matching and registration quality on retinal fundus images. They found that while SuperPoint outperforms all the others relative to the matching performance, LIFT demonstrates the highest results in terms of registration quality, closely followed by KAZE and SIFT. The highlighted issue was that even the best-performing detectors produce feature points which are densely positioned and as a result may be associated with a similar descriptor. This can lead to false matches and thus inaccurate or failed registrations.</p><p>Our goal is to tackle this problem by introducing a novel semi-supervised learned method for keypoint detection. Detectors are often optimized for repeatability (such as LF-NET <ref type="bibr" target="#b35">[37]</ref>) and not for the quality of the associated matches between image pairs. Our training procedure uses a reward concept akin to RL to extract repeatable, stable interest points with a uniform coverage and it is specifically designed to maximize correct matching on a specific domain, as shown for challenging retinal slit lamp images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>Our trained network predicts the location of stable interest points, called GLAMpoints, on a full-sized gray-scale image. In this section, we explain how our training set was produced and our training procedure. As we used standard convolutional network architecture, we only briefly discuss it in the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>We trained our model on a dataset from the ophthalmic field, namely slit lamp fundus videos, used in laser treatment (examples in <ref type="figure" target="#fig_1">Figure 3</ref>). In this application, live registration is required for an accurate ablation of the retinal tissue. Our training dataset consists of 1336 images with different resolutions, ranging from 300 px to 700 px by 150 px to 400 px. These images were acquired with multiple cameras and devices to cover large variability of appearances. They come from eye examination of 10 different patients, who were healthy or with diabetic retinopathy.</p><p>From the original fundus images, image pairs are synthetically created and used for training. Let B be a particular base image from the training dataset, of size H ? W . At every step i, an image pair I i , I i is generated from image B by applying two separate, randomly sampled homography transforms g i , g i . Images I i and I i are thus related according to the homography H Ii,I i = g i * g ?1 i (see supplementary material). On top of the geometric transformations, standard data augmentation methods are used: gaussian noise, changes of contrast, illumination, gamma, motion blur and the inverse of image. A subset of these appearance transformations is randomly chosen for each image of the pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>We define our learned function f ? (I) ? ? S, where S denotes the pixel-wise feature point probability map of size H ?W . Lacking a direct ground truth of keypoint locations, a delayed reward can be computed instead. We base this reward on the matching success, computed after registration. The training proceeds as follows:</p><p>1. Given a pair of images I ? R H?W and I ? R H?W related with the ground truth homography H = H I,I , our model provides a score map for each image, S = f ? (I) and S = f ? (I ).</p><p>2. The locations of interest points are extracted on both score maps using standard non-differentiable Non-Max-Supression (NMS), with a window size w.  Let T denote the set of true positive key points. If a given detected feature point ends up in the set of true positive points, it gets a positive reward. All other points/pixels are given a reward of 0. Consequently, the reward matrix R ? R H?W for a keypoint (x, y) can be defined as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A 128 root-SIFT</head><formula xml:id="formula_0">R x,y = 1, for (x, y) ? T 0, otherwise<label>(1)</label></formula><p>This leads to the following loss function:</p><formula xml:id="formula_1">L simple (?, I) = (f ? (I) ? R) 2<label>(2)</label></formula><p>However, a major drawback of this formulation is the large class imbalance between positively rewarded points and null-rewarded ones, where latter prevails by far, especially in the first stages of training. Given a reward R with mostly zero values, the f ? converges to a zero output. Hard mining has been shown to boost training of descriptors <ref type="bibr" target="#b42">[43]</ref>. Thus, negative hard mining on the false positive matches might also enhance performance in our method, but has not been investigated in this work. Instead, to counteract the imbalance, we use sample mining: we select all n true positive points and randomly sample additional n from the set of false positives. We only back-propagate through the 2n true positive feature points and mined false positive key points. If there are more true positives than false positives, gradients are backpropagated through all found matches. This mining is mathematically formulated as a binary pixel-wise mask M , equal to 1 at the locations of the true positive key points and that of the subset of mined feature points, and equal to 0 otherwise. The final loss is thus formulated as follows:</p><formula xml:id="formula_2">L(?, I) = (f ? (I) ? R) 2 ? M M<label>(3)</label></formula><p>where ? denotes the element-wise multiplication.</p><p>An overview of the training steps is given in <ref type="figure" target="#fig_0">Figure 2</ref>. Importantly, only step 1 is differentiable with respect to the loss. We learn directly on a reward which is the result of non differentiable actions, without supervision.</p><p>It should be noted that the descriptor we used is the root-SIFT version without rotation invariance. The reason is that it performs better on slitlamp images than root-SIFT detector/descriptor with rotation invariance (see supplementary material for details). The aim of this paper is to investigate the detector only and therefore we used rotation-dependent root-SIFT for consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network</head><p>A standard 4-level deep Unet <ref type="bibr" target="#b38">[40]</ref> with a final sigmoid activation was used to learn f ? . It comprises of 3x3 convolution blocks with batch normalization and Rectified Linear Unit (ReLU) activations (see <ref type="figure" target="#fig_0">Figure 2</ref>,c). Since the task of keypoint detection is similar to pixel-wise binary segmentation (class interest point or not), Unet was a promising choice due to its past successes in binary and semantic segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In this section, we describe the testing dataset and the evaluation protocol. We then compare state-of-the-art detectors, quantitatively and qualitatively to our proposed GLAMpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Testing datasets</head><p>In this study we used the following test datasets:</p><p>1. The slit lamp dataset: from retinal videos of 3 patients (different from the ones used for training), a random set of 206 frame pairs was selected as testing samples, with size 338 px to 660 px by 190 px to 350 px. Examples are shown in <ref type="figure" target="#fig_1">Figure 3</ref>. The pairs were selected to have an overlap ranging from 20 to 100%. They are related by affine transformations and rotations up to 15 degrees. Using a dedicated software tool, all pairs of images were manually annotated following common procedures <ref type="bibr" target="#b12">[14]</ref> with at least 5 corresponding points, which were then used to estimate the ground truth homographies relating the pairs. As the slit lamp images depict small area of retina, it is justified to apply the planar assumption in generating homographies <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b22">24]</ref>.</p><p>2. The FIRE dataset <ref type="bibr" target="#b25">[27]</ref>: a publicly available retinal image registration dataset with ground truth annotations. It consists of 129 retinal images forming 134 image pairs. The original images of 2912x2912 pixels weredown scaled to 15% of their original size, to match the resolution of the training set. Examples of such images are shown in <ref type="figure">Figure 5</ref>.</p><p>As a pre-processing step for testing on fundus images, we isolated the green channel, applied adaptive histogram equalization and a bilateral filter to reduce noise and enhance the appearance of edges as proposed in <ref type="bibr" target="#b17">[19]</ref>. The effect of pre-processing can be seen in <ref type="figure">Figure 1</ref>. Even though the focus of this paper is on the retinal images, we also tested the generalization capabilities of our model by evaluating it on natural images. We used the Oxf ord <ref type="bibr" target="#b34">[36]</ref>, EF <ref type="bibr" target="#b54">[55]</ref>, W ebcam <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b26">28]</ref> and V iewP oint <ref type="bibr" target="#b52">[53]</ref> datasets. More details are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation criteria</head><p>We evaluated the performance using the following metrics:</p><p>1. Repeatability describes the percentage of detected points x ? P in image I that are within an -distance ( = 3) to points x ? P in I after transformation with H I,I , where P and P are the sets of extracted points found in common regions to both images:</p><formula xml:id="formula_3">|{x ? P, x ? P | | H I,I * x ? x &lt; ?}| |P | + |P |<label>(4)</label></formula><p>2. Matching performance. Matches were found using the Nearest Neighbor Distance Ratio (NNDR) strategy, as proposed in <ref type="bibr" target="#b32">[34]</ref>: two keypoints are matched if the descriptor distance ratio between the first and the second nearest neighbor is below a certain threshold t. Then, the following metrics were evaluated:</p><p>(a) AUC, area under the ROC curve created by varying the value of t, following <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b49">50]</ref>. (b) M.score, the ratio of correct matches over the total number of keypoints extracted by the detector in the shared viewpoint region <ref type="bibr" target="#b33">[35]</ref>. (c) Coverage fraction, measures the coverage of an image by correctly matched key points. A coverage mask was generated from true positive key points, each one adding a disk of fixed radius (25px) as in <ref type="bibr" target="#b4">[6]</ref>.</p><p>We computed the homography? relating the reference to the transformed image by applying RanSaC algorithm to remove outliers from the detected matches.</p><p>3. Registration success rate. We furthermore evaluated the registration accuracy achieved after using key points computed by different detectors as in <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>To do so, we compared the reprojection error of six fixed points of the reference image (denoted as c i , i = {1, .., 6}) onto the other. For each image pair for which a homography was found, the quality of the registration was assessed with the median error (MEE) and the maximum error (MAE) of the distances between corresponding points after transformation.</p><p>Using these metrics, we defined different thresholds on MEE and MAE that define "acceptable", "inaccurate" and "f ailed" registrations. We consider registration "f ailed" if not enough keypoints or matches were found to compute a homography (minimum 4), if it involves a flip or if the estimated scaling component is greater than 4 or smaller than 0.1. We classified the result as "acceptable" when M EE &lt; 10 and M AE &lt; 30 and as "inaccurate" otherwise. The values for the thresholds were found empirically by post-viewing the results. Using the above definitions, we calculated the success rate of each class, equal to the percentage of image pairs for which the registration falls into each category. These metrics are the most important quantitative evaluation criteria of the overall performance in a real-world setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Baselines and implementation details</head><p>To evaluate the performance of our GLAMpoints detector associated with root-SIFT descriptor, we compared its matching ability and registration quality against well known detectors and descriptors. Among them, SIFT <ref type="bibr" target="#b0">[2]</ref>, KAZE <ref type="bibr" target="#b3">[5]</ref> and LIFT <ref type="bibr" target="#b51">[52]</ref> were shown to perform well on fundus images by Truong et al. <ref type="bibr" target="#b46">[47]</ref>. Moreover, we compared our method to other CNN-based detectorsdescriptors: LF-NET <ref type="bibr" target="#b35">[37]</ref> and SuperPoint <ref type="bibr" target="#b18">[20]</ref>. We used the authors' implementation of LIFT (pretrained on Picadilly), SuperPoint and LF-NET (pretrained on indoor data, which gives better results on fundus images than the version pretrained on outdoor data) and OpenCV implementation for SIFT and KAZE. A rotation-dependent version of root-SIFT descriptor is used due to its better performance on our test set compared to the rotation invariant version. For the remainder of the paper, SIFT descriptor refers to root-SIFT, rotation-dependent, except if otherwise stated.</p><p>Training of GLAMpoints was performed using Tensorflow <ref type="bibr" target="#b1">[3]</ref> with mini-batch size of 5 and the Adam optimizer <ref type="bibr" target="#b28">[30]</ref> with learning rate = 0.001 and ? = (0.9, 0.999) for 35 epochs. For each batch we randomly cropped 256 ? 256 patches of the full-resolution image to speed up the computation. GLAMpoints (NMS10) was trained and tested with a NMS window w equal to 10px. It must be noted that other NMS windows can be applied, which obtain similar performance. <ref type="table" target="#tab_1">Table 1</ref> presents the success rate of registration evaluated on the slit lamp dataset. Without pre-processing, most detectors show lower performance compared to the preprocessed images, but GLAMpoints performs well even on raw images. While the success rate of acceptable registrations of SIFT, KAZE and SuperPoint drops by 20 to 30% between pre-processed and raw images, GLAMpoints as well as LIFT and LF-NET show only a decrease of 3 to 6%. Besides, LF-NET, LIFT and GLAMpoints detect a steady average number of keypoints (around 485 for preprocessed and 350 non-preprocessed) independently of the pre-processing, whereas the other detectors see a reduction half. In general, GLAMpoints shows the highest performance for both raw and pre-processed images in terms of registration success rate. The robust results of our method indicate that while our detector performs as well or better on good quality images compared to the heuristic-based methods, its performance does not drop on lower quality images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Quantitative results on the slit lamp dataset</head><p>While SIFT extracts a large number of keypoints (205.69 on average for unprocessed images and 431.03 for preprocessed), they appear in clusters as shown in <ref type="figure">Figure 1</ref>. As a result, even if the repeatability is relatively high, the close positioning of the interest points leads to a large number of rejected matches, as the nearest-neighbours are very close to each other. This is evidenced by the low coverage fraction, M.score and AU C <ref type="figure" target="#fig_2">(Figure 4)</ref>. With a similar value of repeatability, our approach extracts interest points widely spread and trained for their matching ability (highest coverage fraction), resulting in more true positive matches (second highest M.score and AUC), as shown in <ref type="figure" target="#fig_2">Figure 4</ref>. LF-NET, similar to SIFT, shows high repeatability, which can be explained by its training strategy, which preferred repeatability over accurate matching objective. However, its M.score and AUC are in the bottom part of the ranking <ref type="figure" target="#fig_2">(Figure 4</ref>). While the performance of LF-NET may increase if it was trained on fundus images, its training procedure requires images pairs with their relative pose and corresponding depth maps, which would be extremely difficult -if not impossible -to obtain for fundus images.</p><p>It is worth noting that SuperPoint scored the highest M.score and AU C but in this case the metrics are artificially inflated because very few keypoints are detected <ref type="bibr" target="#b33">(35,</ref><ref type="bibr">88</ref> and 59,21 on average for raw and pre-processed images respectively). This translates to relatively small coverage fraction and one of the lowest repeatability, leading to few possible correct matches.</p><p>As part of an ablation study, we trained GLAMpoints with different descriptors (   To benchmark the detection results, we used the descriptors that were developed/trained jointly with the given detector and thus can be considered as optimal. For instance in <ref type="bibr" target="#b51">[52]</ref>, the combination of the LIFT/LIFT detector/descriptor outperformed LIFT/SIFT. For completeness, we present the registration results of baseline detectors combined with root-SIFT descriptor in <ref type="table" target="#tab_1">Table 1b</ref>, center. As can be seen, using root-SIFT descriptor does not improve the result compared to the original descriptor.</p><p>Finally, to verify that the performance gain of GLAMpoints does not come solely from the uniform and dense spread of the detected keypoints, we computed the success rate for keypoints in a random, uniformly distributed grid <ref type="table" target="#tab_1">(Table 1b</ref>, bottom), which underperforms in comparison. This shows that our detector predicts not only uniform but also significant points. <ref type="table" target="#tab_3">Table 2</ref> shows the results for success rates of registrations on FIRE. Our method outperforms baselines both in terms of success rate and global accuracy of non-failed registrations. As all the images in FIRE dataset present good quality with highly contrasted vascularization, we did not apply pre-processing. We also did not find it necessary to use the available background masks to filter out keypoints detected outside of the retina as generally they were not matched and did not contribute to the final registration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Quantitative results on FIRE dataset</head><p>It is interesting to note the gap of 33.6% in the success rate of acceptable registrations between GLAMpoints and SIFT. As both use the same descriptor, this difference can be only explained by the quality of the detector. As can be seen in <ref type="figure">Figure 5</ref>, SIFT detects a restricted number of keypoints densely positioned solely on the vascular tree and in the image borders, while GLAMpoints extracts interest points over the entire retina, including challenging areas such as the fovea and avascular zones, leading to a substantial rise in the number of correct matches.</p><p>Even though GLAMpoints outperforms all other detectors, LIFT and SuperPoint also present high performance on the FIRE dataset. This dataset contains images with welldefined corners on a clearly contrasted vascular tree and LIFT extracts keypoints spread over the entire image, while SuperPoint was trained to detect corners on synthetic primitive shapes. However, as evidenced on the slit lamp dataset, the performance of SuperPoint strongly deteriorates on images with less clear features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Results on natural images</head><p>To further demonstrate a possible extension of our method to other image domains, we computed its predictions on natural images. Note that we used the same GLAMpoints model trained on slit lamp images.</p><p>Globally, GLAMpoints reaches a success rate of 75.38% for acceptable registrations, against 85.13% for the best performing detector -SIFT with rotation invariance -and 83.59% for SuperPoint. In terms of AU C, M.score and coverage fraction it scores respectively second, second and first best. In contrast, repeatability of GLAMpoints is only second to last after SIFT, KAZE and LF-NET even though it successfully registers more images. This result shows once again that repeatability is not the most adequate metric to measure the performance of a detector. The detailed results can be found in the supplementary material.</p><p>Finally, it should be noted that the outdoor images of this dataset are significantly different from medical fundus images and contain much greater variability of structures, which indicates a promising generalization of our model to unseen image domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Qualitative results</head><p>In case of slit lamp videos, the end goal is to create retinal mosaics. Using 10 videos containing 25 to 558 images, we generated mosaics by registering consecutive frames using keypoints detected by different methods. We calculated a) SIFT b) GLAMpoints the average number of frames before the registration failed (due to the lack of extracted keypoints or correct matches between a pair of images). Over those 10 videos, the average number of registered frames before failure is 9.98 for GLAMpoints and only 1.04 for SIFT. Example mosaics are presented in <ref type="figure">Figure 6</ref>. For the same video, SIFT failed after 34 frames when the data was preprocessed and only after 11 frames on the original data. In contrast, GLAMpoints successfully registered 53 consecutive raw images, without visual errors. The mosaics were created with frame to frame matching with the blending method of <ref type="bibr" target="#b17">[19]</ref> and without bundle adjustment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Run time</head><p>The run time of detection is computed over 84 pairs of images with a resolution of 660px by 350px. The GLAMpoints architecture was run on a Nvidia GeForce  <ref type="table" target="#tab_4">Table 3</ref>. GLAMpoints is on average significantly faster than SIFT. Importantly, it does not require any time-consuming pre-processing step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we introduce GLAMpoints -a keypoint detector optimized for matching performance. This is in contrast to other detectors that are optimized for repeatability of keypoints, ignoring their correctness for matching. GLAMpoints detects significantly more keypoints that lead to correct matches even in low textured images, which do not present many features. As a result, no explicit preprocessing of the images is required. We train our detector on generated image pairs avoiding the need for ground truth correspondences. Our method produces state-of-theart matching and registration results of medical fundus images and our experiments show that it can be further extended to other domains, such as natural images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material</head><p>In this supplementary material, we first provide additional details on the training methodology in Section A. We then give additional qualitative and quantitative evaluation results on fundus images in Section B. Finally, in Section C, we show the generalization capabilities of GLAMpoints by presenting evaluation results on natural images. Importantly, for all results, we use the same model weights trained on fundus images. For the entire supplementary material, SIFT descriptor <ref type="bibr" target="#b32">[34]</ref> refers to root-SIFT <ref type="bibr" target="#b7">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary details on the training method</head><p>A.1. Performance comparison between SIFT descriptor with or without rotation invariance</p><p>GLAMpoints detector was trained and tested in association with SIFT descriptor rotation-dependent because SIFT descriptor without rotation invariance performs better than the rotation invariant version on fundus images. The details of the metrics evaluated on the pre-processed slitlamp dataset for both versions of SIFT descriptor are shown in <ref type="table" target="#tab_5">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Method for homography generation</head><p>For training of our GLAMpoints, we rely on pairs of images synthetically created by applying randomly sampled homography transformations to a set of base slitlamp images. Let B denote a particular base image from the training dataset, of size H ? W . At every step i, an image pair I i , I i is generated from image B by applying two separate, randomly sampled homography transforms g i , g i . Each of those homographies is a composition of rotation, shearing, perspective, scaling and translation elements. The minimum and maximum values of the geometric transformation parameters that we used are given in table 5.</p><p>Nevertheless, it must be noted that the degree of the geometric transformations applied during training should be chosen so that the resulting synthetic training set resembles the test set. In our case, our test set composed of retinal images only showed rotation up to 30 degrees and only little scaling changes, therefore we limited the geometric transformations of the training set accordingly. However, any degree of geometric transformations or even non-linear ones can be applied to the original images to synthetically create pairs of training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of results on fundus images</head><p>Here, we provide more detailed quantitative experiments on fundus images as well as additional qualitative results. <ref type="table" target="#tab_7">Table 6</ref> and 7 show the mean and standard deviation of the median error M EE and the root mean squared error RM SE for respectively the FIRE dataset and the slitlamp dataset. In both cases, GLAMpoints (NMS10) presents the highest registration accuracy for inaccurate registrations and globally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Details of MEE and RMSE per registration class on the retinal images dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Supplementary examples of matching on the FIRE dataset</head><p>We show additional examples of matches obtained by GLAMpoints, SIFT, KAZE, SuperPoint, LIFT and LF-NET on two pairs of images from the F IRE dataset in <ref type="figure" target="#fig_5">Figure 8</ref>. Again, our keypoints GLAMpoints are homogeneously spread and they lead to substantially more true-positive matches (in green in the figure) than any other method.    The metrics computed on the aforementioned datasets are shown in <ref type="figure" target="#fig_6">Figure 9</ref>. We use the same thresholds as in the main paper to determine acceptable, inaccurate and failed registration. We used the LF-NET pretrained on outdoor data, since most images of those datasets are outdoor. It is worth mentioning the gap in performance between SIFT descriptor with or without rotation invariance on the EF and the V iewpoints datasets. Those images exhibit large rotations and therefore a rotation invariant descriptor is necessary, which is not currently the case of our detector associated with SIFT. This explains why GLAMpoints performs poorly on those datasets.</p><p>Besides, it is interesting to note that on the Viewpoints dataset, LF-NET scores extremely low in all metrics except for repeatability. Indeed, on those images, even though the extracted key-points are repeatable, most of them are useless for matching. Therefore, LF-Net finds only very few true positive matches compared to the number of detected keypoints and matches, leading to poor evaluation results. This emphasize the importance of designing a detector specifically optimized for matching purposes.</p><p>Finally, on the Oxf ord dataset, GLAMpoints outperforms all other detectors in terms of M.score, coverage fraction and AU C while scoring second in repeatability.</p><p>Finally, to further extend GLAMpoints to natural images, which often show more drastic view-point changes than retinal images, one could adapt the parameters of the synthetic homographies and appearance augmentations applied during training to fit better the test set of interest. Indeed, in the case of slitlamp retinal images, our test set composed of retinal images only showed rotation up to 30 degrees and only little scaling changes, therefore we limited the geometric transformations of the training set accordingly. However, any degree of geometric transformation or even non-linear ones can be applied to the original images to synthetically create pairs of training images.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>a) Training steps for an image pair I i and I i at epoch i created from a particular base image B. I i and I i are created by warping B according to homographies g i and g i respectively. a i and a i refer to the additional appearance augmentations applied to each image. b) Loss computation corresponding to situation a. c) Schematic representation of Unet-4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Examples of images from the slit lamp dataset showing challenging conditions for registration. From left to right: low vascularization and over-exposure leading to weak contrasts and lack of corners, motion blur, focus blur, acquisition artifacts and reflections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Summary of detector/descriptor performance metrics evaluated over 206 pairs of the slit lamp dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Interest points detected by a) SIFT and b) GLAMpoints and corresponding matches for a pair of images from the FIRE (top) and Oxford (bottom) datasets. Detected points are in white, green matches correspond to true positive, red to false positive. GLAMpoints finds considerably more true positive points than SIFT. Mosaics obtained from registration of consecutive images until failure. a) SIFT, raw images; b) SIFT, preprocessed data; c) GLAMpoints, raw data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Examples of image pairs from the Oxford dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Matches on the F IRE dataset. Detected points are in white, green lines are true positive matches while red ones are false positive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Summary of detector/descriptor performance metrics evaluated over 195 pairs of natural images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Success rates (%) per registration class for each detector on the 206 images of the slit lamp dataset. When the original descriptor is not used in association with detector, the descriptor used is indicated in parenthesis.</figDesc><table><row><cell></cell><cell cols="2">(a) Raw data</cell><cell></cell></row><row><cell></cell><cell cols="3">Failed [%] Inaccurate [%] Acceptable [%]</cell></row><row><cell>SIFT</cell><cell>14.56</cell><cell>63.11</cell><cell>22.33</cell></row><row><cell>KAZE</cell><cell>24.27</cell><cell>61.65</cell><cell>14.08</cell></row><row><cell>SuperPoint</cell><cell>17.48</cell><cell>48.54</cell><cell>33.98</cell></row><row><cell>LIFT</cell><cell>0.0</cell><cell>43.69</cell><cell>56.31</cell></row><row><cell>LF-NET</cell><cell>0.0</cell><cell>39.81</cell><cell>60.19</cell></row><row><cell cols="2">GLAMpoints (SIFT) 0.0</cell><cell>36.41</cell><cell>63.59</cell></row><row><cell></cell><cell cols="2">(b) Pre-processed data</cell><cell></cell></row><row><cell>Detector</cell><cell cols="3">Failed [%] Inaccurate [%] Acceptable [%]</cell></row><row><cell>ORB</cell><cell>9.71</cell><cell>83.01</cell><cell>7.28</cell></row><row><cell>GLAMpoints (ORB)</cell><cell>0.0</cell><cell>88.35</cell><cell>11.65</cell></row><row><cell>BRISK</cell><cell>16.99</cell><cell>66.02</cell><cell>16.99</cell></row><row><cell cols="2">GLAMpoints (BRISK) 1.94</cell><cell>75.73</cell><cell>22.33</cell></row><row><cell>SIFT</cell><cell>1.94</cell><cell>47.75</cell><cell>50.49</cell></row><row><cell>KAZE</cell><cell>1.46</cell><cell>54.85</cell><cell>43.69</cell></row><row><cell>KAZE (SIFT)</cell><cell>4.37</cell><cell>57.28</cell><cell>38.35</cell></row><row><cell>SuperPoint</cell><cell>7.77</cell><cell>51.46</cell><cell>40.78</cell></row><row><cell>SuperPoint (SIFT)</cell><cell>6.80</cell><cell>54.37</cell><cell>38.83</cell></row><row><cell>LIFT</cell><cell>0.0</cell><cell>39.81</cell><cell>60.19</cell></row><row><cell>LF-NET</cell><cell>0.0</cell><cell>36.89</cell><cell>63.11</cell></row><row><cell>LF-NET (SIFT)</cell><cell>0.0</cell><cell>40.29</cell><cell>59.71</cell></row><row><cell>GLAMpoints (SIFT)</cell><cell>0.0</cell><cell>31.55</cell><cell>68.45</cell></row><row><cell>Random grid (SIFT)</cell><cell>0.0</cell><cell>62.62</cell><cell>37.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1b</head><label>1b</label><figDesc>, top). While it performs best with the SIFT descriptor, the results show that for every considered descriptor (SIFT, ORB, BRISK), GLAMpoints improves upon the corresponding original detector.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Success rates (%) of each detector on FIRE.</figDesc><table><row><cell></cell><cell cols="3">Failed [%] Inaccurate [%] Acceptable [%]</cell></row><row><cell>SIFT</cell><cell>2.24</cell><cell>36.57</cell><cell>61.19</cell></row><row><cell>KAZE</cell><cell>14.18</cell><cell>58.21</cell><cell>27.61</cell></row><row><cell>SuperPoint</cell><cell>0.0</cell><cell>13.43</cell><cell>86.57</cell></row><row><cell>LIFT</cell><cell>0.0</cell><cell>10.45</cell><cell>89.55</cell></row><row><cell>LF-NET</cell><cell>0.0</cell><cell>38.06</cell><cell>61.94</cell></row><row><cell cols="2">GLAMpoints (OURS) 0.0</cell><cell>5.22</cell><cell>94.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Average detection run time [ms] per image for GLAMpoints and SIFT detectors.</figDesc><table><row><cell></cell><cell>GLAMpoints</cell><cell>SIFT</cell></row><row><cell>Pre-processing</cell><cell>0.0</cell><cell>16.64 ? 0.93</cell></row><row><cell>Detection image I</cell><cell>CNN: 16.28 ? 96.86 NMS: 11.2 ? 1.05</cell><cell>28.94 ? 1.88</cell></row><row><cell>Total</cell><cell>27.48 ? 98.74</cell><cell>45.58 ? 4.69</cell></row><row><cell cols="3">GTX 1080 GPU while NMS and SIFT used CPU. Mean and</cell></row><row><cell cols="3">standard deviation of run time for GLAMpoints and SIFT</cell></row><row><cell>are presented in</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Evaluation metrics calculated over 206 pre-processed pairs of the slitlamp dataset.</figDesc><table><row><cell>SIFT with rotation invariance SIFT rotation-dependent</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Parameter range used for random homography generation during training.</figDesc><table><row><cell>Scaling</cell><cell>Perspective</cell><cell>Translation</cell><cell>Shearing</cell><cell>Rotation</cell></row><row><cell cols="5">min scaling 0.7 min perspective parameter 0.000001 max horizontal displacement 100 min/max horizontal shearing -0.2 / 0.2 max angle 25</cell></row><row><cell cols="2">max scaling 1.3 max perspective parameter 0.0008</cell><cell>max vertical displacement</cell><cell>100 min/max vertical shearing</cell><cell>-0.2 / 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Means and standard deviations of median errors (MEE) and RMSE in pixels for non-preprocessed images of the FIRE dataset. Acceptable registrations are defined as having (M EE &lt; 10 and M AE &lt; 30).</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Generalization of the model on natural images</head><p>Our method GLAMpoints was designed for application on medical retinal images. However, to show its generalisation properties, we also evaluate our network on natural images. Importantly, it must be noted that here, we use the model weights trained on slitlamp images.</p><p>Our method was tested on several natural image datasets, with following specifications: For all of the aforementioned datasets, the images pairs are related by homography transforms. Indeed, the scenes are either planar, purely rotative or the images are taken at sufficient distance so that the planar assumption holds. In <ref type="figure">figure 7</ref> are represented examples of images pairs from the Oxford dataset.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<idno>OpenCV: cv::xfeatures2d</idno>
		<title level="m">SIFT Class Reference</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martn</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<editor>Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vigas</editor>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FREAK: Fast Retina Keypoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphal</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Fern?ndez Alcantarilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bartoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Features</surname></persName>
		</author>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ondrej Chum, and Jiri Matas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Aldana-Iuit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Saddle: Chasing Fast and Repeatable Features. In International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="675" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to Match Aerial Images with Deep Attentive Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hani</forename><surname>Altwaijry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to Detect and Match Keypoints with Deep Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hani</forename><surname>Altwaijry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">PN-Net: Conjoined Triple Deep Network for Learning Local Image Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassileios</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<idno>abs/1601.05030</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">HPatches: A Benchmark and Evaluation of Handcrafted and Learned Local Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassileios</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3852" to="3861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Retina Mosaicing Using Local Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?bor</forename><surname>Sz?kely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Partial Intensity Invariant Feature Descriptor for Multimodal Retinal Image Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Laine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1707" to="1718" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Partial Intensity Invariant Feature Descriptor for Multimodal Retinal Image Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">F</forename><surname>Laine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Registration of Ocular Fundus Images: an Algorithm Using Cross-correlation of Triple Invariant Image Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artur</forename><forename type="middle">V</forename><surname>Cideciyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Engineering in Medicine and Biology Magazine</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="58" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finding the Best Feature Detector-Descriptor Combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><forename type="middle">L</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Aanaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><forename type="middle">S</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="318" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Real-Time Simultaneous Localisation and Mapping with a Single Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Retinal Slit Lamp Video Mosaicking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><forename type="middle">De</forename><surname>Zanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Richa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tappeiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Sznitman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1035" to="1041" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SuperPoint: Self-Supervised Interest Point Detection and Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">KCNN: Extremely-Efficient Hardware Keypoint Detection With a Compact Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Di Febbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><forename type="middle">Dal</forename><surname>Mutto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kinh</forename><surname>Tieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Descriptor Matching with Convolutional Neural Networks: a Comparison to SIFT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno>1405.5769</idno>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Textureless Macula Swelling Detection With Multiple Retinal Fundus Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Giancardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Meriaudeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Karnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobin</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Grisan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfredo</forename><surname>Ruggeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Chaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="795" to="799" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Retinal Image Registration Based on the Feature of Bifurcation Point</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliu</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiqun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Congress on Image and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Combined Corner and Edge Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth Alvey Vision Conference</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">FIRE : Fundus Image Registration dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Hernandez-Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xenophon</forename><surname>Zabulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Areti</forename><surname>Triantafyllou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiota</forename><surname>Anyfanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Douma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonis</forename><surname>Argyros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal for Modeling in Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Consistent Temporal Variations in Many Outdoor Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Phase Correlation-based Iris Image Registration Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li Ma Jun-Zhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Niu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="419" to="425" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving Accuracy and Efficiency of Mutual Information for Multi-modal Retinal Image Registration using Adaptive Probability Density Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="597" to="606" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BRISK: Binary Robust Invariant Scalable Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margarita</forename><surname>Chli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Registration of OCT Fundus Images with Color Fundus Images Based on Invariant Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cloud Computing and Security</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="471" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2004-11-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Performance Evaluation of Local Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1615" to="1630" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Comparison of Affine Region Detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timor</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">LF-Net: Learning Local Features from Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang Moo</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mutual Information Based Registration of Medical Images: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Josien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><forename type="middle">A</forename><surname>Antoine Maintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Viergever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="986" to="1004" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Feature-Based Retinal Image Registration Using D-Saddle Feature</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Healthcare Engineering</title>
		<editor>Roziana Ramli, Mohd Yamani Idna Idris, Khairunnisa Hasikin, Noor Khairiah A Karim, Ainuddin Wahid Abdul Wahab, Ismail Ahmedy, Fatimah Ahmedy, Nahrizul Adib Kadri, and Hamzah Arof</editor>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ORB: An Efficient Alternative to SIFT or SURF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>C?sar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>S?nchez-Galeana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Eytan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parag</forename><forename type="middle">A</forename><surname>Blumenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Linda</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Using Optical Imaging Summary Data to Detect Glaucoma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">N</forename><surname>Weinreb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opthamology</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1812" to="1818" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Discriminative Learning of Deep Convolutional Feature Point Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franscesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bundle Adjustment -A Modern Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">F</forename><surname>Mclauchlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision Algorithms: Theory and Practice</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="298" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">GLAMpoints : Github project page in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<ptr target="https://github.com/PruneTruong/GLAMpoints_pytorch,2019.2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Apostolopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Mosinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Stucky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Ciller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><forename type="middle">De</forename><surname>Zanet</surname></persName>
		</author>
		<ptr target="https://gitlab.com/retinai_sandro/glampoints,2019.2" />
		<title level="m">GLAMpoints : GitLab project page in Tensor-Flow</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Comparison of Feature Detectors for Retinal Image Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><forename type="middle">De</forename><surname>Zanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Apostolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ARVO</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">TILDE: A Temporally Invariant Learned DEtector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Verdie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang Moo</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robust Point Matching Method for Multimodal Retinal Image Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning Local Image Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Winder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Picking the Best DAISY</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Winder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">LIFT: Learned Invariant Feature Transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Kwang Moo Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning to Assign Orientations to Feature Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Kwang Moo Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Verdie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The Detection and Quantification of Retinopathy Using Digital Angiograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Rzeszotarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">J</forename><surname>Singerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeanne</forename><forename type="middle">M</forename><surname>Chokreff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="619" to="626" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Edge Foci Interest Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnan</forename><surname>Ramnath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
