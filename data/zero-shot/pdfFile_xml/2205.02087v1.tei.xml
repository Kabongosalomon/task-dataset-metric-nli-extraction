<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hypercomplex Image-to-Image Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Grassucci</surname></persName>
							<email>eleonora.grassucci@uniroma1.it.</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. Information Engineering, Electronics and Telecommunications (DIET)</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Sigillo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. Information Engineering, Electronics and Telecommunications (DIET)</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Uncini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. Information Engineering, Electronics and Telecommunications (DIET)</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Comminiello</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. Information Engineering, Electronics and Telecommunications (DIET)</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hypercomplex Image-to-Image Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Hypercomplex Neural Networks</term>
					<term>Generative Adversarial Networks</term>
					<term>Image-to-Image Translation</term>
					<term>Lightweight Models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image-to-image translation (I2I) aims at transferring the content representation from an input domain to an output one, bouncing along different target domains. Recent I2I generative models, which gain outstanding results in this task, comprise a set of diverse deep networks each with tens of million parameters. Moreover, images are usually three-dimensional being composed of RGB channels and common neural models do not take dimensions correlation into account, losing beneficial information. In this paper, we propose to leverage hypercomplex algebra properties to define lightweight I2I generative models capable of preserving pre-existing relations among images dimensions, thus exploiting additional input information. On manifold I2I benchmarks, we show how the proposed Quaternion StarGANv2 and parameterized hypercomplex StarGANv2 (PHStarGANv2) reduce parameters and storage memory amount while ensuring high domain translation performance and good image quality as measured by FID and LPIPS score. Full code is available at https://github.com/ispamm/HI2I.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Image-to-image translation (I2I) aims at transferring the content representation from an input domain to an output one, bouncing along different target domains. Recent I2I generative models, which gain outstanding results in this task, comprise a set of diverse deep networks each with tens of million parameters. Moreover, images are usually three-dimensional being composed of RGB channels and common neural models do not take dimensions correlation into account, losing beneficial information. In this paper, we propose to leverage hypercomplex algebra properties to define lightweight I2I generative models capable of preserving pre-existing relations among images dimensions, thus exploiting additional input information. On manifold I2I benchmarks, we show how the proposed Quaternion StarGANv2 and parameterized hypercomplex StarGANv2 (PHStarGANv2) reduce parameters and storage memory amount while ensuring high domain translation performance and good image quality as measured by FID and LPIPS score. Full code is available at https://github.com/ispamm/HI2I.</p><p>Index Terms-Hypercomplex Neural Networks, Generative Adversarial Networks, Image-to-Image Translation, Lightweight Models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The aim of image-to-image translation (I2I) is to learn a function G S?T able to generate an image x ST by translating an input image x S ? S from the source domain S to a target domain T . More formally,</p><formula xml:id="formula_0">x ST = G S?T (x S ), x ST ? T,<label>(1)</label></formula><p>in which the target domain can be injected by learning domain features from a reference image or by sampling latent vectors from the domain space <ref type="bibr" target="#b0">[1]</ref>.</p><p>Recently, I2I applications are becoming widespread, including a plethora of diverse tasks such as attribute manipulation <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>, sketch-to-image <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, style transfer <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, semantic synthesis <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and others <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b13">[14]</ref>. Among these, generative adversarial networks (GANs) <ref type="bibr" target="#b14">[15]</ref> are particularly suitable for this task due to few restrictions on the generator network. Indeed, GANs are deep generative models composed of a network whose goal is to generate new images given an input noise vector and a discriminator network that aims at distinguishing generated images from real one. These are trained in an adversarial fashion requiring very few constraints on models architecture, especially on the generator. The latter network covers the role of the mapping function G S?T in Eq. 1 for GAN-based I2I applications. Latest works on generative models, including I2I ones, have achieved impressive results by scaling-up models in terms of trainable parameters, computational complexity and memory requirements <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>. Therefore, most of them are difficult to train with a lower budget, undermining their replicability. Furthermore, low attention has been paid to how multidimensional inputs such as color images are processed by these models. The human eye perceives an image with lots of color shades that are the result of interactions among the three RGB channels. Therefore, channels interplays are crucial for a proper image processing. Actually, common real-valued models do not leverage this detail treating each channel as a separate entity, causing an information loss.</p><p>To overcome these limitations, quaternion neural networks (QNNs) have been proposed. Such models have recently gained increasing attention due to their ability to exploit quaternion algebra properties that lead to a consistent parameters reduction while ensuring high performance <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b21">[22]</ref>. This is because, despite the lower number of parameters due to the vector multiplication which, in this domain, takes a different form with respect to the real-valued domain, QNNs preserve correlations among channels. Indeed, they process RGB images as a single entity, thus exploiting relations within input dimensions, building a more suitable representation <ref type="bibr" target="#b22">[23]</ref>. Both GANs and variational autoencoders (VAEs) have been defined in the quaternion domain demonstrating encouraging scores and an improved generation ability <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>. However, QNNs accept 4D inputs only, thus a padding channel has to be annexed to RGB images to compose a pure quaternion. This useless information may undermine QNNs performance.</p><p>Lately, novel approaches have been proposed to extend QNNs advantages to any nD input <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b29">[30]</ref>. Thanks to a parameterized sum of Kronecker products, these methods learn hypercomplex algebra regulations directly from data. Thus, they can be defined in any nD domain of user choice without needing pre-fixed multiplication rules as it is instead for QNNs.</p><p>In this paper, we propose to exploit hypercomplex algebra properties to define lightweight and replicable models for multi-domain I2I tasks. Therefore, we introduce a Quaternion StarGANv2 and the parameterized hypercomplex StarGANv2 (PHStarGANv2), a novel element in the family of parameterized hypercomplex neural networks (PHNNs). The PHStar-GANv2 is able to behave like the Quaternion StarGANv2 by involving n = 4 or to process RGB images in their natural domain with n = 3. These models save trainable parameters and storage memory, thus are more accessible with a lower budget. Moreover, thanks to hypercomplex vector multiplications, including the Hamilton product, the Quaternion StarGANv2 and the PHStarGANv2 grasp relations among image channels thus learning more information despite the lower number of parameters. We prove these abilities with an empirical evaluation on multiple benchmarks and evaluate the performance with visual inspections and different objective metrics. Accordingly, our contributions follow. 1) We introduce a set of hypercomplex StarGANv2 for image-to-image translation, including the Quaternion StarGANv2 and the parameterized hypercomplex Star-GANv2 (PHStarGANv2), which leverage hypercomplex algebra properties to reduce the overall number of parameters and exploiting channels correlations. The latter leads to better performance in terms of generation quality and translation accuracy. To the best of our knowledge, this is the first generative model defined with parameterized hypercomplex layers. 2) We define the instance normalization for hypercomplex domains. This novel technique increases performance of quaternion and parameterized hypercomplex models as measured by FID score. 3) We propose a novel method to initialize parameterized hypercomplex layers that avoids training degeneracy, which is due to the almost-zero values of the weight matrix. We show how weights are distributed more properly with our approach and that this leads to a crucial improvement in generation results, as measured by objective metrics. The rest of the paper is organized as follows. Section II introduces theoretical concepts of quaternion generative models and parameterized hypercomplex networks. In Section III, we expound the proposed I2I models, in Section IV we conceive the hypercomplex normalizations, while in Section V we introduce the novel inizialization. Then, Section VI proves the experimental validity of our methods. Finally, conclusions are drawn in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. QUATERNION AND HYPERCOMPLEX GENERATIVE MODELS</head><p>Quaternion and hypercomplex neural networks were born from a system number based on a set of hypercomplex numbers H, whose additions and multiplications are regulated by a collection of algebra rules. Among these numbers, quaternions are identified by three imaginary units, namely?, ij and?, and four real-valued coefficients q c , c = {0, 1, 2, 3} as:</p><formula xml:id="formula_1">q = q 0 + q 1? + q 2 ij + q 3? .<label>(2)</label></formula><p>While the addition of two quaternions p and q is intuitive, being p + q = (p 0 + q 0 ) + (p 1 + q 1 )? + (p 2 + q 2 )ij + (p 3 + q 3 )?, a more detailed formula is needed to model imaginary units interplays when vector multiplication is performed. Indeed, this operation is not commutative in the quaternion domain since?ij = ?ij?,?? = ???, ij? = ??ij, and so on. To this end, for a proper quaternion vector multiplication, the Hamilton product has been introduced. The Hamilton product is the core of quaternion neural networks (QNNs), where a quaternion weight matrix W = W 0 + W 1? + W 2 ij + W 3? is multiplied by a quaternion input x with the same structure as:</p><formula xml:id="formula_2">W?x = ? ? ? ? W 0 ?W 1 ?W 2 ?W 3 W 1 W 0 ?W 3 W 2 W 2 W 3 W 0 ?W 1 W 3 ?W 2 W 1 W 0 ? ? ? ? ? ? ? ? ? x 0 x 1 x 2 x 3 ? ? ? ? . (3)</formula><p>Weight submatrices are reused and shared among input components, thus while the dimension of the matrix W is the same as a real-valued one, it involves just 1/4 parameters of its real-valued counterpart. Furthermore, by sharing weights among different input dimensions, quaternion layers exploit correlations contained within components thus preserving the original multidimensional structure of the input while gaining advantages from it. This ensures good performance despite the lower number of trainable parameters. Nevertheless, QNNs are limited to 4D data, thus when processing RGB images with three channels, an uninformative further channel has to be padded in order to build the four-dimensional input.</p><p>Recently, a novel approach for parameterizing hypercomplex models has been proposed <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. It aims at building the hypercomplex weight matrix as a sum of Kronecker products parameterized by a user-defined hyperparameter n. These methods allow the definition of fully parameterized hypercomplex neural networks (PHNNs) that completely work in the chosen hypercomplex domain. More in detail, the weight matrix H of a generic PHNN is defined as</p><formula xml:id="formula_3">H = n i=1 A i ? F i ,<label>(4)</label></formula><p>whereby A i describe the hypercomplex algebra rules by learning them directly from data (i.e., the Hamilton product for the quaternion domain) and F i are batch of weights that can be scalars for fully connected (FC) layers, or groups of filters for convolutional ones. In the first case, we deal with parameterized hypercomplex multiplication (PHM) layers <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, while in the second one we employ parameterized hypercomplex convolutional (PHC) layers <ref type="bibr" target="#b27">[28]</ref>. PHNNs involve 1/n free parameters of their real-valued counterparts while being more efficient than quaternion models and obtaining better results due to the fully-learnable structure of their layers. By fixing n = 4, through Eq. 4, we can express the Hamilton product in Eq. 3. PHNNs exceed QNNs thanks to their ability to grasp a more suitable weights organization (i.e., the rules defining the algebra) from data.</p><p>Lately, these techniques have been applied to generative models. Indeed, state-of-the-art generative models usually comprise tens of million parameters and are often employed with multidimensional inputs such as color images or multichannel audio signals <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b30">[31]</ref>. The quaternion-valued variational autoencoder and the family of quaternion generative adversarial networks have demonstrated to obtain comparable performance while reducing the storage memory amount due to the parameters reduction <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Encouraged by these results, we propose to expolit novel PHNNs methods to define a more advanced generative model for image-to-image translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. QUATERNION AND PARAMETERIZED HYPERCOMPLEX STARGANV2 NETWORKS</head><p>In this Section, we present the Quaternion StarGANv2 and the Parameterized Hypercomplex StarGANv2 (PHStar-GANv2), we describe QNNs and PHNNs involved to build such models and training losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Models</head><p>We rebuild the real-valued StarGANv2 <ref type="bibr" target="#b0">[1]</ref> as a quaternion model first, and then as a parameterized hypercomplex one to operate in any user-defined hypercomplex domain by easily setting the hyperparameter n. Both the quaternion model and the parameterized one are composed of four different networks.</p><p>The generator network (G) takes an input image x and a style code s and translates x according to s producing a new sample. It is composed of PHC residual blocks (quaternion convolutional blocks for quaternion model), instance normalization and adaptive instance normalization. We carefully redefine the latter in hypercomplex domains, as Section IV shows more in detail. A mapping network (M), instead, takes care of generating a style code s from a sampled latent vector z and a random domain y. It is built by interleaving PHM layers and ReLU activation function. As before, for the quaternion version, we employ quaternion layers instead of PHM ones. Third, a style encoder network (S) extracts the style code s from a reference image x. Similar to the generator, the encoder comprises several convolutional residual blocks that are built by PHC layers for PHStarGANv2 models or by Hamiltonbased convolutions for quaternion one. However, differently from the generator, the style encoder ends up with a stack of PHM/quaternion layers. Finally, the discriminator network (D) is a multi-branch binary classifier that learns to distinguish whether the image x is a real image of the given domain y or a fake one. This network stacks various residual blocks similar to the alreadydefined ones with a final fully-connected branch for each domain.</p><p>On one hand, the Quaternion StarGANv2 defines these networks in the quaternion domain, involving quaternion operations and layers, thus operating in a pre-defined hypercomplex domain. On the other hand, our PHStarGANv2 is free to run in different domains, thus reproducing the quaternion one setting n = 4, the complex one with n = 2 or processing images padding any additional channel by employing n = 3.</p><p>Therefore, we propose two different approaches to perform image-to-image translation in hypercomplex domains. We explore the task in the quaternion domain to leverage the Hamilton product properties and to investigate the model performance with a rigid pre-defined algebra rule as backbone.</p><p>This method ensures the largest memory saving. We introduce more flexibility thanks to parameterized hypercomplex approaches that guarantee high performance while giving the possibility of choosing the amount of parameters reduction or memory saving and the domain in which the model operates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Losses</head><p>In the following, we expound the losses involved to build the structure of the proposed PHStarGANv2. As the original StarGANv2 <ref type="bibr" target="#b0">[1]</ref>, we employ two approaches for training, both of them based on the same equations. Firstly, we perform a latent-guided analysis, thus generating style codes from latent vectors. Secondly, we produce style codes from reference images in the reference-guided synthesis.</p><p>The adversarial loss is the known GAN loss <ref type="bibr" target="#b14">[15]</ref> composed of two binary cross-entropies. Here, it is computed for each branch of the discriminator corresponding to each domain y: <ref type="figure">D?(G(x,s)</ref>))].</p><formula xml:id="formula_4">? adv = E x,y [log D y (x)] + E x,?,z [log(1 ?</formula><p>(5) The style reconstruction loss enforces the generator to employ the style codes when generating images as G(x,s),</p><formula xml:id="formula_5">? sty = E x,?,z [ s ? S?(G(x,s)) 1 ].<label>(6)</label></formula><p>The style diversification loss, instead, forces the generator to explore the image space and to produce diverse and varied images as:</p><formula xml:id="formula_6">? ds = E x,?,z1,z2 [ G(x,s 1 ) ? G(x,s 2 ) 1 ],<label>(7)</label></formula><p>wherebys 1 ands 2 , which are the target style codes, are generated by the mapping network from the latent vectors z 1 and z 2 . In the reference analysis, these codes are produced from reference images. The preserving source characteristics loss is a cycle consistency loss aiming at guaranteeing that generated images preserve the domain-invariant features as:</p><formula xml:id="formula_7">? cyc = E x,y,?,z [ x ? G(G(x,s),?) 1 ],<label>(8)</label></formula><p>in which? = S y (x) is the input x style code learnt from the style encoder S and y is the true domain of the image. Finally, the full loss is composed by</p><formula xml:id="formula_8">min G,M,S max D ? adv + ? sty ? sty ? ? ds ? ds + ? cyc ? cyc ,<label>(9)</label></formula><p>with ? sty , ? ds , ? cyc are hyperparameters that balance losses importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. HYPERCOMPLEX INSTANCE NORMALIZATIONS</head><p>Recent models dealing with style losses, such as the original StarGANv2 <ref type="bibr" target="#b0">[1]</ref>, replace batch normalization (BN) with a more suitable normalization technique, namely instance normalization (IN) <ref type="bibr" target="#b32">[33]</ref>. This method aims at reducing the contrast on the input image in order to make the model focus on the contrast of the reference one only. Therefore, IN normalizes over a single image, differently from BN which normalizes the whole batch. More formally, given a tensor x ? R N ?C?H?W having as element x nchw , where N is the batch index, C channels while H and W the spatial dimensions, mean ? nc and variance ? 2 nc are computed over the latter dimensions and the normalization is applied as</p><formula xml:id="formula_9">y nchw = ? x nchw ? ? nc ? 2 nc + + ?.<label>(10)</label></formula><p>When dealing with quaternion or hypercomplex inputs, image channels are encapsulated in a single element, thus applying different normalizations per channel, as done in Eq. 10, may break relations among components. Therefore, we propose a proper method for computing and employing instance normalization in hypercomplex domains based on the quaternion batch normalization in <ref type="bibr" target="#b33">[34]</ref>. Suppose to operate in the quaternion domain, the input tensor will be q ? H N ?C?H?W and consequently split in the four quaternion components. Then, the mean ? nc is still a quaternion and it is computed per component:</p><formula xml:id="formula_10">? nc (q) = 1 HW H h=1 W w=1 (q 0,hw + q 1,hw? + q 2,hw ij + q 3,hw? ) =q 0 +q 1? +q 2 ij +q 3? .<label>(11)</label></formula><p>The variance is computed in a similar way, however, the percomponent values are then averaged and a single variance is then employed for normalization:</p><formula xml:id="formula_11">? 2 nc (q) = 1 HW H h=1 W w=1 (? 2 q 0,hw + ? 2 q 1,hw + ? 2 q 2,hw + ? 2 q 3,hw ).<label>(12)</label></formula><p>This is an approximation to the optimal variance that is, however, computationally expensive to be calculated due to the particular form of quaternion covariance matrix <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b36">[37]</ref>. Finally, the hypercomplex instance normalization (HIN) can be applied following Eq. 10 considering that also the shift parameter ? = ? 0 + ? 1? + ? 2 ij + ? 3? is a quaternion. Following a similar approach, it is possible to redefine also the adaptive instance normalization (AdaIN) <ref type="bibr" target="#b37">[38]</ref>. As IN, AdaIN was specifically conceived for this kind of applications, and thus it aims at learning shifting and scaling parameters directly from the style. More concretely, AdaIN has no ? and ? parameters since it aligns channel-wise mean and variance of the content image x to the one of the style y. For quaternion inputs, we propose to compute normalization statistics of the input x as in Eq. 11 and Eq. 12. Moreover, we devise to learn style statistics ?(y) and ?(y) through a quaternion (or PHM for hypercomplex inputs) layer to preserve the multidimensional structure of the style input and adaptively learn the parameters. Formally, the hypercomplex AdaIN (HAdaIN) is defined by</p><formula xml:id="formula_12">HAdaIN(q, y) = ?(y) q ? ? nc (q) ? nc (q) + ?(y).<label>(13)</label></formula><p>In the experimental Section VI, we empirically test the proposed techniques and show how they increase the generation ability of quaternion and PHStarGANv2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. WEIGHTS INITIALIZATION FOR PARAMETERIZED HYPERCOMPLEX LAYERS</head><p>Due to the multidimensional structure of inputs and weights, quaternion and hypercomplex networks initialization has become crucial <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. While the former has been widely investigated, less attention have been paid to the latest parameterized hypercomplex layers. The original work <ref type="bibr" target="#b27">[28]</ref> initializes both A i and F i with Xavier or Kaiming methods. However, due to Kronecker products among these matrices, these values become smaller and closer to 0. As a consequence, the resulting weight matrix H is composed of almost-zero values, as the blue line in <ref type="figure" target="#fig_1">Fig. 1</ref> shows. This may cause training degeneracy <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, undermining the learning procedure. Therefore, since matrices A i play the role of switching on/off the scalar weights or the filters by defining the algebra rules for multiplication and convolutions, we propose an integer initialization for these PH weights. We select randomly each value in the set {?1, 0, 1} meaning that for 0, the corresponding weight in F i is not inserted in the final H, while for ?1 or 1, it is considered with minus or plus, respectively. This is inspired to a quaternion-like initialization where the four matrices A i are set to reproduce the Hamilton product according to Eq. 3. This method ensures that the weight matrix H has a similar distribution to a counterpart real-valued weight matrix, that is the distribution of scalar weights for PHM and filter ones for PHC. We show these comparisons in <ref type="figure" target="#fig_1">Fig. 1</ref>, where the red line is the density of a real-valued convolutional layer initialized with Xavier normal, the yellow one the distribution of a PHC layer initialized rigidly following the Hamilton product in Eq. 3, thus with A i ? {?1, 0, 1} while the green line is the H density of a layer initialized with our RandInteger Init. As it is clear, the final weight matrix is distributed similar to real-valued weights, thus ensuring an analogous behaviour during training, without degeneracy. While the Quat Init can be employed only for 4D inputs, our method can be generalized to any nD inputs and applied with different values of n.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL EVALUATION</head><p>In this Section, we conduct a meticulous experimental evaluation of our proposed approaches. We first delineate the experimental setup, including the architectural details and then we report the empirical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>We conduct the experimental evaluation on the CelebA-HQ dataset, which is a high quality version of the CelebA dataset, containing 30000 images at 1024 ? 1024 resolution that we rescale at 128 ? 128. Moreover, we consider a latent code sampled from a Gaussian distribution with dimension 16 and a learnt style code of 64. In order to have a fair comparison among the networks we consider, we employ the same hyperparameters for real-valued, quaternion and PH models. Except for the batch size, which we set equal to 12 and not to 8, the hyperparameters are set as in the original paper <ref type="bibr" target="#b0">[1]</ref>. The losses weights ? sty and ? cyc are fixed to 1, while ? ds starts from 1 and decreases after each iteration. Learning is performed via Adam optimizers with ? 1 = 0 and ? 2 = 0.99, with a learning rate equal to 10 ?4 for all networks except for the mapping one where it is lower lr = 10 ?6 . We train all networks for 100k iterations.</p><p>To delineate models architecture, first we define the blocks we employ in the networks. The first residual block (ResBlock) interleaves convolutional layers, instance normalization and Leaky ReLU activation functions. The second one involves adaptive instance normalization instead of the standard one, therefore we name it AdaResBlock.</p><p>The generator network is built with an initial convolution and with seven ResBlocks, five of them with average pooling for downsampling, seven AdaResBlocks follow, the last five with upsampling. A final refiner convolution is then applied. As the original work <ref type="bibr" target="#b0">[1]</ref>, we involve a pretrained support network which helps in detecting and generating human faces contours and details <ref type="bibr" target="#b42">[43]</ref>. The mapping network has a simple structure composed by interleaving fully connected (FC) layers and ReLU activations. Four of these layers are shared among the domains, while a four-layer branch is created for each domain. The style encoder has six shared ResBlocks with a final convolution and a fully connected layer for each domain, whose output is the learnt style code. Finally, the discriminator network has a similar structure of the style encoder. However, in this case final layers output the probability of an image from the given domain to be real or generated. We test different configurations for the multi-branch last layer and we notice that adding a real-valued FC layer increases the performance, as <ref type="table" target="#tab_1">Table II</ref> shows. Each hyperparameter in these networks are set equal to the original paper, including number of filters, kernel size, and stride, among others <ref type="bibr" target="#b0">[1]</ref>.</p><p>The training procedure is depicted in <ref type="figure" target="#fig_2">Fig. 2</ref>, in which the generator and discriminator networks are employed both for reference (purple) and for latent analysis (pink). The generator gets a real image and a style code and translates the image according to the style code. The discriminator distinguishes between fake and real images given the domain.</p><p>To assess the performance of our approaches we compute two objective metrics. First, the Fr?chet Inception Distance (FID) measures how much the real and generated distributions are far from each other <ref type="bibr" target="#b43">[44]</ref>. Since we ideally want the distributions to be equal, the lower is the FID value, the better is the generation of the model. Second, the Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b44">[45]</ref> measures the diversity of generated images. In this case, the higher is the LPIPS value, the more diverse are the images among each others, meaning a better result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Results</head><p>To be consistent with the original StarGANv2 evaluation, we replicate both the reference guided analysis and latent guided one. In the former, the style code is extracted by the style encoder from a reference image. Instead, in the latter the style code is learnt by the mapping network starting from a sampled latent code. <ref type="figure">Figure 3</ref> displays results for reference analysis, where the style encoder learns to extract the style (hairstyle, color, beard, mouth, eyebrows, among others) from the reference image. The first column of the figure is the source image, while the second one represents the reference. Samples from StarGANv2, Quaternion StarGANv2, PHStarGANv2 n = 3 and PHStarGANv2 n = 4 are then showed. While the overall quality of samples is good at a human eye, the PH models better grasp the style from the reference, as it can be seen from the beard of the first row, or from the hairstyle of the third one and from the shape of the face in the last one. The generation quality and samples diversity are measured through objective metrics in <ref type="table" target="#tab_1">Table I</ref> (first column for reference). PHStarGANv2 versions always obtain a better LPIPS scores proving the previous qualitative inspection. <ref type="figure" target="#fig_3">Figure 4</ref>, instead, shows samples from the latent analysis. Here, the style is learnt by the mapping network from the sampled latent code. Even in this test, generated samples from PHStarGANv2 models show a better injection of the style code into the source image, thus displaying a more interesting translation while preserving the input pose. Samples from Quaternion StarGANv2 are less diverse and of a lower quality, proving that the high flexibility of PH layers gain advantages with respect to the rigid structure of QNNs. Moreover, PH samples show an improved quality (for example, the last three rows) and human faces are better defined with respect to the real-valued StarGANv2. These results are confirmed by FID and LPIPS results in <ref type="table" target="#tab_1">Table I</ref>, in which the proposed approach outperforms StarGANv2 in each metric considered. More interestingly, these improved results are obtained with a consistent parameters reduction, equal to ?67% employing n = 3 and to ?75% for the n = 4 model. This translates in a crucial storage memory saving for checkpoints and inference, as can be seen in <ref type="table" target="#tab_1">Table I</ref>. The choice between different n values can be then left as a user choice or a device-guided one, whether needing an improved diversity generation or a greater storage memory saving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hypercomplex Instance Normalization Results and Architectural Choices</head><p>In the following, we demonstrate the effectiveness of our hypercomplex instance normalization and we empirically justify some architectural choices. For these experiments we consider as baseline a PHStarGANv2 n = 4 and real-valued instance normalization, then we compute FID score both for reference and for latent analysis. We then perform the same experiments involving the proposed hypercomplex instance normalization with fixed and with adaptive parameters, noting a great improvement in the FID score (second row of <ref type="table" target="#tab_1">Table II)</ref>, proving that our method can help the generation process in hypercomplex domains. Finally, we add a final real-valued fully connected layer to the discriminator network to output the scalar decision value that further slightly increases the FID. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Weights Inizialization Results</head><p>In the following, we report experiments with different initializations for the matrices A i in Eq. 4 of parameterized hypercomplex layers. We perform three different tests on the CelebA-HQ dataset and we measure performance with FID and LPIPS scores that are reported in <ref type="table" target="#tab_1">Table III</ref>. We fix n = 4 for the PHStarGANv2 and we initialize A i with the original method <ref type="bibr" target="#b27">[28]</ref>, following the rigid structure of the quaternion product in Eq. 3, and with our proposed approach with random integers in {?1, 0, 1}. With the first initialization (Xavier Init) the PHStarGANv2 can not train some weights due to their values close to 0 and produces almost flat losses. Employing the quaternion initialization (Quat Init), instead, results drastically improve, proving that an integer initialization for matrices A i is a good choice. However, produced samples are sometimes blurred and the LPIPS score is very low. Our RanInteger Init, instead, generates high quality images both visually and as measured by FID score, while gaining the best LPIPS values in latent and reference synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional I2I Experiments</head><p>We perform additional experiments to investigate the generation ability of our approach with different values of the hyperparameter n in different benchmarks. For this purpose, we consider the AFHQ dataset containing 15000 samples with aligned animal faces and three different domains <ref type="bibr" target="#b0">[1]</ref>. As for CelebA-HQ, we conduct both reference and latent analysis and keep the same hyperparameters except for ? ds equal to 2 and for n which we test equal to 2, 3 and 4. Moreover, we discard the support network since it works for human faces only. <ref type="figure">Figure 5</ref> shows results from the former synthesis, where generated samples preserve source pose while translating attributes from the reference image. As for previous experiments, our PHStarGANv2 is capable of learning the proper style from the reference and then of injecting it in the generator network to perform the domain translation. Indeed, generated samples maintain pose and structure of the input image while modifying animal attributes similarly to the reference (for instance, the ears of the last row). As well, our method demonstrates good I2I translation abilities in latent analysis too, whichever the hypercomplex domain we choose, as <ref type="figure">Fig. 6</ref> reports. Therefore, we demonstrate how our approach is flexible to operate in different hypercomplex domains and on diverse I2I benchmarks with multiple translation domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Reference PHStarGANv2 n=2</p><p>PHStarGANv2 n=3</p><p>PHStarGANv2 n=4 <ref type="figure">Fig. 5</ref>. Reference analysis on AFHQ for PHStarGANv2 with n = 2, 3, 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PHStarGANv2 n=3</head><p>PHStarGANv2 n=2</p><p>PHStarGANv2 n=4 <ref type="figure">Fig. 6</ref>. Latent analysis on AFHQ for PHStarGANv2 with n = 2, 3, 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we present a quaternion and various hypercomplex approaches for image-to-image translation (I2I) tasks. We propose to exploit quaternion and hypercomplex algebra for lightweight and more reproducible I2I models able to preserve relations among image channels thus grasping additional information from the input. Moreover, we conceive specific normalization modules for hypercomplex domains and a novel initialization for parameterized hypercomplex neural networks. With an empirical evaluation on multiple benchmarks, we demonstrate that our approaches achieve improved results both qualitatively and quantitatively while saving up to 75% parameters and storage memory for inference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This research was funded by "Progetti di Ricerca" of Sapienza University of Rome under grant numbers RM120172AC5A564C and RG11916B88E1942F.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Different initializations for matrices A i of a generic PHC layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Quaternion StarGANv2 and PHStarGANv2 architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Latent analysis on CelebA-HQ dataset. The source image is in the first column, followed by generated samples from StarGANv2, Quaternion StarGANv2, PHStarGANv2 n = 4 and PHStarGANv2 n = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I QUANTITATIVE</head><label>I</label><figDesc>COMPARISON ON THE CELEBA-HQ DATASET FOR REFERENCE-GUIDED (LEFT) AND LATENT-GUIDED SYNTHESIS (RIGHT).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Reference</cell><cell>Latent</cell></row><row><cell></cell><cell cols="2">Model</cell><cell></cell><cell cols="3">Params Storage Mem Savings ?</cell><cell cols="2">FID ? LPIPS ? FID ?</cell><cell>LPIPS ?</cell></row><row><cell></cell><cell cols="2">StarGANv2</cell><cell></cell><cell>87M</cell><cell>307MB</cell><cell>0%</cell><cell>21.24</cell><cell>0.24</cell><cell>17.16</cell><cell>0.25</cell></row><row><cell></cell><cell cols="3">Quaternion StarGANv2</cell><cell>22M</cell><cell>76MB</cell><cell>75%</cell><cell>23.09</cell><cell>0.22</cell><cell>27.90</cell><cell>0.12</cell></row><row><cell></cell><cell cols="3">PHStarGANv2 n = 3</cell><cell>29M</cell><cell>137MB</cell><cell>67%</cell><cell>28.11</cell><cell>0.29</cell><cell>16.63</cell><cell>0.33</cell></row><row><cell></cell><cell cols="3">PHStarGANv2 n = 4</cell><cell>22M</cell><cell>76MB</cell><cell>75%</cell><cell>24.33</cell><cell>0.27</cell><cell>16.54</cell><cell>0.29</cell></row><row><cell>Source</cell><cell>Reference</cell><cell>StarGANv2</cell><cell>Quaternion</cell><cell>PHStarGANv2</cell><cell>PHStarGANv2</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>StarGANv2</cell><cell>n=3</cell><cell>n=4</cell><cell></cell><cell></cell></row><row><cell cols="6">Fig. 3. Reference analysis on CelebA-HQ dataset. Source image in the</cell><cell></cell><cell></cell></row><row><cell cols="6">first column, then reference image in the second one. Generated samples</cell><cell></cell><cell></cell></row><row><cell cols="6">from StarGANv2, Quaternion StarGANv2, PHStarGANv2 n = 3 and</cell><cell></cell><cell></cell></row><row><cell cols="2">PHStarGANv2 n = 4 follow.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II FID</head><label>II</label><figDesc>IMPROVEMENTS WITH ARCHITECTURAL CHANGES (10K ITER). IN PH LAYERS IN PHSTARGANV2 n = 4.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="4">FID Ref ? FID Lat ?</cell></row><row><cell cols="2">PHStarGANv2 n = 4</cell><cell></cell><cell>46.36</cell><cell>48.73</cell><cell></cell></row><row><cell cols="2">+ HIN &amp; HAdaIN</cell><cell></cell><cell>28.89</cell><cell>29.79</cell><cell></cell></row><row><cell cols="3">+ Last FC layer discriminator</cell><cell>27.76</cell><cell>27.77</cell><cell></cell></row><row><cell></cell><cell></cell><cell>TABLE III</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">INITIALIZATIONS OF A i Reference</cell><cell cols="2">Latent</cell><cell></cell></row><row><cell>Method</cell><cell>FID ?</cell><cell>LPIPS ?</cell><cell>FID ?</cell><cell>LPIPS ?</cell><cell>Note</cell></row><row><cell>Xavier Init [28]</cell><cell>169.05</cell><cell>0.14</cell><cell>218.62</cell><cell>0.00</cell><cell>Flat loss</cell></row><row><cell>Quat Init</cell><cell>19.10</cell><cell>0.11</cell><cell>18.30</cell><cell>0.13</cell><cell>Blurred</cell></row><row><cell>RandInteger Init</cell><cell>24.33</cell><cell>0.27</cell><cell>16.54</cell><cell>0.29</cell><cell>Good</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">StarGAN v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8185" to="8194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">AttGAN: Facial attribute editing by only changing what you want</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5464" to="5478" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">StarGAN: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunghun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jaegul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring explicit domain supervision for latent space disentanglement in unpaired imageto-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1254" to="1266" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Augmented CycleGAN: Learning many-to-many mappings from unpaired data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="195" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-domain correspondence learning for exemplar-based image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5142" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">U-GAT-IT: unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Drit++: Diverse image-to-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distilling portable generative adversarial networks for image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3585" to="3592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">StyleSpace analysis: Disentangled controls for StyleGAN image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">GANs N&apos; Roses: Stable, controllable, diverse image to image translation (works for videos too!)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06561</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Third time&apos;s the charm? image and video editing with StyleGAN3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.13433</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Process. Systems (NIPS)</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representation (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A U-Net based discriminator for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sch?nfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8207" to="8216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey of quaternion neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Linar?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hypercomplex-valued recurrent correlation neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Lobo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">432</biblScope>
			<biblScope unit="page" from="111" to="123" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Quaternion convolutional neural networks for detection and localization of 3D sound events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoust., Speech and Signal Process. (ICASSP)</title>
		<meeting><address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="8533" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient sound event localization and detection in the quaternion domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brignone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grassucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. on Circuits and Systems II: Express Briefs</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="2453" to="2457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Quaternion convolutional neural networks for heterogeneous image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Linar?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoust., Speech and Signal Process. (ICASSP)</title>
		<meeting><address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="8514" to="8518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Quaternion generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grassucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Generative Adversarial Learning: Architectures and Applications</title>
		<editor>R. Razavi-Far, A. Ruiz-Garcia, V. Palade, and J. Schmidhuber</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="57" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A quaternion-valued variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grassucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoust., Speech and Signal Process. (ICASSP)</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An information-theoretic perspective on proper quaternion variational autoencoders</title>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">PHNNs: Lightweight neural networks via parameterized hypercomplex convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grassucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04176</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Compacter: Efficient lowrank hypercomplex adapter layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Mahabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04647</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Parameterized hypercomplex graph neural networks for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>No?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Clevert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16584</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">StyleCLIP: Text-driven manipulation of StyleGAN imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Int. Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2085" to="2094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Quaternion generative adversarial networks for inscription detection in Byzantine monuments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sfikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Giotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Retsinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nikou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition. ICPR International Workshops and Challenges</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="171" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4105" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Compressing deep-quaternion neural networks with targeted regularisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vecchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAAI Trans. Intell. Technol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="172" to="176" />
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A quaternion gradient operator and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Mandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jahanchahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheong Took</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="50" />
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Augmented second-order statistics of quaternion random signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheong</forename><surname>Took</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Mandic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-02" />
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
	<note>Signal Process</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Algebranets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07360</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Quaternion neural networks for 3D sound source localization in reverberant environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Celsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Workshop on Machine Learning for Signal Process</title>
		<meeting><address><addrLine>Espoo, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Quaternion recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Linar?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<meeting><address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Zero initialization: Initializing residual networks with only zeros and ones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sch?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.12661</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rezero is all you need: Fast convergence at large depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Bachlechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Adaptive wing loss for robust face alignment via heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
