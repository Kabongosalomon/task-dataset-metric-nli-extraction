<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CODIFIED AUDIO LANGUAGE MODELING LEARNS USEFUL REPRESENTATIONS FOR MUSIC INFORMATION RETRIEVAL</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Castellon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Donahue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CODIFIED AUDIO LANGUAGE MODELING LEARNS USEFUL REPRESENTATIONS FOR MUSIC INFORMATION RETRIEVAL</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We demonstrate that language models pre-trained on codified (discretely-encoded) music audio learn representations that are useful for downstream MIR tasks. Specifically, we explore representations from Jukebox [1]: a music generation system containing a language model trained on codified audio from 1M songs. To determine if Jukebox's representations contain useful information for MIR, we use them as input features to train shallow models on several MIR tasks. Relative to representations from conventional MIR models which are pre-trained on tagging, we find that using representations from Jukebox as input features yields 30% stronger performance on average across four MIR tasks: tagging, genre classification, key detection, and emotion recognition. For key detection, we observe that representations from Jukebox are considerably stronger than those from models pre-trained on tagging, suggesting that pre-training via codified audio language modeling may address blind spots in conventional approaches. We interpret the strength of Jukebox's representations as evidence that modeling audio instead of tags provides richer representations for MIR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>It is conventional in MIR 1 to pre-train models on large labeled datasets for one or more tasks (commonly tagging), and reuse the learned representations for different downstream tasks <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. Such transfer learning approaches decrease the amount of labeled data needed to perform well on downstream tasks, which is particularly useful in MIR where labeled data for many important tasks is scarce <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Historically-speaking, improvement on downstream tasks is enabled by finding ever-larger sources of labels for pre-training-in chronological order: tags <ref type="bibr" target="#b2">[3]</ref>, metadata <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, and recently, co-listening data <ref type="bibr" target="#b8">[9]</ref>. However, it stands to reason that directly modeling music audio (as opposed to labels) could yield richer repre-sentations. Recently, contrastive learning <ref type="bibr" target="#b12">[13]</ref> has been proposed as an MIR pre-training strategy which learns representations from audio <ref type="bibr" target="#b13">[14]</ref>, but this paradigm has yet to exceed the performance of label-based pre-trained models on downstream tasks.</p><p>Outside of the discriminative MIR landscape, a recent system called Jukebox <ref type="bibr" target="#b0">[1]</ref> demonstrated promising performance for generating music audio. To achieve this result, Jukebox leverages recent architectural developments from natural language processing (NLP) by codifying audio-encoding high-rate continuous audio waveforms into lower-rate discrete sequences which can be fed in directly to NLP models. Specifically, Jukebox trains a Transformer <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> language model, an autoregressive generative model, on codified audio from 1M songs. Purely for convenience, we refer to Jukebox's training procedure as codified audio language modeling (CALM).</p><p>While Jukebox already demonstrates that CALM is useful for music generation, in this work we demonstrate that CALM is also useful as a pre-training procedure for discriminative MIR tasks. To this end, we repurpose Jukebox for MIR by first using it to extract audio feature representations, and then training shallow models (probes <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>) on downstream tasks using these features as input <ref type="figure">(Figure 1</ref>). Relative to representations from models pre-trained with tagging, we find that representations from Jukebox are 30% more effective on average when used to train probes on four downstream MIR tasks: tagging, genre classification, key detection, and emotion recognition. We also observe that representations from Jukebox are much more useful for key detection than those from models pre-trained on tagging, which suggests that CALM pre-training may be particularly beneficial for tasks which have little to do with tagging. This simple setup of training shallow models on representations from Jukebox is even competitive with purpose-built state-of-the-art methods on several tasks.</p><p>To facilitate reproducibility and encourage further investigation of these representations and tasks <ref type="bibr" target="#b10">[11]</ref>, we release all of our code for this project, alongside images for Docker containers which provide full provenance for our experiments. <ref type="bibr" target="#b1">2</ref> We note that, while CALM pre-training at the scale of Jukebox requires substantial computational resources, our post hoc experiments with Jukebox only require a single commodity GPU with 12 GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1.</head><p>Conventional MIR pre-training (left) trains convolutional neural networks on audio spectrograms using manually-annotated labels from tagging datasets. In contrast, CALM MIR pre-training (middle) involves training a language model on codified audio, which has been previously explored for music generation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1]</ref>-here, we propose to use it for discriminative MIR tasks. To determine if CALM pre-training is effective for MIR, we probe for information about particular MIR tasks (right) in resultant representations. Specifically, we extract features from the learned language model for the audio in small, task-specific labeled datasets, and use these features to train shallow probing models on each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CALM PRE-TRAINING</head><p>CALM was first proposed by van den Oord et al. and used for unconditional speech generation <ref type="bibr" target="#b19">[20]</ref>. As input, CALM takes a collection of raw audio waveforms (and optionally, conditioning metadata), and learns a distribution p(audio | metadata). To this end, CALM adopts a threestage approach: (1) codify a high-rate continuous audio signal into lower-rate discrete codes, (2) train a language model on the resulting codified audio and optional metadata, i.e., learn p(codified audio | metadata), and (3) decode sequences generated by the language model to raw audio. <ref type="bibr" target="#b2">3</ref> The original paper <ref type="bibr" target="#b19">[20]</ref> also proposed a strategy for codifying audio called the vector-quantized variational auto-encoder (VQ-VAE), and the language model was a WaveNet <ref type="bibr" target="#b20">[21]</ref>. Within music, CALM was first used by Dieleman et al. for unconditional piano music generation <ref type="bibr" target="#b16">[17]</ref>, and subsequently, Dhariwal et al. used CALM to build a music generation system called Jukebox <ref type="bibr" target="#b0">[1]</ref> with conditioning on genre, artist, and optionally, lyrics.</p><p>Despite promising results on music audio generation, CALM has not yet been explored as a pre-training strategy for discriminative MIR. We suspect that effective music audio generation necessitates intermediate representations that would also contain useful information for MIR. This hypothesis is further motivated by an abundance of previous work in NLP suggesting that generative and selfsupervised pre-training can yield powerful representations for discriminative tasks <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>.</p><p>To explore this potential, we repurpose Jukebox for MIR. While Jukebox was designed only for generation, its internal language model was trained on codified audio from a corpus of 1.2M songs from many genres and <ref type="bibr" target="#b2">3</ref> This third stage is not necessary for transfer learning.</p><p>artists, making its representations potentially suitable for a multitude of downstream MIR tasks. Jukebox consists of two components-the first is a small (2M parameters) VQ-VAE model <ref type="bibr" target="#b19">[20]</ref> that learns to codify high-rate (44.1 kHz), continuous audio waveforms into lower-rate (?345 Hz), discrete code sequences with a vocabulary size of 2048 (11 bits). The second component is a large (5B parameters) language model that learns to generate codified audio using a Transformer decoder-an architecture originally designed for modeling natural language <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. By training on codified audio (as in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1]</ref>) instead of raw audio (as in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b15">16]</ref>), language models are (empirically) able to learn longer-term structure in music, while simultaneously using significantly less memory to model the same amount of audio.</p><p>Like conventional MIR models which pre-train on tagging and/or metadata, Jukebox also makes use of genre and artist labels during training, providing them as conditioning information to allow for increased user control over the music generation process. Hence, while CALM in general is an unsupervised strategy that does not require labels, transfer learning from Jukebox specifically should not be considered an unsupervised approach (especially for downstream tasks like genre detection). However, by modeling the audio itself instead of modeling the labels (as in conventional MIR pre-training), we hypothesize that Jukebox learns richer representations for MIR tasks than conventional strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXTRACTING SUITABLE REPRESENTATIONS FROM JUKEBOX</head><p>Here we describe how we extract audio representations from Jukebox which are suitable as input features for  training shallow models. While several pre-trained Jukebox models exist with different sizes and conditioning information, here we use the 5B-parameter model without lyrics conditioning (named "5b"), which is a sparse transformer <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> containing 72 layers. Each layer yields 4800-dimensional activations for each element in the codified audio sequence, i.e., approximately 345 times per second. To extract representations from this model for a particular audio waveform, we (1) resample the waveform to 44.1kHz, (2) normalize it, (3) codify it using the Jukebox VQ-VAE model, and (4) input the codified audio into the language model, interpreting its layer-wise activations as representations. Jukebox was trained on ?24-second audio clips (codified audio sequences of length 8192)-we feed in this same amount of audio at a time when extracting representations. In addition to the genre and artist conditioning fields mentioned previously, Jukebox expects two additional fields: total song length and clip offset-to ensure that representations only depend on the input audio, we simply pass in "unknown" for artist and genre, one minute for song length, and zero seconds for clip offset. <ref type="bibr" target="#b3">4</ref> The Jukebox language model yields an unwieldy amount of data-for every 24-second audio clip, it emits 24 ? 345 ? 72 ? 4800 numbers, i.e., over 10GB if stored naively as 32-bit floating point. We reduce the amount of data by mean pooling across time, a common strategy in MIR transfer learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>, which aggregates more than 10GB of activations to around 1MB (72 ? 4800).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Layer selection</head><p>While pooling across time dramatically reduced the dimensionality of Jukebox's outputs, training shallow classifiers on 72 ? 4800 features is still computationally expensive. To further reduce the dimensionality, we use only one of the layers from Jukebox-the middle layer (36)yielding a total of 4800 features per 24 second audio clip. <ref type="bibr" target="#b3">4</ref> We observed in initial experiments that passing in ground-truth conditioning information had little effect on downstream performance. Hence, we elected to pass in placeholder metadata to maintain the typical type signature for audio feature extraction (audio as the only input).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>Size Metrics #Out</p><p>Tagging <ref type="bibr" target="#b30">[31]</ref> 25860 AUC/AP 50 Genre classification <ref type="bibr" target="#b31">[32]</ref> 930 Accuracy 10 Key detection <ref type="bibr" target="#b32">[33]</ref> 1763 Score 24 Emotion recognition <ref type="bibr" target="#b33">[34]</ref> 744 A/V R 2 2 <ref type="table">Table 1</ref>. Basic information about the four tasks we consider in this work, including the size of each task-specific dataset in terms of number of labeled examples, relevant metrics for each task, and the number of model outputs required for each dataset.</p><p>Unlike conventional pre-training, where the strongest representations for transfer learning typically lie at the end of the model <ref type="bibr" target="#b25">[26]</ref>, the strongest representations from pretrained language models tend to lie towards the middle of the network <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>. To confirm this observation in our context, we trained linear models using representations from different layers of Jukebox on our downstream MIR tasks-average performance indeed peaked at the middle layers ( <ref type="figure" target="#fig_1">Figure 2</ref>). In addition to using the middle layer, we experimented with two other layer selection strategies: (1) sub-sampling layers across the network, and (2) selecting relevant layers in a task-specific fashion. <ref type="bibr" target="#b4">5</ref> We found that the simplest strategy of using only the middle layer was equally effective and more computationally practical 6 than the other two layer selection strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DOWNSTREAM TASK DESCRIPTIONS</head><p>We select four downstream MIR tasks to constitute a benchmark for comparing different audio feature representations: (1) tagging, (2) genre classification, (3) key detection, and (4) emotion recognition. A summary of the datasets used for each task appears in <ref type="table">Table 1</ref>. These tasks were selected to cover a wide range of dataset sizes (744 examples for emotion recognition vs. 26k examples for tagging) and subjectivity (emotion recognition is more subjective vs. key detection is more objective). Additionally, each task has an easily-accessible dataset with standard evaluation criteria. We describe each of these tasks and metrics below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tagging</head><p>Tagging involves determining which tags from a fixed set of tags apply to a particular song. Categories of tags include genre (e.g., jazz), instrumentation (e.g., violin), emotions (e.g., happy), and characteristics (e.g., fast). There are two large datasets for tagging, which both contain human-annotated tags for 30-second clips: MagnaTa-gATune <ref type="bibr" target="#b30">[31]</ref> (MTT) which contains around 26k clips, and a tagged subset of 240k clips from the Million Song Dataset <ref type="bibr" target="#b34">[35]</ref> (MSD). While both datasets contain a large vocabulary of tags, typical usage involves limiting the vocabulary to the 50 most common tags in each.</p><p>Because it is the largest non-proprietary MIR dataset, MSD is commonly used for pre-training models for transfer learning. To mitigate an unfair advantage of methods which pre-train on MSD, we use MTT instead of MSD to benchmark representations on tagging performance. While both datasets are superficially similar (choosing from 50 tags for 30-second clips), their label distributions are quite different: MSD is skewed towards genre tags, while MTT is skewed towards instrumentation tags.</p><p>We use the standard (12:1:3) train, validation, and test split for MTT <ref type="bibr" target="#b2">[3]</ref>. Additionally, we report both common metrics (both are macro-averaged over tags as is conventional): area under the receiver operating characteristic curve (MTT AUC ), and average precision (MTT AP ). <ref type="bibr" target="#b6">7</ref> We note that inconsistencies in handling unlabeled examples for past work on MTT have been observed <ref type="bibr" target="#b35">[36]</ref>-some work discards examples without top-50 tags during training, evaluation, or both. In this work, we do not discard any examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Genre classification</head><p>Genre classification involves assigning the most appropriate genre from a fixed list for a given song. For this task, we report accuracy on the GTZAN dataset <ref type="bibr" target="#b36">[37]</ref>, which contains 30-second clips from 10 distinct genres. We adopt the "fault-filtered" split from <ref type="bibr" target="#b31">[32]</ref> which addresses some of the reported issues with this dataset <ref type="bibr" target="#b37">[38]</ref>. We note that this task has a high degree of overlap with tagging, as tagging datasets typically have a number of genres within their tag vocabulary. In fact, seven of ten genres in GTZAN are present in the tag list of MSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Key detection</head><p>Key detection involves predicting both the scale and tonic pitch class for the underlying key of a song. We investigate the Giantsteps-MTG and Giantsteps datasets <ref type="bibr" target="#b32">[33]</ref> which include songs in major and minor scales for all pitch classes, i.e., a 24-way classification task. As in past work <ref type="bibr" target="#b38">[39]</ref>, we use the former for training and the latter for testing. Because no standard validation split exists for Giantsteps-MTG, we follow <ref type="bibr" target="#b31">[32]</ref> and create an artiststratified 4:1 split for training and validation, which we include in our codebase for reproducibility. The music in this dataset is all electronic dance music, and the clips are two minutes in length. We report the typical weighted score metric for Giantsteps (GS): an accuracy measure which gives partial credit for reasonable mistakes such as predicting the relative minor key for the major ground truth <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Emotion recognition</head><p>Emotion recognition involves predicting human emotional response to a song. Data is collected by asking hu- <ref type="bibr" target="#b6">7</ref> Most past work refers to the quantity of average precision as area under the precision-recall curve.  <ref type="bibr" target="#b2">[3]</ref> 4194 CLMR <ref type="bibr" target="#b13">[14]</ref> Contrastive <ref type="bibr" target="#b12">[13]</ref> 512 JUKEBOX <ref type="bibr" target="#b0">[1]</ref> CALM <ref type="bibr" target="#b19">[20]</ref> 4800 <ref type="table">Table 2</ref>. Basic statistics about the six representations we examine in this work.</p><p>mans to report their emotional response on a two dimensional valence-arousal plane <ref type="bibr" target="#b40">[41]</ref>, where valence indicates positive versus negative emotional response, and arousal indicates emotional intensity. We use the Emomusic dataset <ref type="bibr" target="#b33">[34]</ref>, which contains 744 clips of 45 seconds in length. We investigate the static version of this task where original time-varying annotations are averaged together to constitute a clip-level annotation. Because this dataset does not have a standard split, it is difficult to directly compare with past work. To simplify comparison going forward, we created an artist-stratified split of Emomusic, which is released in our codebase. We take the highest reported numbers from past work to characterize "state-of-the-art" performance, though we note that these numbers are not directly comparable to our own due to differing splits. We report the coefficient of determination between the model predictions and human annotations for arousal (Emo A ) and valence (Emo V ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">PROBING EXPERIMENTS</head><p>Here we describe our protocol for probing for information about MIR tasks in representations from Jukebox and other pre-trained models, i.e., measuring performance of shallow models trained on these tasks using different representations as input features. We borrow the term "probing" from analogous investigations in NLP <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, however such methodology is common in transfer learning for MIR [2-5, 7-10].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Descriptions of representations</head><p>In addition to probing representations from Jukebox (an exemplar of CALM pre-training), we probe four additional representations which are emblematic of three other MIR pre-training strategies ( <ref type="table">Table 2</ref>). Before pre-training, handcrafted features were commonplace in MIR-as archetypal examples, we probe constant-Q chromagrams (CHROMA) and Mel-frequency cepstral coefficients (MFCC), extracted with librosa <ref type="bibr" target="#b47">[49]</ref> using the default settings. As in <ref type="bibr" target="#b3">[4]</ref>, we concatenate the mean and standard deviation across time of both the features and their first-and secondorder discrete differences. We also probe two examples of the current conventional paradigm which pre-trains on tagging using MSD: a convolutional model proposed by Choi et al. <ref type="bibr" target="#b3">[4]</ref> (CHOI), and a more modern convolutional model from <ref type="bibr" target="#b7">[8]</ref> (MUSICNN). Finally, we com- For all six metrics, the max score is 100 and higher is better-see Section 4 for a full description of tasks/metrics. For each metric, the best probing-based approach and the best approach overall are bolded. We also report an average score across all four tasks; tasks with multiple evaluation metrics are averaged beforehand. On all metrics, probing JUKEBOX is more effective than probing representations from other pre-trained models. Probing JUKEBOX is competitive with task-specific state-of-the-art approaches for all tasks/metrics except key detection (GS). Note that the ordering of citations in the bottom section corresponds to respective column ordering. * indicates that past work on Emomusic evaluates on different subsets of the dataset than our work and hence numbers are not directly comparable-see Section 4.4 for details.</p><p>pare to a recently-proposed strategy for MIR pre-training called contrastive learning of musical representations <ref type="bibr" target="#b13">[14]</ref> (CLMR), though we note that the only available pretrained model from this work was trained on far less audio (a few thousand songs) than the other pre-trained models (CHOI, MUSICNN, and JUKEBOX).</p><p>All of these strategies operate at different frame rates, i.e., they produce a different number of representation vectors for a fixed amount of input audio. To handle this, we follow common practice of mean pooling representations across time <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>. While CHROMA, MFCC, and CLMR produce a single canonical representation per frame, we note that the other three produce multiple representations per frame, i.e., the outputs of individual layers in each model. For CHOI, we concatenate all layer representations together, which was shown to have strong performance on all downstream tasks in <ref type="bibr" target="#b3">[4]</ref>. For MUSICNN, we concatenate together the mean and max pool of threesecond windows (before mean pooling across these windows), i.e., the default configuration for that approach. For JUKEBOX, we use the middle layer of the network as motivated in Section 3.1. By using a single layer, we also mitigate the potential of a superficial dimensionality advantage for JUKEBOX, as this induces a dimensionality similar to that of MUSICNN (4800 and 4194 respectively; see <ref type="table">Table 2</ref>).</p><p>Unlike other representations which operate on short context windows, CHOI and JUKEBOX were trained on long windows of 29 seconds and 24 seconds of audio respectively. Accordingly, for the three datasets with short clips (tagging, genre classification, and emotion recognition all have clips between 30 and 45 seconds in length), we adopt the policy from <ref type="bibr" target="#b3">[4]</ref> and simply truncate the clips to the first window when computing representations for CHOI and JUKEBOX. Because clips from the key detection dataset are much longer (two minutes), we split the clips into 30-second windows for all methods and train probes on these shorter windows. At test time, we ensemble window-level predictions into clip-level predictions before computing the score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Probing protocol</head><p>To probe representations for relevant information about downstream MIR tasks, we train shallow supervised models (linear models and one-layer MLPs) on each task using these representations as input features. As some representations may require different hyperparameter configurations for successful training, we run a grid search over the following hyperparameters (216 total configurations) for each representation and task (24 total grid searches), using early stopping based on task-specific metrics computed on the validation set of each task:</p><p>? While we use this same hyperparameter grid for all tasks, the learning objective varies by task (cross-entropy for genre classification and key detection, independent binary cross-entropy per tag for tagging, and mean squared error for emotion recognition) as does the number of probe outputs <ref type="table">(Table 1</ref>). Some tasks have multiple metrics-we early stop on MTT AUC for tagging as it is a more com-mon metric than MTT AP , and on the average of Emo A and Emo V for emotion recognition. We take the model with the best early stopping performance from each grid search and compute its performance on the task-specific test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RESULTS AND DISCUSSION</head><p>In <ref type="table">Table 3</ref>, we report performance of all representations on all tasks and metrics, as well as average performance across all tasks. Results are indicative that CALM is a promising paradigm for MIR pre-training. Specifically, we observe that probing the representations from JUKEBOX (learned through CALM pre-training) achieves an average of 69.9, which is 30% higher relative to the average of the best representation pre-trained with tagging (MUSICNN achieves an average of 53.7). Performance of JUKEBOX on all individual metrics is also higher than that of any other representation. Additionally, JUKEBOX achieves an average performance that is 38% higher than that of CLMR. Representations from all pre-trained models outperform hand-crafted features (CHROMA and MFCC) on average. Note that these results are holistic comparisons across different model architectures, model sizes, and amounts of pre-training data (e.g., CLMR was trained on far less data than JUKEBOX), and hence not sufficient evidence to claim that CALM is the "best" music pre-training strategy in general.</p><p>We also observe that JUKEBOX contains substantially more information relevant for key detection than other representations. While CHROMA (spectrogram projected onto musical pitch classes) contains information relevant to key detection by design, all other representations besides JUKEBOX yield performance on par with that of a majority classifier (outputting "F minor" for every example scores 15.0)-hence, these representations contain almost no information about this task. For models pre-trained with tagging (CHOI and MUSICNN), intuition suggests that this is because none of the tags in MSD relate to key signature. For CLMR, we speculate that the use of transposition as a data augmentation strategy also results in a model that contains little useful information about key signature. While tagging and CLMR were not designed with the intention of supporting transfer to key detection, we argue that it is generally desirable to have a unified music representation which performs well on a multitude of downstream MIR tasks. Hence, we interpret the comparatively stronger performance of JUKEBOX on key detection as evidence that CALM pre-training addresses blind spots present in other MIR pre-training paradigms.</p><p>In the bottom section of <ref type="table">Table 3</ref>, we also report state-ofthe-art performance for purpose-built methods on all tasks, which is further broken down by models which use any form of pre-training (including pre-training on additional task-specific data as in <ref type="bibr" target="#b45">[47]</ref>) vs. ones that are trained from scratch. Surprisingly, we observe that probing JUKEBOX is competitive with state-of-the-art for all tasks except for key detection, and achieves an average only 4% lower relative to that of state-of-the-art. On tagging, probing JUKEBOX achieves similar MTT AUC to a strategy which pre-trains on a proprietary dataset of 10M songs using supervision <ref type="bibr" target="#b8">[9]</ref>. We interpret the strong performance of this simple probing setup as evidence that CALM pre-training is a promising path towards models that are useful for many MIR tasks.</p><p>We believe that CALM pre-training is promising for MIR not just because of the strong performance of an existing pre-trained model (Jukebox), but also because there are numerous avenues which may yield further improvements for those with the data and computational resources to explore them. Firstly, CALM could be scaled up to pre-train even larger models on more data (Jukebox was trained on 1M songs, while Spotify has an estimated 70M songs in its catalog). In <ref type="bibr" target="#b48">[50]</ref>, it is observed that increasing model and dataset size yields predictable improvements to cross-entropy for language modeling in NLP, an insight which may also hold for CALM pre-training for MIR. Secondly, we anticipate that fine-tuning a model pre-trained with CALM would outperform our probing setup. Finally, taking a cue from related findings in NLP, we speculate that CALM pre-training with a bidirectional model and masked language modeling (as in BERT <ref type="bibr" target="#b22">[23]</ref>) would outperform the generative setup of Jukebox (that of GPT <ref type="bibr" target="#b49">[51]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">RELATED WORK</head><p>Transfer learning has been an active area of study in MIR for over a decade. An early effort seeking to replace hand-crafted features used neural networks to automatically extract context-independent features from unlabeled audio <ref type="bibr" target="#b50">[52]</ref> and used those features for a supervised learning task. Other early efforts focused on learning shared embedding spaces between audio and metadata <ref type="bibr" target="#b51">[53,</ref><ref type="bibr" target="#b1">2]</ref> or directly using outputs from pre-trained tagging models for music similarity judgements <ref type="bibr" target="#b52">[54]</ref>.</p><p>The predominant strategy for MIR pre-training using large tagging datasets was first proposed by van den Oord et al. 2014 <ref type="bibr" target="#b2">[3]</ref>. This work pre-trained deep neural networks on MSD and demonstrated promising performance on other tagging and genre classification tasks. Choi et al. 2017 <ref type="bibr" target="#b3">[4]</ref> pre-trained on MSD but using a convolutional neural network and also explored a more diverse array of downstream tasks-we use their pre-trained model as one of our baselines. More recent improvements use the same approach with different architectures <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>, the latest of which is another one of our baselines.</p><p>Other strategies for MIR transfer learning have been proposed. Some work pre-trains on music metadata (e.g., artist, album) instead of tags <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>. In contrast to the manual annotations required for tagging-based pre-training, metadata is much cheaper to obtain, but performance of pre-training on metadata is comparable to that of pretraining on tagging. Kim et al. 2020 <ref type="bibr" target="#b9">[10]</ref> improve over <ref type="bibr" target="#b3">Choi et al. 2017</ref> [4] using a multi-task approach that pretrains on both tags and metadata. Huang et al. <ref type="bibr" target="#b8">[9]</ref> demonstrate that metadata can be combined with proprietary colistening data for pre-training on 10M songs to achieve state-of-the-art performance on MTT-probing representations from CALM pre-training on 1M songs achieves comparable performance on MTT <ref type="table">(Table 3</ref>). Finally, con-trastive learning <ref type="bibr" target="#b12">[13]</ref> has been proposed as a strategy for MIR pre-training <ref type="bibr" target="#b53">[55,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b13">14]</ref>-we compare to such a model from Spijkervet and Burgoyne 2021 <ref type="bibr" target="#b13">[14]</ref>.</p><p>While CALM has not previously been explored for MIR transfer learning, it has been explored for other purposes. van den Oord et al. 2017 <ref type="bibr" target="#b19">[20]</ref> first proposed CALM and used it for unconditional speech generation. Variations of CALM have been used as pre-training for speech recognition <ref type="bibr" target="#b55">[57,</ref><ref type="bibr" target="#b56">58]</ref> and urban sound classification <ref type="bibr" target="#b57">[59]</ref>. CALM has also been explored for music generation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1]</ref>. CALM is related to past work on language modeling of raw (i.e., not codified) waveforms <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b58">60,</ref><ref type="bibr" target="#b59">61]</ref>, which tends to be less effective for capturing long-term dependencies compared to modeling codified audio. Language models have also been used extensively for modeling symbolic music <ref type="bibr" target="#b60">[62]</ref><ref type="bibr" target="#b61">[63]</ref><ref type="bibr" target="#b62">[64]</ref>, including some work on pre-training on large corpora of scores for transfer learning <ref type="bibr" target="#b63">[65,</ref><ref type="bibr" target="#b64">66]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION</head><p>In this work we demonstrated that CALM is a promising pre-training strategy for MIR. Compared to conventional approaches, CALM learns richer representations by modeling audio instead of labels. Moreover, CALM allows MIR researchers to repurpose NLP methodologyhistorically, repurposing methodology from another field (computer vision) has provided considerable leverage for MIR. Finally, CALM suggests a direction for MIR research where enormous models pre-trained on large music catalogs break new ground on MIR tasks, analogous to ongoing paradigm shifts in other areas of machine learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Normalized validation performance of linear models trained on representations from specific layers of Jukebox across four downstream MIR tasks. On average, the strongest representations for these tasks come from the middle of Jukebox.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">This procedure selected layers that were the most jointly informative in a greedy fashion, measured by task performance with a linear probe.<ref type="bibr" target="#b5">6</ref> While the entirety of Jukebox does not fit on a single commodity GPU with 12GB memory, the first 36 layers do fit.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">ACKNOWLEDGEMENTS</head><p>We would like to thank Nelson Liu, Mina Lee, John Hewitt, Janne Spijkervet, Minz Won, Jordi Pons, Ethan Chi, Michael Xie, Ananya Kumar, and Glen Husman for helpful conversations about this work. We also thank all reviewers for their helpful feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00341</idno>
		<title level="m">Jukebox: A generative model for music</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Transfer learning in MIR: Sharing learned latent representations for music audio classification and similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoshii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Transfer learning by supervised pre-training for audio-based music classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Transfer learning for music classification and regression tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Representation learning of music using artist labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06648</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SampleCNN: End-to-end deep convolutional neural networks using very small filters for music classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Representation learning of music using artist, album, and track information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.11783</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">musicnn: Pre-trained convolutional neural networks for music audio tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR Late-breaking Demos</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale weaklysupervised content embeddings for music recommendation and tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">One deep music representation to rule them all? A comparative analysis of different representation learning strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Urbano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Liem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Open-source practices for music signal processing research: Recommendations for transparent, sustainable, and reproducible audio research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cartwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Data usage in MIR: history &amp; future recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moriarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Villalobos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spijkervet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Burgoyne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09410</idno>
		<title level="m">Contrastive learning of musical representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The challenge of realistic music generation: modelling raw audio at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Understanding intermediate layers using linear classifier probes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01644</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visualisation and &apos;diagnostic classifiers&apos; reveal how recurrent and recursive neural networks process hierarchical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Veldhoen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00937</idno>
		<title level="m">Neural discrete representation learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">WaveNet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<title level="m">GPT understands, too</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08855</idno>
		<title level="m">Linguistic knowledge and transferability of contextual representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Finding universal grammatical relations in multilingual bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A primer in BERTology: What we know about how BERT works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evaluation of algorithms using games: The case of music tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Downie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning and music adversaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kereliuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Two data sets for tempo estimation and key detection in electronic dance music annotated from user corrections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Knees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vogl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>B?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>H?rschl?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Le</forename><surname>Goff</surname></persName>
		</author>
		<editor>IS-MIR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">1000 songs for emotional analysis of music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Caro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Workshop on Crowdsourcing for Multimedia</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The Million Song Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bertin-Mahieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Whitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamere</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Evaluation of CNN-based automatic music tagging models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bogdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00751</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Musical genre classification of audio signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The GTZAN dataset: its contents, its faults, their effects on evaluation, and its future use</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Sturm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.1461</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-to-end musical key estimation using a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Korzeniowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Signal Processing Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">mir_eval: A transparent implementation of common mir metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<editor>IS-MIR</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automated music emotion recognition: A systematic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of New Music Research</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">What you can cram into a single vector: Probing sentence embeddings for linguistic properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01070</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A structural probe for finding syntax in word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On-line continuous-time music mood regression with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Comparison and analysis of deep audio embeddings for music emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dubnov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06517</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">MIREX 2019 submission: Crowd annotation for audio key estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Carlton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">MIREX</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Masked conditional neural networks for audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Medhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chesmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">librosa: Audio and music signal analysis in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Python in Science Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning features from music audio with deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multi-tasking with joint semantic spaces for large-scale music annotation and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of New Music Research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">From improved auto-taggers to improved music similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seyerlehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schedl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sonnleitner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hauger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Adaptive Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Learning contextual tag embeddings for cross-modal alignment of audio and tags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Drossos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14171</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Enriched music representations with multiple cross-modal contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Drossos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bogdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Effectiveness of self-supervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03912</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11477</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">A framework for contrastive and generative learning of audio representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11459</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">SampleRNN: An unconditional end-to-end neural audio generation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Efficient neural audio synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Finding temporal structure in music: Blues improvisation with LSTM recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Neural Networks for Signal Processing</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Performance RNN: Generating music with expressive timing and dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oore</surname></persName>
		</author>
		<ptr target="https://magenta.tensorflow.org/performance-rnn" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Music transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Z</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">LakhNES: Improving multi-instrumental music generation with cross-domain pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Improving automatic jazz melody generation by transfer learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia-Pacific Signal and Information Processing Association Annual Summit and Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
