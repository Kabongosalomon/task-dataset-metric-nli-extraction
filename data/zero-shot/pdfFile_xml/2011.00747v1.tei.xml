<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual-decoder Transformer for Joint Automatic Speech Recognition and Multilingual Speech Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<country>LIG</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Schwab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<country>LIG</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
							<email>laurent.besacier@univ-grenoble-alpes.frjuancarabina</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<country>LIG</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dual-decoder Transformer for Joint Automatic Speech Recognition and Multilingual Speech Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce dual-decoder Transformer, a new model architecture that jointly performs automatic speech recognition (ASR) and multilingual speech translation (ST). Our models are based on the original Transformer architecture <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref> but consist of two decoders, each responsible for one task (ASR or ST). Our major contribution lies in how these decoders interact with each other: one decoder can attend to different information sources from the other via a dual-attention mechanism. We propose two variants of these architectures corresponding to two different levels of dependencies between the decoders, called the parallel and cross dual-decoder Transformers, respectively. Extensive experiments on the MuST-C dataset show that our models outperform the previously-reported highest translation performance in the multilingual settings, and outperform as well bilingual one-to-one results. Furthermore, our parallel models demonstrate no trade-off between ASR and ST compared to the vanilla multi-task architecture. Our code and pre-trained models are available at https://github.com/formiel/speech-translation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While cascade speech-to-text translation (ST) systems operate in two steps: source language automatic speech recognition (ASR) and source-to-target text machine translation (MT), recent works have attempted to build end-to-end ST without using source language transcription during decoding <ref type="bibr" target="#b3">(B?rard et al., 2016;</ref><ref type="bibr" target="#b36">Weiss et al., 2017;</ref><ref type="bibr" target="#b4">B?rard et al., 2018)</ref>. After two years of extensions to these pioneering works, the last results of the IWSLT 2020 shared task on offline speech translation <ref type="bibr" target="#b1">(Ansari et al., 2020)</ref> demonstrate that end-to-end models are now on par (if not better) than their cascade counterparts. Such a finding motivates even more strongly the works on multilingual (one-to-many, many-to-one, many-tomany) ST <ref type="bibr" target="#b11">Inaguma et al., 2019;</ref><ref type="bibr" target="#b33">Wang et al., 2020a)</ref> for which end-to-end models are well adapted by design. Moreover, of these two approaches: cascade proposes a very loose integration of ASR and MT (even if lattices or word confusion networks were used between ASR and MT before end-to-end models appeared) while most end-to-end approaches simply ignore ASR subtask, trying to directly translate from source speech to target text. We believe that these are two edge design choices and that a tighter coupling of ASR and MT is desirable for future end-to-end ST applications, in which the display of transcripts alongside translations can be beneficial to the users <ref type="bibr">(Sperber et al., 2020)</ref>. This paper addresses multilingual ST and investigates more closely the interactions between speech transcription (ASR) and speech translation (ST) in a multilingual end-to-end architecture based on Transformer. While those interactions were previously investigated as a simple multi-task framework for a bilingual case <ref type="bibr" target="#b0">(Anastasopoulos and Chiang, 2018)</ref>, we propose a dual-decoder with an ASR decoder tightly coupled with an ST decoder and evaluate its effectiveness on one-to-many ST. Our model is inspired by , but the interaction between ASR and ST decoders is much tighter. <ref type="bibr">1</ref> Finally, experiments show that our model outperforms theirs on the MuST-C benchmark .</p><p>Our contributions are summarized as follows: (1) a new model architecture for joint ASR and multilingual ST; (2) an integrated beam search decoding strategy which jointly transcribes and translates, and that is extended to a wait-k strategy where the ASR hypothesis is ahead of the ST hypothesis by k tokens and vice-versa; and (3) competitive performance on the MuST-C dataset in both bilingual and multilingual settings and improvements on previous joint ASR/ST work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multilingual ST Multilingual translation <ref type="bibr" target="#b14">(Johnson et al., 2016)</ref> consists in translating between different language pairs with a single model, thereby improving maintainability and the quality of low resource languages.  adapt this method to one-to-many multilingual speech translation by adding a language embedding to each source feature vector. They also observe that using the source language (English) as one of the target languages improves performance. <ref type="bibr" target="#b11">Inaguma et al. (2019)</ref> simplify the previous approach by prepending a target language token to the decoder and apply it to one-to-many and many-to-many speech translation. They do not investigate many-to-one due to the lack of a large corpus for this. To fill this void, <ref type="bibr" target="#b33">Wang et al. (2020a)</ref> release the CoVoST dataset for ST from 11 languages into English and demonstrate the effectiveness of many-to-one ST.</p><p>Joint ASR and ST Joint ASR and ST decoding was first proposed by <ref type="bibr" target="#b0">Anastasopoulos and Chiang (2018)</ref> through a multi-task learning framework. <ref type="bibr" target="#b6">Chuang et al. (2020)</ref> improve multitask ST by using word embedding as an intermediate level instead of text. A two-stage model that performs first ASR and then passes the decoder states as input to a second ST model was also studied previously <ref type="bibr" target="#b0">(Anastasopoulos and Chiang, 2018;</ref><ref type="bibr" target="#b28">Sperber et al., 2019)</ref>. This architecture is closer to cascaded translation while maintaining end-to-end trainability. <ref type="bibr">Sperber et al. (2020)</ref> introduce the notion of consistency between transcripts and translations and propose metrics to gauge it. They evaluate different model types for the joint ASR and ST task and conclude that end-to-end models with coupled inference procedure are able to achieve strong consistency. In addition to existing models having coupled architectures, they also investigate a model where the transcripts are concatenated to the translations, and the shared encoder-decoder network learns to predict this concatenated outputs. It should be noted that our models have lower latency compared to this approach since the concatenation of outputs makes the two tasks sequential in nature. Our work is closely related to that of  who propose an interactive attention mechanism which enables ASR and ST to be performed synchronously. Both ASR and ST decoders do not only rely on their previous outputs but also on the outputs predicted in the other task. We highlight three differences between their work and ours: (a) we propose a more general framework in which ) is a special case; (b) tighter integration of ASR and ST is proposed in our work; and (c) we experiment in a multilingual ST setting while previous works on joint ASR and ST only investigated bilingual ST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dual-decoder Transformer for Joint ASR and Multilingual ST</head><p>We now present the proposed dual-decoder Transformer for jointly performing ASR and multilingual ST. Our models are based on the Transformer architecture <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref> but consist of two decoders. Each decoder is responsible for one task (ASR or ST). The intuition is that the problem at hand consists in solving two different tasks with different characteristics and different levels of difficulty (multilingual ST is considered more difficult than ASR). Having different decoders specialized in different tasks may thus produce better results. In addition, since these two tasks can be complementary, it is natural to allow the decoders to help each other. Therefore, in our models, we introduce a dual-attention mechanism: in addition to attending to the encoder, the decoders also attend to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model overview</head><p>The model takes as input a sequence of speech features x = (x 1 , x 2 , . . . , x Tx ) in a specific source language (e.g. English) and outputs a transcription y = (y 0 , y 1 , . . . , y Ty ) in the same language as well as translations z 1 , z 2 , . . . , z M in M different target languages (e.g. French, Spanish, etc.). When M = 1, this corresponds to joint ASR and bilingual ST . For simplicity, our presentation considers only a single target language with output z = (z 0 , z 1 , . . . , z Tz ). All results, however, apply to the general multilingual case. In the sequel, denote y &lt;t (y 0 , y 1 , . . . , y t?1 ) and y &gt;t (y t+1 , y t+2 , . . . , y Ty ) (y t is included if "&lt;" and "&gt;" are replaced by "?" and "?" respectively). In addition, assume that y t is ignored if t is outside of the interval [0, T y ]. Notations apply to z as well.</p><p>The dual-decoder model jointly predicts the transcript and translation in an autoregressive fashion:</p><formula xml:id="formula_0">p(y, z | x) = max(Ty,Tz) t=0 p(y t , z t | y &lt;t , z &lt;t , x).</formula><p>(1)</p><p>A natural model would consist of a single decoder followed by a softmax layer. However, even if the capacity of the decoder were large enough for handling both ASR and ST generation, a single softmax would require a very large joint vocabulary (with size V y V z where V y and V z are respectively the vocabulary sizes for y and z). Instead, our dual-decoder consists of two sub-decoders that are specialized in producing outputs tailored to the ASR and ST tasks separately. Formally, our model predicts the next output tokens (? s ,? t ) (where 1 ? s ? T y , 1 ? t ? T z ) given a pair of previous outputs (y &lt;s , z &lt;t ) as:</p><formula xml:id="formula_1">h y s , h z t = DECODER dual (y &lt;s , z &lt;t , ENCODER(x)) ? R dy ? R dz , (2) p(y s | y &lt;s , z &lt;t , x) = [softmax(W y h y s + b y )] ys ,? s = argmax ys p(y s | y &lt;s , z &lt;t , x), (3) p(z t | y &lt;s , z &lt;t , x) = [softmax(W z h z t + b z )] zt ,? t = argmax zt p(z t | y &lt;s , z &lt;t , x),<label>(4)</label></formula><p>where [v] i denotes the i th element of the vector v. Note that y s and z t are token indices (1 ? y s ? V y , 1 ? z t ? V z ). In <ref type="formula">(3)</ref> and <ref type="formula" target="#formula_1">(4)</ref>, we detail the intermediate quantities p(y s | ?) and p(z t | ?) as obtained from the probability distributions over the output vocabulary. In the above, we have made an important assumption about the joint probability p(y s , z t | ?) that it can be factorized into p(y s | ?)p(z t | ?). Therefore, the joint distribution (1) encoded by the dual-decoder Transformer can be rewritten as</p><formula xml:id="formula_2">p(y, z | x) = max(Ty,Tz) t=0 p(y t | y &lt;t , z &lt;t , x)p(z t | y &lt;t , z &lt;t , x).<label>(5)</label></formula><p>We also assumed so far that the sub-decoders start at the same time, which is the most basic configuration. In practice, however, one may allow one sequence to advance k steps compared to the other, known as the wait-k policy <ref type="bibr" target="#b21">(Ma et al., 2019)</ref>. For example, if ST waits for ASR to produce its first k tokens, then the joint distribution becomes</p><formula xml:id="formula_3">p(y, z | x) = p(y &lt;k | x)p(y ?k , z | y &lt;k , x) (6) = k?1 t=0 p(y t | y &lt;t , x) max(Ty?k,Tz) t=0 p(y t+k | y &lt;t+k , z &lt;t , x)p(z t | y &lt;t+k , z &lt;t , x).<label>(7)</label></formula><p>In the next section, we propose two concrete architectures for the dual-decoder, corresponding to different levels of dependencies between the two sub-decoders (ASR and ST). Then, we show that several known models in the literature are special cases of these architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parallel and cross dual-decoder Transformers</head><p>The first architecture is called parallel dual-decoder Transformer, which has the highest level of dependencies: one decoder uses the hidden states of the other to compute its outputs, as illustrated in <ref type="figure" target="#fig_1">Figure 1a</ref>. The encoder consists of an input embedding layer followed by a positional embedding and a number of self-attention and feed-forward network (FFN) layers whose inputs are normalized <ref type="bibr">(Ba et al., 2016)</ref>. 2 This is almost the same as the encoder of the original Transformer <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref> (we refer to the corresponding paper for further details), except that the embedding layer in our encoder is a small convolutional neural network (CNN) <ref type="bibr" target="#b8">(Fukushima and Miyake, 1982;</ref><ref type="bibr" target="#b19">LeCun et al., 1989)</ref> of two layers with ReLU activations and a stride of 2, therefore reducing the input length by 4.  The parallel dual-decoder consists of: (a) two decoders that follow closely the common Transformer decoder structure, and (b) four additional multi-head attention layers (called dual-attention layers). Each dual-attention layer is complementary to a corresponding main attention layer. We recall that an attention layer receives as inputs a query Q ? R d k , a key K ? R d k , a value V ? R dv and outputs Attention(Q, K, V). 3 A dual-attention layer receives Q from the main branch and K, V from the other decoder at the same level (i.e. at the same depth in Transformer architecture) to compute hidden representations that will be merged back into the main branch (i.e. one decoder attends to the other in parallel). We present in more detail this merging operation in Section 3.4.</p><formula xml:id="formula_4">N ? N ? A M A M A A A M A M A A (b) Parallel N ? L N ? A M A M A A A M A M A A L (c) Cross</formula><p>Our second proposed architecture is called cross dual-decoder Transformer, which is similar to the previous one, except that now the dual-attention layers receive K, V from the previous decoding step outputs of the other decoder, as illustrated in <ref type="figure" target="#fig_1">Figure 1c</ref>. Thanks to this design, each prediction step can be performed separately on the two decoders. The hidden representations h y s , h z t in (2) produced by the decoders can thus be decomposed into: 4</p><formula xml:id="formula_5">h y s = DECODER asr (y &lt;s , z &lt;t , ENCODER(x)) ? R dy ,<label>(8)</label></formula><formula xml:id="formula_6">h z t = DECODER st (z &lt;t , y &lt;s , ENCODER(x)) ? R dz .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Special cases</head><p>In this section, we present some special cases of our dual-decoder architecture and discuss their links to existing models in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Independent decoders</head><p>When there is no dual-attention, the two decoders become independent. In this case, the prediction joint probability can be factorized simply as p(y s , z t | y &lt;s , z &lt;t ,</p><formula xml:id="formula_7">x) = p(y s | y &lt;s , x)p(z t | z &lt;t , x)</formula><p>. Therefore, all prediction steps are separable and thus this model is the most computationally efficient. In the literature, this model is often referred to as multi-task <ref type="bibr" target="#b0">(Anastasopoulos and Chiang, 2018;</ref><ref type="bibr">Sperber et al., 2020)</ref>.</p><p>Chained decoders Another special case corresponds to the extreme wait-k policy, in which one decoder waits for the other to completely finish before starting its own decoding. For example, if ST waits for ASR, then the prediction joint probability reads p(y s , z t | y &lt;s , z &lt;t ,</p><formula xml:id="formula_8">x) = p(y s | y &lt;s , x)p(z t | z &lt;t , y, x)</formula><p>. This model is called triangle in previous work <ref type="bibr" target="#b0">(Anastasopoulos and Chiang, 2018;</ref><ref type="bibr">Sperber et al., 2020)</ref>. A special case of this model is when the second decoder in the chain is not directly connected to the encoder, also referred to as two-stage 5 <ref type="bibr" target="#b28">(Sperber et al., 2019;</ref><ref type="bibr">Sperber et al., 2020)</ref>.</p><p>To summarize the different cases, we show below the joint probability distributions encoded by the presented models, in decreasing level of dependencies:</p><formula xml:id="formula_9">(single output) p(y, z | x) = T t=0 p(y t , z t | y &lt;t , z &lt;t , x),<label>(10)</label></formula><formula xml:id="formula_10">(dual-decoder) p(y, z | x) = T t=0 p(y t | y &lt;t , z &lt;t , x)p(z t | y &lt;t , z &lt;t , x),<label>(11)</label></formula><formula xml:id="formula_11">(chained) p(y, z | x) = T t=0 p(y t | y &lt;t , x)p(z t | z &lt;t , y, x),<label>(12)</label></formula><formula xml:id="formula_12">(independent) p(y, z | x) = T t=0 p(y t | y &lt;t , x)p(z t | z &lt;t , x),<label>(13)</label></formula><p>where T = max(T y , T z ). Similar formalization for the wait-k policy <ref type="formula" target="#formula_3">(7)</ref> can be obtained in a straightforward manner. Note that for independent decoders, the distribution is the same as in non-wait-k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Variants</head><p>The previous section presents special cases of our formulation at a high level. In this section, we introduce different fine-grained variants of the dual-decoder Transformers used in the experiments (Section 5).</p><p>Asymmetric dual-decoder Instead of using all the dual-attention layers, one may want to allow a one-way attention: either ASR attends ST or the inverse, but not both.</p><p>At-self or at-source dual-attention In each decoder block, there are two different attention layers, which we respectively call self-attention (bottom) and source-attention 6 (top). For each, there is an associated dual-attention, named respectively dual-attention at self and dual-attention at source. In the experiments, we study the case where either only the at-self or at-source attention layers are retained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Merging operators</head><p>The Merge layers shown in <ref type="figure" target="#fig_1">Figure 1</ref> combine the outputs of the main attention H main and the dual-attention H dual . We experimented dual-attention with two different merging operators: weighted sum or concatenation. We can formally define the merging operators as</p><formula xml:id="formula_13">H out = Merge(H main , H dual ) ? ? ? ? ? H main if no dual-attention, H main + ?H dual , if sum operator, linear ([H main ; H dual ]) if concat operator.<label>(14)</label></formula><p>5 Also called cascade by <ref type="bibr" target="#b0">Anastasopoulos and Chiang (2018)</ref>. We omit this term to avoid confusion with the common cascade models that are typically not trained end-to-end. Note that our chained-decoder (both triangle and two-stage) are end-to-end. <ref type="bibr">6</ref> Also referred to as cross-attention in the literature. We use a different name to avoid confusion with the cross dual-decoder.</p><p>For the sum operator, in particular, we perform experiments for learnable or fixed ?.</p><p>Remark. The model proposed by  is a special case of our cross dual-decoder Transformer with no dual-attention at source, no layer normalization for the input embeddings <ref type="figure" target="#fig_1">(Figure 1c)</ref>, and sum merging with fixed ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training and Decoding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training</head><p>The objective, L(?,?, y, z) = ?L asr (?, y) + (1 ? ?)L st (?, z), is a weighted sum of the cross-entropy ASR and ST losses, where (?,?) and (y, z) denote the predictions and the ground-truths for (ASR, ST), respectively. The weight ? is set to 0.3 in all experiments. Here we favor the ST task based on the intuition that it is more difficult to train than the ASR one, simply because of the multilinguality. A hyperparameter search may further improve the results. We also employ label smoothing <ref type="bibr" target="#b30">(Szegedy et al., 2016</ref>) with = 0.1. For each language pair, training data is sorted by the number of frames. Each mini-batch contains all languages such that their numbers of frames are roughly the same. We follow <ref type="bibr" target="#b11">Inaguma et al. (2019)</ref> and prepend a language-specific token to the target sentence. Preliminary experiments showed that this approach was more effective than adding a target language embedding along the temporal dimension to the speech feature inputs (Di .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Decoding</head><p>We present the beam search strategy used by our model. Since there are two different outputs (ASR and ST), one may naturally think about two different beams (with possibly some interactions). However, we found that a single joint beam works best for our model. In this beam search strategy, each hypothesis includes a tuple of ASR and ST sub-hypotheses. The two sub-hypotheses are expanded together and the score is computed based on the sum of log probabilities of the output token pairs. For a beam size B, the B best hypotheses are retained based on this score. In this setup, both sub-hypotheses evolve jointly, which resembles the training process more than in the case of two different beams. A limitation of this joint-beam strategy is that, in extreme cases, one of the task (ASR or ST) may only have a single hypothesis. Indeed, at a decoding step t + 1, we take the best B predictions (? t ,? t ) in terms of their sum of scores s(y t , z t ) log p(y t | y &lt;t , z &lt;t ) + log p(z t | y &lt;t , z &lt;t ); it can happen that, e.g., some? t has a so dominant score that it is selected for all the hypotheses, i.e. the B (different) hypotheses have a single? t and B different? t . We leave the design of a joint-beam strategy with enforced diversity to future work. Finally, to produce translations for multiple target languages in our system, it suffices to feed different language-specific tokens to the dual-decoder at decoding time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>To build a one-to-many model that can jointly transcribe and translate, we use MuST-C (Di , which is currently the largest publicly available one-to-many speech translation dataset. 7 MuST-C covers language pairs from English to eight different target languages including Dutch, French, German, Italian, Portuguese, Romanian, Russian, and Spanish. Each language direction includes a triplet of source input speech, source transcription, and target translation, with size ranging from 385 hours (Portuguese) to 504 hours (Spanish). We refer to the original paper for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training and decoding details</head><p>Our implementation is based on the ESPnet-ST toolkit <ref type="bibr" target="#b12">(Inaguma et al., 2020)</ref>. <ref type="bibr">8</ref> In the following, we provide details for reproducing the results. The pipeline is identical for all experiments.</p><p>Models All experiments use the same encoder architecture with 12 layers. The decoder has 6 layers, except for the independent-decoder model where we also include a 8-layer version (independent++) to compare the effects of dual-attention against simply increasing the number of model parameters.</p><p>Text pre-processing Transcriptions and translations were normalized and tokenized using the Moses tokenizer <ref type="bibr" target="#b18">(Koehn et al., 2007)</ref>. The transcription was lower-cased and the punctuation was stripped. A joint BPE <ref type="bibr" target="#b27">(Sennrich et al., 2016)</ref> with 8000 merge operations was learned on the concatenation of the English transcription and all target languages. We also experimented with two separate dictionaries (one for English and another for all target languages), but found that the results are worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speech features</head><p>We used Kaldi <ref type="bibr" target="#b26">(Povey et al., 2011)</ref> to extract 83-dimensional features (80-channel log Mel filter-bank coefficients and 3-dimensional pitch features) that were normalized by the mean and standard deviation computed on the training set. Following common practice <ref type="bibr" target="#b11">(Inaguma et al., 2019;</ref><ref type="bibr" target="#b35">Wang et al., 2020c)</ref>, utterances having more than 3000 frames or more than 400 characters were removed. For data augmentation, we used speed pertubation <ref type="bibr" target="#b17">(Ko et al., 2015)</ref> with three factors of 0.9, 1.0, and 1.1 and SpecAugment <ref type="bibr" target="#b24">(Park et al., 2019)</ref> with three types of deterioration including time warping (W ), time masking (T ) and frequency masking (F ), where W = 5, T = 40, and F = 30.</p><p>Optimization Following standard practice for training Transformer, we used the Adam optimizer (Kingma and <ref type="bibr" target="#b16">Ba, 2015)</ref> with Noam learning rate schedule <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>, in which the learning rate is linearly increased for the first 25K warm-up steps then decreased proportionally to the inverse square root of the step counter. We set the initial learning rate to 1e?3 and the Adam parameters to ? 1 = 0.9, ? 2 = 0.98, = 1e?9. We used a batch size of 32 sentences per GPU, with gradient accumulation of 2 training steps. All models were trained on a single-machine with 8 32GB GPUs for 250K steps unless otherwise specified. As for model initialization, we trained an independent-decoder model with the two decoders having shared weights for 150K steps and used its weights to initialize the other models. This resulted in much faster convergence for all models. We also included this shared model in the experiments, and for a fair comparison, we trained it for additional 250K steps. Finally, for decoding, we used a beam size of 10 with length penalty of 0.5. 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results and analysis</head><p>In this section, we report detokenized case-sensitive BLEU 10 <ref type="bibr" target="#b23">(Papineni et al., 2002)</ref> on the MuST-C dev sets <ref type="table">(Table 1)</ref>. Results on the test sets are discussed in Section 5.4. Following previous work <ref type="bibr" target="#b12">(Inaguma et al., 2020)</ref>, we remove non-verbal tokens in evaluation. <ref type="bibr">11</ref> In <ref type="table">Table 1</ref>, there are 3 main groups of models, corresponding to independent-decoder, cross dual-decoder (crx), and parallel dual-decoder (par), respectively. In particular, independent++ corresponds to a 8-decoder-layer model and will serve as our strongest baseline for comparison. <ref type="figure" target="#fig_3">Figure 2</ref> shows the relative performance of some representative models with respect to this baseline, together with their validation accuracies. In the following, when comparing models, we implicitly mean "on average" (over the 8 languages), except otherwise specified.</p><p>Parallel vs. cross Under the same configurations, parallel models outperform their cross counterparts in terms of translation (line 5 vs. line 13, line 6 vs. line 14, and line 7 vs. line 16), showing an improvement of 0.7 BLEU on average. In terms of recognition, however, the parallel architecture has on average a 0.33% higher (worse) WER compared to the cross models. On the other hand, parallel dual-decoders perform better than independent decoders in both translation and recognition tasks, except for the asymmetric case (line 12), the at-self and at-source with sum merging configuration (line 17), and the wait-k model where ST is ahead of ASR (line 19). This shows that both the translation and recognition tasks can benefit from the tight interaction between the two decoders, i.e. it is possible to achieve no trade-off between BLEUs and WERs for the parallel models compared to the independent architecture. This is not the case, however, for the cross dual-decoders that feature weaker interaction than the parallel ones. no normalization for dual-attention input, ? sum merging has ? = 0.3 fixed, R3 ASR is 3 steps ahead of ST, T3 ST is 3 steps ahead of ASR. <ref type="table">Table 1</ref>: BLEU and (average) WER on MuST-C dev set. In the second column (type), crx and parallel denote the cross and parallel dual-decoder, respectively. In the third column (side), st means only ST attends to ASR. Line 1 corresponds to the independent-decoder model where the weights of the two decoders are shared, and independent++ corresponds to the model with 8 decoder layers (instead of 6). It should be noted that line 10 corresponds to the model proposed by . The values that are better than the baseline (independent++) are underlined and colored in blue, while the best values are highlighted in bold.</p><p>Interestingly, there is a slight trade-off between the parallel and cross designs: the parallel models are better in terms of BLEUs but worse in terms of WERs. This is to some extent similar to previous work where models having different types of trade-offs between BLEUs and WERs <ref type="bibr" target="#b10">(He et al., 2011;</ref><ref type="bibr">Sperber et al., 2020;</ref><ref type="bibr" target="#b6">Chuang et al., 2020)</ref>. It should be emphasized that most of the dual-decoder models have fewer or same numbers of parameters compared to independent++. This confirms our intuition that the tight connection between the two decoders in the parallel architecture improves performance. The cross dual-decoders perform relatively well compared to the baseline of two independent decoders with the same number of layers (6), but not so well compared to the stronger baseline with 8 layers.</p><p>Symmetric vs. asymmetric In some experiments, we only allow the ST decoder to attend to the ASR decoder. For the cross dual-decoder, this did not yield noticeable improvements in terms of BLEU (21.72 at line 4 vs. 21.71 at line 6), while for the parallel architecture, the results are worse (21.93 at line 12 vs. 22.70 at line 16). The symmetric models also outperform the asymmetric counterparts in terms of WER (12.7 at line 4 vs. 12.2 at line 6, 13.0 at line 12 vs. 12.7 at line 16). It is confirmed again that the two tasks are complementary and can help each other: removing the ASR-to-ST attention hurts performance. In fact, examining the learnable ? in the sum merging operator shows that the decoders learn to attend to each other, though at different rates. We observed that for the same layer depth, the ST decoder always attends more to the ASR one, and for both of them ? increases with the depth of the layer.</p><p>At-self dual-attention vs. at-source dual-attention For the parallel dual-decoder, the at-source dualattention produces better results than the at-self counterpart <ref type="bibr">(BLEU: 22.54 vs. 22.26,</ref><ref type="bibr">WER: 12.7 vs. 12.8</ref> at line 14 vs. line 15), while the combination of both does not improve the results <ref type="bibr">(BLEU 22.16,</ref><ref type="bibr">WER 12.8 at line 17)</ref>. For the concat merging, using both yields better results in terms of translation but slightly hurts the recognition task (BLEU: 22.70 vs. 22.32, WER: 12.7 vs. 12.5 at line 16 vs. line 13).</p><p>Sum vs. concat merging The impact of merging operators is not consistent across different models. If we focus on the parallel dual-decoder, sum is better for models with only at-source attention (line 13 vs. line 14) and concat is better for models using both at-self and at-source attention (line 16 vs. line 17).   <ref type="bibr">16, 8, 7, 3, 2 in</ref>  <ref type="table">Table 1</ref>. The baseline used for relative BLEU is independent++. One can observe that the parallel models consistently outperform the others in terms of validation accuracy.</p><p>Input normalization and learnable sum Some experiments confirm the importance of normalizing the input fed to the dual-attention layers (i.e. the LayerNorm layers shown in <ref type="figure" target="#fig_1">Figure 1c</ref>). The results show that normalization substantially improves the performance (BLEU: 22.17 vs. 21.34, WER: 12.1 vs. 12.3 at line 8 vs. at line 11). It is also beneficial to use learnable weights compared to a fixed value for the sum merging operator (Equation <ref type="formula" target="#formula_1">(14)</ref>) (BLEU: 21.26 vs. 20.39 at line 9 vs. line 10). Note that the fixed weight and non-normalization configuration corresponds to the model of  (line 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wait-k policy</head><p>We compare a non-wait-k parallel dual-decoder (line 14) with its wait-k (k = 3) counterparts. From the results, one can observe that letting ASR be ahead of ST (line 18) improves the performance (BLEU: 22.78 vs. 22.54, WER: 12.6 vs. 12.7), while letting ST be ahead of ASR (line 19) considerably worsen the results (BLEU: 21.85, WER: 13.6). This confirms our intuition that the ST task is more difficult and should not take the lead in the dual-decoder models.</p><p>ASR results From the results (last column of <ref type="table">Table 1</ref>), one can observe that the dual-decoder models outperform the baseline indepedent++, except for the asymmetric case and the wait-k model where ST is 3 steps ahead of ASR. While using a single decoder leads to an average of 14.2% WER, all other symmetric architectures with two decoders (except the ASR-waits-for-ST) have better and rather stable WERs (from 12.1% to 13.0%). Detailed results for each data subset are provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison to state-of-the-art</head><p>To avoid a hyper-parameter search over the test set, we only select three of our best models together with the baseline independent++ for evaluation. All of the three models are symmetric parallel dualdecoders, the first one has at-source dual-attention with sum merging, the second one has both at-self and at-source dual-attentions with concat merging, and the last one is a wait-k model in which ASR is 3 steps ahead of ST. These models correspond to lines 5, 6, and 7 of <ref type="table" target="#tab_2">Table 2</ref>, and will be referred to respectively as par++, par, and par R3 in the sequel. For par++ we increase the number of decoder layers from 6 to 8, thus increasing the number of parameters from 48M to 51.2M, matching that of the baseline. We do not do this for par R3 (48M) as this model already has a higher latency due to the wait-k. All models are trained for 550K steps, corresponding to 25 epochs. Following <ref type="bibr" target="#b12">Inaguma et al. (2020)</ref>, we use the average of five checkpoints with the best validation accuracies on the dev sets for evaluation.</p><p>We compare the results with the previous work  in the multilingual setting. In addition, to demonstrate the competitive performance of our models, we also include the best existing translation performance on MuST-C <ref type="bibr" target="#b12">(Inaguma et al., 2020)</ref>, although these results were obtained with bilingual systems and from a sophisticated training recipe. Indeed, to obtain the results for each language pair (e.g. en-de), <ref type="bibr" target="#b12">Inaguma et al. (2020)</ref> pre-trained an ASR model and an MT model to initialize the  weights of (respectively) the encoder and decoder for ST training. This means that to obtain the results for the 8 language pairs, 24 independent trainings had to be performed in total (3 for each language pair). The results in <ref type="table" target="#tab_2">Table 2</ref> show that our models achieved very competitive performance compared to bilingual one-to-one models <ref type="bibr" target="#b12">(Inaguma et al., 2020)</ref>, despite being trained for only half the number of epochs. In particular, the par++ model achieved the best results, consistently surpassing the others on all languages (except on Russian where it is outperformed by the bilingual model). Our results also surpassed those of  by a large margin. We observe the largest improvements on Portuguese (+1.94 at line 5, +1.50 at line 6, and +1.47 at line 7, compared to the bilingual result at line 1), which has the least data among the 8 language pairs in MuST-C. This phenomenon is also common in multilingual neural machine translation where multilingual joint training has been shown to improve performance on low-resource languages <ref type="bibr" target="#b15">(Johnson et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced a novel dual-decoder Transformer architecture for synchronous speech recognition and multilingual speech translation. Through a dual-attention mechanism, the decoders in this model are at the same time able to specialize in their tasks while being helpful to each other. The proposed model also generalizes previously proposed approaches using two independent (or weakly tied) decoders or chaining ASR and ST. It is also flexible enough to experiment with settings where ASR is ahead of ST which makes it promising for (one-to-many) simultaneous speech translation. Experiments on the MuST-C dataset showed that our model achieved very competitive performance compared to state-of-the-art.  <ref type="figure">Figure 3</ref>: The cross dual-decoder Transformer. Unlike the parallel dual-decoder Transformer, here one decoder attends to the previous decoding-step outputs of the other and there is no interaction between their hidden states.  <ref type="table">Table 3</ref>: Word error rate on MuST-C dev set. The values that are better than the baseline (independent++) are underlined and colored in blue.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The parallel dual-decoder Transformer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The dual-decoder Transformers. Figure (a) shows the detailed architecture of the parallel dual-decoder Transformer, and Figure (b) shows its simplified view. The cross dual-decoder Transformer is very similar to the parallel one, except that the keys and values fed to the dual-attention layers come from the previous output, which is illustrated by Figure (c). From the above figures, one can easily infer the detailed architecture of the cross Transformer, which can be found in the Appendix. Abbreviations: A (Attention), M (Merge), L (LayerNorm).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Relative BLEU on MuST-C dev set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Results on the MuST-C dev set. The models listed in the legends correspond respectively to lines 17,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>27.77 24.65 19.93 21.53 24.24 18.19 10.99 20.84 14.2 2 independent 44.8M 20.11 28.18 25.61 20.76 21.83 25.45 18.45 11.31 21.46 12.6 3 independent++ 51.2M 20.25 29.48 26.10 21.05 22.34 26.71 19.67 12.10 22.21 12.9</figDesc><table><row><cell cols="3">No type side</cell><cell cols="4">self src merge params de</cell><cell>es</cell><cell>fr</cell><cell>it</cell><cell>nl</cell><cell>pt</cell><cell>ro</cell><cell>ru</cell><cell>avg</cell><cell>WER</cell></row><row><cell cols="12">1 19.40 4 independent (shared) 31.3M crx st -sum 46.4M 20.01 28.57 25.86 20.66 22.26 25.36 19.06 12.00 21.72 12.7</cell></row><row><cell>5</cell><cell>crx</cell><cell>both</cell><cell>-</cell><cell cols="2">concat 51.2M</cell><cell cols="6">20.36 28.51 25.80 21.18 22.10 25.24 19.55 11.89 21.83 12.3</cell></row><row><cell>6</cell><cell>crx</cell><cell>both</cell><cell>-</cell><cell>sum</cell><cell>48.0M</cell><cell cols="6">19.99 28.87 26.09 20.94 21.67 25.42 18.85 11.83 21.71 12.2</cell></row><row><cell>7</cell><cell>crx</cell><cell>both</cell><cell></cell><cell cols="2">concat 54.3M</cell><cell cols="6">20.07 28.73 26.01 20.93 22.59 25.60 19.08 12.46 21.93 12.4</cell></row><row><cell>8</cell><cell>crx</cell><cell>both</cell><cell></cell><cell>sum</cell><cell>51.2M</cell><cell cols="6">20.38 28.90 26.64 21.07 22.61 26.23 19.44 12.12 22.17 12.1</cell></row><row><cell>9</cell><cell>crx</cell><cell>both</cell><cell>-</cell><cell>sum</cell><cell>48.0M</cell><cell cols="6">19.72 27.96 25.49 20.52 21.56 25.01 18.53 11.33 21.26 12.8</cell></row><row><cell cols="2">10 crx</cell><cell>both</cell><cell>-</cell><cell>sum  ?</cell><cell>48.0M</cell><cell cols="6">18.62 27.11 24.41 19.73 20.47 24.49 17.23 11.09 20.39 12.8</cell></row><row><cell cols="2">11 crx</cell><cell>both</cell><cell></cell><cell>sum</cell><cell>51.2M</cell><cell cols="6">19.54 28.17 25.68 20.95 21.55 24.77 18.76 11.28 21.34 12.3</cell></row><row><cell cols="2">12 par</cell><cell>st</cell><cell></cell><cell cols="2">concat 49.6M</cell><cell cols="6">20.57 28.84 26.08 20.85 22.11 25.70 19.36 11.90 21.93 13.0</cell></row><row><cell cols="2">13 par</cell><cell>both</cell><cell>-</cell><cell cols="2">concat 51.2M</cell><cell cols="6">20.84 29.51 26.44 21.53 22.68 25.94 19.04 12.60 22.32 12.5</cell></row><row><cell cols="2">14 par</cell><cell>both</cell><cell>-</cell><cell>sum</cell><cell>48.0M</cell><cell cols="6">20.85 29.18 26.38 22.14 22.87 26.49 19.70 12.74 22.54 12.7</cell></row><row><cell cols="2">15 par</cell><cell>both</cell><cell>-</cell><cell>sum</cell><cell>48.0M</cell><cell cols="6">20.56 29.21 26.54 21.07 22.51 25.75 19.64 12.80 22.26 12.8</cell></row><row><cell cols="2">16 par</cell><cell>both</cell><cell></cell><cell cols="2">concat 54.3M</cell><cell cols="6">21.22 29.50 26.66 21.74 22.76 26.66 20.25 12.79 22.70 12.7</cell></row><row><cell cols="2">17 par</cell><cell>both</cell><cell></cell><cell>sum</cell><cell>51.2M</cell><cell cols="6">20.95 28.67 26.45 21.31 22.29 25.87 19.53 12.24 22.16 12.8</cell></row><row><cell cols="2">18 par</cell><cell cols="2">both R3 -</cell><cell>sum</cell><cell>48.0M</cell><cell cols="6">21.22 30.12 26.53 22.06 23.37 26.59 19.82 12.54 22.78 12.6</cell></row><row><cell cols="2">19 par</cell><cell cols="2">both T3 -</cell><cell>sum</cell><cell>48.0M</cell><cell cols="6">20.35 28.61 25.94 21.22 22.12 25.19 19.36 11.99 21.85 13.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Bilingual one-to-one<ref type="bibr" target="#b12">(Inaguma et al., 2020)</ref> 50 22.91 27.96 32.69 23.75 27.43 28.01 21.90 15.75 25.05 12.0 2 Multilingual one-to-many (Gangi et al., 2019) 17.70 20.90 26.50 18.00 20.00 22.60 ----3 Multilingual one-to-many (Gangi et al., 2019) 16.50 18.90 24.50 16.20 17.80 20.80 15.90 9.80 17.55 -4 independent++ 25 22.82 27.20 32.11 23.34 26.67 28.98 21.37 14.34 24.60 11.6 5 par++ both -sum 25 23.63 28.12 33.45 24.18 27.55 29.95 22.87 15.21 25.62 11.4 6 par both concat 25 22.74 27.59 32.86 23.50 26.97 29.51 21.94 14.88 25.00 11.</figDesc><table><row><cell cols="2">No type</cell><cell cols="2">side self src merge</cell><cell cols="2">epochs de</cell><cell>es</cell><cell>fr</cell><cell>it</cell><cell>nl</cell><cell>pt</cell><cell>ro</cell><cell>ru</cell><cell>avg</cell><cell>WER</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6</cell></row><row><cell>7</cell><cell cols="2">par R3 both -</cell><cell>sum</cell><cell>25</cell><cell cols="8">22.84 27.92 32.12 23.61 27.29 29.48 21.16 14.50 24.87 11.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>BLEU on MuST-C tst-COMMON test set. Line 2 corresponds to the best multilingual models of trained separately for {de,nl} and {es,fr,it,pt}, while line 3 is a single model trained on 8 languages. The WER in line 1 corresponds to the best result reported by<ref type="bibr" target="#b12">Inaguma et al. (2020)</ref>.12    </figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The model of does not have interaction between internal hidden states of the decoders (c.f . Section 3.4). arXiv:2011.00747v1 [cs.CL] 2 Nov 2020</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">All the illustrations in this paper are for the so-called pre-LayerNorm configuration, in which the input of the layer is normalized. Likewise, if the output is normalized instead, the configuration is called post-LayerNorm. Since pre-LayerNorm is known to perform better than post-LayerNorm<ref type="bibr" target="#b22">Nguyen and Salazar, 2019)</ref>, we only conducted experiments for the former, although our implementation supports both.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3"> Following Vaswani et al. (2017), we use multi-head attention, in which the inputs are linearly projected multiple times before feeding to the function, then the outputs are concatenated and projected back to the original dimension. 4 This decomposition is clearly not possible for the parallel dual-decoder Transformer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Smaller datasets includeEuroparl-ST (Iranzo-S?nchez et al., 2020)  andMaSS (Boito et al., 2020). Recently, a very large many-to-many dataset called CoVoST-2<ref type="bibr" target="#b34">(Wang et al., 2020b)</ref> has been released, while its predecessor CoVoST<ref type="bibr" target="#b33">(Wang et al., 2020a)</ref> only covers the many-to-one scenario. 8 https://github.com/espnet/espnet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">For a hypothesis of length L, a length penalty p means a score of pL will be added to the (log probability) score of that hypothesis (Section 4.2). Therefore, longer hypotheses are favored, or equivalently, shorter hypotheses are "penalized".10  We also tried sacreBLEU<ref type="bibr" target="#b25">(Post, 2018)</ref> and found that the results are identical. 11 This is for a fair comparison with the results of<ref type="bibr" target="#b12">Inaguma et al. (2020)</ref>, presented in Section 5.4.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by a Facebook AI SRA grant, and was granted access to the HPC resources of IDRIS under the allocations 2020-AD011011695 and 2020-AP011011765 made by GENCI. It was also done as part of the Multidisciplinary Institute in Artificial Intelligence MIAI@Grenoble-Alpes (ANR-19-P3IA-0003). We thank the anonymous reviewers for their insightful feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We present the detailed model architecture for the cross dual-decoder Transformer in <ref type="figure">Figure 3</ref>. Detailed WER results on MuST-C dev set are presented in <ref type="table">Table 3</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tied multitask learning for neural speech translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<editor>Marilyn A. Walker, Heng Ji, and Amanda Stent</editor>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">FINDINGS OF THE IWSLT 2020 EVALUATION CAMPAIGN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebrahim</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roldano</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Nagesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Salesky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>St?ker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://github.com/espnet/espnet/blob/master/egs/must_c/asr1/RESULTS.md" />
	</analytic>
	<monogr>
		<title level="m">of the 17th International Conference on Spoken Language Translation</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="1" to="34" />
		</imprint>
	</monogr>
	<note>Proceedings 12</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Listen and translate: A proof of concept for end-to-end speech-to-text translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>B?rard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on End-to-end Learning for Speech and Audio Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">End-to-end automatic speech translation of audiobooks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>B?rard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Can</forename><surname>Kocabiyikoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<idno>abs/1802.04200. 1</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mass: A large and clean multilingual corpus of sentence-aligned spoken utterances extracted from the bible</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcely Zanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Boito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahault</forename><surname>Havard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ric</forename><surname>Garnerin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Le Ferrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Besacier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
		<meeting>The 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6486" to="6493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Worse wer, but better bleu? leveraging word embedding as intermediate in multitask end-to-end speech translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Wei</forename><surname>Shun-Po Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<ptr target="AssociationforComputa-tionalLinguistics.8" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5998" to="6003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Must-c: a multilingual speech translation corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Mattia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roldano</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunihiko</forename><surname>Fukushima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sei</forename><surname>Miyake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Competition and cooperation in neural nets</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1982" />
			<biblScope unit="page" from="267" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">One-to-many multilingual end-to-end speech translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia Antonino Di</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2019</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Why word error rate is not a good metric for speech recognizer training for the speech translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5632" to="5635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multilingual end-to-end speech translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirofumi</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2019</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-12-14" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Espnet-st: All-in-one speech translation toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirofumi</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeki</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">Enrique</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalta</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10234.6</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Europarl-st: A multilingual corpus for speech translation of parliamentary debates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Iranzo-S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><forename type="middle">Albert</forename><surname>Silvestre-Cerd?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nahuel</forename><surname>Rosell?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adri?</forename><surname>Gim?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Sanchis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfons</forename><surname>Juan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8229" to="8233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><forename type="middle">B</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="339" to="351" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Synchronous speech recognition and speech-to-text translation with interactive decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8417" to="8424" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">STACL: Simultaneous translation with implicit anticipation and controllable latency using prefix-to-prefix framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaibo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baigong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Transformers without tears: Improving the normalization of selfattention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Toan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salazar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05895.3</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>ISCA. 7</idno>
	</analytic>
	<monogr>
		<title level="m">20th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-19" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
	<note>Gernot Kubin and Zdravko Kacic</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting bleu scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 workshop on automatic speech recognition and understanding. IEEE Signal Processing Society</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers. The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention-passing models for robust and data-efficient end-to-end speech translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Udhyakumar Nallasamy, and Matthias Paulik. 2020. Consistent transcription and translation of speech. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Gollan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deep transformer models for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1810" to="1822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Covost: A diverse multilingual speech-to-text translation corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
		<meeting>The 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France, May</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Covost 2: A massively multilingual speech-to-text translation corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10310.6</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Curriculum pre-training for end-to-end speech translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenglu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3728" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence models can directly translate foreign speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno>ISCA. 1</idno>
	</analytic>
	<monogr>
		<title level="m">18th Annual Conference of the International Speech Communication Association</title>
		<editor>Francisco Lacerda</editor>
		<meeting><address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-20" />
			<biblScope unit="page" from="2625" to="2629" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
