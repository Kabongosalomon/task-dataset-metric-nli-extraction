<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FIDNet: LiDAR Point Cloud Semantic Segmentation with Fully Interpolation Decoding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Bai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinming</forename><surname>Huang</surname></persName>
						</author>
						<title level="a" type="main">FIDNet: LiDAR Point Cloud Semantic Segmentation with Fully Interpolation Decoding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note:</head><p>After the submission of IROS, we have some follow-up updates, please scroll to the end and take a look at the supplementary file after the reference section. Further details can be found in our code:</p><p>https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI Abstract-Projecting the point cloud on the 2D spherical range image transforms the LiDAR semantic segmentation to a 2D segmentation task on the range image. However, the LiDAR range image is still naturally different from the regular 2D RGB image; for example, each position on the range image encodes the unique geometry information. In this paper, we propose a new projection-based LiDAR semantic segmentation pipeline that consists of a novel network structure and an efficient postprocessing step. In our network structure, we design a FID (fully interpolation decoding) module that directly upsamples the multi-resolution feature maps using bilinear interpolation. Inspired by the 3D distance interpolation used in PointNet++, we argue this FID module is a 2D version distance interpolation on (? , ? ) space. As a parameter-free decoding module, the FID largely reduces the model complexity by maintaining good performance. Besides the network structure, we empirically find that our model predictions have clear boundaries between different semantic classes. This makes us rethink whether the widely used K-nearest-neighbor post-processing is still necessary for our pipeline. Then, we realize the many-to-one mapping causes the blurring effect that some points are mapped into the same pixel and share the same label. Therefore, we propose to process those occluded points by assigning the nearest predicted label to them. This NLA (nearest label assignment) post-processing step shows a better performance than KNN with faster inference speed in the ablation study. On SemanticKITTI dataset, our pipeline achieves the best performance among all projection-based methods with 64 ? 2048 resolution and all point-wise solutions. With a ResNet-34 as the backbone, both the training and testing of our model can be finished on a single RTX 2080 Ti with 11G memory. The code is released here. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>LiDAR sensor is playing an important role in outdoor robots, especially autonomous cars. With the booming of deep learning techniques, recent research topics focus on extracting object and semantic information from the point cloud. LiDAR semantic segmentation is such a task that a neural network predicts the semantic label of each point <ref type="bibr" target="#b3">[1]</ref>. As shown in <ref type="figure">Fig. 1</ref>, solving this task builds the 3D understanding of the nearby environment.</p><p>Authors are with Department of Electrical and Computer Engineering, Worcester Polytechnic Institute, Massachusetts 01609, USA. yzhao7@wpi.edu 1 https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI <ref type="figure">Fig. 1</ref>. The LiDAR point cloud semantic segmentation predicts a semantic label for each point to help the car to understand the 3D surroundings. This is a sample from the validation sequence in the SemanticKITTI dataset.</p><p>Though the first dataset specifically aimed at benchmarking the LiDAR semantic segmentation was published in 2019 <ref type="bibr" target="#b5">[2]</ref>, the interest in processing 3D point clouds with neural networks arose earlier in two communities. Researchers from the computer vision society investigated how to design a permutation invariant network to deal with more general unordered point clouds <ref type="bibr" target="#b6">[3]</ref>, <ref type="bibr" target="#b7">[4]</ref>. In contrast, the solution design from researchers on the robotic side considered more about the LiDAR sensing mechanism by projecting the point cloud on the 2D spherical range image <ref type="bibr" target="#b8">[5]</ref>. The projectionbased solution allows the well-designed 2D image semantic segmentation models to be directly used for the LiDAR semantic segmentation task. It lacks the ability to process more general unordered point clouds, but it shows practical advantages such as better performance in terms of both the speed and accuracy <ref type="bibr" target="#b9">[6]</ref>, <ref type="bibr" target="#b10">[7]</ref>. To chase a better performance, recent researchers further design models by combining multiview projections or voxelization with point-wise features <ref type="bibr" target="#b11">[8]</ref>, <ref type="bibr" target="#b12">[9]</ref>, <ref type="bibr" target="#b13">[10]</ref>.</p><p>Motivation The major goal of this paper is to argue how should we design the network for projection-based LiDAR segmentation. We hope to keep the structure as common as possible as well as maintain a good performance. Most image-based network structures rely on an encoder-decoder structure. This paper rethinks this structure by considering the difference between the spherical representation image and the regular image. On the spherical representation image, each position is a 3D point with its unique location information. However, on the regular image, two different pixels may have the same spectral information that makes only a pattern of pixels meaningful. To design a better structure for range image, we propose several modules, including an input module with 1 ? 1 convolution, a backbone network to extract multi-scale features, a fully interpolation decoding module, and a final post-processing step that only assigning labels for those occluded points.</p><p>Contribution This paper proposes a new pipeline to solve the LiDAR semantic segmentation in a projection fashion. The whole solution is clean and effective. Both the training and testing can be conducted on a single RTX 2080 Ti with 11 G memory. The solution's performance is better than all projection-based methods by following the same 64 ? 2048 input resolution. There are two major technical contributions that we think will be helpful for other related methods:</p><p>? We propose a parameter-free fully interpolation decoding module that only contains bilinear interpolation operation. Most existing decoder structures need transpose convolution or special combined convolution with interpolation to upsample low-resolution feature maps. We demonstrate that only using bilinear interpolation can already achieve state-of-the-art performance. This setting reduces the model complexity by avoiding a large amount parameters in the decoder used by other models <ref type="bibr" target="#b14">[11]</ref>, <ref type="bibr" target="#b10">[7]</ref>. ? We replace the widely used K-nearest-neighbor postprocessing with a more efficient and intuitive step. Knearest-neighbor was proposed to solve the boundaryblurring effect generated by two reasons <ref type="bibr" target="#b3">[1]</ref>, which are the ambiguous output of CNNs and the many-toone mapping on spherical range image. However, we empirically find the blurring effect on our predictions is negligible. Then, we specifically focus on the many-toone mapping that some points will be mapped on the same location with others, thus not having directly predicted labels from the network. To solve this problem, we assign the predicted label of the nearest point in 3D space to those unpredicted points. Compared with KNN, this NLA (nearest label assignment) post-processing step has better performance and faster inference speed.</p><p>Both of the above two contributions bring practical improvements to the network design of projection-based Li-DAR semantic segmentation task, which have not been discussed by other literature as far as we know. There are also some other minor contributions such as an input module with 1 ? 1 convolution or nearby point feature aggregation with atrous convolution <ref type="bibr" target="#b15">[12]</ref>. Since similar ideas have been explored in recent papers <ref type="bibr" target="#b16">[13]</ref>, we will introduce them in the method part, but will not claim them as the major contribution of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Point-based Networks</head><p>PointNets PointNet <ref type="bibr" target="#b6">[3]</ref> summarizes several key properties of the general point cloud, including unordered, invariance under transformations, and interaction among points. Those unique challenges stimulate them to design a MLP (multilayer perceptron) based network structure and a per-point feature vector concatenated by global and point-wise features. Besides only the concatenation of global and pointwise features, PointNet++ <ref type="bibr" target="#b7">[4]</ref> propose novel set learning layers to adaptively combine features from multiple scales. However, the local query and grouping limit the model performance on a large point cloud. This attracts some successive papers <ref type="bibr" target="#b17">[14]</ref>, <ref type="bibr" target="#b9">[6]</ref>, the best pure point-based solution still lags behind on LiDAR point cloud semantic segmentation <ref type="bibr" target="#b9">[6]</ref>.</p><p>PointConvs Along with the network structure designing, solving the point cloud segmentation with special convolutional kernels is also a hot topic. Some ideas, such as PointCNN <ref type="bibr" target="#b18">[15]</ref> and KPConv <ref type="bibr" target="#b19">[16]</ref>, have been tried on various datasets and show a strong generality. Some of those special convolutional kernels have been merged as part of other solutions <ref type="bibr" target="#b20">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Voxel-based Networks</head><p>Split the 3D world in discretized voxels is a straightforward idea to process the point cloud. After the voxelization, the regular 3D convolution is able to be used on 3D space just like 2D convolution on 2D image <ref type="bibr" target="#b21">[18]</ref>, <ref type="bibr" target="#b22">[19]</ref>. However, the improvement of those methods in the outdoor LiDAR point cloud remains limited. Some recent papers are trying to consider a better 3D voxelization method to cut the 3D space by incorporating how LiDAR sensor generate the point cloud <ref type="bibr" target="#b23">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Projection-based Network</head><p>Single-View projection The success of 2D convolution neural networks on the image raises the question of whether it is possible to project the 3D point cloud on 2D space for the sake of using well-developed existing models. The scanning mechanism of the LiDAR sensor suggests the spherical range projection <ref type="bibr" target="#b3">[1]</ref>. This idea has been explored by many recent papers from different aspects <ref type="bibr" target="#b10">[7]</ref>, <ref type="bibr" target="#b24">[21]</ref>, <ref type="bibr" target="#b16">[13]</ref>. Besides the range view, bird-eye-view is also considered by recent papers <ref type="bibr" target="#b12">[9]</ref>. All those single view projection methods have the benefit of using 2D convolution networks, like the controllable inference speed.</p><p>Multi-View projection With all those newly developed methods above, it is natural to start thinking about how to combine part of different ideas together. In recent multiview projection solutions <ref type="bibr" target="#b25">[22]</ref>, <ref type="bibr" target="#b26">[23]</ref>, MLP based feature extractor is used first, then feature tensors from different views are fused together to be processed by one decoder. However, those methods need to take the extra cost to prepare multi-view projection, and it is also not clear how much improvement compared with single view projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Practical Considerations</head><p>As a new and important challenge, how to design models to solve the LiDAR semantic segmentation is still an open problem. The solution should consider both the numerical indicator and the practical feasibility. Single view projection methods are able to directly use 2D convolutional neural networks, thus do not need to worry about feasibility as there are a bunch of techniques to optimize 2D networks for various application requirements <ref type="bibr" target="#b27">[24]</ref>, <ref type="bibr" target="#b28">[25]</ref>. This paper specifically works on providing a pipeline for spherical range single view projection solutions with better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>The range view projection methods map each point from (x, y, z) Cartesian coordinate to (r, ? , ? ) spherical coordinate. After discretizing the 2D (? , ? ) space, each point will have a mapped position on the (? , ? ) image. The same as RGB three channels on regular images, the LiDAR point cloud range image has five channels (x, y, z, r, remission).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Input and Backbone Modules</head><p>Though the input is like a 2D image with five channels, each position still represents the information of a point. Therefore, we process the input with two 1 ? 1 convolutional layers that map each point to a high dimension tensor. This input processing module is analogous to the PointNet <ref type="bibr" target="#b6">[3]</ref> which extracts point features by using MLP (multi-layer perceptron). Then, we feed the high dimensional tensor consists of point-wise features into a regular backbone network. The backbone network can be any structures, such as faster MobileNet <ref type="bibr" target="#b29">[26]</ref> or more accurate HRNet <ref type="bibr" target="#b30">[27]</ref>. All those CNN structures take a tensor in and generate multi-resolution feature maps. In this paper, we use the standard ResNet-34 <ref type="bibr" target="#b31">[28]</ref> as the backbone.</p><p>The question is, why the regular backbone network designed for images still can work on point-wise feature tensors? We argue that the LiDAR point cloud is ordered in spherical coordinate as the laser scanner emits laser beams line by line. This mechanism makes it possible to build the special 2D spherical range representation of the LiDAR point cloud. The regular 3 ? 3 convolutional operator in modern backbone networks is equivalent to the eight nearest queries and grouping on the 2D (? , ? ) space. Thus, using the backbone network designed for the image to process point-wise features is the 2D correspondence of the 3D Knearest query and grouping for unordered point cloud used in PointNet++ <ref type="bibr" target="#b7">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fully Interpolation Decoding</head><p>In our network structure shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, we use 1 ? 1 convolution to map each point vector to a high dimension tensor, then extract multi-scale point features with a backbone network that keeps processing nearby point features on 2D (? , ? ) space. The next question is, how to fuse that information together in a point-wise way. In PointNet <ref type="bibr" target="#b6">[3]</ref>, the per-point feature vector is concatenated with the global feature vector as the final feature vector for each point. In PointNet++ <ref type="bibr" target="#b7">[4]</ref>, the distance-based interpolation is used to upsample a subset of points to a larger set. Those two unique operations inspire us to design a similar module aiming to fuse the multi-scale information.</p><p>The distance interpolation defined in PointNet++ <ref type="bibr" target="#b7">[4]</ref> is:</p><formula xml:id="formula_0">f (x) = ? k i=1 w i (x) f i ? k i=1 w i (x) , where w i (x) = 1 d(x, x i ) .</formula><p>However, on the 2D (? , ? ) space, if we set number of nearest points k = 4 and define the distance function d(x, x i ) as l 1 distance, the distance interpolation will exactly degenerate to bilinear upsample. Using the bilinear upsample to interpolate the low-resolution feature maps gives us five point-wise feature tensors which have the same resolution but encode different level information. The same as PointNet <ref type="bibr" target="#b6">[3]</ref>, we concatenate all those feature tensors together. Then, after going through the network structure in <ref type="figure" target="#fig_0">Fig. 2</ref>, each point is mapped from (x, y, z, r, remission) to a high dimensional vector with multi-level information. We name this module FID (fully interpolation decoding). Compared with regular decoder used in other networks, FID is completely parameter-free that largely reduces the model complexity and memory cost. This advantage also brings benefits to the network structure design. For example, one can only focus on customizing the backbone to meet the requirement of the hardware limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Classification Head</head><p>After the FID (fully interpolation decoding) module, we get a per-point feature tensor. To help each point vector better fuse the nearby information, we further use atrous convolution to extract local features and aggregate them by simple concatenation with the original feature tensor. This operation is similar to ASPP (atrous spatial pyramid pooling) <ref type="bibr" target="#b32">[29]</ref>, the difference is we do not apply average pooling but concatenate the outputs with original feature tensor together. The final feature tensor is sent into 1 ? 1 classification layers to predict the semantic label of each point. This classification head is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Post-processing with Nearest Label Assignment</head><p>Almost all recent developed projection-based LiDAR semantic segmentation solutions adopt a KNN (K-nearestneighbor) post-processing module to alleviate the boundaryblurring effect. As claimed in a recent paper <ref type="bibr" target="#b3">[1]</ref>, two reasons make this effect happen: blurring outputs from the neural network and the many-to-one mapping on the range image. However, after visualizing some of our network outputs, we realize the FID (fully interpolation decoding) module is already able to give predictions with clear object boundaries. In <ref type="figure">Fig. 4</ref>, we show our observations. At the top, we visualize two examples by only displaying those points that are processed by the network. We can see there is almost no blurring effect even for small poles. At the bottom of <ref type="figure">Fig. 4</ref>, it is clear to see adding those occluded points creates blurring boundaries. This stimulates us to rethink if it is still necessary to keep the KNN post-processing step in our pipeline.</p><p>Here we firstly give more discussions about the many-toone mapping problem. The mapping from (x, y, z) Cartesian coordinate to (r, ? , ? ) spherical coordinate is a one-to-one continuous mapping. However, discretizing (r, ? , ? ) on 2D (? , ? ) image will group some close points in one cell. After projection, only the information of one point of the cell will be processed by the neural network. Note, those points mapped on the same cell are only close on 2D (? , ? ) space, and they may have a large distance with the processed one. The potential large distance indicates those occluded points may have different labels with the predicted point.</p><p>Based on the phenomenon in <ref type="figure">Fig. 4</ref>, we argue the manyto-one mapping is the major issue in our pipeline. From the discussion above, we know those points mapped on the same cell may have large distances with each other, thus do not share the same label. This implies a simple solution that we can directly assign the label of the nearest point in 3D space to those occluded points. Similar KNN, we design our NLA (nearest label assignment), in Alg. 1, on range image to find the nearest point in a local patch that is GPU enabled. At the (top), we visualize those points that are mapped on the range image. Those labels are directly predicted from the neural network and have clear boundaries. At the (bottom), we add the other points that are mapped on already occupied pixels (more than one point mapped on one pixel). Some of those points are from object boundaries, thus do not share the same label as the predicted one on the pixel. Directly assigning predicted labels will cause the blurred boundary effect as  Compared with KNN, our NLA post-processing step does not need Gaussian weighting and range cutoff, thus is a less complex solution. In the ablation study, we show our nearest label assignment post-processing step has better mIoU with faster inference speed than KNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Other Training Settings</head><p>For the data augmentation, we followed other papers to do the rotation and flipping along the y axis <ref type="bibr" target="#b26">[23]</ref>, <ref type="bibr" target="#b10">[7]</ref>. We set the batch size as 2 and adopted the Adam optimizer with a onecycle learning rate policy. The maximum learning rate was set to 0.002, and the total training epoch was set to 30. For the loss function, we combined the weighted cross-entropy loss <ref type="bibr" target="#b41">[38]</ref> and the Lov?sz-Softmax loss <ref type="bibr" target="#b42">[39]</ref> together. Thanks to the parameter-free FID (fully interpolation decoding) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>The SemanticKITTI dataset <ref type="bibr" target="#b5">[2]</ref> is a recent large-scale dataset that provides dense point-wise annotations for the entire KITTI Odometry Benchmark <ref type="bibr" target="#b43">[40]</ref>. The dataset consists of 22 sequences in total. In this paper, we strictly follow the official split to train the model on sequences 00 to 07, and sequences 09, 10. Sequence 08 is used as the validation set to help us choose the best checkpoint. We submit our predictions on sequences 11 to 21 and report the result from the leaderboard to compare with others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Comparison</head><p>We compare our model performance with other methods in <ref type="table" target="#tab_1">Table I</ref>. As we design the structure to process the projected range image by considering point properties, we mainly compare our method with the point-wise solutions as well as the projection-based solutions. By following the same settings, our pipeline outperforms all point-wise solutions with a large margin and achieves similar performance with the best projection-based solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>In this paper, we empirically find our network predictions do not have a strong edge blurring effect. The blurring on the boundary is mainly caused by the many-to-one mapping problem. To solve this, we design a new post-processing algorithm that simply assigns the label of the nearest point in 3D space to those points without predictions from the neural network. In <ref type="table" target="#tab_1">Table II</ref>, we can see this NLA (Nearest Label Assignment) post-processing algorithm achieves better performance with faster inference speed than commonly used KNN. The experiment is conducted on the validation set of SemanticKITTI. The hardware used here is Nvidia RTX 2080 TI. It is also worth saying the CNN structure only needs 11ms to process one sample. Although we use the mix-precision choice provided by PyTorch during training, this inference speed is still remarkable. We credit this benefit to the parameter-free FID module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we discuss how to design the neural network solution for projection-based LiDAR point cloud segmentation. We propose a pipeline with an input module, a regular backbone, a FID module for upsampling, a classification head, and a post-processing step named NLA. In each module, we first try to choose the most common network settings, then argue why the chosen setting works.</p><p>Our pipeline achieves good performance on the benchmark and keeps a simple structure which we believe is hardwarefriendly for both GPUs and onboard processors. We released our code, to make further development easier.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of our network structure. The input module has two 1 ? 1 layers mapping each point to a high dimensional space. The backbone can be any regular standard network, like ResNet-34 used in this paper. The FID module upsamples all low-resolution feature maps to the original size and concatenates them together. The last classification head takes in the merged large tensor and outputs the label of each point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of our classification head. This head aggregates the original tensor from the FID (fully interpolation decoding) module with tensors from atrous convolution layers. The final point-wise feature tensor is processed by regular 1 ? 1 convolution layers to get the semantic label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 4. At the (top), we visualize those points that are mapped on the range image. Those labels are directly predicted from the neural network and have clear boundaries. At the (bottom), we add the other points that are mapped on already occupied pixels (more than one point mapped on one pixel). Some of those points are from object boundaries, thus do not share the same label as the predicted one on the pixel. Directly assigning predicted labels will cause the blurred boundary effect as Middle-Red points around the car on the bottom-left image and the Dark-Blue points around poles on the bottom-right image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Labels</head><label></label><figDesc>? empty list [ ], k ? 5 S(h, w, k) ? ?(h n , w m ), where (h n , w m ) in the k ? k local patch centered at (h, w); foreach i in 1 : R all .length() do min di f f ? +?; foreach position (h n , w m ) in S(h all [i], w all [i], k) do if abs(I r (h n , w m ) ? R all [i]) &lt; min di f f then label each = I label (h n , w m ) min di f f = abs(I r (h n , w m ) ? R all [i]) end Labels.append(label each) end end return Labels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Middle-Red points around the car on the bottom-left image and the Dark-Blue points around poles on the bottom-right image. Range image I r with size H ?W , predicted label map I label with size H ?W , vector R all with range values for all points, vector h all with projected h values for all points, vector w all with projected w values for all points, local kernel size k. Output: Vector Labels with predicted labels for all points.</figDesc><table><row><cell>Algorithm 1 Nearest Label Assignment (NLA)</cell></row><row><cell>Input :</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I THE</head><label>I</label><figDesc>PERFORMANCE COMPARISON ON SEMANTICKITTI TEST SET (SEQUENCE 11 TO SEQUENCE 21). THE UPPER-HALF ARE POINT-WISE METHODS, AND THE LOWER-HALF ARE PROJECTION-BASED METHODS.</figDesc><table><row><cell>Methods</cell><cell>Size</cell><cell>mean-IoU</cell><cell>car</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>truck</cell><cell>other-vehicle</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>road</cell><cell>parking</cell><cell>sidewalk</cell><cell>other-ground</cell><cell>building</cell><cell>fence</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>pole</cell><cell>traffic-sign</cell></row><row><cell>PointNet [3]</cell><cell>50K pts</cell><cell cols="20">14.6 46.3 1.3 0.3 0.1 0.8 0.2 0.2 0.0 61.6 15.8 35.7 1.4 41.4 12.9 31.0 4.6 17.6 2.4 3.7</cell></row><row><cell>PointNet++ [4]</cell><cell>50 K pts</cell><cell cols="20">20.1 53.7 1.9 0.2 0.9 0.2 0.9 1.0 0.0 72.0 18.7 41.8 5.6 62.3 16.9 46.5 13.8 30.0 6.0 8.9</cell></row><row><cell>SPGraph [14]</cell><cell>50K pts</cell><cell cols="20">20.0 68.3 0.9 4.5 0.9 0.8 1.0 6.0 0.0 49.5 1.7 24.2 0.3 68.2 22.5 59.2 27.2 17.0 18.3 10.5</cell></row><row><cell>SPLATNet [30]</cell><cell>50K pts</cell><cell cols="20">22.8 66.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 70.4 0.8 41.5 0.0 68.7 27.8 72.3 35.9 35.8 13.8 0.0</cell></row><row><cell>TangentConv [31]</cell><cell>50K pts</cell><cell cols="20">35.9 86.8 1.3 12.7 11.6 10.2 17.1 20.2 0.5 82.9 15.2 61.7 9.0 82.8 44.2 75.5 42.5 55.5 30.2 22.2</cell></row><row><cell>PointASNL [32]</cell><cell>8K pts</cell><cell cols="20">46.8 87.9 0.0 25.1 39.0 29.2 34.2 57.6 0.0 87.4 24.3 74.3 1.8 83.1 43.9 84.1 52.2 70.6 57.8 36.9</cell></row><row><cell cols="8">LatticeNet [33] 50 K pts module, all our experiments were conducted on a single RTX</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">2080 Ti with the mix-precision choice in PyTorch.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>COMPARISON BETWEEN PROPOSED NEAREST LABEL ASSIGNMENT AND COMMONLY USED KNN.</figDesc><table><row><cell>resolution</cell><cell>Network Modules</cell><cell cols="2">mIoU (%) process time(ms)</cell></row><row><cell></cell><cell>CNN +</cell><cell>55.4</cell><cell>11</cell></row><row><cell>64 ? 2048</cell><cell>K-Nearest-Neighbor</cell><cell>58.7</cell><cell>2.7</cell></row><row><cell></cell><cell>Nearest Label Assignment</cell><cell>58.9</cell><cell>1.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">x 1 Conv, F = 64 1 x 1 Conv, F = 128 64 x 2048 x 5, ( x, y, z, remission, r )</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">X 1, channel = 128 1 X 1, channel = 20 Softmax 1 X 1, channel = 512 Predictions Fig. 5. Illustration of our updated network structure. The input module has two 1 ? 1 layers mapping each point to a high dimensional space. The backbone can be any regular standard network, like ResNet-34 used in this paper. The FID module upsamples all low-resolution feature maps to the original size and concatenates them together. The last classification head takes in the merged large tensor and outputs the label of each point.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SUPPLEMENTARY FILE WITH FOLLOW-UP UPDATES A. The Goal of the Paper</head><p>Implementing complicated network modules with only one or two points of improvement on hardware is tedious. So here we propose a LiDAR semantic segmentation pipeline on 2D range image just with the most commonly used operators: regular convolutional operator, batch normalization, relu, and bilinear upsample operator. The designed network structure is simple but efficient. We make it achieve comparable performance with state-of-the-art projection-based solutions. The training can be done on a single RTX 2080 Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Change after Official IROS Paper</head><p>The Input Tensor. In the original paper, the input tensor has five channels, x, y, z, range, and remission. We empirically find adding a normal vector for each point will make the training more stable. So we update the input as a eight-channel tensor with x, y, z, range, remission, n 1 , n 2 , and n 3 . (n 1 , n 2 , n 3 ) is the normal vector calculated by following <ref type="bibr" target="#b44">[41]</ref>, <ref type="bibr" target="#b45">[42]</ref>. The Input Module. The input module in the original paper contains two 1 ? 1 convolutional layers. We further use five layers to map each point to a higher dimension space. The Classification Head. We realize the use of the ASPP module is computational heavy. So the updated classification head only contains two 1 ? 1 convolutional layers with a final softmax layer. In Summary. We show the updated network structure in <ref type="figure">Fig. 5</ref>. The new structure can achieve around 60.0 test mIoU with around 6M parameters in total. Thanks to the half-precision ability of PyTorch, the inference speed is 0.01s for each frame.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S-Bki</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>34] all 51.3 83.8 30.6 43.0 26.0 19.6 8.5 3.4 0.0 92.6 65.3 77.4 30.1 89.7 63.7 83.4 64.3 67.4 58.6 67.1</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kpconv</surname></persName>
		</author>
		<idno>16] 50K pts 58.8 96.0 30.2 42</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">61</biblScope>
		</imprint>
	</monogr>
	<note>6 11.8 88.8 61.3 72.7 31.6 90.5 64.2 84.8 69.2 69.1 56.4 47.4</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polarnet</surname></persName>
		</author>
		<idno>3D-MiniNet-KNN [13] 64 ? 2048 55.8 90.5 42.3 42.1 28.5 29.4 47</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">51</biblScope>
		</imprint>
	</monogr>
	<note>8 44.1 14.5 91.6 64.2 74.5 25.4 89.4 60.8 82.8 60.8 66.7 48.0 56.6 Ours 64 ? 2048 59.5 93.9 54.7 48.9 27.6 23.9 62.3 59.8 23.7 90.6 59.1 75.8 26.7 88.9 60.5 84.5 64.4 69.0 53.3 62.8</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rangenet++: Fast and accurate lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="4213" to="4220" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9297" to="9307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1887" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of largescale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Salsanext: Fast semantic segmentation of lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cortinhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzelepis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Aksoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03653</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">2-s3net: Attentive feature fusion with adaptive feature selection for sparse semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Razani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04530</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep fusionnet for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi projection fusion for real-time semantic segmentation of 3d lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Alnaggar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Afifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhelw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1800" to="1809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">485</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d-mininet: Learning a 2d representation from point clouds for fast and efficient 3d lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Riazuelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5432" to="5439" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on ?-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="828" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Kprnet: Improving projection-based lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kochanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">K</forename><surname>Nejadasl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Booij</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12668</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Occuseg: Occupancy-aware 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2940" to="2949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cylindrical and asymmetrical 3d convolution networks for lidar segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10033</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Amvnet: Assertion-based multi-view fusion network for lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Widjaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Chong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Tornado-net: multiview total variation semantic segmentation with diamond inception module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gerdzhev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Razani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.10544</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep neural network compression with single and multiple level quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Importance estimation for neural network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5589" to="5598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Latticenet: Fast point cloud segmentation using permutohedral lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05905</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bayesian spatial kernel smoothing for scalable dense semantic mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Grizzle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Eustice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghaffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="790" to="797" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for roadobject segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Salsanet: Fast road and vehicle segmentation in lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cavdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="926" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Polarnet: An improved grid representation for online lidar point clouds semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9601" to="9610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07836</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The lov?sz-softmax loss: A tractable surrogate for the optimization of the intersectionover-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4413" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast and accurate computation of surface normals from range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Badino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3084" to="3091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A surface geometry model for lidar depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4457" to="4464" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
