<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distributional Reinforcement Learning with Quantile Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Rowland</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G Bellemare</forename><surname>Google</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brain</forename><surname>R?mi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munos</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Distributional Reinforcement Learning with Quantile Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In reinforcement learning an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by . First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In reinforcement learning, the value of an action a in state s describes the expected return, or discounted sum of rewards, obtained from beginning in that state, choosing action a, and subsequently following a prescribed policy. Because knowing this value for the optimal policy is sufficient to act optimally, it is the object modelled by classic value-based methods such as SARSA (Rummery and Niranjan 1994) and Q-Learning <ref type="bibr" target="#b31">(Watkins and Dayan 1992)</ref>, which use Bellman's equation <ref type="bibr" target="#b6">(Bellman 1957)</ref> to efficiently reason about value.</p><p>Recently,  showed that the distribution of the random returns, whose expectation constitutes the aforementioned value, can be described by the distributional analogue of Bellman's equation, echoing previous results in risk-sensitive reinforcement learning <ref type="bibr" target="#b13">(Heger 1994;</ref><ref type="bibr" target="#b20">Morimura et al. 2010;</ref><ref type="bibr" target="#b8">Chow et al. 2015)</ref>. In this previous work, however, the authors argued for the usefulness in modeling this value distribution in and of itself. Their claim was asserted by exhibiting a distributional reinforcement learning algorithm, C51, which achieved state-of- * Contributed during an internship at DeepMind. the-art on the suite of benchmark Atari 2600 games <ref type="bibr" target="#b3">(Bellemare et al. 2013)</ref>.</p><p>One of the theoretical contributions of the C51 work was a proof that the distributional Bellman operator is a contraction in a maximal form of the Wasserstein metric between probability distributions. In this context, the Wasserstein metric is particularly interesting because it does not suffer from disjoint-support issues <ref type="bibr" target="#b2">(Arjovsky, Chintala, and Bottou 2017)</ref> which arise when performing Bellman updates. Unfortunately, this result does not directly lead to a practical algorithm: as noted by the authors, and further developed by , the Wasserstein metric, viewed as a loss, cannot generally be minimized using stochastic gradient methods.</p><p>This negative result left open the question as to whether it is possible to devise an online distributional reinforcement learning algorithm which takes advantage of the contraction result. Instead, the C51 algorithm first performs a heuristic projection step, followed by the minimization of a KL divergence between projected Bellman update and prediction. The work therefore leaves a theory-practice gap in our understanding of distributional reinforcement learning, which makes it difficult to explain the good performance of C51. Thus, the existence of a distributional algorithm that operates end-to-end on the Wasserstein metric remains an open question.</p><p>In this paper, we answer this question affirmatively. By appealing to the theory of quantile regression <ref type="bibr" target="#b17">(Koenker 2005)</ref>, we show that there exists an algorithm, applicable in a stochastic approximation setting, which can perform distributional reinforcement learning over the Wasserstein metric. Our method relies on the following techniques:</p><p>? We "transpose" the parametrization from C51: whereas the former uses N fixed locations for its approximation distribution and adjusts their probabilities, we assign fixed, uniform probabilities to N adjustable locations; ? We show that quantile regression may be used to stochastically adjust the distributions' locations so as to minimize the Wasserstein distance to a target distribution. ? We formally prove contraction mapping results for our overall algorithm, and use these results to conclude that our method performs distributional RL end-to-end under the Wasserstein metric, as desired.</p><p>The main interest of the original distributional algorithm was its state-of-the-art performance, despite still acting by maximizing expectations. One might naturally expect that a direct minimization of the Wasserstein metric, rather than its heuristic approximation, may yield even better results. We derive the Q-Learning analogue for our method (QR-DQN), apply it to the same suite of Atari 2600 games, and find that it achieves even better performance. By using a smoothed version of quantile regression, Huber quantile regression, we gain an impressive 33% median score increment over the already state-of-the-art C51.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distributional RL</head><p>We model the agent-environment interactions by a Markov decision process (MDP) (X , A, R, P, ?) <ref type="bibr" target="#b23">(Puterman 1994)</ref>, with X and A the state and action spaces, R the random variable reward function, P (x |x, a) the probability of transitioning from state x to state x after taking action a, and ? ? [0, 1) the discount factor. A policy ?(?|x) maps each state x ? X to a distribution over A.</p><p>For a fixed policy ?, the return, Z ? = ? t=0 ? t R t , is a random variable representing the sum of discounted rewards observed along one trajectory of states while following ?. Standard RL algorithms estimate the expected value of Z ? , the value function,</p><formula xml:id="formula_0">V ? (x) := E [Z ? (x)] = E ? t=0 ? t R(x t , a t ) | x 0 = x .</formula><p>(1) Similarly, many RL algorithms estimate the action-value function,</p><formula xml:id="formula_1">Q ? (x, a) := E [Z ? (x, a)] = E ? t=0 ? t R(x t , a t ) , (2) x t ? P (?|x t?1 , a t?1 ), a t ? ?(?|x t ), x 0 = x, a 0 = a.</formula><p>The -greedy policy on Q ? chooses actions uniformly at random with probability and otherwise according to arg max a Q ? (x, a).</p><p>In distributional RL the distribution over returns (i.e. the probability law of Z ? ), plays the central role and replaces the value function. We will refer to the value distribution by its random variable. When we say that the value function is the mean of the value distribution we are saying that the value function is the expected value, taken over all sources of intrinsic randomness <ref type="bibr" target="#b12">(Goldstein, Misra, and Courtage 1981)</ref>, of the value distribution. This should highlight that the value distribution is not designed to capture the uncertainty in the estimate of the value function <ref type="bibr" target="#b9">(Dearden, Friedman, and Russell 1998;</ref><ref type="bibr" target="#b10">Engel, Mannor, and Meir 2005)</ref>, that is the parametric uncertainty, but rather the randomness in the returns intrinsic to the MDP.</p><p>Temporal difference (TD) methods significantly speed up the learning process by incrementally improving an estimate of Q ? using dynamic programming through the Bellman operator <ref type="bibr" target="#b6">(Bellman 1957)</ref>,</p><formula xml:id="formula_2">T ? Q(x, a) = E [R(x, a)] + ?E P,? [Q(x , a )] . T ? Z z 1 z 2 q 1 q 2 T ? Z D KL ( T ? ZkZ) T ? Z 4z 4z 4z</formula><p>4z <ref type="figure">Figure 1</ref>: Projection used by C51 assigns mass inversely proportional to distance from nearest support. Update minimizes KL between projected target and estimate.</p><p>Similarly, the value distribution can be computed through dynamic programming using a distributional Bellman operator ,</p><formula xml:id="formula_3">T ? Z(x, a) : D = R(x, a) + ?Z(x , a ), (3) x ? P (?|x, a), a ? ?(?|x ),</formula><p>where Y : D = U denotes equality of probability laws, that is the random variable Y is distributed according to the same law as U .</p><p>The C51 algorithm models Z ? (x, a) using a discrete distribution supported on a "comb" of fixed locations z 1 ? ? ? ? ? z N uniformly spaced over a predetermined interval. The parameters of that distribution are the probabilities q i , expressed as logits, associated with each location z i . Given a current value distribution, the C51 algorithm applies a projection step ? to map the target T ? Z onto its finite element support, followed by a Kullback-Leibler (KL) minimization step (see <ref type="figure">Figure 1</ref>). C51 achieved state-of-the-art performance on Atari 2600 games, but did so with a clear disconnect with the theoretical results of . We now review these results before extending them to the case of approximate distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Wasserstein Metric</head><p>The p-Wasserstein metric W p , for p ? [1, ?], also known as the Mallows metric <ref type="bibr" target="#b7">(Bickel and Freedman 1981)</ref> or the Earth Mover's Distance (EMD) when p = 1 <ref type="bibr" target="#b18">(Levina and Bickel 2001)</ref>, is an integral probability metric between distributions. The p-Wasserstein distance is characterized as the L p metric on inverse cumulative distribution functions (inverse CDFs) <ref type="bibr" target="#b21">(M?ller 1997)</ref>. That is, the p-Wasserstein metric between distributions U and Y is given by, 1</p><formula xml:id="formula_4">W p (U, Y ) = 1 0 |F ?1 Y (?) ? F ?1 U (?)| p d? 1/p ,<label>(4)</label></formula><p>where for a random variable Y , the inverse CDF</p><formula xml:id="formula_5">F ?1 Y of Y is defined by F ?1 Y (?) := inf{y ? R : ? ? F Y (y)} ,<label>(5)</label></formula><p>where F Y (y) = P r(Y ? y) is the CDF of Y . <ref type="figure">Figure 2</ref> illustrates the 1-Wasserstein distance as the area between two CDFs.</p><p>Recently, the Wasserstein metric has been the focus of increased research due to its appealing properties of respecting the underlying metric distances between outcomes (Arjovsky, Chintala, and Bottou 2017; . Unlike the Kullback-Leibler divergence, the Wasserstein metric is a true probability metric and considers both the probability of and the distance between various outcome events. These properties make the Wasserstein well-suited to domains where an underlying similarity in outcome is more important than exactly matching likelihoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convergence of Distributional Bellman Operator</head><p>In the context of distributional RL, let Z be the space of action-value distributions with finite moments:</p><formula xml:id="formula_6">Z = {Z : X ? A ? P(R)| E [|Z(x, a)| p ] &lt; ?, ?(x, a), p ? 1}.</formula><p>Then, for two action-value distributions Z 1 , Z 2 ? Z, we will use the maximal form of the Wasserstein metric introduced by (Bellemare, Dabney, and Munos 2017),</p><formula xml:id="formula_7">d p (Z 1 , Z 2 ) := sup x,a W p (Z 1 (x, a), Z 2 (x, a)).<label>(6)</label></formula><p>It was shown thatd p is a metric over value distributions. Furthermore, the distributional Bellman operator T ? is a contraction ind p , a result that we now recall. Lemma 1 (Lemma 3, Bellemare, Dabney, and Munos 2017). T ? is a ?-contraction: for any two Z 1 , Z 2 ? Z,</p><formula xml:id="formula_8">d p (T ? Z 1 , T ? Z 2 ) ? ?d p (Z 1 , Z 2 ).</formula><p>Lemma 1 tells us thatd p is a useful metric for studying the behaviour of distributional reinforcement learning algorithms, in particular to show their convergence to the fixed point Z ? . Moreover, the lemma suggests that an effective way in practice to learn value distributions is to attempt to minimize the Wasserstein distance between a distribution Z and its Bellman update T ? Z, analogous to the way that TDlearning attempts to iteratively minimize the L 2 distance between Q and T Q.</p><p>Unfortunately, another result shows that we cannot in general minimize the Wasserstein metric (viewed as a loss) using stochastic gradient descent.</p><p>Theorem 1 (Theorem 1, ). Let? m := 1 m m i=1 ? Yi be the empirical distribution derived from samples Y 1 , . . . , Y m drawn from a Bernoulli distribution B. Let B ? be a Bernoulli distribution parametrized by ?, the probability of the variable taking the value 1. Then the minimum of the expected sample loss is in general different from the minimum of the true Wasserstein loss; that is,</p><formula xml:id="formula_9">arg min ? E Y1:m W p (? m , B ? ) = arg min ? W p (B, B ? ).</formula><p>This issue becomes salient in a practical context, where the value distribution must be approximated. Crucially, the C51 algorithm is not guaranteed to minimize any p-Wasserstein metric. This gap between theory and practice in distributional RL is not restricted to C51. Morimura et </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Space of Returns</head><formula xml:id="formula_10">Probability Space ? 1 ? 2 ? 3 ? 4 = 1 ? 0 = 0 ? 1 ? 2 ? 3 ? 4 q 1 q 2 q 3 q 4 Z 2 Z ? W 1 Z 2 Z Q z 1 = F 1 Z (? 1 ) z 2 z 3 z 4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approximately Minimizing Wasserstein</head><p>Recall that C51 approximates the distribution at each state by attaching variable (parametrized) probabilities q 1 , . . . , q N to fixed locations z 1 ? ? ? ? ? z N . Our approach is to "transpose" this parametrization by considering fixed probabilities but variable locations. Specifically, we take uniform weights, so that q i = 1/N for each i = 1, . . . , N . Effectively, our new approximation aims to estimate quantiles of the target distribution. Accordingly, we will call it a quantile distribution, and let Z Q be the space of quantile distributions for fixed N . We will denote the cumulative probabilities associated with such a distribution (that is, the discrete values taken on by the CDF) by ? 1 , . . . , ? N , so that ? i = i N for i = 1, . . . , N . We will also write ? 0 = 0 to simplify notation.</p><p>Formally, let ? :</p><formula xml:id="formula_11">X ?A ? R N be some parametric model. A quantile distribution Z ? ? Z Q maps each state-action pair (x, a) to a uniform probability distribution supported on {? i (x, a)}. That is, Z ? (x, a) := 1 N N i=1 ? ?i(x,a) ,<label>(7)</label></formula><p>where ? z denotes a Dirac at z ? R.</p><p>Compared to the original parametrization, the benefits of a parameterized quantile distribution are threefold. First, (1) we are not restricted to prespecified bounds on the support, or a uniform resolution, potentially leading to significantly more accurate predictions when the range of returns vary greatly across states. This also (2) lets us do away with the unwieldy projection step present in C51, as there are no issues of disjoint supports. Together, these obviate the need for domain knowledge about the bounds of the return distribution when applying the algorithm to new tasks. Finally, (3) this reparametrization allows us to minimize the Wasserstein loss, without suffering from biased gradients, specifically, using quantile regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Quantile Approximation</head><p>It is well-known that in reinforcement learning, the use of function approximation may result in instabilities in the learning process <ref type="bibr" target="#b28">(Tsitsiklis and Van Roy 1997)</ref>. Specifically, the Bellman update projected onto the approximation space may no longer be a contraction. In our case, we analyze the distributional Bellman update, projected onto a parameterized quantile distribution, and prove that the combined operator is a contraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantile Projection We are interested in quantifying the projection of an arbitrary value distribution</head><formula xml:id="formula_12">Z ? Z onto Z Q , that is ? W1 Z := arg min Z ? ?Z Q W 1 (Z, Z ? ),</formula><p>Let Y be a distribution with bounded first moment and U a uniform distribution over N Diracs as in <ref type="formula" target="#formula_11">(7)</ref>, with support</p><formula xml:id="formula_13">{? 1 , . . . , ? N }. Then W 1 (Y, U ) = N i=1 ?i ?i?1 |F ?1 Y (?) ? ? i |d?.</formula><p>Lemma 2. For any ?, ? ? [0, 1] with ? &lt; ? and cumulative distribution function F with inverse F ?1 , the set of ? ? R minimizing</p><formula xml:id="formula_14">? ? |F ?1 (?) ? ?|d? ,</formula><p>is given by</p><formula xml:id="formula_15">? ? R F (?) = ? + ? 2 .</formula><p>In particular, if F ?1 is the inverse CDF, then F ?1 ((? + ? )/2) is always a valid minimizer, and if F ?1 is continuous at</p><formula xml:id="formula_16">(? + ? )/2, then F ?1 ((? + ? )/2) is the unique minimizer.</formula><p>These quantile midpoints will be denoted by? i = ?i?1+?i 2 for 1 ? i ? N . Therefore, by Lemma 2, the values for <ref type="figure">Figure 2</ref> shows an example of the quantile projection ? W1 Z minimizing the 1-Wasserstein distance to Z. 2</p><formula xml:id="formula_17">{? 1 , ? 1 , . . . , ? N } that minimize W 1 (Y, U ) are given by ? i = F ?1 Y (? i ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantile Regression</head><p>The original proof of Theorem 1 only states the existence of a distribution whose gradients are biased. As a result, we might hope that our quantile parametrization leads to unbiased gradients. Unfortunately, this is not true. Proposition 1. Let Z ? be a quantile distribution, and? m the empirical distribution composed of m samples from Z. Then for all p ? 1, there exists a Z such that</p><formula xml:id="formula_18">arg min E[W p (? m , Z ? )] = arg min W p (Z, Z ? ).</formula><p>However, there is a method, more widely used in economics than machine learning, for unbiased stochastic approximation of the quantile function. Quantile regression, and conditional quantile regression, are methods for approximating the quantile functions of distributions and conditional distributions respectively <ref type="bibr" target="#b17">(Koenker 2005</ref>). These methods have been used in a variety of settings where outcomes have intrinsic randomness <ref type="bibr" target="#b16">(Koenker and Hallock 2001)</ref>; from food expenditure as a function of household income <ref type="bibr" target="#b11">(Engel 1857)</ref>, to studying value-at-risk in economic models <ref type="bibr" target="#b26">(Taylor 1999)</ref>.</p><p>The quantile regression loss, for quantile ? ? [0, 1], is an asymmetric convex loss function that penalizes overestimation errors with weight ? and underestimation errors with weight 1?? . For a distribution Z, and a given quantile ? , the value of the quantile function F ?1 Z (? ) may be characterized as the minimizer of the quantile regression loss</p><formula xml:id="formula_19">L ? QR (?) := E? ?Z [? ? (? ? ?)] , where ? ? (u) = u(? ? ? {u&lt;0} ), ?u ? R.<label>(8)</label></formula><p>More generally, by Lemma 2 we have that the minimizing values of {? 1 , . . . , ? N } for W 1 (Z, Z ? ) are those that minimize the following objective:</p><formula xml:id="formula_20">N i=1 E? ?Z [?? i (? ? ? i )]</formula><p>In particular, this loss gives unbiased sample gradients. As a result, we can find the minimizing {? 1 , . . . , ? N } by stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantile Huber Loss</head><p>The quantile regression loss is not smooth at zero; as u ? 0 + , the gradient of Equation 8 stays constant. We hypothesized that this could limit performance when using non-linear function approximation. To this end, we also consider a modified quantile loss, called the quantile Huber loss. 3 This quantile regression loss acts as an asymmetric squared loss in an interval [??, ?] around zero and reverts to a standard quantile loss outside this interval.</p><p>The Huber loss is given by <ref type="bibr" target="#b14">(Huber 1964)</ref>,</p><formula xml:id="formula_21">L ? (u) = 1 2 u 2 , if |u| ? ? ?(|u| ? 1 2 ?), otherwise .<label>(9)</label></formula><p>The quantile Huber loss is then simply the asymmetric variant of the Huber loss,</p><formula xml:id="formula_22">? ? ? (u) = |? ? ? {u&lt;0} |L ? (u).<label>(10)</label></formula><p>For notational simplicity we will denote ? 0 ? = ? ? , that is, it will revert to the standard quantile regression loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combining Projection and Bellman Update</head><p>We are now in a position to prove our main result, which states that the combination of the projection implied by quantile regression with the Bellman operator is a contraction. The result is in ?-Wasserstein metric, i.e. the size of the largest gap between the two CDFs. Proposition 2. Let ? W1 be the quantile projection defined as above, and when applied to value distributions gives the projection for each state-value distribution. For any two value distributions Z 1 , Z 2 ? Z for an MDP with countable state and action spaces,</p><formula xml:id="formula_23">d ? (? W1 T ? Z 1 , ? W1 T ? Z 2 ) ? ?d ? (Z 1 , Z 2 ).</formula><p>(11) We therefore conclude that the combined operator ? W1 T ? has a unique fixed point? ? , and the repeated application of this operator, or its stochastic approximation, converges to? ? . Becaused p ?d ? , we conclude that convergence occurs for all p ? [1, ?]. Interestingly, the contraction property does not directly hold for p &lt; ?; see Lemma 5 in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distributional RL using Quantile Regression</head><p>We can now form a complete algorithmic approach to distributional RL consistent with our theoretical results. That is, approximating the value distribution with a parameterized quantile distribution over the set of quantile midpoints, defined by Lemma 2. Then, training the location parameters using quantile regression <ref type="formula" target="#formula_19">(Equation 8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantile Regression Temporal Difference Learning</head><p>Recall the standard TD update for evaluating a policy ?,</p><formula xml:id="formula_24">V (x) ? V (x) + ?(r + ?V (x ) ? V (x)), a ? ?(?|x), r ? R(x, a), x ? P (?|x, a)</formula><p>. TD allows us to update the estimated value function with a single unbiased sample following ?. Quantile regression also allows us to improve the estimate of the quantile function for some target distribution, Y (x), by observing samples y ? Y (x) and minimizing Equation 8.</p><p>Furthermore, we have shown that by estimating the quantile function for well-chosen values of ? ? (0, 1) we can obtain an approximation with minimal 1-Wasserstein distance from the original (Lemma 2). Finally, we can combine this with the distributional Bellman operator to give a target distribution for quantile regression. This gives us the quantile regression temporal difference learning (QRTD) algorithm, summarized simply by the update,</p><formula xml:id="formula_25">? i (x) ? ? i (x) + ?(? i ? ? {r+?z &lt;?i(x))} ), (12) a ? ?(?|x), r ? R(x, a), x ? P (?|x, a), z ? Z ? (x ),</formula><p>where Z ? is a quantile distribution as in <ref type="formula" target="#formula_11">(7)</ref>, and ? i (x) is the estimated value of F ?1 Z ? (x) (? i ) in state x. It is important to note that this update is for each value of? i and is defined for a single sample from the next state value distribution. In general it is better to draw many samples of z ? Z(x ) and minimize the expected update. A natural approach in this case, which we use in practice, is to compute the update for all pairs of (? i (x), ? j (x )). Next, we turn to a control algorithm and the use of non-linear function approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantile Regression DQN</head><p>Q-Learning is an off-policy reinforcement learning algorithm built around directly learning the optimal action-value function using the Bellman optimality operator <ref type="bibr" target="#b31">(Watkins and Dayan 1992)</ref>,</p><formula xml:id="formula_26">T Q(x, a) = E [R(x, a)] + ? E x ?P max a Q(x , a ) .</formula><p>The distributional variant of this is to estimate a stateaction value distribution and apply a distributional Bellman optimality operator,</p><formula xml:id="formula_27">T Z(x, a) = R(x, a) + ?Z(x , a ),<label>(13)</label></formula><formula xml:id="formula_28">x ? P (?|x, a), a = arg max a E z?Z(x ,a ) [z] .</formula><p>Notice in particular that the action used for the next state is the greedy action with respect to the mean of the next stateaction value distribution.</p><p>For a concrete algorithm we will build on the DQN architecture <ref type="bibr" target="#b19">(Mnih et al. 2015)</ref>. We focus on the minimal changes necessary to form a distributional version of DQN. Specifically, we require three modifications to DQN. First, we use a nearly identical neural network architecture as DQN, only changing the output layer to be of size |A| ? N , where N is a hyper-parameter giving the number of quantile targets. Second, we replace the Huber loss used by DQN 4 , L ? (r t + ? max a Q(x t+1 , a ) ? Q(x t , a t )) with ? = 1, with a quantile Huber loss (full loss given by Algorithm 1). Finally, we replace RMSProp <ref type="bibr" target="#b27">(Tieleman and Hinton 2012)</ref> with Adam <ref type="bibr" target="#b15">(Kingma and Ba 2015)</ref>. We call this new algorithm quantile regression DQN (QR-DQN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Quantile Regression Q-Learning</head><formula xml:id="formula_29">Require: N, ? input x, a, r, x , ? ? [0, 1) # Compute distributional Bellman target Q(x , a ) := j q j ? j (x , a ) a * ? arg max a Q(x, a ) T ? j ? r + ?? j (x , a * ), ?j # Compute quantile regression loss (Equation 10) output N i=1 E j ? ? ?i (T ? j ? ? i (x, a))</formula><p>Unlike C51, QR-DQN does not require projection onto the approximating distribution's support, instead it is able to expand or contract the values arbitrarily to cover the true range of return values. As an additional advantage, this means that QR-DQN does not require the additional hyper-parameter giving the bounds of the support required by C51. The only additional hyper-parameter of QR-DQN not shared by DQN is the number of quantiles N , which controls with what resolution we approximate the value distribution. As we increase N , QR-DQN goes from DQN to increasingly able to estimate the upper and lower quantiles of the value distribution. It becomes increasingly capable of distinguishing low probability events at either end of the cumulative distribution over returns.</p><p>x S 0 1 2 2 2 0 0 0 0 0 0 </p><formula xml:id="formula_30">x G Z(x S ) F Z(xS ) Returns Returns Z ? MC Z ? (a) (b) (c) [V ? MC (xS) V (xS)] 2 W1(Z ? MC (xS), Z(xS)) Episodes (d) (e)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>In the introduction we claimed that learning the distribution over returns had distinct advantages over learning the value function alone. We have now given theoretically justified algorithms for performing distributional reinforcement learning, QRTD for policy evaluation and QR-DQN for control. In this section we will empirically validate that the proposed distributional reinforcement learning algorithms: (1) learn the true distribution over returns, (2) show increased robustness during training, and (3) significantly improve sample complexity and final performance over baseline algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Value Distribution Approximation Error</head><p>We begin our experimental results by demonstrating that QRTD actually learns an approximate value distribution that minimizes the 1-Wasserstein to the ground truth distribution over returns. Although our theoretical results already establish convergence of the former to the latter, the empirical performance helps to round out our understanding. We use a variant of the classic windy gridworld domain (Sutton and Barto 1998), modified to have two rooms and randomness in the transitions. <ref type="figure" target="#fig_0">Figure 3(a)</ref> shows our version of the domain, where we have combined the transition stochasticity, wind, and the doorway to produce a multimodal distribution over returns when anywhere in the first room. Each state transition has probability 0.1 of moving in a random direction, otherwise the transition is affected by wind moving the agent northward. The reward function is zero until reaching the goal state x G , which terminates the episode and gives a reward of 1.0. The discount factor is ? = 0.99.</p><p>We compute the ground truth value distribution for optimal policy ?, learned by policy iteration, at each state by performing 1K Monte-Carlo (MC) rollouts and recording the observed returns as an empirical distribution, shown in <ref type="figure" target="#fig_0">Figure 3(b)</ref>. Next, we ran both TD(0) and QRTD while following ? for 10K episodes. Each episode begins in the designated start state (x S ). Both algorithms started with a learning rate of ? = 0.1. For QRTD we used N = 32 and drop ? by half every 2K episodes.</p><p>Let Z ? M C (x S ) be the MC estimated distribution over returns from the start state x S , similarly V ? M C (x S ) its mean.</p><p>In <ref type="figure" target="#fig_0">Figure 3</ref> we show the approximation errors at x S for both algorithms with respect to the number of episodes. In (d) we evaluated, for both TD(0) and QRTD, the squared error, (V ? M C ? V (x S )) 2 , and in (e) we show the 1-Wasserstein metric for QRTD, W 1 (Z ? M C (x S ), Z(x S )), where V (x S ) and Z(x S ) are the expected returns and value distribution at state x S estimated by the algorithm. As expected both algorithms converge correctly in mean, and QRTD minimizes the 1-Wasserstein distance to Z ? M C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on Atari 2600</head><p>We now provide experimental results that demonstrate the practical advantages of minimizing the Wasserstein metric end-to-end, in contrast to the C51 approach. We use the 57 Atari 2600 games from the Arcade Learning Environment (ALE) <ref type="bibr" target="#b3">(Bellemare et al. 2013)</ref>. Both C51 and QR-DQN build on the standard DQN architecture, and we expect both to benefit from recent improvements to DQN such as the dueling architectures <ref type="bibr" target="#b30">(Wang et al. 2016</ref>) and prioritized replay ). However, in our evaluations we compare the pure versions of C51 and QR-DQN without these additions. We present results for both a strict quantile loss, ? = 0 (QR-DQN-0), and with a Huber quantile loss with ? = 1 (QR-DQN-1). We performed hyper-parameter tuning over a set of five training games and evaluated on the full set of 57 games using these best settings (? = 0.00005, ADAM = 0.01/32, and N = 200). 5 As with DQN we use a target network when computing the distributional Bellman update. We also allow to decay at the same rate as in DQN, but to a lower value of 0.01, as is common in recent work <ref type="bibr" target="#b30">Wang et al. 2016;</ref><ref type="bibr" target="#b29">van Hasselt, Guez, and Silver 2016)</ref>.</p><p>Out training procedure follows that of <ref type="bibr" target="#b19">Mnih et al. (2015)</ref>'s, and we present results under two evaluation protocols: best agent performance and online performance. In both evaluation protocols we consider performance over all 57 Atari 2600 games, and transform raw scores into humannormalized scores (van Hasselt, Guez, and Silver 2016). <ref type="bibr">5</ref> We swept over ? in (10 ?3 , 5 ? 10 ?4 , 10 ?4 , 5 ? 10 ?5 , 10 ?5 ); ADAM in (0.01/32, 0.005/32, 0.001/32); N (10, 50, 100, 200)    <ref type="bibr" target="#b22">(Nair et al. 2015)</ref>.</p><p>Best agent performance To provide comparable results with existing work we report test evaluation results under the best agent protocol. Every one million training frames, learning is frozen and the agent is evaluated for 500K frames while recording the average return. Evaluation episodes begin with up to 30 random no-ops <ref type="bibr" target="#b19">(Mnih et al. 2015)</ref>, and the agent uses a lower exploration rate ( = 0.001). As training progresses we keep track of the best agent performance achieved thus far. <ref type="table" target="#tab_2">Table 1</ref> gives the best agent performance, at 200 million frames trained, for QR-DQN, C51, DQN, Double DQN (van Hasselt, Guez, and Silver 2016), Prioritized replay , and Dueling architecture <ref type="bibr" target="#b30">(Wang et al. 2016)</ref>. We see that QR-DQN outperforms all previous agents in mean and median human-normalized score.</p><p>Online performance In this evaluation protocol <ref type="figure" target="#fig_1">(Figure 4)</ref> we track the average return attained during each testing (left) and training (right) iteration. For the testing performance we use a single seed for each algorithm, but show online performance with no form of early stopping. For training performance, values are averages over three seeds. Instead of reporting only median performance, we look at the distribution of human-normalized scores over the full set of games. Each bar represents the score distribution at a fixed percentile <ref type="figure" target="#fig_0">(10th, 20th, 30th, 40th, and 50th)</ref>. The upper percentiles show a similar trend but are omitted here for visual clarity, as their scale dwarfs the more informative lower half.</p><p>From this, we can infer a few interesting results.</p><p>(1) Early in learning, most algorithms perform worse than random for at least 10% of games. (2) QRTD gives similar improvements to sample complexity as prioritized replay, while also improving final performance. (3) Even at 200 million frames, there are 10% of games where all algorithms reach less than 10% of human. This final point in particular shows us that all of our recent advances continue to be severely limited on a small subset of the Atari 2600 games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>The importance of the distribution over returns in reinforcement learning has been (re)discovered and highlighted many times by now. In  the idea was taken a step further, and argued to be a central part of approximate reinforcement learning. However, the paper left open the question of whether there exists an algorithm which could bridge the gap between Wasserstein-metric theory and practical concerns.</p><p>In this paper we have closed this gap with both theoretical contributions and a new algorithm which achieves stateof-the-art performance in Atari 2600. There remain many promising directions for future work. Most exciting will be to expand on the promise of a richer policy class, made possible by action-value distributions. We have mentioned a few examples of such policies, often used for risk-sensitive decision making. However, there are many more possible decision policies that consider the action-value distributions as a whole.</p><p>Additionally, QR-DQN is likely to benefit from the improvements on DQN made in recent years. For instance, due to the similarity in loss functions and Bellman operators we might expect that QR-DQN suffers from similar overestimation biases to those that Double DQN was designed to address <ref type="bibr" target="#b29">(van Hasselt, Guez, and Silver 2016)</ref>. A natural next step would be to combine QR-DQN with the nondistributional methods found in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>In particular, if F ?1 is the inverse CDF, then F ?1 ((? + ? )/2) is always a valid minimizer, and if F ?1 is continuous at (? + ? )/2, then F ?1 ((? + ? )/2) is the unique minimizer.</p><p>Proof. For any ? ? [0, 1], the function ? ? |F ?1 (?) ? ?| is convex, and has subgradient given by</p><formula xml:id="formula_31">? ? ? ? ? 1 if ? &lt; F ?1 (?) [?1, 1] if ? = F ?1 (?) ?1 if ? &gt; F ?1 (?) ,</formula><p>so the function ? ? ? ? |F ?1 (?) ? ?|d? is also convex, and has subgradient given by</p><formula xml:id="formula_32">? ? F (?) ? ?1d? + ? F (?) 1d? .</formula><p>Setting this subgradient equal to 0 yields</p><formula xml:id="formula_33">(? + ? ) ? 2F (?) = 0 ,<label>(14)</label></formula><p>and since F ? F ?1 is the identity map on [0, 1], it is clear that ? = F ?1 ((? + ? )/2) satisfies Equation 14. Note that in fact any ? such that F (?) = (? + ? )/2 yields a subgradient of 0, which leads to a multitude of minimizers if F ?1 is not continuous at (? + ? )/2. Proposition 1. Let Z ? be a quantile distribution, and? m the empirical distribution composed of m samples from Z. Then for all p ? 1, there exists a Z such that</p><formula xml:id="formula_34">arg min E[W p (? m , Z ? )] = arg min W p (Z, Z ? ). Proof. Write Z ? = N i=1 1 N ? ?i , with ? 1 ? ? ? ? ? ? N .</formula><p>We take Z to be of the same form as Z ? . Specifically, consider Z given by</p><formula xml:id="formula_35">Z = N i=1 1 N ? i ,</formula><p>supported on the set {1, . . . , N }, and take m = N . Then clearly the unique minimizing Z ? for W p (Z, Z ? ) is given by taking Z ? = Z. However, consider the gradient with respect to ? 1 for the objective</p><formula xml:id="formula_36">E[W p (? N , Z ? )] .</formula><p>We have</p><formula xml:id="formula_37">? ?1 E[W p (? N , Z ? )]| ?1=1 = E[? ?1 W p (? N , Z ? )| ?1=1 ] .</formula><p>In the event that the sample distribution? N has an atom at 1, then the optimal transport plan pairs the atom of Z ? at ? 1 = 1 with this atom of? N , and gradient with respect to ? 1 of W p (? N , Z ? ) is 0. If the sample distribution? N does not contain an atom at 1, then the left-most atom of? N is greater than 1 (since Z is supported on {1, . . . , N }. In this case, the gradient on ? 1 is negative. Since this happens with non-zero probability, we conclude that</p><formula xml:id="formula_38">? ?1 E[W p (? N , Z ? )]| ?1=1 &lt; 0 ,</formula><p>and therefore Z ? = Z cannot be the minimizer of</p><formula xml:id="formula_39">E[W p (? N , Z ? )].</formula><p>Proposition 2. Let ? W1 be the quantile projection defined as above, and when applied to value distributions gives the projection for each state-value distribution. For any two value distributions Z 1 , Z 2 ? Z for an MDP with countable state and action spaces,</p><formula xml:id="formula_40">d ? (? W1 T ? Z 1 , ? W1 T ? Z 2 ) ? ?d ? (Z 1 , Z 2 ).<label>(11)</label></formula><p>Proof. We assume that instantaneous rewards given a stateaction pair are deterministic; the general case is a straightforward generalization. Further, since the operator T ? is a ?-contraction in d ? , it is sufficient to prove the claim in the case ? = 1. In addition, since Wasserstein distances are invariant under translation of the support of distributions, it is sufficient to deal with the case where r(x, a) ? 0 for all (x, a) ? X ? A. The proof then proceeds by first reducing to the case where every value distribution consists only of single Diracs, and then dealing with this reduced case using Lemma 3. We write Z(x, a) = N k=1 1 N ? ? k (x,a) and Y (x, a) = N k=1 1 N ? ? k (x,a) , for some functions ?, ? : X ? A ? R n . Let (x, a) be a state-action pair, and let ((x i , a i )) i?I be all the state-action pairs that are accessible from (x , a ) in a single transition, where I is a (finite or countable) indexing set. Write p i for the probability of transitioning from (x , a ) to (x i , a i ), for each i ? I. We now construct a new MDP and new value distributions for this MDP in which all distributions are given by single Diracs, with a view to applying Lemma 3. The new MDP is of the following form. We take the state-action pair (x , a ), and define new states, actions, transitions, and a policy ?, so that the stateaction pairs accessible from (x , a ) in this new MDP are given by (( x j i , a j i ) i?I ) N j=1 , and the probability of reaching the state-action pair ( x j i , a j i ) is p i /n. Further, we define new value distributions Z, Y as follows. For each i ? I and j = 1, . . . , N , we set:</p><formula xml:id="formula_41">Z( x j i , a j i ) = ? ?j (xi,ai) Y ( x j i , a j i ) = ? ?j (xi,ai) .</formula><p>The construction is illustrated in <ref type="figure" target="#fig_2">Figure 5</ref>. Since, by Lemma 4, the d ? distance between the 1-Wasserstein projections of two real-valued distributions is the max over the difference of a certain set of quantiles, we may appeal to Lemma 3 to obtain the following:</p><formula xml:id="formula_42">d ? (? W1 (T ? Z)(x , a ), ? W1 (T ? Y )(x , a )) ? sup i=1?I j=1,...,N |? j (x i , a i ) ? ? j (x i , a i )| = sup i=1?I d ? (Z(x i , a i ), Y (x i , a i ))<label>(15)</label></formula><p>Now note that by construction, (T ? Z)(x , a ) (respectively, (T ? Y )(x , a )) has the same distribution as (T ? Z)(x , a ) (respectively, (T ? Y )(x , a )), and so</p><formula xml:id="formula_43">d ? (? W1 (T ? Z)(x , a ), ? W1 (T ? Y )(x , a )) = d ? (? W1 (T ? Z)(x , a ), ? W1 (T ? Y )(x , a )) .</formula><p>Therefore, substituting this into the Inequality 15, we obtain</p><formula xml:id="formula_44">d ? (? W1 (T ? Z)(x , a ), ? W1 (T ? Y )(x , a )) ? sup i?I d ? (Z(x i , a i ), Y (x i , a i )) .</formula><p>Taking suprema over the initial state (x , a ) then yields the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supporting results</head><p>Lemma 3. Consider an MDP with countable state and action spaces. Let Z, Y be value distributions such that each state-action distribution Z(x, a), Y (x, a) is given by a single Dirac. Consider the particular case where rewards are identically 0 and ? = 1, and let ? ? [0, 1]. Denote by ? ? the projection operator that maps a probability distribution onto a Dirac delta located at its ? th quantile. Then</p><formula xml:id="formula_45">d ? (? ? T ? Z, ? ? T ? Y ) ? d ? (Z, Y )</formula><p>Proof. Let Z(x, a) = ? ?(x,a) and Y (x, a) = ? ?(x,a) for each state-action pair (x, a) ? X ? A, for some functions ?, ? : X ? A ? R. Let (x , a ) be a state-action pair, and let ((x i , a i )) i?I be all the state-action pairs that are accessible from (x , a ) in a single transition, with I a (finite or countably infinite) indexing set. To lighten notation, we write ? i for ?(x i , a i ) and ? i for ?(x i , a i ). Further, let the probability of transitioning from (x , a ) to (x i , a i ) be p i , for all i ? I.</p><p>Then we have</p><formula xml:id="formula_46">(T ? Z)(x , a ) = i?I p i ? ?i (16) (T ? Y )(x , a ) = i?I p i ? ?i .<label>(17)</label></formula><p>Now consider the ? th quantile of each of these distributions, for ? ? [0, 1] arbitrary. Let u ? I be such that ? u is equal to this quantile of (T ? Z)(x , a ), and let v ? I such that ? v is equal to this quantile of (T ? Y )(x , a ). Now note that</p><formula xml:id="formula_47">d ? (? ? T ? Z(x , a ), ? ? T ? Y (x , a )) = |? u ? ? v | We now show that |? u ? ? v | &gt; |? i ? ? i | ?i ? I<label>(18)</label></formula><p>is impossible, from which it will follow that</p><formula xml:id="formula_48">d ? (? ? T ? Z(x , a ), ? ? T ? Y (x , a )) ? d ? (Z, Y ) ,</formula><p>and the result then follows by taking maxima over stateaction pairs (x , a ). To demonstrate the impossibility of (18), without loss of generality we take ? u ? ? v . We now introduce the following partitions of the indexing set I. Define:</p><formula xml:id="formula_49">I ??u = {i ? I|? i ? ? u } , I &gt;?u = {i ? I|? i &gt; ? u } , I &lt;?v = {i ? I|? i &lt; ? v } , I ??v = {i ? I|? i ? ? v } ,</formula><p>and observe that we clearly have the following disjoint unions:</p><formula xml:id="formula_50">I = I ??u ? I &gt;?u , I = I &lt;?v ? I ??v .</formula><p>If <ref type="formula" target="#formula_19">(18)</ref> is to hold, then we must have I ??u ? I ??v = ?. Therefore, we must have I ??u ? I &lt;?v . But if this is the case, then since ? u is the ? th quantile of (T ? Z)(x , a ), we must have i?I ??u p i ? ? , and so consequently i?I &lt;?v</p><formula xml:id="formula_51">p i ? ? ,</formula><p>from which we conclude that the ? th quantile of (T ? Y )(x , a ) is less than ? v , a contradiction. Therefore (18) cannot hold, completing the proof.</p><p>Lemma 4. For any two probability distributions ? 1 , ? 2 over the real numbers, and the Wasserstein projection operator ? W1 that projects distributions onto support of size n, we have that</p><formula xml:id="formula_52">d ? (? W1 ? 1 , ? W1 ? 2 ) = max i=1,...,n F ?1 ?1 2i ? 1 2n ? F ?1 ?2 2i ? 1 2n .</formula><p>Proof. By the discussion surrounding Lemma 2, we have that</p><formula xml:id="formula_53">? W1 ? k = n i=1 1 n ? F ?1 ? k ( 2i?1</formula><p>2n ) for k = 1, 2. Therefore, the optimal coupling between ? W1 ? 1 and ? W1 ? 2 must be given by F ?1 ?1 ( 2i?1 2n ) ? F ?1 ?2 ( 2i?1 2n ) for each i = 1, . . . , n. This immediately leads to the expression of the lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further theoretical results</head><p>Lemma 5. The projected Bellman operator ? W1 T ? is in general not a non-expansion in d p , for p ? [1, ?).</p><p>Proof. Consider the case where the number of Dirac deltas in each distribution, N , is equal to 2, and let ? = 1. We consider an MDP with a single initial state, x, and two terminal states, x 1 and x 2 . We take the action space of the MDP to be trivial, and therefore omit it in the notation that follows. Let the MDP have a 2/3 probability of transitioning from x to x 1 , and 1/3 probability of transitioning from x to x 2 . We take all rewards in the MDP to be identically 0. Further, consider two value distributions, Z and Y , given by:</p><formula xml:id="formula_54">Z(x 1 ) = 1 2 ? 0 + 1 2 ? 2 , Y (x 1 ) = 1 2 ? 1 + 1 2 ? 2 , Z(x 2 ) = 1 2 ? 3 + 1 2 ? 5 , Y (x 2 ) = 1 2 ? 4 + 1 2 ? 5 , Z(x) = ? 0 , Y (x) = ? 0 .</formula><p>Then note that we have</p><formula xml:id="formula_55">d p (Z(x 1 ), Y (x 1 )) = 1 2 |1 ? 0| 1/p = 1 2 1/p , d p (Z(x 2 ), Y (x 2 )) = 1 2 |4 ? 3| 1/p = 1 2 1/p , d p (Z(x), Y (x)) = 0 ,</formula><p>and so d p (Z, Y ) = 1 2 1/p . We now consider the projected backup for these two value distributions at the state x. We first compute the full backup:</p><formula xml:id="formula_56">(T ? Z)(x) = 1 3 ? 0 + 1 3 ? 2 + 1 6 ? 3 + 1 6 ? 5 , (T ? Y )(x) = 1 3 ? 1 + 1 3 ? 2 + 1 6 ? 4 + 1 6 ? 5 .</formula><p>Appealing to Lemma 2, we note that when projected these distributions onto two equally-weighted Diracs, the locations of these Diracs correspond to the 25% and 75% quantiles of the original distributions. We therefore have</p><formula xml:id="formula_57">(? W1 T ? Z)(x) = 1 2 ? 0 + 1 2 ? 3 , (? W1 T ? Y )(x) = 1 2 ? 1 + 1 2 ? 4 ,</formula><p>and we therefore obtain</p><formula xml:id="formula_58">d 1 (? W1 T ? Z, ? W1 T ? Y ) = 1 2 (|1 ? 0| p + |4 ? 3| p ) 1/p =1 &gt; 1 2 1/p = d 1 (Z, Y ) ,</formula><p>completing the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation</head><p>Human-normalized scores are given by <ref type="bibr" target="#b29">(van Hasselt, Guez, and Silver 2016)</ref>,</p><formula xml:id="formula_59">score = agent ? random human ? random ,</formula><p>where agent, human and random represent the per-game raw scores for the agent, human baseline, and random agent baseline. Figure 7: Raw scores across all games, starting with 30 no-op actions. Reference values from <ref type="bibr" target="#b30">Wang et al. (2016)</ref> and .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>(a) Two-room windy gridworld, with wind magnitude shown along bottom row. Policy trajectory shown by blue path, with additional cycles caused by randomness shown by dashed line. (b, c) (Cumulative) Value distribution at start state x S , estimated by MC, Z ? M C , and by QRTD, Z ? . (d, e) Value function (distribution) approximation errors for TD(0) and QRTD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Online evaluation results, in human-normalized scores, over 57 Atari 2600 games for 200 million training samples. (Left) Testing performance for one seed, showing median over games. (Right) Training performance, averaged over three seeds, showing percentiles (10, 20, 30, 40, and 50) over games.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Initial MDP and value distribution Z (top), and transformed MDP and value distribution Z (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Mean and median of best scores across 57 Atari 2600 games, measured as percentages of human baseline</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Notation used in the paper , ? 1 , . . . , ? N Cumulative probabilities with ? 0 := 0 ? 1 , . . . ,? NFigure 6: Online training curves for DQN, C51, and QR-DQN on 57 Atari 2600 games. Curves are averages over three seeds, smoothed over a sliding window of 5 iterations, and error bands give standard deviations.</figDesc><table><row><cell>Symbol</cell><cell>Description of usage</cell></row><row><cell></cell><cell>Reinforcement Learning</cell></row><row><cell>M X A R, R t</cell><cell>MDP (X , A, R, P , ?) State space of MDP Action space of MDP Reward function, random variable reward</cell></row><row><cell>P ? x, x t ? X a, a  *  , b ? A r, r t ? R ? T ? T V ? , V Q ? , Q</cell><cell>Transition probabilities, P (x |x, a) Discount factor, ? ? [0, 1) States Actions Rewards Policy (dist.) Bellman operator (dist.) Bellman optimality operator Value function, state-value function Action-value function</cell></row><row><cell>?</cell><cell>Step-size parameter, learning rate</cell></row><row><cell></cell><cell>Exploration rate, -greedy</cell></row><row><cell>ADAM</cell><cell>Adam parameter</cell></row><row><cell>?</cell><cell>Huber-loss parameter</cell></row><row><cell>L ?</cell><cell>Huber-loss with parameter ?</cell></row><row><cell></cell><cell>Distributional Reinforcement Learning</cell></row><row><cell>Z ? , Z Z ? M C Z Z ? z ? Z p</cell><cell>Random return, value distribution Monte-Carlo value distribution under policy ? Space of value distribution? Fixed point of convergence for ? W1 T ? Instantiated return sample Metric order</cell></row><row><cell>W p L p</cell><cell>p-Wasserstein metric Metric order p</cell></row><row><cell>d p</cell><cell>maximal form of Wasserstein</cell></row><row><cell>?</cell><cell>Projection used by C51</cell></row><row><cell>? W1</cell><cell>1-Wasserstein projection</cell></row><row><cell>? ? ? ? ? q 1 , . . . , q N</cell><cell>Quantile regression loss Huber quantile loss Probabilities, parameterized probabilities</cell></row><row><cell cols="2">? 0 Midpoint quantile targets</cell></row><row><cell>?</cell><cell>Sample from unit interval</cell></row><row><cell>? z ?</cell><cell>Dirac function at z ? R Parameterized function</cell></row><row><cell>B</cell><cell>Bernoulli distribution</cell></row><row><cell>B ? Z Q Z ?</cell><cell>Parameterized Bernoulli distribution Space of quantile (value) distributions Parameterized quantile (value) distribution</cell></row><row><cell>Y</cell><cell>Random variable over R</cell></row><row><cell>Y 1 , . . . , Y m</cell><cell>Random variable sample?</cell></row><row><cell>Y m</cell><cell>Empirical distribution from m-Diracs</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For p = ?, W?(Y, U ) = sup ??[0,1] |F ?1 Y (?) ? F ?1 U (?)|.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We save proofs for the appendix due to space limitations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Our quantile Huber loss is related to, but distinct from that ofAravkin et al. (2014).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">DQN uses gradient clipping of the squared error that makes it equivalent to a Huber loss with ? = 1.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors acknowledge the vital contributions of their colleagues at DeepMind. Special thanks to Tom Schaul, Audrunas Gruslys, Charles Blundell, and Benigno Uria for their early suggestions and discussions on the topic of quantile regression. Additionally, we are grateful for feedback from David Silver, Yee Whye Teh, Georg Ostrovski, Joseph Modayil, Matt Hoffman, Hado van Hasselt, Ian Osband, Mohammad Azar, Tom Stepleton, Olivier Pietquin, Bilal Piot; and a second acknowledgement in particular of Tom Schaul for his detailed review of an previous draft.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix Proofs Lemma 2. For any ?, ? ? [0, 1] with ? &lt; ? and cumulative distribution function F with inverse F ?1 , the set of ? ? R minimizing ? ? |F ?1 (?) ? ?|d? , is given by</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Aravkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luss</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Sparse Quantile Huber Regression for Efficient and Robust Estimation. arXiv</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wasserstein Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Arcade Learning Environment: An Evaluation Platform for General Agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>The Cramer Distance as a Solution to Biased Wasserstein Gradients. arXiv</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<title level="m">A Distributional Perspective on Reinforcement Learning. Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Some Asymptotic Theory for the Bootstrap. The Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Freedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="page" from="1196" to="1217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Risk-Sensitive and Robust Decision-Making: a CVaR Optimization Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1522" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bayesian Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dearden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reinforcement Learning with Gaussian Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Meir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Die Productions-und Consumtionsverh?ltnisse des K?nigreichs Sachsen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Engel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zeitschrift des Statistischen Bureaus des K?niglich S?chsischen Ministeriums des Innern</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="1857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On Intrinsic Randomness of Dynamical Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courtage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Physics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="111" to="126" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Consideration of Risk in Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Machine Learning</title>
		<meeting>the 11th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="105" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust Estimation of a Location Parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="101" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quantile Regression: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hallock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Perspectives</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="43" to="56" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Quantile Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koenker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Earth Mover&apos;s Distance is the Mallows Distance: Some Insights from Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 8th IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human-level Control through Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Parametric Return Density Estimation for Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Morimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hachiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>the Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Integral Probability Metrics and their Generating Classes of Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="429" to="443" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alcicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fearon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De Maria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<title level="m">Massively Parallel Methods for Deep Reinforcement Learning. In ICML Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Markov Decision Processes: Discrete stochastic dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Rummery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
		</imprint>
		<respStmt>
			<orgName>Cambridge University Engineering Department</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>On-line Qlearning using Connectionist Systems</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Prioritized Experience Replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Quantile Regression Approach to Estimating the Distribution of Multiperiod Returns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Derivatives</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="78" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rmsprop. COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An Analysis of Temporal-Difference Learning with Function Approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="674" to="690" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning with Double Q-Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dueling Network Architectures for Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
