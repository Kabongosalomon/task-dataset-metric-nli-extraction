<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transductive Decoupled Variational Inference for Few-Shot Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<addrLine>The Netherlands</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Jamali-Rad</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<addrLine>The Netherlands</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shell Global Solutions International B.V</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transductive Decoupled Variational Inference for Few-Shot Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The versatility to learn from a handful of samples is the hallmark of human intelligence. Few-shot learning is an endeavour to transcend this capability down to machines. Inspired by the promise and power of probabilistic deep learning, we propose a novel variational inference network for few-shot classification (coined as TRIDENT) to decouple the representation of an image into semantic and label latent variables, and simultaneously infer them in an intertwined fashion. To induce task-awareness, as part of the inference mechanics of TRIDENT, we exploit information across both query and support images of a few-shot task using a novel built-in attention-based transductive feature extraction module (we call AttFEX). Our extensive experimental results corroborate the efficacy of TRIDENT and demonstrate that, using the simplest of backbones, it sets a new state-of-the-art in the most commonly adopted datasets miniImageNet and tieredImageNet (offering up to 4% and 5% improvements, respectively), as well as for the recent challenging crossdomain miniImagenet ? CUB scenario offering a significant margin (up to 20% improvement) beyond the best existing cross-domain baselines. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Deep learning algorithms are usually data hungry and require massive amounts of training data to reach a satisfactory level of performance on any task. To tackle this limitation, few-shot classification aims to learn to classify images from various unseen tasks in a data-deficient setting. In this exciting space, metric learning proposes to learn a shared feature extractor to embed the samples into a metric space of class prototypes <ref type="bibr" target="#b57">(Sung et al. 2018;</ref><ref type="bibr" target="#b61">Vinyals et al. 2016;</ref><ref type="bibr" target="#b54">Snell, Swersky, and Zemel 2017;</ref><ref type="bibr" target="#b62">Wang et al. 2019;</ref><ref type="bibr" target="#b31">Liu, Song, and Qin 2020;</ref><ref type="bibr" target="#b4">Bateni et al. 2020</ref>). Due to limited data per class, these prototypes suffer from sample-bias and fail to efficiently represent class characteristics. Furthermore, sharing a feature extractor across tasks implies that the discriminative information learnt from the seen classes are equally effective on any arbitrary unseen classes, which is not true in most cases. Task-aware few-shot learning approaches <ref type="bibr" target="#b3">(Bateni et al. 2022;</ref><ref type="bibr" target="#b69">Ye et al. 2020)</ref> address these limitations by exploiting information hidden in the unlabeled data. As a result, the model learns task-specific embeddings by aligning the features of the labelled and unlabelled task instances for optimal distance metric based label assignment. Since the alignment of these embeddings is still subject to the relevance of the characteristics captured by the shared feature extractors, task-aware methods sometimes fail to extract meaningful representations particularly relevant to classification. Probabilistic methods address sample-bias by relaxing the need to find point estimates to approximate datadependent distributions of either high-dimensional model weights <ref type="bibr" target="#b39">(Nguyen, Do, and Carneiro 2019;</ref><ref type="bibr" target="#b47">Ravi and Beatson 2019;</ref><ref type="bibr" target="#b15">Gordon et al. 2019;</ref><ref type="bibr" target="#b19">Hu et al. 2020)</ref> or lowerdimensional class prototypes <ref type="bibr" target="#b56">(Sun et al. 2021;</ref><ref type="bibr" target="#b72">Zhang et al. 2019)</ref>. However, inferring a high-dimensional posterior of model parameters is inefficient in low-data regimes and estimating distributions of class prototypes involves using handcrafted non-parametric aggregation techniques which may not be well suited for every unseen task.</p><p>Although fit for purpose, all these approaches seem to overlook an important perspective. An image is composed of different attributes such as style, design, and context, which are not necessarily relevant discriminative characteristics for classification. Here, we refer to these attributes as semantic information. On the other hand, other class-characterizing attributes (such as wings of a bird, trunk of an elephant, hump on a camel's back) are critical for classification, irrespective of context. We refer to such attributes as label information. Typically, contextual information is majorly governed by semantic attributes, whereas the label characteristics are subtly embedded throughout an image. In other words, semantic information can be predominantly present across an image, whereas attending to subtle label information determines how effective a classification algorithm would be. Thus, we argue that attention to label-specific information should be ingrained into the mechanics of the classifier, decoupling it from semantic information. This becomes even more important in a few-shot setting where the network has to quickly learn from little data. Building upon this idea, we propose transductive variational inference of decoupled latent variables (coined as TRIDENT), to simultaneously infer decoupled label and semantic information using two intertwined variational networks. To induce taskawareness while constructing the variational inference mechanics of TRIDENT, we introduce a novel atention-based <ref type="figure">Figure 1</ref>: High-level process flow of TRIDENT. Inferred label latent variable z l contains class-characterizing information, as is reflected by better separation of the distributions when compared to their semantic latent counterparts zs. AttFEX module generates task-aware feature maps by exploiting information from both support and query images, which compensates for the lack of label vectors Y in inferring z l .</p><p>transductive feature extraction module (we call AttFEX) which further enhances the discriminative power of the inferred label attributes. This way TRIDENT infers distributions instead of point estimates and injects a handcrafted inductive-bias into the network to guide the classification process. Our main contributions can be summarized as:</p><p>1. We propose TRIDENT, a variational inference network to simultaneously infer two salient decoupled attributes of an image (label and semantic), by inferring these two using two intertwined variational sub-networks ( <ref type="figure">Fig. 1</ref>). 2. We introduce an attention-based transductive feature extraction module, AttFEX, to enable TRIDENT see through and compare all images within a task, inducing task-cognizance in the inference of label information. 3. We perform extensive evaluations to demonstrate that TRIDENT sets a new state-of-the-art by outperforming all existing baselines on the most commonly adopted datasets miniImagenet and tieredImagenet (up to 4% and 5%), as well as for the challenging cross-domain scenario of miniImagenet ? CUB (up to 20% improvement).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Metric-based learning. This body of work revolves around mapping input samples into a lower-dimensional embedding space and then classifying the unlabelled samples based on a distance or similarity metric. By parameterizing these mappings with neural networks and using differentiable similarity metrics for classification, these networks can be trained in an episodic manner <ref type="bibr" target="#b61">(Vinyals et al. 2016)</ref>  Transductive Feature-Extraction and Inference. Transductive feature extraction or task-aware learning is a variant of the metric-learning with an adaptation mechanism that aligns support and query feature vectors in the embed-ding space for better representation of task-specific discriminative information. This not only improves the discriminative ability of classifiers across tasks, but also alleviates the problem of overfitting on limited support set since information from the query set is also used for extracting features of images in a task. CNAPS <ref type="bibr" target="#b50">(Requeima et al. 2019)</ref>, Transductive-CNAPS <ref type="bibr" target="#b3">(Bateni et al. 2022)</ref>, FEAT <ref type="bibr" target="#b69">(Ye et al. 2020)</ref>, Assoc-Align (Afrasiyabi, Lalonde, and Gagn? 2020), TPMN ) and CTM <ref type="bibr" target="#b29">(Li et al. 2019)</ref> are prime examples of such methods. Next to transduction for taskaware feature extraction, there are methods that use transductive inference to classify all the query samples at once by jointly assigning them labels, as opposed to their inductive counterparts where prediction is done on the samples one at a time. This is either done by iteratively propagating labels from the support to the query samples or by fine-tuning a pre-trained backbone using an additional entropy loss on all query samples, which encourages confident class predictions at query samples. TPN , Ent-Min <ref type="bibr" target="#b9">(Dhillon et al. 2020)</ref>, TIM <ref type="bibr" target="#b5">(Boudiaf et al. 2020</ref>), Transductive-CNAPS <ref type="bibr" target="#b3">(Bateni et al. 2022)</ref>, LaplacianShot , DPGN  and <ref type="bibr">ReRank (SHEN et al. 2021</ref>) are a few notable examples in this space that usually report state-of-the-art results in certain few-shot classification settings .</p><p>Optimization-based meta-learning. These methods search for model parameters that are sensitive to task objective functions for fast gradient-based adaptation to new tasks. MAML <ref type="bibr" target="#b12">(Finn, Abbeel, and Levine 2017)</ref>, its variants (Rajeswaran et al. <ref type="bibr" target="#b40">Nichol, Achiam, and Schulman 2018a)</ref> and SNAIL <ref type="bibr" target="#b38">(Mishra et al. 2018</ref>) are a few prominent examples while LEO <ref type="bibr" target="#b51">(Rusu et al. 2019</ref>) efficiently meta-updates its parameters in a lower dimensional latent space. Probabilistic learning. The estimated parameters of typical gradient-based meta-learning methods discussed earlier <ref type="bibr" target="#b12">(Finn, Abbeel, and Levine 2017;</ref><ref type="bibr" target="#b51">Rusu et al. 2019;</ref><ref type="bibr" target="#b38">Mishra et al. 2018;</ref><ref type="bibr" target="#b40">Nichol, Achiam, and Schulman 2018a;</ref><ref type="bibr">Rajeswaran et al. 2019)</ref>, have high variance due to the small task sample size. To deal with this, a natural extension is to model the uncertainty by treating these parameters as latent variables in a Bayesian framework as proposed in Neural Statistician <ref type="bibr" target="#b11">(Edwards and Storkey 2017)</ref>, PLATI-PUS <ref type="bibr" target="#b13">(Finn, Xu, and Levine 2018)</ref>, VAMPIRE <ref type="bibr" target="#b39">(Nguyen, Do, and Carneiro 2019)</ref>, ABML <ref type="bibr" target="#b47">(Ravi and Beatson 2019)</ref>, VERSA , SIB , SAMOVAR <ref type="bibr" target="#b22">(Iakovleva, Verbeek, and Alahari 2020)</ref>. Methods like ABPML <ref type="bibr" target="#b56">(Sun et al. 2021)</ref> and VariationalFSL <ref type="bibr" target="#b72">(Zhang et al. 2019)</ref> infer latent variables of class prototypes to perform classification and avoid inferring highdimensional model parameters. ABPML <ref type="bibr" target="#b56">(Sun et al. 2021)</ref> and VariationalFSL <ref type="bibr" target="#b72">(Zhang et al. 2019)</ref> are the closest to our approach. In contrast to these two methods, we avoid handcrafting class-level aggregations, as well as we enhance variational inference by incorporating an inductive bias through decoupling of label and semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Definition</head><p>Consider a labelled dataset  <ref type="bibr" target="#b61">(Vinyals et al. 2016;</ref><ref type="bibr" target="#b57">Sung et al. 2018;</ref><ref type="bibr" target="#b54">Snell, Swersky, and Zemel 2017)</ref>, we use episodic training on a set of tasks T i ? p(T ). The tasks are constructed by drawing K random samples from N different classes, which we denote as an (N -way, Kshot) task. Concretely, each task T i is composed of a support and a query set. The support set S = {(x S kn , y S kn ) | k ? [1, K], n ? [1, N ]} contains K samples per class and the query set Q = {(x Q kn , y Q kn ) | k ? [1, Q], n ? [1, N ]} contains Q samples per class. For a given task, the N Q query and N K support images are mutually exclusive to assess the generalization performance.</p><formula xml:id="formula_0">D = {(x i , y i ) | i ? [1, N ]}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Proposed Method: TRIDENT</head><p>Let us start with the high-level idea. The proposed approach is devised to learn meaningful representations that capture two pivotal characteristics of an image by modelling them as separate latent variables: (i) z s representing semantics, and (ii) z l embodying class labels. Inferring these two latent variables simultaneously allows z l to learn meaningful distributions of class-discriminating characteristics decoupled from semantic features represented by z s . We argue that learning z l as the sole latent variable for classification results in capturing a mixture of true label and other semantic information. This in turn can lead to sub-optimal classification performance, especially in a few-shot setting where the information per class is scarce and the network has to adapt and generalize quickly. By inferring decoupled label and semantics latent variables, we inject a handcrafted inductivebias that incorporates only relevant characteristics, and thus, ameliorates the network's classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generative Process</head><p>The directed graphical model in <ref type="figure" target="#fig_0">Fig. 2</ref> illustrates the common underlying generative process p such that p i = p(x i , y i | z li , z si ). For the sake of brevity, in the following we drop the sample index i as we always refer to terms associated with a single data sample. We work on the logical premise that the label latent variable z l is responsible for generating class label as well as for image reconstruction, whereas the semantic latent variable z s is only responsible for image reconstruction (solid lines in the <ref type="figure">figure)</ref>. Formally, the data is explained by the generative processes:</p><formula xml:id="formula_1">p ?1 (y | z l ) = Cat(y | z l ) and p ?2 (x | z l , z s ) = g ?2 (x; z l , z s ),</formula><p>where Cat(.) refers to a multinomial distribution and g ?2 (x; z l , z s ) is a suitable likelihood function such as a Gaussian or Bernoulli distribution. The likelihoods of both these generative processes are parameterized using deep neural networks and the priors of the latent variables are chosen to be standard multivariate Gaussian distributions :</p><formula xml:id="formula_2">p(z s ) = N (z s | 0, I) and p(z l ) = N (z l | 0, I).</formula><p>Variational Inference of Decoupled Z l and Z s Computing exact posterior distributions is intractable due to high dimensionality and non-linearity of the deep neural network parameter space. Following , we instead construct an approximate posterior over the latent variables by introducing a fixed-form distribution q(z l , z s | x, y) parameterized by ?. By using q ? (.) as an inference network, the inference is rendered tractable, scalable and amortized since ? now acts as the global variational parameter. We assume q ? has a fac-</p><formula xml:id="formula_3">torized form q ? (z s , z l | x, y) = q ?1 (z l | x, z s ) q ?2 (z s | x),</formula><p>where q ?1 (.), q ?2 (.) are assumed to be multivariate Gaussian distributions. As is also depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>, we use z s as input to q ?1 (.) to infer z l because of their conditional dependence given x. This way we forge a path to allow necessary semantic latent information flow through the label inference network. On the other hand, the opposite direction (using z l to infer z s ) is unnecessary, because label information does not directly contribute to the extraction of semantic features. We will further reflect on this design choice in the next subsection. Neural networks are then used to parameterize both inference networks as:</p><formula xml:id="formula_4">q ? 2 (zs | x) = N zs | ? ? 2 (x), diag(? 2 ? 2 (x)) , q ? 1 (z l | x, zs) = N z l | ? ? 1 (x, zs), diag(? 2 ? 1 (x, zs)) .<label>(1)</label></formula><p>To find the optimal approximate posterior, we derive the evidence lower bound (ELBO) on the marginal likelihood of the data to form our objective function:</p><formula xml:id="formula_5">p(x, y) = p(x, y | zs, z l ) p(zs,z l ) dzs dz l , = E q(zs,z l | x) p(x | z l , zs)p(y | z l )p(z l )p(zs) q(z l , zs | x) . ln p(x, y) E q(zs,z l |x) ln p(x | z l , zs)p(y | z l )p(z l )p(zs) q(zs, z l | x) , = Eq ? 2 Eq ? 1 ln p(x | zs, z l )p(y | z l )p(zs)p(z l ) q(zs | x)q(z l | x, zs) .</formula><p>Denoting ? = (? 1 , ? 2 , ? 1 , ? 2 ), the ELBO can be given by</p><formula xml:id="formula_6">L(?) = ?Eq ? 2 Eq ? 1 [ln p ? 2 (x | zs, z l ) + ln p ? 1 (y | z l )] + DKL q ? 1 (z l | x, zs) p(z l ) + DKL q ? 2 (zs | x) p(zs) ,<label>(2)</label></formula><p>where the second line follows the graphical model in <ref type="figure" target="#fig_0">Fig 2,</ref> and E(.) and ln(.) denote the expectation operator and the natural logarithm, respectively. We avoid computing biased gradients by following the re-parameterization trick from . Assuming Gaussian distributions for the priors as well as the variational distributions allows us to compute the KL Divergences of z l and z s (last two terms in <ref type="formula" target="#formula_6">(2)</ref>) analytically . By considering a multivariate Gaussian distribution and a multinomial distribution as the likelihood functions for p ?2 (x | z s , z l ) and p ?1 (y | z l ), respectively, the negative log-likelihood of x becomes the mean squared error (MSE) between the reconstructed imagesx and the ground-truth images x while the negative log-likelihood of y becomes the cross-entropy between the actual labels y and the predicted labels?. After working (2) out, we arrive at our overall objective function L = L R + L C , where:</p><formula xml:id="formula_7">LR = ?1 x ?x 2 ? KL(?s, ?s), LC = ??2 N n=1 yn ln p ? 1 (? = n | z l ) ? KL(? l , ? l ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_8">KL(?, ?) = 1 2 D d=1 1+2 ln(? d )?(? d ) 2 ?(? d ) 2 ,<label>D</label></formula><p>denotes the dimension of the latent space, N is the total number of classes in an (N -way, K-shot) task, ? 1 , ? 2 are constant scaling factors, ? s and ? 2 s denote the mean and variance vectors of semantic latent distribution, and ? l and ? 2 l denote the mean and variance vectors of label latent distribution. The hyper-parameters ? 1 , ? 2 only scale the evidence lower-bound appropriately, since the reconstruction loss is in practice three orders of magnitude greater than the cross-entropy loss. As such, this scaling helps convergence but impacts the tightness of the ELBO slightly; nonetheless, (2) and (3) are still considered variational inference by consensus among the literature <ref type="bibr" target="#b17">(Higgins et al. 2017;</ref><ref type="bibr" target="#b24">Joy et al. 2021;</ref><ref type="bibr" target="#b36">Mathieu et al. 2019;</ref><ref type="bibr" target="#b10">Dupont 2018)</ref>. The loss is calculated for each given task on query and support sets separately; i.e.,</p><formula xml:id="formula_9">L g = L g R + L g C with g ? {S i , Q i }.</formula><p>Note that in (1) we deliberately choose to exclude the label information y as input to q ?1 (.) to be able to exploit the associated generative network p ?1 (y | z l ) as a classifier. The consequence and the proposed solution to accommodate this design choice are discussed in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support Set Feature Maps</head><p>Query Set Feature Maps <ref type="figure">Figure 3</ref>: AttFEX module depicting colors as images and shades as feature maps. We illustrate only 3 image feature maps and 3 channels instead of 32 for N, for the sake of simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AttFEX for Transductive Feature Extraction</head><p>We first extract the feature maps of all images in the task using a convolutional block</p><formula xml:id="formula_10">F = ConvEnc(X) where X ? R N (K+Q)?C?W ?H , F ? R N (K+Q)?C ?W ?H .</formula><p>The feature map tensor F is then transposed into F ? R C ?N (K+Q)?W ?H and fed into two consecutive 1 ? 1 convolution blocks. This helps the network utilize information across corresponding pixels of all images in a task T i which acts as a parametric comparison of classes. We leverage the fact that ConvEnc already extracts local pixel information by using larger kernels, and thus, use parameter-light 1 ? 1 convolutions subsequently to focus only on individual pixels. Let F i denote the i th channel (or feature map layer) out of total of C available and ReLU denote the rectified linear unit activation. The 1 ? 1 convolution block (Conv 1?1 ) is formulated as follows:</p><formula xml:id="formula_11">M i = ReLU Conv 1?1 (F i , W M ) , ?i ? [1, C ]; N j = ReLU Conv 1?1 (M j , W N ) , ?j ? [1, C ];<label>(4)</label></formula><p>where N ? R C ?32?W ?H and W M ? R 64?N (K+Q)?1?1 , W N ? R 32?64?1?1 denote the learnable weights. Next, we want to blend information across feature maps for which we use a self-attention mechanism <ref type="bibr" target="#b60">(Vaswani et al. 2017</ref>) across N j , ?j ? [1, 32]. To do so, we feed N to query, key and value extraction networks f q (, ;</p><formula xml:id="formula_12">W Q ), f k (.; W K ), f v (.; W V )</formula><p>which are also designed to be 1 ? 1 convolutions as:</p><formula xml:id="formula_13">Q i = ReLU (Conv 1?1 (N i , W Q )) , ?i ? [1, C ]; K i = ReLU (Conv 1?1 (N i , W K )) , ?i ? [1, C ]; V i = ReLU (Conv 1?1 (N i , W V )) , ?i ? [1, C ];<label>(5)</label></formula><p>where W Q , W K , W V ? R 1?32?1?1 are the learnable weights and Q, K, V ? R C ?1?W ?H are the query, key and value tensors. Next, each feature map N j is mapped to its output tensor G j by computing a weighted sum of the values, where each weight (within parentheses in (6)) measures the compatibility (or similarity) between the query and its corresponding key tensor using an inner-product:</p><formula xml:id="formula_14">G i = C j=1 exp (Q i ? K j ) ? d k . C k=1 exp (Q i ? K k ) V i ,<label>(6)</label></formula><p>where d k = W ? H , and G i ? R 1?C ?W ?H , ?i. Finally, we transform the original feature maps F by applying a Hadamard product between the feature mask G and F, thus, rendering the required feature maps transductive:</p><formula xml:id="formula_15">F S = G ? F S orF Q = G ? F Q .</formula><p>Here, F S and F Q represent the feature maps corresponding to the support and query images, respectively. As a result of operating on this channel-pixel distribution across images in a task, F S and F Q are rendered task-aware. Note that the query tensor Q must not be confused with the query set Q of a task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithmic Overview and Training Strategy</head><p>Figure 4: TRIDENT is comprised of two intertwined variational networks. Z g s is concatenated with the output of AttFEX, and used for inferring Z g l , where g ? {S, Q}. Next, both Z g l and Z g s are used to reconstruct imagesX g while Z g l is used to extract? g .</p><p>Overview of TRIDENT. The complete architecture of TRIDENT is illustrated in <ref type="figure">Fig. 4</ref>. The ConvEnc feature extractor and the linear layers ? ?2 (.), ? 2 ?2 (.) constitute the inference network q ?2 of the semantic latent variable (bottom row of <ref type="figure">Fig. 4)</ref>. The AttFEX module, another ConvEnc, and linear layers ? ?1 (.) and ? 2 ?1 (.) make up the inference network q ?1 of the label latent variable (top row of <ref type="figure">Fig. 4)</ref>. The proposed approach, TRIDENT, is described in Algorithm 1. Note that TRIDENT is trained in a MAML <ref type="bibr" target="#b12">(Finn, Abbeel, and Levine 2017)</ref> fashion, where depending on the inner or outer loop, the support or query set (g ? {S, Q}) will be the reference, respectively. First, the lower ConvEnc block extracts feature maps X g CE = ConvEnc(X g ). X g CE 's are then flattened and passed onto ? ?2 (.), ? 2 ?2 (.), which respectively output the mean and variance vectors of the semantic latent distribution, as discussed in <ref type="formula" target="#formula_4">(1)</ref>. This is done either for the entire support or the query images X g , where g ? {S, Q} for a given task T i . We then sample a set of vectors Z g s (subscript s for semantic) from their corresponding Gaussian distributions using the re-parameterization trick (line 1, Algorithm 1). Upon passing X = X S ? X Q through the upper ConvEnc, the AttFEX module of q ?1 comes into play to create taskcognizant feature mapsF g for either S or Q (line 2). Z g s together withF g are passed onto the linear layers ? ?1 (.), ? 2 ?1 (.) to generate the mean and variance vectors of the label</p><formula xml:id="formula_16">Algorithm 1: TRIDENT Require: X S , X Q , Y g , X g CE , where g ? {S, Q} 1 Sample: Z g s ? q ?2 Z s | ? ?2 (X g CE ), diag ? 2 ?2 (X g CE ) 2</formula><p>Compute task-cognizant embeddings:</p><formula xml:id="formula_17">[F S ,F Q ] = AttFEX(ConvEnc(X)); X = X S ? X Q 3 Concatenate Z g s andF g into [F g , Z g s ] and sample: Z g l ? q ?1 Z l | ? ?1 ([F g , Z g s ]), diag(? 2 ?1 ([F g , Z g s ])) 4 Reconstruct X g usingX g = p ?2 (X | Z g l , Z g s ) 5</formula><p>Extract class-conditional probabilities using:</p><formula xml:id="formula_18">p ? g | Z g l = softmax p ?1 (Y g | Z g l ) 6 Compute L g = L g R + L g C using (3) Return: L g Algorithm 2: End to End Meta-Training of TRIDENT Require: D tr , ?, ?, B 1 Randomly initialise ? = (? 1 , ? 2 , ? 1 , ? 2 ) 2 while not converged do 3 Sample B tasks T i = S i ? Q i from D tr 4 for each task T i do 5 for number of adaptation steps do 6 Compute L Si (?) = TRIDENT(T i ? {Y Qi }) 7 Evaluate ? (?) L Si (?) 8 ? ? ? ? ?? ? L Si (?) 9 end 10 (? ) i = ? 11 end 12 Compute L Qi (? i ) = TRIDENT(T i ? {Y Si }); ?i ? [1, B] 13 Meta-update on Q i : ? ? ? ? ?? ? B i=1 L Qi (? i ) 14 end</formula><p>latent Gaussian distributions (line 3). After sampling the set of vectors Z g l (subscript l for label) from their corresponding distributions, we use Z g l and Z g s to reconstruct the input imagesX g using the generative network p ?2 (line 4). Next, Z g l 's are input to the classifier network p ?1 to generate the class logits, which are normalized using a softmax(.), resulting in class-conditional probabilities p(? g | Z g l ) (line 5). Finally (in line 6), using the outputs of all the components discussed earlier, we calculate the loss L g as formulated in <ref type="formula" target="#formula_6">(2)</ref> and <ref type="formula" target="#formula_7">(3)</ref>. Training strategy. An important aspect of the training procedure of TRIDENT is that its set of parameters ? = (? 1 , ? 2 , ? 1 , ? 2 ) are meta-learnt by backpropagating through the adaptation procedure on the support set, as proposed in MAML <ref type="bibr" target="#b12">(Finn, Abbeel, and Levine 2017)</ref> and illustrated here in Algorithm 2. This increases the sensitivity of the parameters ? towards the loss function for fast adaptation to unseen tasks and reduces generalization errors on the query set Q. First, we randomly initialize the parameters ? (line 1, Algorithm 2) to compute the objective function over the support set L Si (?) using equation <ref type="formula" target="#formula_7">(3)</ref> in the main manuscript, and perform a number of gradient de- <ref type="table">Table 1</ref>: Accuracies in (% ? std). The predominant methodology of the baselines: Ind.: inductive inference, TF: transductive feature extraction methods, TI: transductive inference methods. Conv: convolutional blocks, RN: ResNet backbone, ?: extra data. Style: best and second best. TRIDENT employs a transductive feature extraction module (TF), and the simplest of backbones (Conv4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>miniImagenet tieredImagenet mini?CUB</head><p>Methods Backbone Approach 5-way 1-shot 5-way 5-shot 5-way 1-shot 5-way 5-shot 5-way 1-shot 5-way 5-shot scent steps on the parameters ? to adapt them to the support set (lines 5 to 9). This is called the inner-update and is done separately for all the support sets corresponding to their B different tasks (line 3). Once the inner-update is computed for each of the B parameter sets, the loss is evaluated on the query set L Qi (? i ) (line 12), following which a meta-update is conducted over all the corresponding query sets, which involves computing a gradient through a gradient procedure as described in <ref type="bibr">(Finn, Abbeel, and</ref> Levine 2017) (line 13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Evaluation</head><p>The goal of this section is to address the following four questions: (i) How well does TRIDENT perform when compared against the state-of-the-art methods for few-shot classification? (ii) How reliable is TRIDENT in terms of the confidence and uncertainty metrics? (iii) How well does TRIDENT perform in a cross-domain setting where there is a domain shift between the training and testing datasets? (iv) Does TRIDENT actually decouple latent variables? Benchmark Datasets. We evaluate TRIDENT on the three most commonly adopted datasets: miniImagenet (Ravi and Larochelle 2017), tieredImagenet <ref type="bibr" target="#b49">(Ren et al. 2018</ref>) and CUB <ref type="bibr" target="#b63">(Welinder et al. 2010)</ref>. miniImagenet and tieredImagenet are subsets of ImageNet <ref type="bibr" target="#b8">(Deng et al. 2009</ref>) utilized for few-shot classification. Further details on these datasets can be found in the Appendix. Implementational Details. We use PyTorch <ref type="bibr" target="#b44">(Paszke et al. 2019)</ref> and learn2learn <ref type="bibr" target="#b2">(Arnold et al. 2020</ref>) for all our implementations. We use a commonly adopted Conv4 architecture <ref type="bibr" target="#b48">(Ravi and Larochelle 2017;</ref><ref type="bibr" target="#b12">Finn, Abbeel, and Levine 2017;</ref><ref type="bibr" target="#b45">Patacchiola et al. 2020;</ref><ref type="bibr" target="#b0">Afrasiyabi, Lalonde, and Gagn? 2020;</ref><ref type="bibr" target="#b62">Wang et al. 2019;</ref><ref type="bibr" target="#b5">Boudiaf et al. 2020)</ref> as ConvEnc to obtain the generic feature maps. Following the standard setting in the literature <ref type="bibr" target="#b12">(Finn, Abbeel, and Levine 2017;</ref><ref type="bibr" target="#b48">Ravi and Larochelle 2017)</ref>, the Conv4 has four convolutional blocks where each block has a 3 ? 3 convolution layer with 32 feature maps, followed by a batch normalization (BN) (Ioffe and Szegedy 2015) layer, a 2 ? 2 max-pooling layer and a LeakyReLU(0.2) activation. The generative network p ?1 for z l is a classifier with two linear layers and a LeakyReLU(0.2) activation in between, while p ?2 for z s consists of four blocks of a 2-D upsampling layer, followed by a 3 ? 3 convolution and LeakyReLU(0.2) activation. Both latent variables z l and z s have a dimensionality of 64. Following <ref type="bibr" target="#b41">(Nichol, Achiam, and Schulman 2018b;</ref><ref type="bibr" target="#b32">Liu et al. 2019;</ref><ref type="bibr" target="#b60">Vaswani et al. 2017)</ref>, images are resized to 84 ? 84 for all configurations and we train and report test accuracy of (5-way, 1 and 5-shot) settings with 10 query images per class for all datasets. Hyperparameter settings can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Results</head><p>We report test accuracies indicating 95% confidence intervals over 600 tasks for miniImagenet, and 2000 tasks for both tieredImagenet and CUB, as is customary across the literature <ref type="bibr" target="#b9">Dhillon et al. 2020;</ref><ref type="bibr" target="#b3">Bateni et al. 2022)</ref>. We compare our performance against a wide variety of state-of-the-art few-shot classification methods such as: (i) metric-learning <ref type="bibr" target="#b4">Bateni et al. 2020;</ref><ref type="bibr" target="#b0">Afrasiyabi, Lalonde, and Gagn? 2020;</ref><ref type="bibr" target="#b68">Yang et al. 2020</ref>), (ii) transductive feature-extraction based <ref type="bibr" target="#b43">(Oreshkin, Rodr?guez L?pez, and Lacoste 2018;</ref><ref type="bibr" target="#b69">Ye et al. 2020;</ref><ref type="bibr" target="#b29">Li et al. 2019;</ref><ref type="bibr">Xu et al. 2021</ref>), (iii) optimization-based <ref type="bibr" target="#b12">(Finn, Abbeel, and Levine 2017;</ref><ref type="bibr" target="#b38">Mishra et al. 2018;</ref><ref type="bibr" target="#b42">Oh et al. 2021;</ref><ref type="bibr" target="#b28">Lee et al. 2019;</ref><ref type="bibr" target="#b51">Rusu et al. 2019)</ref>, (iv) transductive inference-based <ref type="bibr" target="#b3">(Bateni et al. 2022;</ref><ref type="bibr" target="#b5">Boudiaf et al. 2020</ref>;  <ref type="bibr" target="#b74">Ziko et al. 2020;</ref><ref type="bibr" target="#b32">Liu et al. 2019)</ref>, and (v) Bayesian (Iakovleva, Verbeek, and Alahari 2020; <ref type="bibr" target="#b72">Zhang et al. 2019;</ref><ref type="bibr" target="#b19">Hu et al. 2020;</ref><ref type="bibr" target="#b45">Patacchiola et al. 2020;</ref><ref type="bibr" target="#b47">Ravi and Beatson 2019)</ref> approaches. Previous works , <ref type="bibr" target="#b18">(Hou et al. 2019)</ref> have demonstrated the superiority of transductive inference methods over their inductive counterparts. In this light, we compare against a larger number of transductive (18 baselines) rather than inductive (7 baselines) methods for a fair comparison.</p><p>It is important to note that TRIDENT is only a transductive feature-extraction based method as we utilize the query set images to extract task-aware feature embeddings; it is not a transductive inference based method since we perform inference of class-labels over the entire domain of definition and not just for the selected query samples <ref type="bibr" target="#b59">(Vapnik 2006;</ref><ref type="bibr" target="#b14">Gammerman, Vovk, and Vapnik 1998)</ref>. The results on miniImagenet and tieredImagenet for both (5-way, 1 and 5-shot) settings are summarized in <ref type="table">Table 1</ref>. We accentuate on the fact that we also compare against Transd-CNAPS+FETI <ref type="bibr" target="#b3">(Bateni et al. 2022)</ref>, where the authors pretrain the ResNet-18 backbone on the entire train split of Imagenet. We, however, avoid training on additional datasets, in favor of fair comparison with the rest of literature. Regardless of the choice of backbone (simplest in our case), TRIDENT sets a new state-of-the-art on miniImagenet and tieredImagenet for both (5-way, 1 and 5-shot) settings, offering up to 5% gain over the prior art. Recently, a more challenging cross-domain setting has been proposed for fewshot classification to assess its generalization capabilities to unseen datasets. The commonly adopted setting is where one trains on miniImagenet and tests on CUB . The results of this experiment are also presented in <ref type="table">Table 1</ref>. We compare against any existing baselines for which this cross-domain experiment has been conducted. As can be seen, and to the best of our knowledge, TRIDENT again sets a new state-of-the-art by a significant margin of 20% for (5way, 1-shot) setting, and 1.5% for (5-way, 5-shot) setting. Computational Complexity. Most of the reported baselines in <ref type="table">Table 1</ref> use stronger backbones such as ResNet12, ResNet18 and WRN which contain 11.5, 12.4 and 36.4 millions of parameters respectively. On the other hand, we use three Conv4s along with two fully connected layers and an AttFEX module which accounts for 410,958 and 412,238 parameters in the (5-way, 1-shot) and (5-way, 5-shot) scenarios, respectively. This is summarized in details in <ref type="table" target="#tab_3">Table 2</ref>. Even though we are more parameter heavy than approaches that use a single Conv4 as feature extractor, TRIDENT's total parameters still lies in the same order of magnitude as these approaches. In summary, when it comes to complexity in parameter space, we are considerably more efficient than the vast majority of the cited competitors. Reliability Metrics. A complementary set of metrics are typically used in probabilistic settings to measure the uncertainty and reliability of predictions. More specifically, expected calibration error (ECE) and maximum calibration error (MCE) respectively measure the expected and maximum binned difference between confidence and accuracy <ref type="bibr" target="#b16">(Guo et al. 2017)</ref>. This is illustrated in <ref type="table" target="#tab_4">Table 3</ref> where TRIDENT offers superior calibration on miniImagenet (5-way, 1 and 5-shot) as compared to other probabilistic approaches, and MAML <ref type="bibr" target="#b12">(Finn, Abbeel, and Levine 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We analyze the classification performance of TRIDENT across various paramaters and hyper-parameters, as is summarized in <ref type="table" target="#tab_6">Table 4</ref>. We use miniImagenet (5-way, 1-shot) setting to carry out ablation study experiments. To cover different design perspectives, we carry out ablation on: (i) MAML-style training parameters: meta-batch size B and number of inner adaption steps n, (ii) latent space dimensionality: z l and z s to assess the impact of their size, (iii) Regarding latent space dimensions, a correlation between a higher dimension of z l and z s and a better performance can be observed. Even though, the results show that increasing both dimensions beyond 64 leads to performance degradation. As such, (64, 64) seems to be the sweet spot. Finally, on feature space dimensions of AttFEX, the performance improves when W M &gt; W N , and the best performance is achieved when the parameters are set to <ref type="bibr">(64,</ref><ref type="bibr">32)</ref>. Notably, the exact set of parameters return the best performance for (5-way, 5-shot) setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoupling Analysis</head><p>As a qualitative demonstration, we visualize the label and semantic latent means (? l and ? s ) of query images for a randomly selected (5-way, 5-shot) task from miniImagenet, before and after the MAML meta-update procedure. The UMAP (McInnes, Healy, and Melville 2018) plots in <ref type="figure">Fig. 5</ref> illustrate significant improvement in class-conditional separation of query samples for label latent space upon metaadaption, whereas negligible improvement is visible on the semantic latent space. This is a qualitative evidence that Z L captures more class-discriminating information as compared to Z S . To substantiate this quantitatively, the clustering capacity of these latent spaces is also measured by the Davies-Bouldin score (DBI) <ref type="bibr" target="#b7">(Davies and Bouldin 1979)</ref>, where, the lower the DBI score, the better both the inter-cluster separation and intra-cluster "tightness". <ref type="figure">Fig. 5</ref> shows that the DBI score drops significantly more after meta-adaptation in the case of Z L as compared to Z S , indicating better clustering of features in the former than the latter. This aligns with the proposed decoupling strategy of TRIDENT and corroborates the validity of our proposition to put an emphasis on label latent information for the downstream few-shot task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concluding Remarks</head><p>We introduce a novel variational inference network (coined as TRIDENT) that simultaneously infers decoupled latent variables representing semantic and label information of an image. The proposed network is comprised of two intertwined variational sub-networks responsible for inferring the semantic and label information separately, the latter being enhanced using an attention-based transductive feature extraction module (AttFEX). Our extensive experimental results corroborate the efficacy of this transductive decoupling strategy on a variety of few-shot classification settings demonstrating superior performance and setting a new stateof-the-art for the most commonly adopted dataset mini and tieredImagenet as well as for the recent challenging crossdomain scenario of miniImagenet ? CUB. As future work, we plan to demonstrate the applicability of TRIDENT in semi-supervised and unsupervised settings by including the likelihood of unlabelled samples derived from the graphical model. This would render TRIDENT as an all-inclusive holistic approach towards solving few-shot classification.  . We use this dataset to simulate the effect of a domain shift where the model is first trained on a (5-way, 1 or 5shot) configuration of miniImagenet and then tested on the test classes of CUB, as used in <ref type="bibr" target="#b5">Boudiaf et al. 2020;</ref><ref type="bibr" target="#b74">Ziko et al. 2020;</ref><ref type="bibr" target="#b33">Long et al. 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementational Details</head><p>Let ? 1 and ? 2 respectively denote the scaling factors of the MSE and cross-entropy terms in our objective functions L R and L C , as already defined in Subsection 4.2. The terms ? and ? respectively denote the learning rates of the inner and meta updates whereas B and n respectively denote the number of sampled tasks and adaptation steps of the inner-update of our end-to-end training process, as described in Algorithm 2. The hyperparameter values (H.P.) used for training TRIDENT on miniImagenet and tieredImagenet are shown in <ref type="table" target="#tab_8">Table 6</ref>. We apply the same hyperparameters for the cross-domain testing scenario of miniImagenet ? CUB used for training TRIDENT on miniImagenet, for the given (N -way, K-shot) configuration. Hyperparameters are kept fixed throughout training, validation and testing for a given configuration. Adam <ref type="bibr" target="#b25">(Kingma and Ba 2015)</ref> optimizer is used for inner and meta-updates. Finally, the query, key and value extraction networks f q (, ; W Q ), f k (.; W K ), f v (.; W V ) of the AttFEX module only use Conv 1?1 (.) and not the LeakyReLU(0.2) activation function for (5-way, 1-shot) tasks, irrespective of the dataset. We observed that utilizing BatchNorm <ref type="bibr" target="#b23">(Ioffe and Szegedy 2015)</ref> in the decoder of z s (p ?2 ) to train TRIDENT on (5-way, 5shot) tasks of miniImagenet and on (5-way, 1-shot) tasks of tieredImagenet leads to better scores and improved stability during training. We used the ReLU activation function instead of LeakyReLU(0.2) to carry out training on (5way, 1-shot) tasks of tieredImagenet. Meta-learning objectives can lead to unstable optimization processes in practice, especially when coupled with stochastic sampling in latent spaces, as also previously observed in (Antreas Antoniou, Harrison Edwards, and Amos J. <ref type="bibr" target="#b1">Storkey 2019;</ref><ref type="bibr" target="#b51">Rusu et al. 2019)</ref>. For ease of experimentation we clip the metagradient norm at an absolute value of 1. TRIDENT converges in 82, 000 and 22, 500 epochs for (5-way, 1-shot) and (5-way, 5-shot) tasks of miniImagenet, respectively and takes 67, 500 and 48, 000 epochs for convergence on (5-way, 1-shot) and (5-way, 5-shot) tasks of tieredImagenet, respectively. This translates to an average training time of 110 hours on an 11GB NVIDIA 1080Ti GPU. Note that we did not employ any data augmentation, feature averaging or any other data apart from the corresponding training subset D tr , during training and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Calibration Results</head><p>To further examine the reliability and calibration of our method, we assess the ECE, MCE <ref type="bibr" target="#b16">(Guo et al. 2017)</ref> and Brier scores (BRIER 1950) of TRIDENT on the challenging cross-domain scenario of miniImagenet ? CUB for (5-way, 5-shot) tasks. This table can be treated as an extension to <ref type="table" target="#tab_4">Table 3</ref> since here, we compare the calibration metrics for an additional scenario. When compared against other baselines that report these metrics on the aforementioned sce- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ECE MCE Brier</head><p>Feature Transfer  0.275 0.646 0.772 Baseline  0.315 0.537 0.716 Matching Nets <ref type="bibr" target="#b61">(Vinyals et al. 2016)</ref> 0.030 0.079 0.630 Proto Nets <ref type="bibr" target="#b54">(Snell, Swersky, and Zemel 2017)</ref> 0.009 0.025 0.604 Relation Net <ref type="bibr" target="#b57">(Sung et al. 2018)</ref> 0.234 0.554 0.730 DKT+Cos <ref type="bibr" target="#b45">(Patacchiola et al. 2020)</ref> 0.236 0.426 0.670 BMAML <ref type="bibr" target="#b70">(Yoon et al. 2018)</ref> 0.048 0.077 0.619 BMAML+Chaser <ref type="bibr" target="#b70">(Yoon et al. 2018)</ref> 0.066 0.260 0.639 LogSoftGP(ML) <ref type="bibr">(Galy-Fajou et al. 2020)</ref> 0.220 0.513 0.709 LogSoftGP(PL) <ref type="bibr">(Galy-Fajou et al. 2020)</ref> 0.022 0.042 0.564 OVE(ML) <ref type="bibr" target="#b55">(Snell and Zemel 2021)</ref> 0.049 0.066 0.576 OVE(PL) <ref type="bibr" target="#b55">(Snell and Zemel 2021)</ref> 0.020 0.032 0.556 TRIDENT(Ours) 0.009 0.02 0.276 nario, TRIDENT proves to be the most calibrated with the best reliability scores. This is shown in <ref type="table" target="#tab_9">Table 7</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Generative Model of TRIDENT. Dotted lines indicate variational inference and solid lines refer to generative processes. The inference and generative parameters are color coded to correspond to their respective architectures indicated inFig.1 and Fig.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>MAML<ref type="bibr" target="#b12">(Finn, Abbeel, and Levine 2017)</ref> Conv4 Ind.48.70 ? 1.84 63.11 ? 0.92 51.67 ? 1.81 70.30 ? 0.08 34.01 ? 1.25 48.83 ? 0.62 ABML<ref type="bibr" target="#b47">(Ravi and Beatson 2019)</ref> </figDesc><table><row><cell></cell><cell>Conv4</cell><cell>Ind.</cell><cell cols="2">40.88 ? 0.25 58.19 ? 0.17</cell><cell>-</cell><cell>-</cell><cell cols="2">31.51 ? 0.32 47.80 ? 0.51</cell></row><row><cell>OVE(PL) (Patacchiola et al. 2020)</cell><cell>Conv4</cell><cell>Ind.</cell><cell cols="2">48.00 ? 0.24 67.14 ? 0.23</cell><cell>-</cell><cell>-</cell><cell cols="2">37.49 ? 0.11 57.23 ? 0.31</cell></row><row><cell>DKT+Cos (Patacchiola et al. 2020)</cell><cell>Conv4</cell><cell>Ind.</cell><cell cols="2">48.64 ? 0.45 62.85 ? 0.37</cell><cell>-</cell><cell>-</cell><cell cols="2">40.22 ? 0.54 55.65 ? 0.05</cell></row><row><cell>BOIL (Oh et al. 2021)</cell><cell>Conv4</cell><cell>Ind.</cell><cell cols="4">49.61 ? 0.16 48.58 ? 0.27 66.45 ? 0.37 69.37 ? 0.12</cell><cell>-</cell><cell>-</cell></row><row><cell>LFWT(Tseng et al. 2020)</cell><cell>RN10</cell><cell>TF+TI</cell><cell cols="2">66.32 ? 0.80 81.98 ? 0.55</cell><cell>-</cell><cell>-</cell><cell cols="2">47.47 ? 0.75 66.98 ? 0.68</cell></row><row><cell>FRN(Wertheimer, Tang, and Hariharan 2021)</cell><cell>RN12</cell><cell>Ind.</cell><cell cols="6">66.45 ? 0.19 82.83 ? 0.13 71.16 ? 0.22 86.01 ? 0.15 54.11 ? 0.19 77.09 ? 0.15</cell></row><row><cell>DPGN(Yang et al. 2020)</cell><cell>RN12</cell><cell>TF+TI</cell><cell>67.77</cell><cell>84.6</cell><cell>72.45</cell><cell>87.24</cell><cell>-</cell><cell>-</cell></row><row><cell>PAL(Ma et al. 2021)</cell><cell>RN12</cell><cell>TF+TI</cell><cell cols="4">69.37 ? 0.64 84.40 ? 0.44 72.25 ? 0.72 86.95 ? 0.47</cell><cell>-</cell><cell>-</cell></row><row><cell>Proto-Completion(Zhang et al. 2021a)</cell><cell>RN12</cell><cell>TF+TI</cell><cell cols="4">73.13 ? 0.85 82.06 ? 0.54 81.04 ? 0.89 87.42 ? 0.57</cell><cell>-</cell><cell>-</cell></row><row><cell>TPMN(Wu et al. 2021)</cell><cell>RN12</cell><cell>TF+TI</cell><cell cols="4">67.64 ? 0.63 83.44 ? 0.43 72.24 ? 0.70 86.55 ? 0.63</cell><cell>-</cell><cell>-</cell></row><row><cell>LIF-EMD(Li, Wang, and Hu 2021)</cell><cell>RN12</cell><cell>TF+TI</cell><cell cols="4">68.94 ? 0.28 85.07 ? 0.50 73.76 ? 0.32 87.83 ? 0.59</cell><cell>-</cell><cell>-</cell></row><row><cell>Transd-CNAPS(Bateni et al. 2022)</cell><cell>RN18</cell><cell>TF+TI</cell><cell>55.6 ? 0.9</cell><cell>73.1 ? 0.7</cell><cell>65.9 ? 1.0</cell><cell>81.8 ? 0.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Baseline++(Chen et al. 2019)</cell><cell>RN18</cell><cell>TF</cell><cell cols="2">51.87 ? 0.77 75.68 ? 0.63</cell><cell>-</cell><cell>-</cell><cell cols="2">42.85 ? 0.69 62.04 ? 0.76</cell></row><row><cell>FEAT(Ye et al. 2020)</cell><cell>RN18</cell><cell>TF</cell><cell>66.78</cell><cell>82.05</cell><cell>70.80</cell><cell>84.79</cell><cell cols="2">50.67 ? 0.78 71.08 ? 0.73</cell></row><row><cell>SimpleShot(Wang et al. 2019)</cell><cell>WRN</cell><cell>Ind.</cell><cell>63.32</cell><cell>80.28</cell><cell>69.98</cell><cell>85.45</cell><cell>48.56</cell><cell>65.63</cell></row><row><cell>Assoc-Align(Afrasiyabi, Lalonde, and Gagn? 2020)</cell><cell>WRN</cell><cell>TF</cell><cell cols="6">65.92 ? 0.60 82.85 ? 0.55 74.40 ? 0.68 86.61 ? 0.59 47.25 ? 0.76 72.37 ? 0.89</cell></row><row><cell>ReRank(SHEN et al. 2021)</cell><cell>WRN</cell><cell>TF+TI</cell><cell>72.4?0.6</cell><cell>80.2?0.4</cell><cell>79.5?0.6</cell><cell>84.8?0.4</cell><cell>-</cell><cell>-</cell></row><row><cell>TIM-GD(Boudiaf et al. 2020)</cell><cell>WRN</cell><cell>TI</cell><cell>77.8</cell><cell>87.4</cell><cell>82.1</cell><cell>89.8</cell><cell>-</cell><cell>71</cell></row><row><cell>LaplacianShot(Ziko et al. 2020)</cell><cell>WRN</cell><cell>TI</cell><cell>74.9</cell><cell>84.07</cell><cell>80.22</cell><cell>87.49</cell><cell>55.46</cell><cell>66.33</cell></row><row><cell>S2M2(Mangla et al. 2020)</cell><cell>WRN</cell><cell>TF</cell><cell cols="6">64.93 ? 0.18 83.18 ? 0.11 73.71 ? 0.22 88.59 ? 0.14 48.24 ? 0.84 70.44 ? 0.75</cell></row><row><cell>MetaQDA(Zhang et al. 2021b)</cell><cell>WRN</cell><cell>TF</cell><cell cols="6">67.83 ? 0.64 84.28 ? 0.69 74.33 ? 0.65 89.56 ? 0.79 53.75 ? 0.72 71.84 ? 0.66</cell></row><row><cell>PT+MAP(Hu, Gripon, and Pateux 2021)</cell><cell>WRN</cell><cell>TF+TI</cell><cell cols="6">82.92 ? 0.26 88.82 ? 0.13 85.67 ? 0.26 90.45 ? 0.14 62.49 ? 0.32 76.51 ? 0.18</cell></row><row><cell>PEM n E-BMS(Hu, Pateux, and Gripon 2022)</cell><cell>WRN</cell><cell>TF+TI</cell><cell cols="6">83.35 ? 0.25 89.53 ? 0.13 86.07 ? 0.25 91.09 ? 0.14 63.90 ? 0.31 79.15 ? 0.18</cell></row><row><cell>Transd-CNAPS+FETI(Bateni et al. 2022)</cell><cell>RN18  ?</cell><cell>TF+TI</cell><cell>79.9 ? 0.8</cell><cell>91.50 ? 0.4</cell><cell>73.8 ? 0.1</cell><cell>87.7 ? 0.6</cell><cell>-</cell><cell>-</cell></row><row><cell>TRIDENT(Ours)</cell><cell>Conv4</cell><cell>TF</cell><cell cols="6">86.11 ? 0.59 95.95 ? 0.28 86.97 ? 0.50 96.57 ? 0.17 84.61 ? 0.33 80.74 ? 0.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Parameter count of TRIDENT against competitors.</figDesc><table><row><cell></cell><cell>Conv4</cell><cell>??</cell><cell>??</cell><cell cols="3">AttFEX TRIDENT Conv4</cell><cell>RN18</cell><cell>WRN</cell></row><row><cell>q? 1</cell><cell cols="3">28896 51264 51264</cell><cell>6994</cell><cell></cell></row><row><cell>q? 2</cell><cell cols="3">28896 51264 51264</cell><cell>-</cell><cell>412,238</cell><cell>190, 410 12.4M 36.482M</cell></row><row><cell>p? 1 + p? 2</cell><cell></cell><cell cols="2">2245 + 132009</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Calibration errors of TRIDENT. Style: best and second best.</figDesc><table><row><cell></cell><cell cols="8">Metrics MAML PLATIPUS ABPML ABML BMAML VAMPIRE TRIDENT</cell></row><row><cell>5-way,</cell><cell>ECE</cell><cell>0.046</cell><cell>0.032</cell><cell>0.013</cell><cell>0.026</cell><cell>0.025</cell><cell>0.008</cell><cell>0.0036</cell></row><row><cell>1-shot</cell><cell>MCE</cell><cell>0.073</cell><cell>0.108</cell><cell>0.037</cell><cell>0.058</cell><cell>0.092</cell><cell>0.038</cell><cell>0.029</cell></row><row><cell>5-way,</cell><cell>ECE</cell><cell>0.032</cell><cell>-</cell><cell>0.006</cell><cell>-</cell><cell>0.027</cell><cell>-</cell><cell>0.0015</cell></row><row><cell>5-shot</cell><cell>MCE</cell><cell>0.044</cell><cell>-</cell><cell>0.030</cell><cell>-</cell><cell>0.049</cell><cell>-</cell><cell>0.018</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation study for miniImagenet (5-way, 1-shot) tasks. Accuracies in (% ? std.). 76.29 ? 0.72 75.44 ? 0.81 79.1 ? 0.57 82.93 ? 0.8 86.11 ? 0.59 85.62 ? 0.52 81.49 ? 0.65 82.89 ? 0.48 84.42 ? 0.59 ? 0.23 77.89 ? 0.39 79.55 ? 0.87 86.11 ? 0.59 84.87 ? 0.45 82.11 ? 0.35 84.67 ? 0.7 85.8 ? 0.58 83.92 ? 0.63Figure 5: Better class separation upon meta-update is confirmed by lower DBI scores. Different colors/markers indicate classes.</figDesc><table><row><cell>(B, n)</cell><cell>(5, 3)</cell><cell>(5, 5)</cell><cell></cell><cell>(10, 3)</cell><cell>(10, 5)</cell><cell></cell><cell>(20, 3)</cell><cell>(20, 5)</cell><cell></cell></row><row><cell></cell><cell>-</cell><cell cols="2">67.43 ? 0.75</cell><cell>69.21 ? 0.66</cell><cell cols="2">74.6 ? 0.84</cell><cell>80.82 ? 0.68</cell><cell cols="2">86.11 ? 0.59</cell></row><row><cell>(dim(z l ),</cell><cell>(32, 32)</cell><cell>(32, 64)</cell><cell>(32, 128)</cell><cell>(64, 32)</cell><cell>(64, 64)</cell><cell>(64, 128)</cell><cell>(128, 32)</cell><cell>(128, 64)</cell><cell>(128, 128)</cell></row><row><cell>dim(z s ))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(dim(W M ),</cell><cell>(32, 32)</cell><cell>(32, 64)</cell><cell>(32, 128)</cell><cell>(64, 32)</cell><cell>(64, 64)</cell><cell>(64, 128)</cell><cell>(128, 32)</cell><cell>(128, 64)</cell><cell>(128, 128)</cell></row><row><cell>dim(W N ))</cell><cell>78.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Additional Details of DatasetsminiImagenet<ref type="bibr" target="#b61">(Vinyals et al. 2016</ref>) is a subset of ImageNet<ref type="bibr" target="#b8">(Deng et al. 2009</ref>) for few-shot classification. It contains 100 classes with 600 samples each. We follow the predominantly adopted settings of<ref type="bibr" target="#b48">(Ravi and Larochelle 2017;</ref><ref type="bibr" target="#b6">Chen et al. 2019)</ref> where we split the entire dataset into 64 classes for training, 16 for validation and 20 for testing. tieredImagenet is a larger subset of ImageNet<ref type="bibr" target="#b8">(Deng et al. 2009</ref>) with 608 classes and 779, 165 total images, which are grouped into 34 higher-level nodes in the ImageNet human-curated hierarchy. This set of nodes is partitioned into 20, 6, and 8 disjoint sets of training, validation, and testing nodes, and the corresponding classes form the respective meta-sets. CUB<ref type="bibr" target="#b63">(Welinder et al. 2010)</ref> dataset has a total of 200 classes, split into training, validation and test sets following</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameter values when training TRIDENT.</figDesc><table><row><cell></cell><cell cols="2">miniImagenet</cell><cell cols="2">tieredImagenet</cell></row><row><cell cols="5">H.P. 5-way, 1-shot 5-way, 5-shot 5-way, 1-shot 5-way, 5-shot</cell></row><row><cell>? 1</cell><cell>1e-2</cell><cell>1e-2</cell><cell>1e-2</cell><cell>1e-2</cell></row><row><cell>? 2</cell><cell>100</cell><cell>100</cell><cell>150</cell><cell>150</cell></row><row><cell>?</cell><cell>1e-3</cell><cell>1e-3</cell><cell>1.5e-3</cell><cell>1.7e-3</cell></row><row><cell>?</cell><cell>1e-4</cell><cell>1e-4</cell><cell>1.5e-4</cell><cell>1.7e-4</cell></row><row><cell>B</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell></row><row><cell>n</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Style: best and second best.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code and experimentation can be found in our GitHub repository: https://github.com/anujinho/trident</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of AttFEX</head><p>In order to study the impact of the transductive feature extractor AttFEX, we exclude it during training and train the remaining architecture. Training proceeds exactly as mentioned before in the manuscript. As can be seen in Ta- ble 5, the exclusion of AttFEX from TRIDENT results in a substantial drop in classification performance across both datasets and task settings. Empirically, this further substantiates the importance of AttFEX's ability to render the feature maps transductive/task-aware. As explained earlier in the main manuscript, it is imperative to include y in the input to q ?1 (.) for mathematical correctness of the variational inference formulation. However, in order to utilize TRIDENT as a classification and not a label reconstruction network, we choose not to input y to q ?1 (.), but rather do so indirectly by inducing a semblance of label characteristics in the features extracted from the images in a task. Thus, it is important to realize that this ability of AttFEX to render feature maps transductive is not just an adhoc performance enhancer, but rather an essential part of TRIDENT since it allows us to not violate our generative and inference mechanics.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Associative alignment for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Afrasiyabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gagn?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="18" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How to train your MAML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). Open-Review.net</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M R</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Zarkias</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
		<respStmt>
			<orgName>A Library for Meta-Learning Research</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enhancing Few-Shot Image Classification With Unlabelled Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Van De Meent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2796" to="2805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved Few-Shot Visual Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Masrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Information Maximization for Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boudiaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc. BRIER, G. W</publisher>
			<date type="published" when="1950" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Closer Look at Few-shot Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Cluster Separation Measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Bouldin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="224" to="227" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Baseline for Few-Shot Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Disentangled Joint Continuous and Discrete Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards a Neural Statistician</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<idno>abs/1606.02185</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-Class Gaussian Process Classification Made Conjugate: Efficient Inference via Data Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Donner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 35th Uncertainty in Artificial Intelligence Conference</title>
		<editor>Adams, R. P.</editor>
		<editor>and Gogate, V.</editor>
		<meeting>The 35th Uncertainty in Artificial Intelligence Conference</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="755" to="765" />
		</imprint>
	</monogr>
	<note>Curran Associates, Inc. Galy-Fajou, T</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Learning by transduction</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page">98</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Meta-Learning Probabilistic Inference for Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On Calibration of Modern Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross Attention Network for Few-shot Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Wallach, H.</editor>
		<editor>Larochelle, H.</editor>
		<editor>Beygelzimer, A.</editor>
		<editor>d&apos;Alch?-Buc, F.</editor>
		<editor>Fox, E.</editor>
		<editor>and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Empirical Bayes Transductive Meta-Learning with Synthetic Gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Leveraging the feature distribution in transfer-based few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="487" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Squeezing backbone feature distributions to the max for efficient few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">147</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Meta-Learning with Shared Amortized Variational Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Iakovleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, Proceedings of Machine Learning Research</title>
		<meeting>the 37th International Conference on Machine Learning, Machine Learning Research</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Capturing Label Characteristics in {VAE}s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rainforth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised Learning with Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Meta-Learning with Differentiable Convex Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Finding Task-Relevant Features for Few-Shot Learning by Category Traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<title level="m">Learning Intact Features by Erasing-Inpainting for Few-shot Classification. Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8401" to="8409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Prototype Rectification for Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-08-23" />
		</imprint>
	</monogr>
	<note>Part I</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to Propagate Labels: Transductive Propagation Network for Few-shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Conditional Adversarial Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Partner-assisted learning for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Abd-Almageed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10573" to="10582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Charting the right manifold: Manifold mixup for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mangla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF winter conference on applications of computer vision</title>
		<meeting>the IEEE/CVF winter conference on applications of computer vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2218" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Disentangling Disentanglement in Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Chaudhuri, K.</editor>
		<editor>and Salakhutdinov, R.</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="4402" to="4412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Simple Neural Attentive Meta-Learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Uncertainty in Model-Agnostic Meta-Learning using Variational Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<title level="m">On First-Order Meta-Learning Algorithms</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">On First-Order Meta-Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno>abs/1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">BOIL: Towards Representation Change for Few-shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">TADAM: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodr?guez L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F P</forename><surname>O&amp;apos;boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Amortized Bayesian Meta-Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>; D&amp;apos;alch?-Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beatson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Optimization as a Model for Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Meta-Learning for Semi-Supervised Few-Shot Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast and Flexible Multi-Task Classification using Conditional Neural Adaptive Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Meta-Learning with Latent Embedding Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Few-Shot Learning with Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Estrach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Re-ranking for image retrieval and transductive fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sbai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aubry</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Beygelzimer, A.</editor>
		<editor>Dauphin, Y.</editor>
		<editor>Liang, P.</editor>
		<editor>and Vaughan, J. W.</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Prototypical Networks for Few-shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bayesian Few-Shot Classification with One-vs-Each P?lya-Gamma Augmented Gaussian Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Amortized Bayesian Prototype Meta-learning: A New Probabilistic Meta-learning Approach to Few-shot Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, Proceedings of Machine Learning Research</title>
		<meeting>The 24th International Conference on Artificial Intelligence and Statistics, Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning to Compare: Relation Network for Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Cross-domain few-shot classification via learned feature-wise transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08735</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Estimation of Dependences Based on Empirical Data. Estimation of Dependences Based on Empirical Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Matching Networks for One Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04623</idno>
		<title level="m">SimpleShot: Revisiting Nearest-Neighbor Classification for Few-Shot Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Few-Shot Classification With Feature Map Reconstruction Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wertheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8012" to="8021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8433" to="8442" />
		</imprint>
	</monogr>
	<note>Task-Aware Part Mining Network for Few-Shot Learning</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>yifan xu</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Attentional Constellation Nets for Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">DPGN: Distribution Propagation Graph Network for Few-Shot Learning. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13387" to="13396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8808" to="8817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Bayesian Model-Agnostic Meta-Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Prototype Completion With Primitive Knowledge for Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3754" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Variational Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1685" to="1694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Shallow bayesian meta learning for real-world fewshot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="page" from="651" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Laplacian Regularized Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11660" to="11670" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
